[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R 数据科学 2e",
    "section": "",
    "text": "Welcome\nThis is the website for the 2nd edition of “R for Data Science”. This book will teach you how to do data science with R: You’ll learn how to get your data into R, get it into the most useful structure, transform it and visualize.\n这是 《R 数据科学》 第二版的网站。本书将教你如何使用 R 进行数据科学：你将学习如何将数据导入 R、将其整理成最有用的结构，并对其进行转换与可视化。\nIn this book, you will find a practicum of skills for data science. Just as a chemist learns how to clean test tubes and stock a lab, you’ll learn how to clean data and draw plots—and many other things besides. These are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with R. You’ll learn how to use the grammar of graphics, literate programming, and reproducible research to save time. You’ll also learn how to manage cognitive resources to facilitate discoveries when wrangling, visualizing, and exploring data.\n在本书中，你将获得一套系统的数据科学实战技能。就像化学家学习如何清洗试管和整理实验室一样，你将学习如何清洗数据、绘制图形，以及更多相关技能。这些技能让数据科学成为可能，而你将在此处学到使用 R 完成这些任务的最佳实践。你将学会使用图形语法（grammar of graphics）、释义式编程（literate programming）以及可重复性研究（reproducible research）来节省时间。你还将学习如何管理认知资源，以便在数据清洗、可视化和探索过程中促进发现。\nThis website is and will always be free, licensed under the CC BY-NC-ND 3.0 License. If you’d like a physical copy of the book, you can order it on Amazon. If you appreciate reading the book for free and would like to give back, please make a donation to Kākāpō Recovery: the kākāpō (which appears on the cover of R4DS) is a critically endangered parrot native to New Zealand; there are only 244 left.\n本网站现在并且永远向所有人免费开放，内容遵循 CC BY‑NC‑ND 3.0 许可证。如果你希望拥有纸质版书籍，可以在 Amazon 订购。如果你喜欢免费阅读本书并希望回馈，请向 Kākāpō Recovery 捐款：封面上的 kākāpō 是新西兰特有的极危鹦鹉，现存仅 244 只。\nIf you speak another language, you might be interested in the freely available translations of the 1st edition:\n如果你使用其他语言，也许会对第一版的免费翻译感兴趣：\n\nSpanish 西班牙语译本：http://es.r4ds.hadley.nz\nItalian 意大利语译本：https://it.r4ds.hadley.nz\nTurkish 土耳其语译本：https://tr.r4ds.hadley.nz\nPortuguese 葡萄牙语译本：https://pt.r4ds.hadley.nz\n\nYou can find suggested answers to exercises in the book at https://mine-cetinkaya-rundel.github.io/r4ds-solutions.\n你可以在 https://mine-cetinkaya-rundel.github.io/r4ds-solutions 找到本书练习题的参考答案。\nPlease note that R4DS uses a Contributor Code of Conduct. By contributing to this book, you agree to abide by its terms.\n请注意，R4DS 遵循 贡献者行为准则。参与本书贡献即表示你同意遵守其条款。",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "preface-2e.html",
    "href": "preface-2e.html",
    "title": "Preface to the second edition",
    "section": "",
    "text": "Welcome to the second edition of “R for Data Science”!\n欢迎阅读 《R for Data Science》 第二版！\nThis is a major reworking of the first edition, removing material we no longer think is useful, adding material we wish we included in the first edition, and generally updating the text and code to reflect changes in best practices.\n这是对第一版的一次重大重构：剔除了不再实用的内容，补充了当初想加入却未能包含的材料，并全面更新了文本和代码，以反映最新的最佳实践。\nWe’re also very excited to welcome a new co-author: Mine Çetinkaya-Rundel, a noted data-science educator and one of our colleagues at Posit (the company formerly known as RStudio).\n我们还非常高兴地迎来新合著者 —— 著名的数据科学教育者、Posit（前身为 RStudio）同事 Mine Çetinkaya-Rundel。\nA brief summary of the biggest changes follows:\n以下简要概述本次最重要的改动：\n\nThe first part of the book has been renamed to “Whole game”.\nThe goal of this section is to give you the rough details of the “whole game” of data science before we dive into the details.\n书的第一部分现已更名为 Whole game（完整全局）。本节旨在让你在深入细节之前，先对数据科学的 “完整游戏” 有一个整体了解。\nThe second part of the book is “Visualize”.\nThis part gives data-visualization tools and best practices a more thorough coverage compared to the first edition.\nThe best place to get all the details is still the ggplot2 book, but now R4DS covers more of the most important techniques.\n第二部分为 Visualize（可视化）。相比第一版，本版更系统地介绍数据可视化工具与最佳实践。详细信息仍以 《ggplot2》 一书为最佳参考，但 R4DS 现已涵盖更多关键方法。\nThe third part of the book is now called “Transform” and gains new chapters on numbers, logical vectors, and missing values.\nThese were previously parts of the data-transformation chapter, but needed much more room to cover all the details.\n第三部分现称为 Transform（转换），并新增了数字、逻辑向量和缺失值等章节。它们原先只是数据转换章节中的一部分，但如今需要更多篇幅深入介绍。\nThe fourth part of the book is called “Import”.\nIt’s a new set of chapters that goes beyond reading flat text files to working with spreadsheets, getting data out of databases, working with big data, rectangling hierarchical data, and scraping data from web sites.\n第四部分为 Import（导入）。这一新篇章不仅涉及读取纯文本文件，还涵盖电子表格、数据库提取、大数据处理、层级数据整形，以及网页数据抓取等主题。\nThe “Program” part remains, but has been rewritten from top-to-bottom to focus on the most important parts of function writing and iteration.\nFunction writing now includes details on how to wrap tidyverse functions (dealing with the challenges of tidy evaluation), since this has become much easier and more important over the last few years.\nWe’ve added a new chapter on important base R functions that you’re likely to see in wild-caught R code.\n“Program” 部分保留，但已彻底重写，重点聚焦函数编写与迭代的核心要素。现在的函数编写章节详细讲解了如何封装 tidyverse 函数（应对 tidy evaluation 的挑战），因为近年来这项任务变得更简单且更关键。我们还新增了讲解常见 base R 函数的新章节，以帮助你读懂“野生” R 代码。\nThe modeling part has been removed.\nWe never had enough room to fully do modelling justice, and there are now much better resources available.\nWe generally recommend using the tidymodels packages and reading Tidy Modeling with R by Max Kuhn and Julia Silge.\n模型部分已被移除。由于篇幅所限，我们无法充分展示建模的全部内容，而如今已有更优秀的资源可供学习。我们推荐使用 tidymodels 套件，并阅读 Max Kuhn 与 Julia Silge 合著的 Tidy Modeling with R。\nThe “Communicate” part remains, but has been thoroughly updated to feature Quarto instead of R Markdown.\nThis edition of the book has been written in Quarto, and it’s clearly the tool of the future.\n“Communicate” 部分仍在，但已全面更新为 Quarto，替代 R Markdown。本书第二版即使用 Quarto 编写，显然它将成为未来的首选工具。",
    "crumbs": [
      "Preface to the second edition"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "What you will learn\nData science is an exciting discipline that allows you to transform raw data into understanding, insight, and knowledge. The goal of “R for Data Science” is to help you learn the most important tools in R that will allow you to do data science efficiently and reproducibly, and to have some fun along the way 😃. After reading this book, you’ll have the tools to tackle a wide variety of data science challenges using the best parts of R.\n数据科学是一门激动人心的学科，它能让你将原始数据转化为理解、洞见和知识。 《R 数据科学》的目标是帮助你学习 R 中最重要的工具，让你能够高效、可复现地进行数据科学工作，并在此过程中获得一些乐趣 😃。 读完本书后，你将拥有使用 R 的精华部分来应对各种数据科学挑战的工具。\nData science is a vast field, and there’s no way you can master it all by reading a single book. This book aims to give you a solid foundation in the most important tools and enough knowledge to find the resources to learn more when necessary. Our model of the steps of a typical data science project looks something like Figure 1.\n数据科学是一个广阔的领域，不可能通过阅读一本书就掌握所有内容。 本书旨在为你打下最重要的工具的坚实基础，并提供足够的知识，以便你在必要时能找到资源进行更深入的学习。 我们对一个典型数据科学项目步骤的模型如 Figure 1 所示。\nFigure 1: In our model of the data science process, you start with data import and tidying. Next, you understand your data with an iterative cycle of transforming, visualizing, and modeling. You finish the process by communicating your results to other humans.\nFirst, you must import your data into R. This typically means that you take data stored in a file, database, or web application programming interface (API) and load it into a data frame in R. If you can’t get your data into R, you can’t do data science on it!\n首先，你必须将数据 导入 (import) R 中。 这通常意味着你将存储在文件、数据库或网络应用程序编程接口 (API) 中的数据加载到 R 的数据框中。 如果你不能将数据导入 R，你就无法对其进行数据科学分析！\nOnce you’ve imported your data, it is a good idea to tidy it. Tidying your data means storing it in a consistent form that matches the semantics of the dataset with how it is stored. In brief, when your data is tidy, each column is a variable and each row is an observation. Tidy data is important because the consistent structure lets you focus your efforts on answering questions about the data, not fighting to get the data into the right form for different functions.\n导入数据后，最好对其进行 整理 (tidy)。 整理数据意味着以一种一致的形式存储数据，使其存储方式与数据集的语义相匹配。 简而言之，当你的数据是整洁的，每一列都是一个变量，每一行都是一个观测。 整洁的数据很重要，因为一致的结构让你能集中精力回答关于数据的问题，而不是费力地将数据转换成不同函数所需的正确形式。\nOnce you have tidy data, a common next step is to transform it. Transformation includes narrowing in on observations of interest (like all people in one city or all data from the last year), creating new variables that are functions of existing variables (like computing speed from distance and time), and calculating a set of summary statistics (like counts or means). Together, tidying and transforming are called wrangling because getting your data in a form that’s natural to work with often feels like a fight!\n一旦你有了整洁的数据，通常的下一步是进行 转换 (transform)。 转换包括筛选感兴趣的观测值（例如某个城市的所有人或去年的所有数据），根据现有变量创建新变量（例如根据距离和时间计算速度），以及计算一组汇总统计数据（例如计数或均值）。 整理和转换一起被称为 数据整理 (wrangling)，因为将数据处理成便于使用的形式通常感觉像一场战斗！\nOnce you have tidy data with the variables you need, there are two main engines of knowledge generation: visualization and modeling. These have complementary strengths and weaknesses, so any real data analysis will iterate between them many times.\n一旦你获得了包含所需变量的整洁数据，便有两种主要的知识生成引擎：可视化和建模。 这两者各有优缺点，互为补充，因此任何实际的数据分析都会在它们之间多次迭代。\nVisualization is a fundamentally human activity. A good visualization will show you things you did not expect or raise new questions about the data. A good visualization might also hint that you’re asking the wrong question or that you need to collect different data. Visualizations can surprise you, but they don’t scale particularly well because they require a human to interpret them.可视化 (Visualization) 本质上是一项人类活动。 好的可视化会向你展示意想不到的事情，或引发关于数据的新问题。 好的可视化也可能暗示你问错了问题，或者你需要收集不同的数据。 可视化可以给你带来惊喜，但它们的可扩展性不是特别好，因为它们需要人来解释。\nModels are complementary tools to visualization. Once you have made your questions sufficiently precise, you can use a model to answer them. Models are fundamentally mathematical or computational tools, so they generally scale well. Even when they don’t, it’s usually cheaper to buy more computers than it is to buy more brains! But every model makes assumptions, and by its very nature, a model cannot question its own assumptions. That means a model cannot fundamentally surprise you.模型 (Models) 是可视化的补充工具。 一旦你的问题足够精确，你就可以使用模型来回答它们。 模型本质上是数学或计算工具，所以它们通常具有良好的可扩展性。 即使它们不具备，购买更多的计算机通常也比购买更多的人脑便宜！ 但是每个模型都有其假设，而模型本质上无法质疑自身的假设。 这意味着模型根本上无法给你带来惊喜。\nThe last step of data science is communication, an absolutely critical part of any data analysis project. It doesn’t matter how well your models and visualization have led you to understand the data unless you can also communicate your results to others.\n数据科学的最后一步是 沟通 (communication)，这是任何数据分析项目中至关重要的一部分。 无论你的模型和可视化让你对数据理解得有多透彻，除非你也能将你的结果传达给他人，否则一切都是徒劳。\nSurrounding all these tools is programming. Programming is a cross-cutting tool that you use in nearly every part of a data science project. You don’t need to be an expert programmer to be a successful data scientist, but learning more about programming pays off because becoming a better programmer allows you to automate common tasks and solve new problems with greater ease.\n围绕所有这些工具的是 编程 (programming)。 编程是一项贯穿始终的工具，你在数据科学项目的几乎每个环节都会用到它。 你不需要成为一名专家程序员才能成为一名成功的数据科学家，但学习更多关于编程的知识是值得的，因为成为一个更好的程序员可以让你自动化处理常见任务，并更轻松地解决新问题。\nYou’ll use these tools in every data science project, but they’re not enough for most projects. There’s a rough 80/20 rule at play: you can tackle about 80% of every project using the tools you’ll learn in this book, but you’ll need other tools to tackle the remaining 20%. Throughout this book, we’ll point you to resources where you can learn more.\n你将在每个数据科学项目中使用这些工具，但对大多数项目来说，它们还不够。 这里有一个大致的 80/20 法则：你可以使用本书中学到的工具处理每个项目中大约 80% 的工作，但你需要其他工具来处理剩下的 20%。 在本书中，我们会为你指出可以学习更多知识的资源。",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#how-this-book-is-organized",
    "href": "intro.html#how-this-book-is-organized",
    "title": "Introduction",
    "section": "How this book is organized",
    "text": "How this book is organized\nThe previous description of the tools of data science is organized roughly according to the order in which you use them in an analysis (although, of course, you’ll iterate through them multiple times). In our experience, however, learning data importing and tidying first is suboptimal because, 80% of the time, it’s routine and boring, and the other 20% of the time, it’s weird and frustrating. That’s a bad place to start learning a new subject! Instead, we’ll start with visualization and transformation of data that’s already been imported and tidied. That way, when you ingest and tidy your own data, your motivation will stay high because you know the pain is worth the effort.\n前文对数据科学工具的描述大致是按照你在分析中使用的顺序来组织的（当然，你会多次迭代使用它们）。 然而，根据我们的经验，首先学习数据导入和整理并非最佳选择，因为 80% 的时间这项工作是常规且乏味的，而另外 20% 的时间则是古怪且令人沮丧的。 这对于开始学习一个新主题来说是个糟糕的起点！ 因此，我们将从已经导入和整理好的数据的可视化和转换开始。 这样，当你处理和整理自己的数据时，你的动力会保持高昂，因为你知道这些辛苦是值得的。\nWithin each chapter, we try to adhere to a consistent pattern: start with some motivating examples so you can see the bigger picture, and then dive into the details. Each section of the book is paired with exercises to help you practice what you’ve learned. Although it can be tempting to skip the exercises, there’s no better way to learn than by practicing on real problems.\n在每一章中，我们都力求遵循一种一致的模式：从一些激励性的例子开始，让你看到全局，然后深入细节。 书中的每一节都配有练习，以帮助你实践所学。 虽然跳过练习很诱人，但没有比通过解决实际问题更好的学习方法了。",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#what-you-wont-learn",
    "href": "intro.html#what-you-wont-learn",
    "title": "Introduction",
    "section": "What you won’t learn",
    "text": "What you won’t learn\nThere are several important topics that this book doesn’t cover. We believe it’s important to stay ruthlessly focused on the essentials so you can get up and running as quickly as possible. That means this book can’t cover every important topic.\n本书未涵盖一些重要主题。 我们认为，严格专注于核心内容，以便你能尽快上手，这一点至关重要。 这意味着本书无法涵盖所有重要主题。\nModeling\nModeling is super important for data science, but it’s a big topic, and unfortunately, we just don’t have the space to give it the coverage it deserves here. To learn more about modeling, we highly recommend Tidy Modeling with R by our colleagues Max Kuhn and Julia Silge. This book will teach you the tidymodels family of packages, which, as you might guess from the name, share many conventions with the tidyverse packages we use in this book.\n建模对于数据科学超级重要，但它是一个很大的主题，不幸的是，我们在这里没有足够的篇幅给予它应有的介绍。 要学习更多关于建模的知识，我们强烈推荐我们的同事 Max Kuhn 和 Julia Silge 编写的 《Tidy Modeling with R》。 这本书将教你 tidymodels 系列的包，正如你可能从名字中猜到的那样，它们与我们在本书中使用的 tidyverse 包有许多共同的约定。\nBig data\nThis book proudly and primarily focuses on small, in-memory datasets. This is the right place to start because you can’t tackle big data unless you have experience with small data. The tools you’ll learn throughout the majority of this book will easily handle hundreds of megabytes of data, and with a bit of care, you can typically use them to work with a few gigabytes of data. We’ll also show you how to get data out of databases and parquet files, both of which are often used to store big data. You won’t necessarily be able to work with the entire dataset, but that’s not a problem because you only need a subset or subsample to answer the question that you’re interested in.\n本书主要且自豪地专注于小型的、内存中的数据集。 这是一个正确的起点，因为除非你有处理小数据的经验，否则你无法处理大数据。 你在本书大部分章节中学到的工具可以轻松处理数百兆字节的数据，稍加注意，你通常可以用它们来处理几千兆字节的数据。 我们还将向你展示如何从数据库和 parquet 文件中获取数据，这两种文件都常用于存储大数据。 你不一定能够处理整个数据集，但这不成问题，因为你只需要一个子集或子样本来回答你感兴趣的问题。\nIf you’re routinely working with larger data (10–100 GB, say), we recommend learning more about data.table. We don’t teach it here because it uses a different interface than the tidyverse and requires you to learn some different conventions. However, it is incredibly faster, and the performance payoff is worth investing some time in learning it if you’re working with large data.\n如果你经常处理更大数据（比如 10-100 GB），我们建议你学习更多关于 data.table 的知识。 我们在这里不教它，因为它使用了与 tidyverse 不同的接口，需要你学习一些不同的约定。 然而，它的速度快得惊人，如果你处理的是大数据，为了性能提升而花时间学习它是值得的。\nPython, Julia, and friends\nIn this book, you won’t learn anything about Python, Julia, or any other programming language useful for data science. This isn’t because we think these tools are bad. They’re not! And in practice, most data science teams use a mix of languages, often at least R and Python. But we strongly believe that it’s best to master one tool at a time, and R is a great place to start.\n在本书中，你不会学到任何关于 Python、Julia 或其他用于数据科学的编程语言的知识。 这并非因为我们认为这些工具不好。 它们很好！ 在实践中，大多数数据科学团队会混合使用多种语言，通常至少包括 R 和 Python。 但我们坚信，最好一次只掌握一种工具，而 R 是一个很好的起点。",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#prerequisites",
    "href": "intro.html#prerequisites",
    "title": "Introduction",
    "section": "Prerequisites",
    "text": "Prerequisites\nWe’ve made a few assumptions about what you already know to get the most out of this book. You should be generally numerically literate, and it’s helpful if you have some basic programming experience already. If you’ve never programmed before, you might find Hands on Programming with R by Garrett to be a valuable adjunct to this book.\n为了让你能从本书中获得最大收益，我们对你已有的知识做了一些假设。 你应该具备基本的数学素养，如果已经有一些基础的编程经验会很有帮助。 如果你以前从未编程过，你可能会发现 Garrett 编写的 《Hands on Programming with R》 是本书的一个有价值的补充。\nYou need four things to run the code in this book: R, RStudio, a collection of R packages called the tidyverse, and a handful of other packages. Packages are the fundamental units of reproducible R code. They include reusable functions, documentation that describes how to use them, and sample data.\n要运行本书中的代码，你需要四样东西：R、RStudio、一个名为 tidyverse 的 R 包集合，以及其他几个包。 包是可复现 R 代码的基本单位。 它们包含可重用的函数、描述如何使用它们的文档以及示例数据。\nR\nTo download R, go to CRAN, the comprehensive R archive network, https://cloud.r-project.org. A new major version of R comes out once a year, and there are 2-3 minor releases each year. It’s a good idea to update regularly. Upgrading can be a bit of a hassle, especially for major versions that require you to re-install all your packages, but putting it off only makes it worse. We recommend R 4.2.0 or later for this book.\n要下载 R，请访问 CRAN（comprehensive R archive network，综合 R 存档网络），网址为 https://cloud.r-project.org。 R 每年发布一个新的主版本，每年还有 2-3 个次要版本发布。 定期更新是个好主意。 升级可能有点麻烦，特别是主版本升级需要你重新安装所有包，但拖延只会让事情变得更糟。 我们推荐为本书使用 R 4.2.0 或更高版本。\nRStudio\nRStudio is an integrated development environment, or IDE, for R programming, which you can download from https://posit.co/download/rstudio-desktop/. RStudio is updated a couple of times a year, and it will automatically let you know when a new version is out, so there’s no need to check back. It’s a good idea to upgrade regularly to take advantage of the latest and greatest features. For this book, make sure you have at least RStudio 2022.02.0.\nRStudio 是 R 编程的集成开发环境（IDE），你可以从 https://posit.co/download/rstudio-desktop/ 下载。 RStudio 每年更新几次，当有新版本发布时，它会自动通知你，所以无需反复检查。 定期升级以利用最新最强大的功能是个好主意。 对于本书，请确保你至少安装了 RStudio 2022.02.0 版本。\nWhen you start RStudio, Figure 2, you’ll see two key regions in the interface: the console pane and the output pane. For now, all you need to know is that you type the R code in the console pane and press enter to run it. You’ll learn more as we go along!1\n当你启动 RStudio 时（见 Figure 2），你会在界面中看到两个关键区域：控制台窗格和输出窗格。 目前，你只需要知道，在控制台窗格中输入 R 代码，然后按 Enter 键来运行它。 随着学习的深入，你会了解更多！1\n\n\n\n\n\n\n\nFigure 2: The RStudio IDE has two key regions: type R code in the console pane on the left, and look for plots in the output pane on the right.\n\n\n\n\nThe tidyverse\nYou’ll also need to install some R packages. An R package is a collection of functions, data, and documentation that extends the capabilities of base R. Using packages is key to the successful use of R. The majority of the packages that you will learn in this book are part of the so-called tidyverse. All packages in the tidyverse share a common philosophy of data and R programming and are designed to work together.\n你还需要安装一些 R 包。 R 包 (package) 是一个集函数、数据和文档于一体的集合，它扩展了基础 R 的功能。 使用包是成功使用 R 的关键。 你将在本书中学到的大部分包都属于所谓的 tidyverse。 tidyverse 中的所有包都共享一套关于数据和 R 编程的共同理念，并且被设计为可以协同工作。\nYou can install the complete tidyverse with a single line of code:\n你可以用一行代码安装完整的 tidyverse：\n\ninstall.packages(\"tidyverse\")\n\nOn your computer, type that line of code in the console, and then press enter to run it. R will download the packages from CRAN and install them on your computer.\n在你的电脑上，在控制台中输入那行代码，然后按回车键运行它。 R 将从 CRAN 下载这些包并安装到你的电脑上。\nYou will not be able to use the functions, objects, or help files in a package until you load it with library(). Once you have installed a package, you can load it using the library() function:\n在你使用 library() 加载一个包之前，你将无法使用该包中的函数、对象或帮助文件。 一旦安装了包，你可以使用 library() 函数来加载它：\n\nlibrary(tidyverse)\n#&gt; ── Attaching core tidyverse packages ───────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n#&gt; ✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n#&gt; ✔ purrr     1.0.4     \n#&gt; ── Conflicts ─────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nThis tells you that tidyverse loads nine packages: dplyr, forcats, ggplot2, lubridate, purrr, readr, stringr, tibble, tidyr. These are considered the core of the tidyverse because you’ll use them in almost every analysis.\n这会告诉你 tidyverse 加载了九个包：dplyr、forcats、ggplot2、lubridate、purrr、readr、stringr、tibble 和 tidyr。 这些被认为是 tidyverse 的 核心 (core)，因为你几乎在每次分析中都会用到它们。\nPackages in the tidyverse change fairly frequently. You can see if updates are available by running tidyverse_update().\ntidyverse 中的包更新相当频繁。 你可以通过运行 tidyverse_update() 来查看是否有可用的更新。\nOther packages\nThere are many other excellent packages that are not part of the tidyverse because they solve problems in a different domain or are designed with a different set of underlying principles. This doesn’t make them better or worse; it just makes them different. In other words, the complement to the tidyverse is not the messyverse but many other universes of interrelated packages. As you tackle more data science projects with R, you’ll learn new packages and new ways of thinking about data.\n还有许多其他优秀的包不属于 tidyverse，因为它们解决的是不同领域的问题，或者遵循不同的底层设计原则。 这并不能说它们更好或更差，只是它们不同。 换句话说，tidyverse 的补充不是 messyverse (混乱宇宙)，而是许多其他相互关联的包的宇宙。 随着你用 R 攻克更多的数据科学项目，你将学到新的包和新的数据思维方式。\nWe’ll use many packages from outside the tidyverse in this book. For example, we’ll use the following packages because they provide interesting datasets for us to work with in the process of learning R:\n在本书中，我们将使用许多来自 tidyverse 之外的包。 例如，我们将使用以下包，因为它们在我们学习 R 的过程中提供了有趣的数据集供我们使用：\n\ninstall.packages(\n  c(\"arrow\", \"babynames\", \"curl\", \"duckdb\", \"gapminder\", \n    \"ggrepel\", \"ggridges\", \"ggthemes\", \"hexbin\", \"janitor\", \"Lahman\", \n    \"leaflet\", \"maps\", \"nycflights13\", \"openxlsx\", \"palmerpenguins\", \n    \"repurrrsive\", \"tidymodels\", \"writexl\")\n  )\n\nWe’ll also use a selection of other packages for one off examples. You don’t need to install them now, just remember that whenever you see an error like this:\n我们还将为一些一次性的例子使用其他一些包。 你现在不需要安装它们，只需记住，当你看到如下错误时：\n\nlibrary(ggrepel)\n#&gt; Error in library(ggrepel) : there is no package called ‘ggrepel’\n\nYou need to run install.packages(\"ggrepel\") to install the package.\n你需要运行 install.packages(\"ggrepel\") 来安装这个包。",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#running-r-code",
    "href": "intro.html#running-r-code",
    "title": "Introduction",
    "section": "Running R code",
    "text": "Running R code\nThe previous section showed you several examples of running R code. The code in the book looks like this:\n上一节向你展示了几个运行 R 代码的例子。 书中的代码看起来是这样的：\n\n1 + 2\n#&gt; [1] 3\n\nIf you run the same code in your local console, it will look like this:\n如果你在本地控制台运行同样的代码，它会看起来是这样的：\n&gt; 1 + 2\n[1] 3\nThere are two main differences. In your console, you type after the &gt;, called the prompt; we don’t show the prompt in the book. In the book, the output is commented out with #&gt;; in your console, it appears directly after your code. These two differences mean that if you’re working with an electronic version of the book, you can easily copy code out of the book and paste it into the console.\n这里有两个主要区别。 在你的控制台里，你在 &gt;（称为 提示符 (prompt)）后面输入；我们在书中不显示提示符。 在书中，输出被 #&gt; 注释掉了；在你的控制台里，它直接出现在你的代码后面。 这两个区别意味着，如果你正在使用本书的电子版，你可以很容易地从书中复制代码并粘贴到控制台。\nThroughout the book, we use a consistent set of conventions to refer to code:\n在整本书中，我们使用一套一致的约定来引用代码：\n- Functions are displayed in a code font and followed by parentheses, like sum() or mean().\n函数会以代码字体显示，并后跟括号，如 sum() 或 mean()。\n- Other R objects (such as data or function arguments) are in a code font, without parentheses, like flights or x.\n其他的 R 对象（比如数据或函数参数）会以代码字体显示，不带括号，如 flights 或 x。\n- Sometimes, to make it clear which package an object comes from, we’ll use the package name followed by two colons, like dplyr::mutate() or nycflights13::flights. This is also valid R code.\n有时，为了明确一个对象来自哪个包，我们会使用包名后跟两个冒号的形式，比如 dplyr::mutate() 或 nycflights13::flights。 这也是有效的 R 代码。",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#acknowledgments",
    "href": "intro.html#acknowledgments",
    "title": "Introduction",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis book isn’t just the product of Hadley, Mine, and Garrett but is the result of many conversations (in person and online) that we’ve had with many people in the R community. We’re incredibly grateful for all the conversations we’ve had with y’all; thank you so much!\n这本书不仅仅是 Hadley、Mine 和 Garrett 的成果，也是我们与 R 社区中许多人进行多次对话（线上和线下）的结晶。 我们非常感谢与各位进行的所有对话；非常感谢你们！\nThis book was written in the open, and many people contributed via pull requests. A special thanks to all 259 of you who contributed improvements via GitHub pull requests (in alphabetical order by username): @a-rosenberg, Tim Becker (@a2800276), Abinash Satapathy (@Abinashbunty), Adam Gruer (@adam-gruer), adi pradhan (@adidoit), A. s. (@Adrianzo), Aep Hidyatuloh (@aephidayatuloh), Andrea Gilardi (@agila5), Ajay Deonarine (@ajay-d), @AlanFeder, Daihe Sui (@alansuidaihe), @alberto-agudo, @AlbertRapp, @aleloi, pete (@alonzi), Alex (@ALShum), Andrew M. (@amacfarland), Andrew Landgraf (@andland), @andyhuynh92, Angela Li (@angela-li), Antti Rask (@AnttiRask), LOU Xun (@aquarhead), @ariespirgel, @august-18, Michael Henry (@aviast), Azza Ahmed (@azzaea), Steven Moran (@bambooforest), Brian G. Barkley (@BarkleyBG), Mara Averick (@batpigandme), Oluwafemi OYEDELE (@BB1464), Brent Brewington (@bbrewington), Bill Behrman (@behrman), Ben Herbertson (@benherbertson), Ben Marwick (@benmarwick), Ben Steinberg (@bensteinberg), Benjamin Yeh (@bentyeh), Betul Turkoglu (@betulturkoglu), Brandon Greenwell (@bgreenwell), Bianca Peterson (@BinxiePeterson), Birger Niklas (@BirgerNi), Brett Klamer (@bklamer), @boardtc, Christian (@c-hoh), Caddy (@caddycarine), Camille V Leonard (@camillevleonard), @canovasjm, Cedric Batailler (@cedricbatailler), Christina Wei (@christina-wei), Christian Mongeau (@chrMongeau), Cooper Morris (@coopermor), Colin Gillespie (@csgillespie), Rademeyer Vermaak (@csrvermaak), Chloe Thierstein (@cthierst), Chris Saunders (@ctsa), Abhinav Singh (@curious-abhinav), Curtis Alexander (@curtisalexander), Christian G. Warden (@cwarden), Charlotte Wickham (@cwickham), Kenny Darrell (@darrkj), David Kane (@davidkane9), David (@davidrsch), David Rubinger (@davidrubinger), David Clark (@DDClark), Derwin McGeary (@derwinmcgeary), Daniel Gromer (@dgromer), @Divider85, @djbirke, Danielle Navarro (@djnavarro), Russell Shean (@DOH-RPS1303), Zhuoer Dong (@dongzhuoer), Devin Pastoor (@dpastoor), @DSGeoff, Devarshi Thakkar (@dthakkar09), Julian During (@duju211), Dylan Cashman (@dylancashman), Dirk Eddelbuettel (@eddelbuettel), Edwin Thoen (@EdwinTh), Ahmed El-Gabbas (@elgabbas), Henry Webel (@enryH), Ercan Karadas (@ercan7), Eric Kitaif (@EricKit), Eric Watt (@ericwatt), Erik Erhardt (@erikerhardt), Etienne B. Racine (@etiennebr), Everett Robinson (@evjrob), @fellennert, Flemming Miguel (@flemmingmiguel), Floris Vanderhaeghe (@florisvdh), @funkybluehen, @gabrivera, Garrick Aden-Buie (@gadenbuie), Peter Ganong (@ganong123), Gerome Meyer (@GeroVanMi), Gleb Ebert (@gl-eb), Josh Goldberg (@GoldbergData), bahadir cankardes (@gridgrad), Gustav W Delius (@gustavdelius), Hao Chen (@hao-trivago), Harris McGehee (@harrismcgehee), @hendrikweisser, Hengni Cai (@hengnicai), Iain (@Iain-S), Ian Sealy (@iansealy), Ian Lyttle (@ijlyttle), Ivan Krukov (@ivan-krukov), Jacob Kaplan (@jacobkap), Jazz Weisman (@jazzlw), John Blischak (@jdblischak), John D. Storey (@jdstorey), Gregory Jefferis (@jefferis), Jeffrey Stevens (@JeffreyRStevens), 蒋雨蒙 (@JeldorPKU), Jennifer (Jenny) Bryan (@jennybc), Jen Ren (@jenren), Jeroen Janssens (@jeroenjanssens), @jeromecholewa, Janet Wesner (@jilmun), Jim Hester (@jimhester), JJ Chen (@jjchern), Jacek Kolacz (@jkolacz), Joanne Jang (@joannejang), @johannes4998, John Sears (@johnsears), @jonathanflint, Jon Calder (@jonmcalder), Jonathan Page (@jonpage), Jon Harmon (@jonthegeek), JooYoung Seo (@jooyoungseo), Justinas Petuchovas (@jpetuchovas), Jordan (@jrdnbradford), Jeffrey Arnold (@jrnold), Jose Roberto Ayala Solares (@jroberayalas), Joyce Robbins (@jtr13), @juandering, Julia Stewart Lowndes (@jules32), Sonja (@kaetschap), Kara Woo (@karawoo), Katrin Leinweber (@katrinleinweber), Karandeep Singh (@kdpsingh), Kevin Perese (@kevinxperese), Kevin Ferris (@kferris10), Kirill Sevastyanenko (@kirillseva), Jonathan Kitt (@KittJonathan), @koalabearski, Kirill Müller (@krlmlr), Rafał Kucharski (@kucharsky), Kevin Wright (@kwstat), Noah Landesberg (@landesbergn), Lawrence Wu (@lawwu), @lindbrook, Luke W Johnston (@lwjohnst86), Kara de la Marck (@MarckK), Kunal Marwaha (@marwahaha), Matan Hakim (@matanhakim), Matthias Liew (@MatthiasLiew), Matt Wittbrodt (@MattWittbrodt), Mauro Lepore (@maurolepore), Mark Beveridge (@mbeveridge), @mcewenkhundi, mcsnowface, PhD (@mcsnowface), Matt Herman (@mfherman), Michael Boerman (@michaelboerman), Mitsuo Shiota (@mitsuoxv), Matthew Hendrickson (@mjhendrickson), @MJMarshall, Misty Knight-Finley (@mkfin7), Mohammed Hamdy (@mmhamdy), Maxim Nazarov (@mnazarov), Maria Paula Caldas (@mpaulacaldas), Mustafa Ascha (@mustafaascha), Nelson Areal (@nareal), Nate Olson (@nate-d-olson), Nathanael (@nateaff), @nattalides, Ned Western (@NedJWestern), Nick Clark (@nickclark1000), @nickelas, Nirmal Patel (@nirmalpatel), Nischal Shrestha (@nischalshrestha), Nicholas Tierney (@njtierney), Jakub Nowosad (@Nowosad), Nick Pullen (@nstjhp), @olivier6088, Olivier Cailloux (@oliviercailloux), Robin Penfold (@p0bs), Pablo E. Garcia (@pabloedug), Paul Adamson (@padamson), Penelope Y (@penelopeysm), Peter Hurford (@peterhurford), Peter Baumgartner (@petzi53), Patrick Kennedy (@pkq), Pooya Taherkhani (@pooyataher), Y. Yu (@PursuitOfDataScience), Radu Grosu (@radugrosu), Ranae Dietzel (@Ranae), Ralph Straumann (@rastrau), Rayna M Harris (@raynamharris), @ReeceGoding, Robin Gertenbach (@rgertenbach), Jajo (@RIngyao), Riva Quiroga (@rivaquiroga), Richard Knight (@RJHKnight), Richard Zijdeman (@rlzijdeman), @robertchu03, Robin Kohrs (@RobinKohrs), Robin (@Robinlovelace), Emily Robinson (@robinsones), Rob Tenorio (@robtenorio), Rod Mazloomi (@RodAli), Rohan Alexander (@RohanAlexander), Romero Morais (@RomeroBarata), Albert Y. Kim (@rudeboybert), Saghir (@saghirb), Hojjat Salmasian (@salmasian), Jonas (@sauercrowd), Vebash Naidoo (@sciencificity), Seamus McKinsey (@seamus-mckinsey), @seanpwilliams, Luke Smith (@seasmith), Matthew Sedaghatfar (@sedaghatfar), Sebastian Kraus (@sekR4), Sam Firke (@sfirke), Shannon Ellis (@ShanEllis), @shoili, Christian Heinrich (@Shurakai), S’busiso Mkhondwane (@sibusiso16), SM Raiyyan (@sm-raiyyan), Jakob Krigovsky (@sonicdoe), Stephan Koenig (@stephan-koenig), Stephen Balogun (@stephenbalogun), Steven M. Mortimer (@StevenMMortimer), Stéphane Guillou (@stragu), Sulgi Kim (@sulgik), Sergiusz Bleja (@svenski), Tal Galili (@talgalili), Alec Fisher (@Taurenamo), Todd Gerarden (@tgerarden), Tom Godfrey (@thomasggodfrey), Tim Broderick (@timbroderick), Tim Waterhouse (@timwaterhouse), TJ Mahr (@tjmahr), Thomas Klebel (@tklebel), Tom Prior (@tomjamesprior), Terence Teo (@tteo), @twgardner2, Ulrik Lyngs (@ulyngs), Shinya Uryu (@uribo), Martin Van der Linden (@vanderlindenma), Walter Somerville (@waltersom), @werkstattcodes, Will Beasley (@wibeasley), Yihui Xie (@yihui), Yiming (Paul) Li (@yimingli), @yingxingwu, Hiroaki Yutani (@yutannihilation), Yu Yu Aung (@yuyu-aung), Zach Bogart (@zachbogart), @zeal626, Zeki Akyol (@zekiakyol).",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#colophon",
    "href": "intro.html#colophon",
    "title": "Introduction",
    "section": "Colophon",
    "text": "Colophon\nAn online version of this book is available at https://r4ds.hadley.nz. It will continue to evolve in between reprints of the physical book. The source of the book is available at https://github.com/hadley/r4ds. The book is powered by Quarto, which makes it easy to write books that combine text and executable code.\n本书的在线版本可在 https://r4ds.hadley.nz 查看。 在实体书再版之间，它会不断演进。 本书的源代码可在 https://github.com/hadley/r4ds 找到。 本书由 Quarto 驱动，它使得编写结合文本和可执行代码的书籍变得容易。",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "Introduction",
    "section": "",
    "text": "If you’d like a comprehensive overview of all of RStudio’s features, see the RStudio User Guide at https://docs.posit.co/ide/user.↩︎",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "whole-game.html",
    "href": "whole-game.html",
    "title": "Whole game",
    "section": "",
    "text": "Our goal in this part of the book is to give you a rapid overview of the main tools of data science: importing, tidying, transforming, and visualizing data, as shown in Figure 1. We want to show you the “whole game” of data science giving you just enough of all the major pieces so that you can tackle real, if simple, datasets. The later parts of the book will hit each of these topics in more depth, increasing the range of data science challenges that you can tackle.  在本书这一部分，我们的目标是快速概览数据科学的主要工具：导入 (importing)、整洁化 (tidying)、转换 (transforming) 和 可视化 (visualizing)，如 Figure 1 所示。 我们希望向你展示数据科学的 “完整全局 (whole game)”，让你对所有关键环节都有足够认识，以便能够处理真正但相对简单的数据集。 后续章节将更深入地探讨这些主题，逐步提升你可应对的数据科学挑战的广度和深度。\n\n\n\n\n\n\n\nFigure 1: In this section of the book, you’ll learn how to import, tidy, transform, and visualize data.\n\n\n\n\nFour chapters focus on the tools of data science:  以下四个章节集中介绍数据科学的核心工具：\n\nVisualization is a great place to start with R programming, because the payoff is so clear: you get to make elegant and informative plots that help you understand data. In 1  Data visualization you’ll dive into visualization, learning the basic structure of a ggplot2 plot, and powerful techniques for turning data into plots.  以可视化入门 R 编程是绝佳选择，因为它的回报十分直观：你可以绘制优雅而信息丰富的图形来理解数据。 在 1  Data visualization 中，你将深入学习可视化，掌握 ggplot2 图形的基本结构，以及将数据转化为图形的强大技巧。\nVisualization alone is typically not enough, so in 3  Data transformation, you’ll learn the key verbs that allow you to select important variables, filter out key observations, create new variables, and compute summaries.  单靠可视化通常不足以完成分析任务，因此在 3  Data transformation 中，你将学习一组关键 “动词 (verbs)”：选择重要变量、筛选关键观测、创建新变量并生成汇总结果。\nIn 5  Data tidying, you’ll learn about tidy data, a consistent way of storing your data that makes transformation, visualization, and modelling easier. You’ll learn the underlying principles, and how to get your data into a tidy form.  在 5  Data tidying 中，你将了解 “整洁数据 (tidy data)” 的概念，这是一种一致的存储方式，可让后续转换、可视化与建模工作更轻松。 你将学习其核心原则，以及如何将数据整理成整洁格式。\nBefore you can transform and visualize your data, you need to first get your data into R. In 7  Data import you’ll learn the basics of getting .csv files into R.  在转换和可视化数据之前，你需要先将数据导入 R。 7  Data import 将教你把 .csv 文件读入 R 的基础方法。\n\nNestled among these chapters are four other chapters that focus on your R workflow. In 2  Workflow: basics, 4  Workflow: code style, and 6  Workflow: scripts and projects you’ll learn good workflow practices for writing and organizing your R code. These will set you up for success in the long run, as they’ll give you the tools to stay organized when you tackle real projects. Finally, 8  Workflow: getting help will teach you how to get help and keep learning.  除了上述内容，本部分还穿插了四个专注于 R 工作流的章节。 在 2  Workflow: basics、4  Workflow: code style 以及 6  Workflow: scripts and projects 中，你将学习撰写并组织 R 代码的良好工作流实践。 这些技能将在长期项目中助你保持条理与高效。 最后，8  Workflow: getting help 将指导你如何寻求帮助并持续学习。",
    "crumbs": [
      "Whole game"
    ]
  },
  {
    "objectID": "data-visualize.html",
    "href": "data-visualize.html",
    "title": "1  Data visualization",
    "section": "",
    "text": "1.1 Introduction\nR has several systems for making graphs, but ggplot2 is one of the most elegant and most versatile. ggplot2 implements the grammar of graphics, a coherent system for describing and building graphs. With ggplot2, you can do more and faster by learning one system and applying it in many places.\nR 有多个用于制作图形的系统，但 ggplot2 是其中最优雅、功能最全面的之一。ggplot2 实现了图形语法 (grammar of graphics)，这是一个用于描述和构建图形的连贯系统。通过学习 ggplot2，你可以在许多地方应用这一系统，从而更快、更好地完成工作。\nThis chapter will teach you how to visualize your data using ggplot2. We will start by creating a simple scatterplot and use that to introduce aesthetic mappings and geometric objects – the fundamental building blocks of ggplot2. We will then walk you through visualizing distributions of single variables as well as visualizing relationships between two or more variables. We’ll finish off with saving your plots and troubleshooting tips.\n本章将教你如何使用 ggplot2 来可视化你的数据。我们将从创建一个简单的散点图开始，并以此介绍美学映射 (aesthetic mappings) 和几何对象 (geometric objects)——它们是 ggplot2 的基本构建模块。然后，我们将引导你学习如何可视化单个变量的分布以及两个或多个变量之间的关系。最后，我们会介绍如何保存你的图形以及一些故障排除技巧。",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data visualization</span>"
    ]
  },
  {
    "objectID": "data-visualize.html#introduction",
    "href": "data-visualize.html#introduction",
    "title": "1  Data visualization",
    "section": "",
    "text": "“The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey\n“简单的图形给数据分析师带来的信息比任何其他设备都多。” — 约翰·图基 (John Tukey)\n\n\n\n\n1.1.1 Prerequisites\nThis chapter focuses on ggplot2, one of the core packages in the tidyverse. To access the datasets, help pages, and functions used in this chapter, load the tidyverse by running:\n本章重点介绍 ggplot2，它是 tidyverse 中的核心包之一。要访问本章中使用的数据集、帮助页面和函数，请通过运行以下代码加载 tidyverse：\n\nlibrary(tidyverse)\n#&gt; ── Attaching core tidyverse packages ───────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n#&gt; ✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n#&gt; ✔ purrr     1.0.4     \n#&gt; ── Conflicts ─────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nThat one line of code loads the core tidyverse; the packages that you will use in almost every data analysis. It also tells you which functions from the tidyverse conflict with functions in base R (or from other packages you might have loaded)1.\n这一行代码加载了 tidyverse 的核心包；这些包几乎在每次数据分析中都会用到。它还会告诉你 tidyverse 中的哪些函数与基础 R（或其他你可能已加载的包）中的函数存在冲突1。\nIf you run this code and get the error message there is no package called 'tidyverse', you’ll need to first install it, then run library() once again.\n如果你运行此代码并收到错误消息 there is no package called 'tidyverse'，你需要先安装它，然后再运行一次 library()。\n\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)\n\nYou only need to install a package once, but you need to load it every time you start a new session.\n你只需要安装一个包一次，但每次开始新会话时都需要加载它。\nIn addition to tidyverse, we will also use the palmerpenguins package, which includes the penguins dataset containing body measurements for penguins on three islands in the Palmer Archipelago, and the ggthemes package, which offers a colorblind safe color palette.\n除了 tidyverse，我们还将使用 palmerpenguins 包，其中包含了 penguins 数据集，该数据集包含帕默群岛三个岛屿上企鹅的身体测量数据；我们还会用到 ggthemes 包，它提供了一个色盲安全的调色板。\n\nlibrary(palmerpenguins)\n#&gt; \n#&gt; Attaching package: 'palmerpenguins'\n#&gt; The following objects are masked from 'package:datasets':\n#&gt; \n#&gt;     penguins, penguins_raw\nlibrary(ggthemes)",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data visualization</span>"
    ]
  },
  {
    "objectID": "data-visualize.html#first-steps",
    "href": "data-visualize.html#first-steps",
    "title": "1  Data visualization",
    "section": "\n1.2 First steps",
    "text": "1.2 First steps\nDo penguins with longer flippers weigh more or less than penguins with shorter flippers? You probably already have an answer, but try to make your answer precise. What does the relationship between flipper length and body mass look like? Is it positive? Negative? Linear? Nonlinear? Does the relationship vary by the species of the penguin? How about by the island where the penguin lives? Let’s create visualizations that we can use to answer these questions.\n鳍状肢较长的企鹅比鳍状肢较短的企鹅重还是轻？你可能已经有了答案，但请尝试让你的答案更精确。鳍状肢长度和体重之间的关系是怎样的？是正相关？负相关？线性的？非线性的？这种关系是否因企鹅的种类而异？又是否因企鹅居住的岛屿而异？让我们创建可视化图表来回答这些问题。\n\n1.2.1 The penguins data frame\nYou can test your answers to those questions with the penguins data frame found in palmerpenguins (a.k.a. palmerpenguins::penguins). A data frame is a rectangular collection of variables (in the columns) and observations (in the rows). penguins contains 344 observations collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER2.\n你可以使用 palmerpenguins 包中的 penguins 数据框（也写作 palmerpenguins::penguins）来检验你对这些问题的回答。数据框是一个矩形集合，包含变量（在列中）和观测值（在行中）。penguins 数据集包含了 344 条观测数据，由 Kristen Gorman 博士和南极帕默站长期生态研究项目（LTER）收集并提供2。\nTo make the discussion easier, let’s define some terms:\n为了让讨论更容易，我们来定义一些术语：\n- A variable is a quantity, quality, or property that you can measure.变量 (variable) 是你可以测量的数量、质量或属性。\n- A value is the state of a variable when you measure it. The value of a variable may change from measurement to measurement.值 (value) 是你在测量时一个变量的状态。一个变量的值可能在不同次测量中发生变化。\n- An observation is a set of measurements made under similar conditions (you usually make all of the measurements in an observation at the same time and on the same object). An observation will contain several values, each associated with a different variable. We’ll sometimes refer to an observation as a data point.观测 (observation) 是在相似条件下进行的一组测量（通常你会在同一时间对同一对象进行一次观测中的所有测量）。一次观测会包含几个值，每个值都与一个不同的变量相关联。我们有时也将一次观测称为一个数据点。\n- Tabular data is a set of values, each associated with a variable and an observation. Tabular data is tidy if each value is placed in its own “cell”, each variable in its own column, and each observation in its own row.表格数据 (Tabular data) 是一组值，每个值都与一个变量和一次观测相关联。如果每个值都放在自己的“单元格”中，每个变量在自己的列中，每个观测在自己的行中，那么这个表格数据就是整洁的。\nIn this context, a variable refers to an attribute of all the penguins, and an observation refers to all the attributes of a single penguin.\n在此背景下，一个变量指的是所有企鹅的一个属性，而一个观测指的是单只企鹅的所有属性。\nType the name of the data frame in the console and R will print a preview of its contents. Note that it says tibble on top of this preview. In the tidyverse, we use special data frames called tibbles that you will learn more about soon.\n在控制台中输入数据框的名称，R 将会打印其内容的预览。注意，在这个预览的顶部显示了 tibble。在 tidyverse 中，我们使用一种特殊的数据框，称为 tibbles，你很快就会学到更多关于它的知识。\n\npenguins\n#&gt; # A tibble: 344 × 8\n#&gt;   species island    bill_length_mm bill_depth_mm flipper_length_mm\n#&gt;   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;\n#&gt; 1 Adelie  Torgersen           39.1          18.7               181\n#&gt; 2 Adelie  Torgersen           39.5          17.4               186\n#&gt; 3 Adelie  Torgersen           40.3          18                 195\n#&gt; 4 Adelie  Torgersen           NA            NA                  NA\n#&gt; 5 Adelie  Torgersen           36.7          19.3               193\n#&gt; 6 Adelie  Torgersen           39.3          20.6               190\n#&gt; # ℹ 338 more rows\n#&gt; # ℹ 3 more variables: body_mass_g &lt;int&gt;, sex &lt;fct&gt;, year &lt;int&gt;\n\nThis data frame contains 8 columns. For an alternative view, where you can see all variables and the first few observations of each variable, use glimpse(). Or, if you’re in RStudio, run View(penguins) to open an interactive data viewer.\n该数据框包含 8 列。若要查看另一种视图，即可以看到所有变量以及每个变量的前几个观测值，请使用 glimpse()。或者，如果你在 RStudio 中，运行 View(penguins) 可以打开一个交互式数据查看器。\n\nglimpse(penguins)\n#&gt; Rows: 344\n#&gt; Columns: 8\n#&gt; $ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, A…\n#&gt; $ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torge…\n#&gt; $ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.…\n#&gt; $ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.…\n#&gt; $ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, …\n#&gt; $ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 347…\n#&gt; $ sex               &lt;fct&gt; male, female, female, NA, female, male, female, m…\n#&gt; $ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2…\n\nAmong the variables in penguins are:penguins 数据框中的变量包括：\n\nspecies: a penguin’s species (Adelie, Chinstrap, or Gentoo).species: 企鹅的种类（阿德利、帽带或金图）。\nflipper_length_mm: length of a penguin’s flipper, in millimeters.flipper_length_mm: 企鹅鳍状肢的长度，单位为毫米。\nbody_mass_g: body mass of a penguin, in grams.body_mass_g: 企鹅的体重，单位为克。\n\nTo learn more about penguins, open its help page by running ?penguins.\n要了解更多关于 penguins 的信息，运行 ?penguins 来打开它的帮助页面。\n\n1.2.2 Ultimate goal\nOur ultimate goal in this chapter is to recreate the following visualization displaying the relationship between flipper lengths and body masses of these penguins, taking into consideration the species of the penguin.\n我们在本章的最终目标是重新创建以下可视化图表，该图表展示了这些企鹅的鳍状肢长度和体重之间的关系，同时考虑了企鹅的种类。\n\n\n\n\n\n\n\n\n\n1.2.3 Creating a ggplot\nLet’s recreate this plot step-by-step.\n让我们一步步地重新创建这个图。\nWith ggplot2, you begin a plot with the function ggplot(), defining a plot object that you then add layers to. The first argument of ggplot() is the dataset to use in the graph and so ggplot(data = penguins) creates an empty graph that is primed to display the penguins data, but since we haven’t told it how to visualize it yet, for now it’s empty. This is not a very exciting plot, but you can think of it like an empty canvas you’ll paint the remaining layers of your plot onto.\n使用 ggplot2 时，你通过 ggplot() 函数开始一个绘图，定义一个绘图对象，然后向其添加图层 (layers)。ggplot() 的第一个参数是要在图中使用的数据集，因此 ggplot(data = penguins) 会创建一个准备好显示 penguins 数据的空图，但由于我们还没有告诉它如何可视化这些数据，所以目前它还是空的。这虽然不是一个很激动人心的图，但你可以把它想象成一块空白的画布，你将在这上面绘制图的其余图层。\n\nggplot(data = penguins)\n\n\n\n\n\n\n\nNext, we need to tell ggplot() how the information from our data will be visually represented. The mapping argument of the ggplot() function defines how variables in your dataset are mapped to visual properties (aesthetics) of your plot. The mapping argument is always defined in the aes() function, and the x and y arguments of aes() specify which variables to map to the x and y axes. For now, we will only map flipper length to the x aesthetic and body mass to the y aesthetic. ggplot2 looks for the mapped variables in the data argument, in this case, penguins.\n接下来，我们需要告诉 ggplot() 如何将我们数据中的信息进行可视化表示。ggplot() 函数的 mapping 参数定义了你数据集中的变量如何映射到图的视觉属性（美学 (aesthetics)）上。mapping 参数总是在 aes() 函数中定义，而 aes() 的 x 和 y 参数指定了将哪些变量映射到 x 轴和 y 轴。现在，我们只将鳍状肢长度映射到 x 美学，将体重映射到 y 美学。ggplot2 会在 data 参数（在这里是 penguins）中寻找被映射的变量。\nThe following plot shows the result of adding these mappings.\n下面的图表展示了添加这些映射后的结果。\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n)\n\n\n\n\n\n\n\nOur empty canvas now has more structure – it’s clear where flipper lengths will be displayed (on the x-axis) and where body masses will be displayed (on the y-axis). But the penguins themselves are not yet on the plot. This is because we have not yet articulated, in our code, how to represent the observations from our data frame on our plot.\n我们空白的画布现在有了更多的结构——很清楚鳍状肢长度将显示在哪里（x 轴上），体重将显示在哪里（y 轴上）。但是企鹅本身还没有出现在图上。这是因为我们还没有在代码中明确说明如何将我们数据框中的观测值在图上表示出来。\nTo do so, we need to define a geom: the geometrical object that a plot uses to represent data. These geometric objects are made available in ggplot2 with functions that start with geom_. People often describe plots by the type of geom that the plot uses. For example, bar charts use bar geoms (geom_bar()), line charts use line geoms (geom_line()), boxplots use boxplot geoms (geom_boxplot()), scatterplots use point geoms (geom_point()), and so on.\n为此，我们需要定义一个 几何对象 (geom)：即图用来表示数据的几何对象。这些几何对象在 ggplot2 中通过以 geom_ 开头的函数提供。人们通常用图所使用的几何对象类型来描述图。例如，条形图使用条形几何对象 (geom_bar())，折线图使用线形几何对象 (geom_line())，箱线图使用箱线图几何对象 (geom_boxplot())，散点图使用点几何对象 (geom_point()) 等等。\nThe function geom_point() adds a layer of points to your plot, which creates a scatterplot. ggplot2 comes with many geom functions that each adds a different type of layer to a plot. You’ll learn a whole bunch of geoms throughout the book, particularly in Chapter 9.\n函数 geom_point() 会在你的图上添加一个点图层，从而创建一个散点图。ggplot2 提供了许多几何函数，每个函数都会给图添加不同类型的图层。你将在本书中学习到许多几何对象，特别是在 Chapter 9 中。\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n#&gt; Warning: Removed 2 rows containing missing values or values outside the scale range\n#&gt; (`geom_point()`).\n\n\n\n\n\n\n\nNow we have something that looks like what we might think of as a “scatterplot”. It doesn’t yet match our “ultimate goal” plot, but using this plot we can start answering the question that motivated our exploration: “What does the relationship between flipper length and body mass look like?” The relationship appears to be positive (as flipper length increases, so does body mass), fairly linear (the points are clustered around a line instead of a curve), and moderately strong (there isn’t too much scatter around such a line). Penguins with longer flippers are generally larger in terms of their body mass.\n现在我们得到了一个看起来像是我们所认为的“散点图”的东西。它还不完全符合我们“最终目标”的图，但通过这张图，我们可以开始回答激发我们探索的问题：“鳍状肢长度和体重之间的关系是怎样的？” 这种关系看起来是正相关的（随着鳍状肢长度的增加，体重也增加），相当线性的（数据点聚集在一条直线周围而不是一条曲线），并且强度适中（围绕这条线的散点不多）。鳍状肢较长的企鹅通常体重也较重。\nBefore we add more layers to this plot, let’s pause for a moment and review the warning message we got:\n在为这张图添加更多图层之前，让我们暂停一下，回顾一下我们收到的警告信息：\n\nRemoved 2 rows containing missing values (geom_point()).\n\n&gt; 已移除 2 个包含缺失值的行 (geom_point()).\nWe’re seeing this message because there are two penguins in our dataset with missing body mass and/or flipper length values and ggplot2 has no way of representing them on the plot without both of these values. Like R, ggplot2 subscribes to the philosophy that missing values should never silently go missing. This type of warning is probably one of the most common types of warnings you will see when working with real data – missing values are a very common issue and you’ll learn more about them throughout the book, particularly in Chapter 18. For the remaining plots in this chapter we will suppress this warning so it’s not printed alongside every single plot we make.\n我们看到这条消息是因为我们的数据集中有两只企鹅的体重和/或鳍状肢长度值缺失，而 ggplot2 在没有这两个值的情况下无法在图上表示它们。和 R 一样，ggplot2 也遵循这样的理念：缺失值不应该悄无声息地消失。这种类型的警告可能是你在处理真实数据时最常看到的警告之一——缺失值是一个非常普遍的问题，你将在本书中，特别是在 Chapter 18 中学到更多相关知识。对于本章中余下的图，我们将抑制此警告，这样它就不会在我们制作的每一张图旁边都打印出来。\n\n1.2.4 Adding aesthetics and layers\nScatterplots are useful for displaying the relationship between two numerical variables, but it’s always a good idea to be skeptical of any apparent relationship between two variables and ask if there may be other variables that explain or change the nature of this apparent relationship. For example, does the relationship between flipper length and body mass differ by species? Let’s incorporate species into our plot and see if this reveals any additional insights into the apparent relationship between these variables.\n散点图对于展示两个数值变量之间的关系很有用，但对两个变量之间的任何明显关系持怀疑态度总是一个好主意，并且应该探究是否还有其他变量可以解释或改变这种明显关系的性质。例如，鳍状肢长度和体重之间的关系是否因物种而异？让我们将物种纳入我们的图中，看看这是否能揭示关于这些变量之间明显关系的更多见解。\nTo achieve this, will we need to modify the aesthetic or the geom? If you guessed “in the aesthetic mapping, inside of aes()”, you’re already getting the hang of creating data visualizations with ggplot2! And if not, don’t worry. Throughout the book you will make many more ggplots and have many more opportunities to check your intuition as you make them.\n为了实现这一点，我们需要修改美学还是几何对象？如果你猜的是“在美学映射中，即 aes() 内部”，那么你已经开始掌握用 ggplot2 创建数据可视化的窍门了！如果没猜对，也别担心。在本书中，你将制作更多的 ggplot 图，并在制作过程中有更多机会来检验你的直觉。\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point()\n\n\n\n\n\n\n\nWhen a categorical variable is mapped to an aesthetic, ggplot2 will automatically assign a unique value of the aesthetic (here a unique color) to each unique level of the variable (each of the three species), a process known as scaling. ggplot2 will also add a legend that explains which values correspond to which levels.\n当一个分类变量被映射到一个美学上时，ggplot2 会自动为该变量的每个唯一水平（这里是三个物种中的每一个）分配一个该美学的唯一值（这里是唯一的颜色），这个过程称为标度变换 (scaling)。ggplot2 还会添加一个图例，解释哪些值对应哪些水平。\nNow let’s add one more layer: a smooth curve displaying the relationship between body mass and flipper length. Before you proceed, refer back to the code above, and think about how we can add this to our existing plot.\n现在我们再加一个图层：一条平滑曲线，用于显示体重和鳍状肢长度之间的关系。在继续之前，请回顾上面的代码，并思考我们如何将这个图层添加到现有的图中。\nSince this is a new geometric object representing our data, we will add a new geom as a layer on top of our point geom: geom_smooth(). And we will specify that we want to draw the line of best fit based on a linear model with method = \"lm\".\n由于这是一个代表我们数据的新的几何对象，我们将在点几何对象之上添加一个新的几何对象图层：geom_smooth()。并且我们将指定我们希望基于一个线性模型（linear model）来绘制最佳拟合线，即设置 method = \"lm\"。\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\nWe have successfully added lines, but this plot doesn’t look like the plot from Section 1.2.2, which only has one line for the entire dataset as opposed to separate lines for each of the penguin species.\n我们成功地添加了线，但这张图看起来不像 Section 1.2.2 中的那张图，那张图只有一条贯穿整个数据集的线，而不是为每种企鹅都画一条单独的线。\nWhen aesthetic mappings are defined in ggplot(), at the global level, they’re passed down to each of the subsequent geom layers of the plot. However, each geom function in ggplot2 can also take a mapping argument, which allows for aesthetic mappings at the local level that are added to those inherited from the global level. Since we want points to be colored based on species but don’t want the lines to be separated out for them, we should specify color = species for geom_point() only.\n当美学映射在 ggplot() 中定义时，即在全局层面，它们会被传递给图中后续的每个几何对象图层。然而，ggplot2 中的每个几何函数也可以接受一个 mapping 参数，这允许在局部层面进行美学映射，这些映射会添加到从全局层面继承的映射之上。因为我们希望点根据物种着色，但不希望线也因此分开，所以我们应该只为 geom_point() 指定 color = species。\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species)) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\nVoila! We have something that looks very much like our ultimate goal, though it’s not yet perfect. We still need to use different shapes for each species of penguins and improve labels.\n瞧！我们得到了一个非常接近我们最终目标的图，尽管还不完美。我们还需要为每种企鹅使用不同的形状，并改进标签。\nIt’s generally not a good idea to represent information using only colors on a plot, as people perceive colors differently due to color blindness or other color vision differences. Therefore, in addition to color, we can also map species to the shape aesthetic.\n通常来说，在图上仅用颜色来表示信息不是一个好主意，因为由于色盲或其他色觉差异，人们对颜色的感知是不同的。因此，除了颜色，我们还可以将 species 映射到 shape 美学上。\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\nNote that the legend is automatically updated to reflect the different shapes of the points as well.\n请注意，图例也会自动更新，以反映点的不同形状。\nAnd finally, we can improve the labels of our plot using the labs() function in a new layer. Some of the arguments to labs() might be self explanatory: title adds a title and subtitle adds a subtitle to the plot. Other arguments match the aesthetic mappings, x is the x-axis label, y is the y-axis label, and color and shape define the label for the legend. In addition, we can improve the color palette to be colorblind safe with the scale_color_colorblind() function from the ggthemes package.\n最后，我们可以通过在一个新图层中使用 labs() 函数来改进我们图的标签。labs() 的一些参数可能是不言自明的：title 为图添加一个标题，subtitle 添加一个副标题。其他参数与美学映射相对应，x 是 x 轴的标签，y 是 y 轴的标签，而 color 和 shape 定义了图例的标签。此外，我们可以使用 ggthemes 包中的 scale_color_colorblind() 函数来改进调色板，使其对色盲友好。\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Body mass and flipper length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper length (mm)\", y = \"Body mass (g)\",\n    color = \"Species\", shape = \"Species\"\n  ) +\n  scale_color_colorblind()\n\n\n\n\n\n\n\nWe finally have a plot that perfectly matches our “ultimate goal”!\n我们终于得到了一个与我们的“最终目标”完全匹配的图！\n\n1.2.5 Exercises\n\nHow many rows are in penguins? How many columns?penguins 数据框中有多少行？多少列？\nWhat does the bill_depth_mm variable in the penguins data frame describe? Read the help for ?penguins to find out.penguins 数据框中的 bill_depth_mm 变量描述了什么？阅读 ?penguins 的帮助文档来找出答案。\nMake a scatterplot of bill_depth_mm vs. bill_length_mm. That is, make a scatterplot with bill_depth_mm on the y-axis and bill_length_mm on the x-axis. Describe the relationship between these two variables.\n制作一个 bill_depth_mm 对 bill_length_mm 的散点图。也就是说，制作一个以 bill_depth_mm 为 y 轴，bill_length_mm 为 x 轴的散点图。描述这两个变量之间的关系。\nWhat happens if you make a scatterplot of species vs. bill_depth_mm? What might be a better choice of geom?\n如果你制作一个 species 对 bill_depth_mm 的散点图会发生什么？什么可能是更好的几何对象选择？\n\nWhy does the following give an error and how would you fix it?\n为什么下面的代码会报错？你将如何修复它？\n\nggplot(data = penguins) +\n  geom_point()\n\n\nWhat does the na.rm argument do in geom_point()? What is the default value of the argument? Create a scatterplot where you successfully use this argument set to TRUE.\ngeom_point()中的na.rm参数有什么作用？该参数的默认值是什么？创建一个散点图，在其中成功地将此参数设置为TRUE`。\nAdd the following caption to the plot you made in the previous exercise: “Data come from the palmerpenguins package.” Hint: Take a look at the documentation for labs().\n在你上一个练习中制作的图上添加以下标题：“数据来自 palmerpenguins 包。” 提示：查看 labs() 的文档。\n\nRecreate the following visualization. What aesthetic should bill_depth_mm be mapped to? And should it be mapped at the global level or at the geom level?\n重新创建以下可视化图。bill_depth_mm 应该映射到哪个美学上？它应该在全局层面还是在几何对象层面进行映射？\n\nggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g)) +\n    geom_point(aes(color = bill_depth_mm)) +\n    geom_smooth()\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n#&gt; Warning: Removed 2 rows containing non-finite outside the scale range\n#&gt; (`stat_smooth()`).\n#&gt; Warning: Removed 2 rows containing missing values or values outside the scale range\n#&gt; (`geom_point()`).\n\n\n\n\n\n\n\n\n\nRun this code in your head and predict what the output will look like. Then, run the code in R and check your predictions.\n在脑海中运行这段代码，并预测输出会是什么样子。然后，在 R 中运行代码并检查你的预测。\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = island)\n) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\n\nWill these two graphs look different? Why/why not? 这两张图看起来会有所不同吗？为什么/为什么不？\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper\\_length\\_mm, y = body\\_mass\\_g)\n) +\n  geom\\_point() +\n  geom\\_smooth()\n\nggplot() +\n  geom\\_point(\n    data = penguins,\n    mapping = aes(x = flipper\\_length\\_mm, y = body\\_mass\\_g)\n  ) +\n  geom\\_smooth(\n    data = penguins,\n    mapping = aes(x = flipper\\_length\\_mm, y = body\\_mass\\_g)\n  )\n```\n:::",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data visualization</span>"
    ]
  },
  {
    "objectID": "data-visualize.html#sec-ggplot2-calls",
    "href": "data-visualize.html#sec-ggplot2-calls",
    "title": "1  Data visualization",
    "section": "\n1.3 ggplot2 calls",
    "text": "1.3 ggplot2 calls\nAs we move on from these introductory sections, we’ll transition to a more concise expression of ggplot2 code. So far we’ve been very explicit, which is helpful when you are learning:\n随着我们从这些介绍性章节继续前进，我们将转向一种更简洁的 ggplot2 代码表达方式。到目前为止，我们一直非常明确，这在学习时很有帮助：\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n\nTypically, the first one or two arguments to a function are so important that you should know them by heart. The first two arguments to ggplot() are data and mapping, in the remainder of the book, we won’t supply those names. That saves typing, and, by reducing the amount of extra text, makes it easier to see what’s different between plots. That’s a really important programming concern that we’ll come back to in Chapter 25.\n通常，函数的前一两个参数非常重要，你应该熟记于心。ggplot() 的前两个参数是 data 和 mapping，在本书的其余部分，我们将不再提供这些名称。这样可以节省打字时间，并且通过减少额外的文本量，更容易看出图与图之间的区别。这是一个非常重要的编程考量，我们将在 Chapter 25 中再次讨论。\nRewriting the previous plot more concisely yields:\n更简洁地重写之前的图会得到：\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()\n\nIn the future, you’ll also learn about the pipe, |&gt;, which will allow you to create that plot with:\n将来，你还会学到管道符 |&gt;，它将允许你用以下方式创建该图：\n\npenguins |&gt; \n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data visualization</span>"
    ]
  },
  {
    "objectID": "data-visualize.html#visualizing-distributions",
    "href": "data-visualize.html#visualizing-distributions",
    "title": "1  Data visualization",
    "section": "\n1.4 Visualizing distributions",
    "text": "1.4 Visualizing distributions\nHow you visualize the distribution of a variable depends on the type of variable: categorical or numerical.\n如何可视化一个变量的分布，取决于该变量的类型：是分类变量还是数值变量。\n\n1.4.1 A categorical variable\nA variable is categorical if it can only take one of a small set of values. To examine the distribution of a categorical variable, you can use a bar chart. The height of the bars displays how many observations occurred with each x value.\n如果一个变量只能取一小组值中的一个，那么它就是分类 (categorical) 变量。要检查分类变量的分布，你可以使用条形图。条形的高度显示了每个 x 值出现了多少次观测。\n\nggplot(penguins, aes(x = species)) +\n  geom_bar()\n\n\n\n\n\n\n\nIn bar plots of categorical variables with non-ordered levels, like the penguin species above, it’s often preferable to reorder the bars based on their frequencies. Doing so requires transforming the variable to a factor (how R handles categorical data) and then reordering the levels of that factor.\n在处理具有无序水平的分类变量的条形图时，比如上面的企鹅 species，通常最好根据它们的频率对条形进行重新排序。这样做需要将变量转换为因子（R 处理分类数据的方式），然后重新排列该因子的水平。\n\nggplot(penguins, aes(x = fct_infreq(species))) +\n  geom_bar()\n\n\n\n\n\n\n\nYou will learn more about factors and functions for dealing with factors (like fct_infreq() shown above) in Chapter 16.\n你将在 Chapter 16 中学习更多关于因子以及处理因子的函数（如上面展示的 fct_infreq()）的知识。\n\n1.4.2 A numerical variable\nA variable is numerical (or quantitative) if it can take on a wide range of numerical values, and it is sensible to add, subtract, or take averages with those values. Numerical variables can be continuous or discrete.\n如果一个变量可以取广泛的数值，并且对这些值进行加、减或求平均是有意义的，那么它就是数值 (numerical)（或定量）变量。数值变量可以是连续的或离散的。\nOne commonly used visualization for distributions of continuous variables is a histogram.\n一种常用于可视化连续变量分布的图是直方图。\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 200)\n\n\n\n\n\n\n\nA histogram divides the x-axis into equally spaced bins and then uses the height of a bar to display the number of observations that fall in each bin. In the graph above, the tallest bar shows that 39 observations have a body_mass_g value between 3,500 and 3,700 grams, which are the left and right edges of the bar.\n直方图将 x 轴划分为等宽的区间（bins），然后用条形的高度来显示落入每个区间的观测数量。在上图中，最高的条形显示有 39 个观测的 body_mass_g 值在 3,500 到 3,700 克之间，这分别是该条形的左右边界。\nYou can set the width of the intervals in a histogram with the binwidth argument, which is measured in the units of the x variable. You should always explore a variety of binwidths when working with histograms, as different binwidths can reveal different patterns. In the plots below a binwidth of 20 is too narrow, resulting in too many bars, making it difficult to determine the shape of the distribution. Similarly, a binwidth of 2,000 is too high, resulting in all data being binned into only three bars, and also making it difficult to determine the shape of the distribution. A binwidth of 200 provides a sensible balance.\n你可以使用 binwidth 参数来设置直方图中区间的宽度，该宽度以 x 变量的单位来衡量。在使用直方图时，你应该总是尝试不同的区间宽度，因为不同的宽度可能会揭示出不同的模式。在下面的图中，20 的区间宽度太窄，导致条形过多，难以确定分布的形状。同样，2000 的区间宽度太高，导致所有数据只被分到三个条形中，也使得确定分布的形状变得困难。200 的区间宽度提供了一个合理的平衡。\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 20)\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 2000)\n\n\n\n\n\n\n\n\n\n\nAn alternative visualization for distributions of numerical variables is a density plot. A density plot is a smoothed-out version of a histogram and a practical alternative, particularly for continuous data that comes from an underlying smooth distribution. We won’t go into how geom_density() estimates the density (you can read more about that in the function documentation), but let’s explain how the density curve is drawn with an analogy. Imagine a histogram made out of wooden blocks. Then, imagine that you drop a cooked spaghetti string over it. The shape the spaghetti will take draped over blocks can be thought of as the shape of the density curve. It shows fewer details than a histogram but can make it easier to quickly glean the shape of the distribution, particularly with respect to modes and skewness.\n数值变量分布的另一种可视化方法是密度图。密度图是直方图的平滑版本，是一种实用的替代方案，尤其适用于来自底层平滑分布的连续数据。我们不会深入探讨 geom_density() 如何估计密度（你可以在函数文档中了解更多信息），但让我们用一个类比来解释密度曲线是如何绘制的。想象一个由木块组成的直方图。然后，想象你把一根煮熟的意大利面条扔在它上面。意大利面条搭在木块上形成的形状可以被看作是密度曲线的形状。它比直方图显示的细节少，但可以更容易地快速了解分布的形状，特别是在众数和偏度方面。\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_density()\n#&gt; Warning: Removed 2 rows containing non-finite outside the scale range\n#&gt; (`stat_density()`).\n\n\n\n\n\n\n\n\n1.4.3 Exercises\n\nMake a bar plot of species of penguins, where you assign species to the y aesthetic. How is this plot different?\n制作一个 penguins 中 species 的条形图，其中你将 species 赋给 y 美学。这个图有什么不同？\n\nHow are the following two plots different? Which aesthetic, color or fill, is more useful for changing the color of bars?\n下面这两个图有什么不同？哪个美学，color 还是 fill，对于改变条形的颜色更有用？\n\nggplot(penguins, aes(x = species)) +\n  geom\\_bar(color = \"red\")\n\nggplot(penguins, aes(x = species)) +\n  geom\\_bar(fill = \"red\")\n\n\nWhat does the bins argument in geom_histogram() do?geom_histogram() 中的 bins 参数有什么作用？\nMake a histogram of the carat variable in the diamonds dataset that is available when you load the tidyverse package. Experiment with different binwidths. What binwidth reveals the most interesting patterns?\n制作一个 diamonds 数据集中 carat 变量的直方图，该数据集在加载 tidyverse 包时可用。尝试不同的区间宽度。哪个区间宽度揭示了最有趣的模式？",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data visualization</span>"
    ]
  },
  {
    "objectID": "data-visualize.html#visualizing-relationships",
    "href": "data-visualize.html#visualizing-relationships",
    "title": "1  Data visualization",
    "section": "\n1.5 Visualizing relationships",
    "text": "1.5 Visualizing relationships\nTo visualize a relationship we need to have at least two variables mapped to aesthetics of a plot. In the following sections you will learn about commonly used plots for visualizing relationships between two or more variables and the geoms used for creating them.\n要可视化一个关系，我们需要将至少两个变量映射到图的美学上。在接下来的部分，你将学习到常用于可视化两个或多个变量之间关系的图，以及用于创建它们的几何对象。\n\n1.5.1 A numerical and a categorical variable\nTo visualize the relationship between a numerical and a categorical variable we can use side-by-side box plots. A boxplot is a type of visual shorthand for measures of position (percentiles) that describe a distribution. It is also useful for identifying potential outliers. As shown in Figure 1.1, each boxplot consists of:\n为了可视化数值变量和分类变量之间的关系，我们可以使用并排的箱线图。箱线图 (boxplot) 是一种描述分布位置度量（百分位数）的视觉简写。它对于识别潜在的异常值也很有用。如 Figure 1.1 所示，每个箱线图都由以下几部分组成：\n- A box that indicates the range of the middle half of the data, a distance known as the interquartile range (IQR), stretching from the 25th percentile of the distribution to the 75th percentile. In the middle of the box is a line that displays the median, i.e. 50th percentile, of the distribution. These three lines give you a sense of the spread of the distribution and whether or not the distribution is symmetric about the median or skewed to one side.\n\n一个箱体，表示数据中间一半的范围，这个距离被称为四分位距 (IQR)，从分布的第 25 百分位数延伸到第 75 百分位数。箱体中间有一条线，显示分布的中位数，即第 50 百分位数。这三条线让你了解分布的离散程度以及分布是否关于中位数对称或偏向一侧。\n\n- Visual points that display observations that fall more than 1.5 times the IQR from either edge of the box. These outlying points are unusual so are plotted individually.\n\n显示离箱体任一边缘超过 1.5 倍 IQR 的观测值的视觉点。这些离群点是不寻常的，所以被单独绘制出来。\n\n- A line (or whisker) that extends from each end of the box and goes to the farthest non-outlier point in the distribution.\n\n从箱体两端延伸出的线（或须），一直延伸到分布中最远的非离群点。\n\n\n\n\n\n\n\n\nFigure 1.1: Diagram depicting how a boxplot is created. 描绘箱线图如何创建的图示。\n\n\n\n\nLet’s take a look at the distribution of body mass by species using geom_boxplot():\n让我们使用 geom_boxplot() 来查看按物种分的体重分布：\n\nggplot(penguins, aes(x = species, y = body_mass_g)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nAlternatively, we can make density plots with geom_density().\n或者，我们可以使用 geom_density() 制作密度图。\n\nggplot(penguins, aes(x = body_mass_g, color = species)) +\n  geom_density(linewidth = 0.75)\n\n\n\n\n\n\n\nWe’ve also customized the thickness of the lines using the linewidth argument in order to make them stand out a bit more against the background.\n我们还使用了 linewidth 参数自定义了线条的粗细，以便它们在背景中更加突出。\nAdditionally, we can map species to both color and fill aesthetics and use the alpha aesthetic to add transparency to the filled density curves. This aesthetic takes values between 0 (completely transparent) and 1 (completely opaque). In the following plot it’s set to 0.5.\n此外，我们可以将 species 同时映射到 color 和 fill 美学，并使用 alpha 美学为填充的密度曲线添加透明度。这个美学的值介于 0（完全透明）和 1（完全不透明）之间。在下面的图中，它被设置为 0.5。\n\nggplot(penguins, aes(x = body_mass_g, color = species, fill = species)) +\n  geom_density(alpha = 0.5)\n\n\n\n\n\n\n\nNote the terminology we have used here:\n注意我们在这里使用的术语：\n- We map variables to aesthetics if we want the visual attribute represented by that aesthetic to vary based on the values of that variable. - Otherwise, we set the value of an aesthetic.\n\n如果我们希望由某个美学代表的视觉属性根据某个变量的值而变化，我们就将变量映射到该美学。\n否则，我们就设置某个美学的值。\n\n1.5.2 Two categorical variables\nWe can use stacked bar plots to visualize the relationship between two categorical variables. For example, the following two stacked bar plots both display the relationship between island and species, or specifically, visualizing the distribution of species within each island.\n我们可以使用堆叠条形图来可视化两个分类变量之间的关系。例如，以下两个堆叠条形图都显示了 island 和 species 之间的关系，或者具体来说，可视化了每个岛屿内 species 的分布。\nThe first plot shows the frequencies of each species of penguins on each island. The plot of frequencies shows that there are equal numbers of Adelies on each island. But we don’t have a good sense of the percentage balance within each island.\n第一张图显示了每个岛屿上每种企鹅的频率。频率图显示，每个岛屿上的阿德利企鹅数量相等。但我们无法很好地了解每个岛屿内部的百分比平衡。\n\nggplot(penguins, aes(x = island, fill = species)) +\n  geom_bar()\n\n\n\n\n\n\n\nThe second plot, a relative frequency plot created by setting position = \"fill\" in the geom, is more useful for comparing species distributions across islands since it’s not affected by the unequal numbers of penguins across the islands. Using this plot we can see that Gentoo penguins all live on Biscoe island and make up roughly 75% of the penguins on that island, Chinstrap all live on Dream island and make up roughly 50% of the penguins on that island, and Adelie live on all three islands and make up all of the penguins on Torgersen.\n第二张图是一个相对频率图，通过在几何对象中设置 position = \"fill\" 创建，它在比较不同岛屿间的物种分布时更有用，因为它不受各岛屿企鹅数量不等的影响。通过这张图我们可以看到，金图企鹅都生活在比斯科岛，约占该岛企鹅总数的 75%；帽带企鹅都生活在梦幻岛，约占该岛企鹅总数的 50%；而阿德利企鹅生活在所有三个岛屿上，并且占了托尔森岛上所有企鹅。\n\nggplot(penguins, aes(x = island, fill = species)) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\nIn creating these bar charts, we map the variable that will be separated into bars to the x aesthetic, and the variable that will change the colors inside the bars to the fill aesthetic.\n在创建这些条形图时，我们将要被分成条形的变量映射到 x 美学，将要改变条形内部颜色的变量映射到 fill 美学。\n\n1.5.3 Two numerical variables\nSo far you’ve learned about scatterplots (created with geom_point()) and smooth curves (created with geom_smooth()) for visualizing the relationship between two numerical variables. A scatterplot is probably the most commonly used plot for visualizing the relationship between two numerical variables.\n到目前为止，你已经学习了用于可视化两个数值变量之间关系的散点图（用 geom_point() 创建）和平滑曲线（用 geom_smooth() 创建）。散点图可能是可视化两个数值变量之间关系最常用的图。\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()\n\n\n\n\n\n\n\n\n1.5.4 Three or more variables\nAs we saw in Section 1.2.4, we can incorporate more variables into a plot by mapping them to additional aesthetics. For example, in the following scatterplot the colors of points represent species and the shapes of points represent islands.\n正如我们在 Section 1.2.4 中看到的，我们可以通过将更多变量映射到其他美学上，将它们融入到图中。例如，在下面的散点图中，点的颜色代表物种，点的形状代表岛屿。\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species, shape = island))\n\n\n\n\n\n\n\nHowever adding too many aesthetic mappings to a plot makes it cluttered and difficult to make sense of. Another way, which is particularly useful for categorical variables, is to split your plot into facets, subplots that each display one subset of the data.\n然而，向图中添加太多的美学映射会使其变得杂乱无章，难以理解。另一种方法，特别是对分类变量很有用，就是将你的图分割成分面 (facets)，即每个子图显示数据的一个子集。\nTo facet your plot by a single variable, use facet_wrap(). The first argument of facet_wrap() is a formula3, which you create with ~ followed by a variable name. The variable that you pass to facet_wrap() should be categorical.\n要按单个变量对图进行分面，请使用 facet_wrap()。facet_wrap() 的第一个参数是一个公式3，你通过 ~ 后跟一个变量名来创建它。传递给 facet_wrap() 的变量应该是分类变量。\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species, shape = species)) +\n  facet_wrap(~island)\n\n\n\n\n\n\n\nYou will learn about many other geoms for visualizing distributions of variables and relationships between them in Chapter 9.\n你将在 Chapter 9 中学习到许多其他用于可视化变量分布和它们之间关系的几何对象。\n\n1.5.5 Exercises\n\nThe mpg data frame that is bundled with the ggplot2 package contains 234 observations collected by the US Environmental Protection Agency on 38 car models. Which variables in mpg are categorical? Which variables are numerical? (Hint: Type ?mpg to read the documentation for the dataset.) How can you see this information when you run mpg?\nggplot2 软件包中包含的 mpg 数据框 (data frame) 含有 234 条观测数据，这些数据由美国环境保护署收集，涵盖了 38 种车型。 mpg 中的哪些变量是分类 (categorical) 变量？ 哪些变量是数值 (numerical) 变量？ (提示：输入 ?mpg 来阅读该数据集的文档。) 当你运行 mpg 时，如何看到这些信息？\nMake a scatterplot of hwy vs. displ using the mpg data frame. Next, map a third, numerical variable to color, then size, then both color and size, then shape. How do these aesthetics behave differently for categorical vs. numerical variables?\n使用 mpg 数据框 (data frame) 创建一个 hwy 与 displ 的散点图 (scatterplot)。 接下来，将第三个数值 (numerical) 变量映射 (map) 到 color，然后是 size，然后是 color 和 size，最后是 shape。 对于分类 (categorical) 变量和数值 (numerical) 变量，这些图形属性 (aesthetics) 的行为有何不同？\nIn the scatterplot of hwy vs. displ, what happens if you map a third variable to linewidth?\n在 hwy 与 displ 的散点图 (scatterplot) 中，如果将第三个变量映射 (map) 到 linewidth 会发生什么？\nWhat happens if you map the same variable to multiple aesthetics?\n如果将同一个变量映射 (map) 到多个图形属性 (aesthetics) 会发生什么？\nMake a scatterplot of bill_depth_mm vs. bill_length_mm and color the points by species. What does adding coloring by species reveal about the relationship between these two variables? What about faceting by species?\n创建一个 bill_depth_mm 与 bill_length_mm 的散点图 (scatterplot)，并按 species 为点着色。 按物种 (species) 着色揭示了这两个变量之间关系的哪些信息？ 按 species 分面 (faceting) 呢？\n\nWhy does the following yield two separate legends? How would you fix it to combine the two legends?\n为什么以下代码会产生两个独立的图例 (legends)？ 你将如何修改它以合并这两个图例 (legends)？\n\nggplot(\n  data = penguins,\n  mapping = aes(\n    x = bill_length_mm, y = bill_depth_mm, \n    color = species, shape = species\n  )\n) +\n  geom_point() +\n  labs(color = \"Species\")\n\n\n\nCreate the two following stacked bar plots. Which question can you answer with the first one? Which question can you answer with the second one?\n创建以下两个堆叠条形图 (stacked bar plots)。 你可以用第一个图回答什么问题？ 你可以用第二个图回答什么问题？\n\nggplot(penguins, aes(x = island, fill = species)) +\n  geom_bar(position = \"fill\")\nggplot(penguins, aes(x = species, fill = island)) +\n  geom_bar(position = \"fill\")",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data visualization</span>"
    ]
  },
  {
    "objectID": "data-visualize.html#sec-ggsave",
    "href": "data-visualize.html#sec-ggsave",
    "title": "1  Data visualization",
    "section": "\n1.6 Saving your plots",
    "text": "1.6 Saving your plots\nOnce you’ve made a plot, you might want to get it out of R by saving it as an image that you can use elsewhere. That’s the job of ggsave(), which will save the plot most recently created to disk:\n一旦你制作了图，你可能希望将其从 R 中导出，保存为可在其他地方使用的图像。这是 ggsave() 的工作，它会将最近创建的图保存到磁盘：\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()\nggsave(filename = \"penguin-plot.png\")\n\nThis will save your plot to your working directory, a concept you’ll learn more about in Chapter 6.\n这会将你的图保存到你的工作目录，关于这个概念你将在 Chapter 6 中学到更多。\nIf you don’t specify the width and height they will be taken from the dimensions of the current plotting device. For reproducible code, you’ll want to specify them. You can learn more about ggsave() in the documentation.\n如果你不指定 width 和 height，它们将取自当前绘图设备的尺寸。为了代码的可复现性，你会希望指定它们。你可以在文档中了解更多关于 ggsave() 的信息。\nGenerally, however, we recommend that you assemble your final reports using Quarto, a reproducible authoring system that allows you to interleave your code and your prose and automatically include your plots in your write-ups. You will learn more about Quarto in Chapter 28.\n然而，通常我们建议你使用 Quarto 来组织你的最终报告，这是一个可复现的创作系统，它允许你将代码和文字交织在一起，并自动将你的图包含在你的报告中。你将在 Chapter 28 中学习更多关于 Quarto 的信息。\n\n1.6.1 Exercises\n\n\nRun the following lines of code. Which of the two plots is saved as mpg-plot.png? Why?\n运行以下代码行。哪张图被保存为 mpg-plot.png？为什么？\n\nggplot(mpg, aes(x = class)) +\n  geom_bar()\nggplot(mpg, aes(x = cty, y = hwy)) +\n  geom_point()\nggsave(\"mpg-plot.png\")\n\n\nWhat do you need to change in the code above to save the plot as a PDF instead of a PNG? How could you find out what types of image files would work in ggsave()?\n你需要如何更改上面的代码，才能将图保存为 PDF 而不是 PNG？你如何找出 ggsave() 支持哪些类型的图像文件？",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data visualization</span>"
    ]
  },
  {
    "objectID": "data-visualize.html#common-problems",
    "href": "data-visualize.html#common-problems",
    "title": "1  Data visualization",
    "section": "\n1.7 Common problems",
    "text": "1.7 Common problems\nAs you start to run R code, you’re likely to run into problems. Don’t worry — it happens to everyone. We have all been writing R code for years, but every day we still write code that doesn’t work on the first try!\n当你开始运行 R 代码时，你很可能会遇到问题。别担心——这发生在每个人身上。我们都写了多年的 R 代码，但每天我们仍然会写出第一次尝试就不能工作的代码！\nStart by carefully comparing the code that you’re running to the code in the book. R is extremely picky, and a misplaced character can make all the difference. Make sure that every ( is matched with a ) and every \" is paired with another \". Sometimes you’ll run the code and nothing happens. Check the left-hand of your console: if it’s a +, it means that R doesn’t think you’ve typed a complete expression and it’s waiting for you to finish it. In this case, it’s usually easy to start from scratch again by pressing ESCAPE to abort processing the current command.\n首先，仔细比较你正在运行的代码和书中的代码。R 非常挑剔，一个放错位置的字符都可能导致天壤之别。确保每个 ( 都与一个 ) 匹配，每个 \" 都与另一个 \" 配对。有时你运行代码后什么也没发生。检查你的控制台左侧：如果它是一个 +，这意味着 R 认为你还没有输入一个完整的表达式，正在等待你完成它。在这种情况下，通常很容易通过按 ESCAPE 键中止当前命令的处理，然后从头开始。\nOne common problem when creating ggplot2 graphics is to put the + in the wrong place: it has to come at the end of the line, not the start. In other words, make sure you haven’t accidentally written code like this:\n在创建 ggplot2 图形时，一个常见的问题是把 + 放在了错误的位置：它必须放在行的末尾，而不是开头。换句话说，确保你没有意外地写出像下面这样的代码：\n\nggplot(data = mpg) \n+ geom_point(mapping = aes(x = displ, y = hwy))\n\nIf you’re still stuck, try the help. You can get help about any R function by running ?function_name in the console, or highlighting the function name and pressing F1 in RStudio. Don’t worry if the help doesn’t seem that helpful - instead skip down to the examples and look for code that matches what you’re trying to do.\n如果你仍然卡住了，试试帮助。你可以通过在控制台运行 ?function_name 来获取任何 R 函数的帮助，或者在 RStudio 中高亮函数名并按 F1。如果帮助看起来不那么有用，别担心——直接跳到示例部分，寻找与你正在尝试做的事情相匹配的代码。\nIf that doesn’t help, carefully read the error message. Sometimes the answer will be buried there! But when you’re new to R, even if the answer is in the error message, you might not yet know how to understand it. Another great tool is Google: try googling the error message, as it’s likely someone else has had the same problem, and has gotten help online.\n如果那也帮不了你，仔细阅读错误信息。有时答案就藏在那里！但是当你刚接触 R 时，即使答案就在错误信息中，你可能还不知道如何理解它。另一个很棒的工具是谷歌：尝试用谷歌搜索错误信息，因为很可能其他人也遇到过同样的问题，并且在网上得到了帮助。",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data visualization</span>"
    ]
  },
  {
    "objectID": "data-visualize.html#summary",
    "href": "data-visualize.html#summary",
    "title": "1  Data visualization",
    "section": "\n1.8 Summary",
    "text": "1.8 Summary\nIn this chapter, you’ve learned the basics of data visualization with ggplot2. We started with the basic idea that underpins ggplot2: a visualization is a mapping from variables in your data to aesthetic properties like position, color, size and shape. You then learned about increasing the complexity and improving the presentation of your plots layer-by-layer. You also learned about commonly used plots for visualizing the distribution of a single variable as well as for visualizing relationships between two or more variables, by leveraging additional aesthetic mappings and/or splitting your plot into small multiples using faceting.\n在本章中，你学习了使用 ggplot2 进行数据可视化的基础知识。我们从支撑 ggplot2 的基本思想开始：可视化是将数据中的变量映射到诸如位置、颜色、大小和形状等美学属性的过程。然后你学习了如何逐层增加图的复杂性并改善其呈现效果。你还学习了常用于可视化单个变量分布以及可视化两个或多个变量之间关系的图，这是通过利用额外的美学映射和/或使用分面将图分割成小倍数图来实现的。\nWe’ll use visualizations again and again throughout this book, introducing new techniques as we need them as well as do a deeper dive into creating visualizations with ggplot2 in Chapter 9 through Chapter 11.\n在本书中，我们将反复使用可视化，在需要时引入新技术，并在 Chapter 9 到 Chapter 11 中更深入地探讨使用 ggplot2 创建可视化。\nWith the basics of visualization under your belt, in the next chapter we’re going to switch gears a little and give you some practical workflow advice. We intersperse workflow advice with data science tools throughout this part of the book because it’ll help you stay organized as you write increasing amounts of R code.\n掌握了可视化的基础知识后，在下一章中，我们将稍微转换一下思路，给你一些实用的工作流程建议。在本书的这一部分，我们将工作流程建议与数据科学工具穿插在一起，因为这将在你编写越来越多的 R 代码时帮助你保持条理清晰。",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data visualization</span>"
    ]
  },
  {
    "objectID": "data-visualize.html#footnotes",
    "href": "data-visualize.html#footnotes",
    "title": "1  Data visualization",
    "section": "",
    "text": "You can eliminate that message and force conflict resolution to happen on demand by using the conflicted package, which becomes more important as you load more packages. You can learn more about conflicted at https://conflicted.r-lib.org.↩︎\nHorst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0. https://allisonhorst.github.io/palmerpenguins/. doi: 10.5281/zenodo.3960218.↩︎\nHere “formula” is the name of the thing created by ~, not a synonym for “equation”.↩︎",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data visualization</span>"
    ]
  },
  {
    "objectID": "workflow-basics.html",
    "href": "workflow-basics.html",
    "title": "2  Workflow: basics",
    "section": "",
    "text": "2.1 Coding basics\nYou now have some experience running R code. We didn’t give you many details, but you’ve obviously figured out the basics, or you would’ve thrown this book away in frustration! Frustration is natural when you start programming in R because it is such a stickler for punctuation, and even one character out of place can cause it to complain. But while you should expect to be a little frustrated, take comfort in that this experience is typical and temporary: it happens to everyone, and the only way to get over it is to keep trying.\n你现在已经有了一些运行 R 代码的经验。我们没有提供太多细节，但你显然已经掌握了基础知识，否则你可能早就沮丧地扔掉这本书了！当你开始用 R 编程时，感到沮丧是正常的，因为它对标点符号要求非常严格，哪怕只有一个字符放错位置 (misplaced) 也会导致它报错。但是，尽管你应该预料到会有些沮丧，但请放心，这种经历是典型且暂时的：每个人都会遇到，而克服它的唯一方法就是不断尝试。\nBefore we go any further, let’s ensure you’ve got a solid foundation in running R code and that you know some of the most helpful RStudio features.\n在我们继续深入之前，让我们确保你在运行 R 代码方面有坚实的基础，并且了解一些 RStudio 最有用的功能。\nLet’s review some basics we’ve omitted so far in the interest of getting you plotting as quickly as possible. You can use R to do basic math calculations:\n让我们回顾一些为了让你尽快开始绘图而省略的基础知识。你可以使用 R 进行基本的数学计算：\n1 / 200 * 30\n#&gt; [1] 0.15\n(59 + 73 + 2) / 3\n#&gt; [1] 44.66667\nsin(pi / 2)\n#&gt; [1] 1\nYou can create new objects with the assignment operator &lt;-:\n你可以使用赋值运算符 &lt;- 创建新对象：\nx &lt;- 3 * 4\nNote that the value of x is not printed, it’s just stored. If you want to view the value, type x in the console.\n注意，x 的值并不会被打印出来，它只是被储存起来了。如果你想查看它的值，可以在控制台 (console) 中输入 x。\nYou can combine multiple elements into a vector with c():\n你可以使用 c() 函数将多个元素 combine (组合) 成一个向量 (vector)：\nprimes &lt;- c(2, 3, 5, 7, 11, 13)\nAnd basic arithmetic on vectors is applied to every element of the vector:\n对向量进行的基本算术运算会应用于向量的每个元素：\nprimes * 2\n#&gt; [1]  4  6 10 14 22 26\nprimes - 1\n#&gt; [1]  1  2  4  6 10 12\nAll R statements where you create objects, assignment statements, have the same form:\n所有创建对象的 R 语句，即赋值 (assignment) 语句，都具有相同的形式：\nobject_name &lt;- value\nWhen reading that code, say “object name gets value” in your head.\n在阅读这段代码时，你可以在脑海中默念“对象名得到值”。\nYou will make lots of assignments, and &lt;- is a pain to type. You can save time with RStudio’s keyboard shortcut: Alt + - (the minus sign). Notice that RStudio automatically surrounds &lt;- with spaces, which is a good code formatting practice. Code can be miserable to read on a good day, so giveyoureyesabreak and use spaces.\n你会进行大量的赋值操作，而手动输入 &lt;- 会很麻烦。你可以使用 RStudio 的键盘快捷键来节省时间：Alt + - (减号)。请注意，RStudio 会自动在 &lt;- 两侧添加空格，这是一个很好的代码格式化习惯。即使在状态好的时候，阅读代码也可能是一件痛苦的事情，所以请善待你的眼睛，多使用空格。",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Workflow: basics</span>"
    ]
  },
  {
    "objectID": "workflow-basics.html#comments",
    "href": "workflow-basics.html#comments",
    "title": "2  Workflow: basics",
    "section": "\n2.2 Comments",
    "text": "2.2 Comments\nR will ignore any text after # for that line. This allows you to write comments, text that is ignored by R but read by other humans. We’ll sometimes include comments in examples explaining what’s happening with the code.\nR 会忽略该行 # 之后的所有文本。这允许你编写注释 (comments)，即被 R 忽略但供人类阅读的文本。我们有时会在示例中加入注释来解释代码的功能。\nComments can be helpful for briefly describing what the following code does.\n注释有助于简要描述后续代码的功能。\n\n# create vector of primes\nprimes &lt;- c(2, 3, 5, 7, 11, 13)\n\n# multiply primes by 2\nprimes * 2\n#&gt; [1]  4  6 10 14 22 26\n\nWith short pieces of code like this, leaving a comment for every single line of code might not be necessary. But as the code you’re writing gets more complex, comments can save you (and your collaborators) a lot of time figuring out what was done in the code.\n对于像这样简短的代码片段，可能没有必要为每一行代码都写注释。但是，当你编写的代码变得越来越复杂时，注释可以为你（和你的合作者）节省大量时间来理解代码的功能。\nUse comments to explain the why of your code, not the how or the what. The what and how of your code are always possible to figure out, even if it might be tedious, by carefully reading it. If you describe every step in the comments, and then change the code, you will have to remember to update the comments as well or it will be confusing when you return to your code in the future.\n使用注释来解释你代码的原因 (why)，而不是方式 (how) 或内容 (what)。通过仔细阅读代码，总是可以搞清楚代码的内容和方式，尽管这可能很乏味。如果你在注释中描述了每一步，然后在修改代码后，你必须记得同时更新注释，否则将来你再回头看代码时会感到困惑。\nFiguring out why something was done is much more difficult, if not impossible. For example, geom_smooth() has an argument called span, which controls the smoothness of the curve, with larger values yielding a smoother curve. Suppose you decide to change the value of span from its default of 0.75 to 0.9: it’s easy for a future reader to understand what is happening, but unless you note your thinking in a comment, no one will understand why you changed the default.\n搞清楚做某件事的原因要困难得多，甚至是不可能的。例如，geom_smooth() 有一个名为 span 的参数，它控制曲线的平滑度，值越大曲线越平滑。假设你决定将 span 的值从默认的 0.75 更改为 0.9：未来的读者很容易理解发生了什么，但除非你在注释中记录下你的想法，否则没人会明白你为什么要更改默认值。\nFor data analysis code, use comments to explain your overall plan of attack and record important insights as you encounter them. There’s no way to re-capture this knowledge from the code itself.\n对于数据分析代码，使用注释来解释你的整体分析计划，并记录你遇到的重要见解。这些信息是无法单从代码本身重新获取的。",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Workflow: basics</span>"
    ]
  },
  {
    "objectID": "workflow-basics.html#sec-whats-in-a-name",
    "href": "workflow-basics.html#sec-whats-in-a-name",
    "title": "2  Workflow: basics",
    "section": "\n2.3 What’s in a name?",
    "text": "2.3 What’s in a name?\nObject names must start with a letter and can only contain letters, numbers, _, and .. You want your object names to be descriptive, so you’ll need to adopt a convention for multiple words. We recommend snake_case, where you separate lowercase words with _.\n对象名称必须以字母开头，并且只能包含字母、数字、_ 和 .。你希望你的对象名称具有描述性，所以你需要为多个单词的命名采纳一种约定。我们推荐使用蛇形命名法 (snake_case)，即用 _ 分隔小写单词。\n\ni_use_snake_case\notherPeopleUseCamelCase\nsome.people.use.periods\nAnd_aFew.People_RENOUNCEconvention\n\nWe’ll return to names again when we discuss code style in Chapter 4.\n我们将在 Chapter 4 中讨论代码风格时再次回到命名的话题。\nYou can inspect an object by typing its name:\n你可以通过输入对象名称来查看它：\n\nx\n#&gt; [1] 12\n\nMake another assignment:\n再进行一次赋值：\n\nthis_is_a_really_long_name &lt;- 2.5\n\nTo inspect this object, try out RStudio’s completion facility: type “this”, press TAB, add characters until you have a unique prefix, then press return.\n要查看这个对象，可以试试 RStudio 的自动补全功能：输入 “this”，按 TAB 键，继续添加字符直到前缀唯一，然后按回车键。\nLet’s assume you made a mistake, and that the value of this_is_a_really_long_name should be 3.5, not 2.5. You can use another keyboard shortcut to help you fix it. For example, you can press ↑ to bring the last command you typed and edit it. Or, type “this” then press Cmd/Ctrl + ↑ to list all the commands you’ve typed that start with those letters. Use the arrow keys to navigate, then press enter to retype the command. Change 2.5 to 3.5 and rerun.\n假设你犯了一个错误，this_is_a_really_long_name 的值应该是 3.5，而不是 2.5。你可以使用另一个键盘快捷键来帮助你修正它。例如，你可以按 ↑ 键调出你输入的上一条命令并进行编辑。或者，输入 “this”，然后按 Cmd/Ctrl + ↑ 来列出所有以这些字母开头的命令。使用箭头键导航，然后按回车键重新输入该命令。将 2.5 改为 3.5 并重新运行。\nMake yet another assignment:\n再进行一次赋值：\n\nr_rocks &lt;- 2^3\n\nLet’s try to inspect it:\n让我们试着查看它：\n\nr_rock\n#&gt; Error: object 'r_rock' not found\nR_rocks\n#&gt; Error: object 'R_rocks' not found\n\nThis illustrates the implied contract between you and R: R will do the tedious computations for you, but in exchange, you must be completely precise in your instructions. If not, you’re likely to get an error that says the object you’re looking for was not found. Typos matter; R can’t read your mind and say, “oh, they probably meant r_rocks when they typed r_rock”. Case matters; similarly, R can’t read your mind and say, “oh, they probably meant r_rocks when they typed R_rocks”.\n这说明了你和 R 之间的一个隐含契约：R 会为你完成繁琐的计算，但作为交换，你必须给出完全精确的指令。否则，你很可能会得到一个错误，提示找不到你想要的对象。拼写错误很重要；R 无法读懂你的心思，然后说：“哦，他们输入 r_rock 时可能指的是 r_rocks”。大小写也很重要；同样，R 也无法读懂你的心思，然后说：“哦，他们输入 R_rocks 时可能指的是 r_rocks”。",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Workflow: basics</span>"
    ]
  },
  {
    "objectID": "workflow-basics.html#calling-functions",
    "href": "workflow-basics.html#calling-functions",
    "title": "2  Workflow: basics",
    "section": "\n2.4 Calling functions",
    "text": "2.4 Calling functions\nR has a large collection of built-in functions that are called like this:\nR 有大量内置函数，调用方式如下：\n\nfunction_name(argument1 = value1, argument2 = value2, ...)\n\nLet’s try using seq(), which makes regular sequences of numbers, and while we’re at it, learn more helpful features of RStudio. Type se and hit TAB. A popup shows you possible completions. Specify seq() by typing more (a q) to disambiguate or by using ↑/↓ arrows to select. Notice the floating tooltip that pops up, reminding you of the function’s arguments and purpose. If you want more help, press F1 to get all the details in the help tab in the lower right pane.\n让我们试试 seq() 函数，它可以生成规则的数字序列 (sequences)，同时我们也可以学习更多 RStudio 的实用功能。输入 se 然后按 TAB 键。一个弹出窗口会显示可能的补全选项。通过输入更多字符（一个 q）来消除歧义，或使用 ↑/↓ 箭头来选择 seq()。注意弹出的浮动提示框，它会提醒你该函数的参数和用途。如果你需要更多帮助，可以按 F1 键，在右下角的帮助 (help) 标签页中获取所有详细信息。\nWhen you’ve selected the function you want, press TAB again. RStudio will add matching opening (() and closing ()) parentheses for you. Type the name of the first argument, from, and set it equal to 1. Then, type the name of the second argument, to, and set it equal to 10. Finally, hit return.\n当你选定想要的函数后，再次按 TAB 键。RStudio 会为你添加匹配的开括号 ( 和闭括号 )。输入第一个参数的名称 from，并将其设置为 1。然后，输入第二个参数的名称 to，并将其设置为 10。最后，按回车键。\n\nseq(from = 1, to = 10)\n#&gt;  [1]  1  2  3  4  5  6  7  8  9 10\n\nWe often omit the names of the first several arguments in function calls, so we can rewrite this as follows:\n在函数调用中，我们经常省略前几个参数的名称，所以我们可以像下面这样重写：\n\nseq(1, 10)\n#&gt;  [1]  1  2  3  4  5  6  7  8  9 10\n\nType the following code and notice that RStudio provides similar assistance with the paired quotation marks:\n输入以下代码，你会发现 RStudio 对成对的引号也提供了类似的辅助功能：\n\nx &lt;- \"hello world\"\n\nQuotation marks and parentheses must always come in a pair. RStudio does its best to help you, but it’s still possible to mess up and end up with a mismatch. If this happens, R will show you the continuation character “+”:\n引号和括号必须总是成对出现。RStudio 会尽力帮助你，但仍然有可能出错，导致括号或引号不匹配。如果发生这种情况，R 会显示一个续行符 +：\n&gt; x &lt;- \"hello\n+\nThe + tells you that R is waiting for more input; it doesn’t think you’re done yet. Usually, this means you’ve forgotten either a \" or a ). Either add the missing pair, or press ESCAPE to abort the expression and try again.\n这个 + 告诉你 R 正在等待更多输入；它认为你还没有完成输入。通常，这意味着你忘记了输入一个 \" 或 )。你可以补上缺失的符号，或者按 ESCAPE 键中止表达式并重试。\nNote that the environment tab in the upper right pane displays all of the objects that you’ve created:\n注意，右上角窗格中的环境 (environment) 标签页会显示你创建的所有对象：",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Workflow: basics</span>"
    ]
  },
  {
    "objectID": "workflow-basics.html#exercises",
    "href": "workflow-basics.html#exercises",
    "title": "2  Workflow: basics",
    "section": "\n2.5 Exercises",
    "text": "2.5 Exercises",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Workflow: basics</span>"
    ]
  },
  {
    "objectID": "workflow-basics.html#exercises-1",
    "href": "workflow-basics.html#exercises-1",
    "title": "2  Workflow: basics",
    "section": "\n2.6 Exercises",
    "text": "2.6 Exercises\n\n\nWhy does this code not work?\n\nmy_variable &lt;- 10\nmy_varıable\n#&gt; Error: object 'my_varıable' not found\n\nLook carefully! (This may seem like an exercise in pointlessness, but training your brain to notice even the tiniest difference will pay off when programming.)\n\n\nTweak each of the following R commands so that they run correctly:\n\nlibary(todyverse)\n\nggplot(dTA = mpg) + \n  geom_point(maping = aes(x = displ y = hwy)) +\n  geom_smooth(method = \"lm)\n\n\nPress Option + Shift + K / Alt + Shift + K. What happens? How can you get to the same place using the menus?\n\nLet’s revisit an exercise from the Section 1.6. Run the following lines of code. Which of the two plots is saved as mpg-plot.png? Why?\n\nmy_bar_plot &lt;- ggplot(mpg, aes(x = class)) +\n  geom_bar()\nmy_scatter_plot &lt;- ggplot(mpg, aes(x = cty, y = hwy)) +\n  geom_point()\nggsave(filename = \"mpg-plot.png\", plot = my_bar_plot)",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Workflow: basics</span>"
    ]
  },
  {
    "objectID": "workflow-basics.html#summary",
    "href": "workflow-basics.html#summary",
    "title": "2  Workflow: basics",
    "section": "\n2.7 Summary",
    "text": "2.7 Summary\nNow that you’ve learned a little more about how R code works, and some tips to help you understand your code when you come back to it in the future. In the next chapter, we’ll continue your data science journey by teaching you about dplyr, the tidyverse package that helps you transform data, whether it’s selecting important variables, filtering down to rows of interest, or computing summary statistics.\n现在你已经对 R 代码的工作原理有了更多了解，也掌握了一些技巧，可以帮助你在未来回顾代码时更好地理解它。在下一章中，我们将继续你的数据科学之旅，教你关于 dplyr 的知识，它是 tidyverse 中的一个包，可以帮助你转换数据，无论是选择重要变量、筛选感兴趣的行，还是计算汇总统计数据。",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Workflow: basics</span>"
    ]
  },
  {
    "objectID": "data-transform.html",
    "href": "data-transform.html",
    "title": "3  Data transformation",
    "section": "",
    "text": "3.1 Introduction\nVisualization is an important tool for generating insight, but it’s rare that you get the data in exactly the right form you need to make the graph you want.\n可视化是产生洞察力的重要工具，但你很少能以你需要的确切形式获得数据来制作你想要的图表。\nOften you’ll need to create some new variables or summaries to answer your questions with your data, or maybe you just want to rename the variables or reorder the observations to make the data a little easier to work with.\n通常，你需要创建一些新的变量或摘要来用你的数据回答你的问题，或者你可能只是想重命名变量或重新排序观测值，以使数据更容易处理。\nYou’ll learn how to do all that (and more!) in this chapter, which will introduce you to data transformation using the dplyr package and a new dataset on flights that departed from New York City in 2013.\n在本章中，你将学习如何做到所有这些 (以及更多！)，本章将向你介绍如何使用 dplyr 包和一个关于 2013 年从纽约市起飞的航班的新数据集来进行数据转换。\nThe goal of this chapter is to give you an overview of all the key tools for transforming a data frame.\n本章的目标是让你对转换数据框 (data frame) 的所有关键工具有一个概览。\nWe’ll start with functions that operate on rows and then columns of a data frame, then circle back to talk more about the pipe, an important tool that you use to combine verbs.\n我们将从操作数据框的行和列的函数开始，然后回过头来更多地讨论管道 (pipe)，这是一个用来组合动词 (verb) 的重要工具。\nWe will then introduce the ability to work with groups.\n然后我们将介绍处理分组的能力。\nWe will end the chapter with a case study that showcases these functions in action.\n我们将以一个案例研究来结束本章，展示这些函数的实际应用。\nIn later chapters, we’ll return to the functions in more detail as we start to dig into specific types of data (e.g., numbers, strings, dates).\n在后面的章节中，我们将更详细地回顾这些函数，因为我们将开始深入研究特定类型的数据 (例如，数字、字符串、日期)。",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data transformation</span>"
    ]
  },
  {
    "objectID": "data-transform.html#introduction",
    "href": "data-transform.html#introduction",
    "title": "3  Data transformation",
    "section": "",
    "text": "3.1.1 Prerequisites\nIn this chapter, we’ll focus on the dplyr package, another core member of the tidyverse.\n在本章中，我们将重点介绍 dplyr 包，它是 tidyverse 的另一个核心成员。\nWe’ll illustrate the key ideas using data from the nycflights13 package and use ggplot2 to help us understand the data.\n我们将使用 nycflights13 包中的数据来说明关键思想，并使用 ggplot2 来帮助我们理解数据。\n\nlibrary(nycflights13)\nlibrary(tidyverse)\n#&gt; ── Attaching core tidyverse packages ───────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n#&gt; ✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n#&gt; ✔ purrr     1.0.4     \n#&gt; ── Conflicts ─────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nTake careful note of the conflicts message that’s printed when you load the tidyverse.\n请仔细注意加载 tidyverse 时打印的冲突消息。\nIt tells you that dplyr overwrites some functions in base R.\n它告诉你 dplyr 覆盖了基础 R 中的一些函数。\nIf you want to use the base version of these functions after loading dplyr, you’ll need to use their full names: stats::filter() and stats::lag().\n如果在加载 dplyr 后想使用这些函数的基础版本，你需要使用它们的全名：stats::filter() 和 stats::lag()。\nSo far, we’ve mostly ignored which package a function comes from because it doesn’t usually matter.\n到目前为止，我们大多忽略了函数来自哪个包，因为它通常不重要。\nHowever, knowing the package can help you find help and find related functions, so when we need to be precise about which package a function comes from, we’ll use the same syntax as R: packagename::functionname().\n然而，知道包可以帮助你找到帮助和相关的函数，所以当我们需​​要精确地指出函数来自哪个包时，我们将使用与 R 相同的语法：packagename::functionname()。\n\n3.1.2 nycflights13\nTo explore the basic dplyr verbs, we will use nycflights13::flights.\n为了探索基本的 dplyr 动词，我们将使用 nycflights13::flights。\nThis dataset contains all 336,776 flights that departed from New York City in 2013.\n这个数据集包含了 2013 年从纽约市出发的所有 336,776 个航班。\nThe data comes from the US Bureau of Transportation Statistics and is documented in ?flights.\n数据来自美国交通统计局，并在 ?flights 中有文档记录。\n\nflights\n#&gt; # A tibble: 336,776 × 19\n#&gt;    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n#&gt; 1  2013     1     1      517            515         2      830            819\n#&gt; 2  2013     1     1      533            529         4      850            830\n#&gt; 3  2013     1     1      542            540         2      923            850\n#&gt; 4  2013     1     1      544            545        -1     1004           1022\n#&gt; 5  2013     1     1      554            600        -6      812            837\n#&gt; 6  2013     1     1      554            558        -4      740            728\n#&gt; # ℹ 336,770 more rows\n#&gt; # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, …\n\nflights is a tibble, a special type of data frame used by the tidyverse to avoid some common gotchas.flights 是一个 tibble，这是 tidyverse 使用的一种特殊类型的数据框，以避免一些常见的陷阱。\nThe most important difference between tibbles and data frames is the way tibbles print; they are designed for large datasets, so they only show the first few rows and only the columns that fit on one screen.\ntibble 和数据框之间最重要的区别在于 tibble 的打印方式；它们专为大型数据集设计，因此只显示前几行和能在一屏内显示的列。\nThere are a few options to see everything.\n有几个选项可以查看所有内容。\nIf you’re using RStudio, the most convenient is probably View(flights), which opens an interactive, scrollable, and filterable view.\n如果你正在使用 RStudio，最方便的可能是 View(flights)，它会打开一个交互式的、可滚动的和可筛选的视图。\nOtherwise you can use print(flights, width = Inf) to show all columns, or use glimpse():\n否则，你可以使用 print(flights, width = Inf) 来显示所有列，或者使用 glimpse()：\n\nglimpse(flights)\n#&gt; Rows: 336,776\n#&gt; Columns: 19\n#&gt; $ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013…\n#&gt; $ month          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n#&gt; $ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n#&gt; $ dep_time       &lt;int&gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 55…\n#&gt; $ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 60…\n#&gt; $ dep_delay      &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2,…\n#&gt; $ arr_time       &lt;int&gt; 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 8…\n#&gt; $ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 8…\n#&gt; $ arr_delay      &lt;dbl&gt; 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7,…\n#&gt; $ carrier        &lt;chr&gt; \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV\", \"B6\"…\n#&gt; $ flight         &lt;int&gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301…\n#&gt; $ tailnum        &lt;chr&gt; \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668DN\", \"N…\n#&gt; $ origin         &lt;chr&gt; \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EWR\", \"LG…\n#&gt; $ dest           &lt;chr&gt; \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FLL\", \"IA…\n#&gt; $ air_time       &lt;dbl&gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149…\n#&gt; $ distance       &lt;dbl&gt; 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 73…\n#&gt; $ hour           &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6…\n#&gt; $ minute         &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59…\n#&gt; $ time_hour      &lt;dttm&gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-0…\n\nIn both views, the variable names are followed by abbreviations that tell you the type of each variable: &lt;int&gt; is short for integer, &lt;dbl&gt; is short for double (aka real numbers), &lt;chr&gt; for character (aka strings), and &lt;dttm&gt; for date-time.\n在这两种视图中，变量名后面都跟着缩写，告诉你每个变量的类型：&lt;int&gt; 是整数 (integer) 的缩写，&lt;dbl&gt; 是双精度浮点数 (double) (也称为实数) 的缩写，&lt;chr&gt; 是字符 (character) (也称为字符串) 的缩写，而 &lt;dttm&gt; 是日期时间 (date-time) 的缩写。\nThese are important because the operations you can perform on a column depend heavily on its “type.”\n这些都很重要，因为你可以对一列执行的操作在很大程度上取决于它的“类型”。\n\n3.1.3 dplyr basics\nYou’re about to learn the primary dplyr verbs (functions), which will allow you to solve the vast majority of your data manipulation challenges.\n你即将学习主要的 dplyr 动词 (函数)，它们将使你能够解决绝大多数的数据操作挑战。\nBut before we discuss their individual differences, it’s worth stating what they have in common:\n但在我们讨论它们的个体差异之前，有必要说明它们的共同点：\n\nThe first argument is always a data frame.\n第一个参数总是一个数据框。\nThe subsequent arguments typically describe which columns to operate on using the variable names (without quotes).\n后续的参数通常使用变量名 (不带引号) 来描述要操作的列。\nThe output is always a new data frame.\n输出总是一个新的数据框。\n\nBecause each verb does one thing well, solving complex problems will usually require combining multiple verbs, and we’ll do so with the pipe, |&gt;.\n因为每个动词都能很好地完成一件事，所以解决复杂问题通常需要组合多个动词，我们将使用管道 |&gt; 来实现这一点。\nWe’ll discuss the pipe more in Section 3.4, but in brief, the pipe takes the thing on its left and passes it along to the function on its right so that x |&gt; f(y) is equivalent to f(x, y), and x |&gt; f(y) |&gt; g(z) is equivalent to g(f(x, y), z).\n我们将在 Section 3.4 中更详细地讨论管道，但简而言之，管道将其左边的东西传递给其右边的函数，因此 x |&gt; f(y) 等同于 f(x, y)，而 x |&gt; f(y) |&gt; g(z) 等同于 g(f(x, y), z)。\nThe easiest way to pronounce the pipe is “then”.\n管道最简单的发音是“然后” (then)。\nThat makes it possible to get a sense of the following code even though you haven’t yet learned the details:\n这使得即使你还没有学习细节，也能理解以下代码的含义：\n\nflights |&gt;\n  filter(dest == \"IAH\") |&gt; \n  group_by(year, month, day) |&gt; \n  summarize(\n    arr_delay = mean(arr_delay, na.rm = TRUE)\n  )\n\ndplyr’s verbs are organized into four groups based on what they operate on: rows, columns, groups, or tables.\ndplyr 的动词根据它们操作的对象分为四组：行、列、组 或 表。\nIn the following sections, you’ll learn the most important verbs for rows, columns, and groups.\n在接下来的部分中，你将学习针对行、列和组的最重要的动词。\nThen, we’ll return to the join verbs that work on tables in Chapter 19.\n然后，我们将在 Chapter 19 中回到处理表的连接动词。\nLet’s dive in!\n让我们开始吧！",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data transformation</span>"
    ]
  },
  {
    "objectID": "data-transform.html#rows",
    "href": "data-transform.html#rows",
    "title": "3  Data transformation",
    "section": "\n3.2 Rows",
    "text": "3.2 Rows\nThe most important verbs that operate on rows of a dataset are filter(), which changes which rows are present without changing their order, and arrange(), which changes the order of the rows without changing which are present.\n操作数据集行的最重要的动词是 filter()，它在不改变行顺序的情况下改变存在的行；以及 arrange()，它在不改变存在的行的情况下改变行的顺序。\nBoth functions only affect the rows, and the columns are left unchanged.\n这两个函数都只影响行，列保持不变。\nWe’ll also discuss distinct() which finds rows with unique values.\n我们还将讨论 distinct()，它能找到具有唯一值的行。\nUnlike arrange() and filter() it can also optionally modify the columns.\n与 arrange() 和 filter() 不同，它还可以选择性地修改列。\n\n3.2.1 filter()\n\nfilter() allows you to keep rows based on the values of the columns1.filter() 允许你根据列的值保留行1。\nThe first argument is the data frame.\n第一个参数是数据框。\nThe second and subsequent arguments are the conditions that must be true to keep the row.\n第二个及后续参数是必须为真才能保留该行的条件。\nFor example, we could find all flights that departed more than 120 minutes (two hours) late:\n例如，我们可以找到所有晚点超过 120 分钟 (两小时) 的航班：\n\nflights |&gt; \n  filter(dep_delay &gt; 120)\n#&gt; # A tibble: 9,723 × 19\n#&gt;    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n#&gt; 1  2013     1     1      848           1835       853     1001           1950\n#&gt; 2  2013     1     1      957            733       144     1056            853\n#&gt; 3  2013     1     1     1114            900       134     1447           1222\n#&gt; 4  2013     1     1     1540           1338       122     2020           1825\n#&gt; 5  2013     1     1     1815           1325       290     2120           1542\n#&gt; 6  2013     1     1     1842           1422       260     1958           1535\n#&gt; # ℹ 9,717 more rows\n#&gt; # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, …\n\nAs well as &gt; (greater than), you can use &gt;= (greater than or equal to), &lt; (less than), &lt;= (less than or equal to), == (equal to), and != (not equal to).\n除了 &gt; (大于)，你还可以使用 &gt;= (大于或等于)、&lt; (小于)、&lt;= (小于或等于)、== (等于) 和 != (不等于)。\nYou can also combine conditions with & or , to indicate “and” (check for both conditions) or with | to indicate “or” (check for either condition):\n你还可以使用 & 或 , 来组合条件，表示“与” (检查两个条件)，或使用 | 表示“或” (检查任一条件)：\n\n# Flights that departed on January 1\nflights |&gt; \n  filter(month == 1 & day == 1)\n#&gt; # A tibble: 842 × 19\n#&gt;    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n#&gt; 1  2013     1     1      517            515         2      830            819\n#&gt; 2  2013     1     1      533            529         4      850            830\n#&gt; 3  2013     1     1      542            540         2      923            850\n#&gt; 4  2013     1     1      544            545        -1     1004           1022\n#&gt; 5  2013     1     1      554            600        -6      812            837\n#&gt; 6  2013     1     1      554            558        -4      740            728\n#&gt; # ℹ 836 more rows\n#&gt; # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, …\n\n# Flights that departed in January or February\nflights |&gt; \n  filter(month == 1 | month == 2)\n#&gt; # A tibble: 51,955 × 19\n#&gt;    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n#&gt; 1  2013     1     1      517            515         2      830            819\n#&gt; 2  2013     1     1      533            529         4      850            830\n#&gt; 3  2013     1     1      542            540         2      923            850\n#&gt; 4  2013     1     1      544            545        -1     1004           1022\n#&gt; 5  2013     1     1      554            600        -6      812            837\n#&gt; 6  2013     1     1      554            558        -4      740            728\n#&gt; # ℹ 51,949 more rows\n#&gt; # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, …\n\nThere’s a useful shortcut when you’re combining | and ==: %in%.\n当你组合 | 和 == 时，有一个有用的快捷方式：%in%。\nIt keeps rows where the variable equals one of the values on the right:\n它会保留变量等于右侧值之一的行：\n\n# A shorter way to select flights that departed in January or February\nflights |&gt; \n  filter(month %in% c(1, 2))\n#&gt; # A tibble: 51,955 × 19\n#&gt;    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n#&gt; 1  2013     1     1      517            515         2      830            819\n#&gt; 2  2013     1     1      533            529         4      850            830\n#&gt; 3  2013     1     1      542            540         2      923            850\n#&gt; 4  2013     1     1      544            545        -1     1004           1022\n#&gt; 5  2013     1     1      554            600        -6      812            837\n#&gt; 6  2013     1     1      554            558        -4      740            728\n#&gt; # ℹ 51,949 more rows\n#&gt; # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, …\n\nWe’ll come back to these comparisons and logical operators in more detail in Chapter 12.\n我们将在 Chapter 12 中更详细地回顾这些比较和逻辑运算符。\nWhen you run filter() dplyr executes the filtering operation, creating a new data frame, and then prints it.\n当你运行 filter() 时，dplyr 会执行筛选操作，创建一个新的数据框，然后打印它。\nIt doesn’t modify the existing flights dataset because dplyr functions never modify their inputs.\n它不会修改现有的 flights 数据集，因为 dplyr 函数从不修改它们的输入。\nTo save the result, you need to use the assignment operator, &lt;-:\n要保存结果，你需要使用赋值运算符 &lt;-：\n\njan1 &lt;- flights |&gt; \n  filter(month == 1 & day == 1)\n\n\n3.2.2 Common mistakes\nWhen you’re starting out with R, the easiest mistake to make is to use = instead of == when testing for equality.\n当你刚开始使用 R 时，最容易犯的错误是在测试相等性时使用 = 而不是 ==。\nfilter() will let you know when this happens:filter() 会在这种情况下通知你：\n\nflights |&gt; \n  filter(month = 1)\n#&gt; Error in `filter()`:\n#&gt; ! We detected a named input.\n#&gt; ℹ This usually means that you've used `=` instead of `==`.\n#&gt; ℹ Did you mean `month == 1`?\n\nAnother mistakes is you write “or” statements like you would in English:\n另一个错误是你像用英语一样写“或”语句：\n\nflights |&gt; \n  filter(month == 1 | 2)\n\nThis “works”, in the sense that it doesn’t throw an error, but it doesn’t do what you want because | first checks the condition month == 1 and then checks the condition 2, which is not a sensible condition to check.\n这“行得通”，因为它不会抛出错误，但它没有做你想要的，因为 | 首先检查条件 month == 1，然后检查条件 2，这不是一个合理的检查条件。\nWe’ll learn more about what’s happening here and why in Section 12.3.2.\n我们将在 Section 12.3.2 中更多地了解这里发生了什么以及为什么会这样。\n\n3.2.3 arrange()\n\narrange() changes the order of the rows based on the value of the columns.arrange() 根据列的值改变行的顺序。\nIt takes a data frame and a set of column names (or more complicated expressions) to order by.\n它接受一个数据框和一组列名 (或更复杂的表达式) 作为排序依据。\nIf you provide more than one column name, each additional column will be used to break ties in the values of the preceding columns.\n如果你提供多个列名，每个额外的列将用于打破前面列值中的平局。\nFor example, the following code sorts by the departure time, which is spread over four columns.\n例如，以下代码按出发时间排序，该时间分布在四列中。\nWe get the earliest years first, then within a year, the earliest months, etc.\n我们首先得到最早的年份，然后在一年内，得到最早的月份，依此类推。\n\nflights |&gt; \n  arrange(year, month, day, dep_time)\n#&gt; # A tibble: 336,776 × 19\n#&gt;    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n#&gt; 1  2013     1     1      517            515         2      830            819\n#&gt; 2  2013     1     1      533            529         4      850            830\n#&gt; 3  2013     1     1      542            540         2      923            850\n#&gt; 4  2013     1     1      544            545        -1     1004           1022\n#&gt; 5  2013     1     1      554            600        -6      812            837\n#&gt; 6  2013     1     1      554            558        -4      740            728\n#&gt; # ℹ 336,770 more rows\n#&gt; # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, …\n\nYou can use desc() on a column inside of arrange() to re-order the data frame based on that column in descending (big-to-small) order.\n你可以在 arrange() 内部对列使用 desc()，以按该列的降序 (从大到小) 重新排序数据框。\nFor example, this code orders flights from most to least delayed:\n例如，此代码将航班从最晚点到最少晚点排序：\n\nflights |&gt; \n  arrange(desc(dep_delay))\n#&gt; # A tibble: 336,776 × 19\n#&gt;    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n#&gt; 1  2013     1     9      641            900      1301     1242           1530\n#&gt; 2  2013     6    15     1432           1935      1137     1607           2120\n#&gt; 3  2013     1    10     1121           1635      1126     1239           1810\n#&gt; 4  2013     9    20     1139           1845      1014     1457           2210\n#&gt; 5  2013     7    22      845           1600      1005     1044           1815\n#&gt; 6  2013     4    10     1100           1900       960     1342           2211\n#&gt; # ℹ 336,770 more rows\n#&gt; # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, …\n\nNote that the number of rows has not changed – we’re only arranging the data, we’re not filtering it.\n请注意，行数没有改变——我们只是在排列数据，而不是筛选数据。\n\n3.2.4 distinct()\n\ndistinct() finds all the unique rows in a dataset, so technically, it primarily operates on the rows.distinct() 在数据集中查找所有唯一的行，因此从技术上讲，它主要作用于行。\nMost of the time, however, you’ll want the distinct combination of some variables, so you can also optionally supply column names:\n然而，大多数时候，你会想要一些变量的独特组合，所以你也可以选择性地提供列名：\n\n# Remove duplicate rows, if any\nflights |&gt; \n  distinct()\n#&gt; # A tibble: 336,776 × 19\n#&gt;    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n#&gt; 1  2013     1     1      517            515         2      830            819\n#&gt; 2  2013     1     1      533            529         4      850            830\n#&gt; 3  2013     1     1      542            540         2      923            850\n#&gt; 4  2013     1     1      544            545        -1     1004           1022\n#&gt; 5  2013     1     1      554            600        -6      812            837\n#&gt; 6  2013     1     1      554            558        -4      740            728\n#&gt; # ℹ 336,770 more rows\n#&gt; # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, …\n\n# Find all unique origin and destination pairs\nflights |&gt; \n  distinct(origin, dest)\n#&gt; # A tibble: 224 × 2\n#&gt;   origin dest \n#&gt;   &lt;chr&gt;  &lt;chr&gt;\n#&gt; 1 EWR    IAH  \n#&gt; 2 LGA    IAH  \n#&gt; 3 JFK    MIA  \n#&gt; 4 JFK    BQN  \n#&gt; 5 LGA    ATL  \n#&gt; 6 EWR    ORD  \n#&gt; # ℹ 218 more rows\n\nAlternatively, if you want to keep the other columns when filtering for unique rows, you can use the .keep_all = TRUE option.\n或者，如果你想在筛选唯一行时保留其他列，可以使用 .keep_all = TRUE 选项。\n\nflights |&gt; \n  distinct(origin, dest, .keep_all = TRUE)\n#&gt; # A tibble: 224 × 19\n#&gt;    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n#&gt; 1  2013     1     1      517            515         2      830            819\n#&gt; 2  2013     1     1      533            529         4      850            830\n#&gt; 3  2013     1     1      542            540         2      923            850\n#&gt; 4  2013     1     1      544            545        -1     1004           1022\n#&gt; 5  2013     1     1      554            600        -6      812            837\n#&gt; 6  2013     1     1      554            558        -4      740            728\n#&gt; # ℹ 218 more rows\n#&gt; # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, …\n\nIt’s not a coincidence that all of these distinct flights are on January 1: distinct() will find the first occurrence of a unique row in the dataset and discard the rest.\n所有这些不同的航班都在 1 月 1 日并非巧合：distinct() 会找到数据集中唯一行的第一次出现，并丢弃其余的。\nIf you want to find the number of occurrences instead, you’re better off swapping distinct() for count().\n如果你想查找出现次数，最好将 distinct() 换成 count()。\nWith the sort = TRUE argument, you can arrange them in descending order of the number of occurrences.\n使用 sort = TRUE 参数，你可以按出现次数的降序排列它们。\nYou’ll learn more about count in Section 13.3.\n你将在 Section 13.3 中学到更多关于 count() 的知识。\n\nflights |&gt;\n  count(origin, dest, sort = TRUE)\n#&gt; # A tibble: 224 × 3\n#&gt;   origin dest      n\n#&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;int&gt;\n#&gt; 1 JFK    LAX   11262\n#&gt; 2 LGA    ATL   10263\n#&gt; 3 LGA    ORD    8857\n#&gt; 4 JFK    SFO    8204\n#&gt; 5 LGA    CLT    6168\n#&gt; 6 EWR    ORD    6100\n#&gt; # ℹ 218 more rows\n\n\n3.2.5 Exercises\n\n\nIn a single pipeline for each condition, find all flights that meet the condition:\n\nHad an arrival delay of two or more hours\nFlew to Houston (IAH or HOU)\nWere operated by United, American, or Delta\nDeparted in summer (July, August, and September)\nArrived more than two hours late but didn’t leave late\nWere delayed by at least an hour, but made up over 30 minutes in flight\n\n\nSort flights to find the flights with the longest departure delays. Find the flights that left earliest in the morning.\nSort flights to find the fastest flights. (Hint: Try including a math calculation inside of your function.)\nWas there a flight on every day of 2013?\nWhich flights traveled the farthest distance? Which traveled the least distance?\nDoes it matter what order you used filter() and arrange() if you’re using both? Why/why not? Think about the results and how much work the functions would have to do.",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data transformation</span>"
    ]
  },
  {
    "objectID": "data-transform.html#columns",
    "href": "data-transform.html#columns",
    "title": "3  Data transformation",
    "section": "\n3.3 Columns",
    "text": "3.3 Columns\nThere are four important verbs that affect the columns without changing the rows: mutate() creates new columns that are derived from the existing columns, select() changes which columns are present, rename() changes the names of the columns, and relocate() changes the positions of the columns.\n有四个重要的动词会影响列而不改变行：mutate() 从现有列派生出新列，select() 改变存在的列，rename() 改变列的名称，relocate() 改变列的位置。\n\n3.3.1 mutate()\n\nThe job of mutate() is to add new columns that are calculated from the existing columns.mutate() 的工作是添加从现有列计算出的新列。\nIn the transform chapters, you’ll learn a large set of functions that you can use to manipulate different types of variables.\n在转换章节中，你将学习大量可用于操作不同类型变量的函数。\nFor now, we’ll stick with basic algebra, which allows us to compute the gain, how much time a delayed flight made up in the air, and the speed in miles per hour:\n现在，我们将坚持使用基本代数，它允许我们计算 gain (延误的航班在空中弥补了多少时间) 和 speed (以英里/小时为单位)：\n\nflights |&gt; \n  mutate(\n    gain = dep_delay - arr_delay,\n    speed = distance / air_time * 60\n  )\n#&gt; # A tibble: 336,776 × 21\n#&gt;    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n#&gt; 1  2013     1     1      517            515         2      830            819\n#&gt; 2  2013     1     1      533            529         4      850            830\n#&gt; 3  2013     1     1      542            540         2      923            850\n#&gt; 4  2013     1     1      544            545        -1     1004           1022\n#&gt; 5  2013     1     1      554            600        -6      812            837\n#&gt; 6  2013     1     1      554            558        -4      740            728\n#&gt; # ℹ 336,770 more rows\n#&gt; # ℹ 13 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, …\n\nBy default, mutate() adds new columns on the right-hand side of your dataset, which makes it difficult to see what’s happening here.\n默认情况下，mutate() 会在数据集的右侧添加新列，这使得很难看清这里发生了什么。\nWe can use the .before argument to instead add the variables to the left-hand side2:\n我们可以使用 .before 参数来将变量添加到左侧2：\n\nflights |&gt; \n  mutate(\n    gain = dep_delay - arr_delay,\n    speed = distance / air_time * 60,\n    .before = 1\n  )\n#&gt; # A tibble: 336,776 × 21\n#&gt;    gain speed  year month   day dep_time sched_dep_time dep_delay arr_time\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n#&gt; 1    -9  370.  2013     1     1      517            515         2      830\n#&gt; 2   -16  374.  2013     1     1      533            529         4      850\n#&gt; 3   -31  408.  2013     1     1      542            540         2      923\n#&gt; 4    17  517.  2013     1     1      544            545        -1     1004\n#&gt; 5    19  394.  2013     1     1      554            600        -6      812\n#&gt; 6   -16  288.  2013     1     1      554            558        -4      740\n#&gt; # ℹ 336,770 more rows\n#&gt; # ℹ 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, …\n\nThe . indicates that .before is an argument to the function, not the name of a third new variable we are creating.. 表示 .before 是函数的一个参数，而不是我们正在创建的第三个新变量的名称。\nYou can also use .after to add after a variable, and in both .before and .after you can use the variable name instead of a position.\n你也可以使用 .after 在变量之后添加，在 .before 和 .after 中，你都可以使用变量名而不是位置。\nFor example, we could add the new variables after day:\n例如，我们可以在 day 之后添加新变量：\n\nflights |&gt; \n  mutate(\n    gain = dep_delay - arr_delay,\n    speed = distance / air_time * 60,\n    .after = day\n  )\n\nAlternatively, you can control which variables are kept with the .keep argument.\n或者，你可以使用 .keep 参数来控制保留哪些变量。\nA particularly useful argument is \"used\" which specifies that we only keep the columns that were involved or created in the mutate() step.\n一个特别有用的参数是 \"used\"，它指定我们只保留在 mutate() 步骤中涉及或创建的列。\nFor example, the following output will contain only the variables dep_delay, arr_delay, air_time, gain, hours, and gain_per_hour.\n例如，以下输出将只包含 dep_delay、arr_delay、air_time、gain、hours 和 gain_per_hour 变量。\n\nflights |&gt; \n  mutate(\n    gain = dep_delay - arr_delay,\n    hours = air_time / 60,\n    gain_per_hour = gain / hours,\n    .keep = \"used\"\n  )\n\nNote that since we haven’t assigned the result of the above computation back to flights, the new variables gain, hours, and gain_per_hour will only be printed but will not be stored in a data frame.\n请注意，由于我们没有将上述计算的结果分配回 flights，新变量 gain、hours 和 gain_per_hour 将只被打印而不会存储在数据框中。\nAnd if we want them to be available in a data frame for future use, we should think carefully about whether we want the result to be assigned back to flights, overwriting the original data frame with many more variables, or to a new object.\n如果我们希望它们在数据框中可供将来使用，我们应该仔细考虑是否要将结果分配回 flights (用更多的变量覆盖原始数据框)，还是分配给一个新对象。\nOften, the right answer is a new object that is named informatively to indicate its contents, e.g., delay_gain, but you might also have good reasons for overwriting flights.\n通常，正确的答案是一个新对象，其名称具有信息性，以指示其内容，例如 delay_gain，但你也可能有充分的理由覆盖 flights。\n\n3.3.2 select()\n\nIt’s not uncommon to get datasets with hundreds or even thousands of variables.\n获得包含数百甚至数千个变量的数据集并不少见。\nIn this situation, the first challenge is often just focusing on the variables you’re interested in.\n在这种情况下，第一个挑战通常只是关注你感兴趣的变量。\nselect() allows you to rapidly zoom in on a useful subset using operations based on the names of the variables:select() 允许你使用基于变量名称的操作快速放大到一个有用的子集：\n\n\nSelect columns by name:\n按名称选择列：\n{r}     #| results: false     flights |&gt;        select(year, month, day)\n\n\nSelect all columns between year and day (inclusive):\n选择从 year 到 day (含) 的所有列：\n{r}     #| results: false     flights |&gt;        select(year:day)\n\n\nSelect all columns except those from year to day (inclusive):\n选择除了从 year 到 day (含) 之外的所有列：\n{r}     #| results: false     flights |&gt;        select(!year:day)\nHistorically this operation was done with - instead of !, so you’re likely to see that in the wild.\n历史上，这个操作是使用 - 而不是 ! 完成的，所以你很可能在野外看到它。\nThese two operators serve the same purpose but with subtle differences in behavior.\n这两个运算符的目的相同，但在行为上有细微的差别。\nWe recommend using ! because it reads as “not” and combines well with & and |.\n我们建议使用 !，因为它读作“非” (not)，并且能很好地与 & 和 | 结合。\n\n\nSelect all columns that are characters:\n选择所有字符类型的列：\n{r}     #| results: false     flights |&gt;        select(where(is.character))\n\n\nThere are a number of helper functions you can use within select():\n在 select() 中你可以使用许多辅助函数：\n\nstarts_with(\"abc\"): matches names that begin with “abc”.\n匹配以 “abc” 开头的名称。\nends_with(\"xyz\"): matches names that end with “xyz”.\n匹配以 “xyz” 结尾的名称。\ncontains(\"ijk\"): matches names that contain “ijk”.\n匹配包含 “ijk” 的名称。\nnum_range(\"x\", 1:3): matches x1, x2 and x3.\n匹配 x1, x2 和 x3。\n\nSee ?select for more details.\n查看 ?select 获取更多细节。\nOnce you know regular expressions (the topic of Chapter 15) you’ll also be able to use matches() to select variables that match a pattern.\n一旦你了解了正则表达式 (regular expressions) (Chapter 15 的主题)，你还可以使用 matches() 来选择匹配模式的变量。\nYou can rename variables as you select() them by using =.\n你可以在 select() 时使用 = 来重命名变量。\nThe new name appears on the left-hand side of the =, and the old variable appears on the right-hand side:\n新名称出现在 = 的左侧，旧变量出现在右侧：\n\nflights |&gt; \n  select(tail_num = tailnum)\n#&gt; # A tibble: 336,776 × 1\n#&gt;   tail_num\n#&gt;   &lt;chr&gt;   \n#&gt; 1 N14228  \n#&gt; 2 N24211  \n#&gt; 3 N619AA  \n#&gt; 4 N804JB  \n#&gt; 5 N668DN  \n#&gt; 6 N39463  \n#&gt; # ℹ 336,770 more rows\n\n\n3.3.3 rename()\n\nIf you want to keep all the existing variables and just want to rename a few, you can use rename() instead of select():\n如果你想保留所有现有变量，而只想重命名少数几个，你可以使用 rename() 代替 select()：\n\nflights |&gt; \n  rename(tail_num = tailnum)\n#&gt; # A tibble: 336,776 × 19\n#&gt;    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n#&gt; 1  2013     1     1      517            515         2      830            819\n#&gt; 2  2013     1     1      533            529         4      850            830\n#&gt; 3  2013     1     1      542            540         2      923            850\n#&gt; 4  2013     1     1      544            545        -1     1004           1022\n#&gt; 5  2013     1     1      554            600        -6      812            837\n#&gt; 6  2013     1     1      554            558        -4      740            728\n#&gt; # ℹ 336,770 more rows\n#&gt; # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, …\n\nIf you have a bunch of inconsistently named columns and it would be painful to fix them all by hand, check out janitor::clean_names() which provides some useful automated cleaning.\n如果你有一堆命名不一致的列，并且手动修复它们会很痛苦，可以看看 janitor::clean_names()，它提供了一些有用的自动清理功能。\n\n3.3.4 relocate()\n\nUse relocate() to move variables around.\n使用 relocate() 来移动变量。\nYou might want to collect related variables together or move important variables to the front.\n你可能想将相关的变量收集在一起，或者将重要的变量移到前面。\nBy default relocate() moves variables to the front:\n默认情况下，relocate() 将变量移动到最前面：\n\nflights |&gt; \n  relocate(time_hour, air_time)\n#&gt; # A tibble: 336,776 × 19\n#&gt;   time_hour           air_time  year month   day dep_time sched_dep_time\n#&gt;   &lt;dttm&gt;                 &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;\n#&gt; 1 2013-01-01 05:00:00      227  2013     1     1      517            515\n#&gt; 2 2013-01-01 05:00:00      227  2013     1     1      533            529\n#&gt; 3 2013-01-01 05:00:00      160  2013     1     1      542            540\n#&gt; 4 2013-01-01 05:00:00      183  2013     1     1      544            545\n#&gt; 5 2013-01-01 06:00:00      116  2013     1     1      554            600\n#&gt; 6 2013-01-01 05:00:00      150  2013     1     1      554            558\n#&gt; # ℹ 336,770 more rows\n#&gt; # ℹ 12 more variables: dep_delay &lt;dbl&gt;, arr_time &lt;int&gt;, …\n\nYou can also specify where to put them using the .before and .after arguments, just like in mutate():\n你也可以像在 mutate() 中一样，使用 .before 和 .after 参数来指定将它们放在哪里：\n\nflights |&gt; \n  relocate(year:dep_time, .after = time_hour)\nflights |&gt; \n  relocate(starts_with(\"arr\"), .before = dep_time)\n\n\n3.3.5 Exercises\n\nCompare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related?\nBrainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights.\nWhat happens if you specify the name of the same variable multiple times in a select() call?\n\nWhat does the any_of() function do? Why might it be helpful in conjunction with this vector?\n\nvariables &lt;- c(\"year\", \"month\", \"day\", \"dep_delay\", \"arr_delay\")\n\n\n\nDoes the result of running the following code surprise you? How do the select helpers deal with upper and lower case by default? How can you change that default?\n\nflights |&gt; select(contains(\"TIME\"))\n\n\nRename air_time to air_time_min to indicate units of measurement and move it to the beginning of the data frame.\n\nWhy doesn’t the following work, and what does the error mean?\n\nflights |&gt; \n  select(tailnum) |&gt; \n  arrange(arr_delay)\n#&gt; Error in `arrange()`:\n#&gt; ℹ In argument: `..1 = arr_delay`.\n#&gt; Caused by error:\n#&gt; ! object 'arr_delay' not found",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data transformation</span>"
    ]
  },
  {
    "objectID": "data-transform.html#sec-the-pipe",
    "href": "data-transform.html#sec-the-pipe",
    "title": "3  Data transformation",
    "section": "\n3.4 The pipe",
    "text": "3.4 The pipe\nWe’ve shown you simple examples of the pipe above, but its real power arises when you start to combine multiple verbs.\n我们在上面向你展示了管道的简单示例，但它真正的威力在于当你开始组合多个动词时。\nFor example, imagine that you wanted to find the fastest flights to Houston’s IAH airport: you need to combine filter(), mutate(), select(), and arrange():\n例如，假设你想找到飞往休斯顿 IAH 机场的最快航班：你需要组合 filter()、mutate()、select() 和 arrange()：\n\nflights |&gt; \n  filter(dest == \"IAH\") |&gt; \n  mutate(speed = distance / air_time * 60) |&gt; \n  select(year:day, dep_time, carrier, flight, speed) |&gt; \n  arrange(desc(speed))\n#&gt; # A tibble: 7,198 × 7\n#&gt;    year month   day dep_time carrier flight speed\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt; &lt;chr&gt;    &lt;int&gt; &lt;dbl&gt;\n#&gt; 1  2013     7     9      707 UA         226  522.\n#&gt; 2  2013     8    27     1850 UA        1128  521.\n#&gt; 3  2013     8    28      902 UA        1711  519.\n#&gt; 4  2013     8    28     2122 UA        1022  519.\n#&gt; 5  2013     6    11     1628 UA        1178  515.\n#&gt; 6  2013     8    27     1017 UA         333  515.\n#&gt; # ℹ 7,192 more rows\n\nEven though this pipeline has four steps, it’s easy to skim because the verbs come at the start of each line: start with the flights data, then filter, then mutate, then select, then arrange.\n尽管这个管道有四个步骤，但它很容易浏览，因为动词都出现在每行的开头：从 flights 数据开始，然后筛选，然后转换，然后选择，然后排列。\nWhat would happen if we didn’t have the pipe?\n如果我们没有管道会怎么样？\nWe could nest each function call inside the previous call:\n我们可以将每个函数调用嵌套在前一个调用中：\n\narrange(\n  select(\n    mutate(\n      filter(\n        flights, \n        dest == \"IAH\"\n      ),\n      speed = distance / air_time * 60\n    ),\n    year:day, dep_time, carrier, flight, speed\n  ),\n  desc(speed)\n)\n\nOr we could use a bunch of intermediate objects:\n或者我们可以使用一堆中间对象：\n\nflights1 &lt;- filter(flights, dest == \"IAH\")\nflights2 &lt;- mutate(flights1, speed = distance / air_time * 60)\nflights3 &lt;- select(flights2, year:day, dep_time, carrier, flight, speed)\narrange(flights3, desc(speed))\n\nWhile both forms have their time and place, the pipe generally produces data analysis code that is easier to write and read.\n虽然这两种形式都有其适用的时机和场合，但管道通常能生成更易于编写和阅读的数据分析代码。\nTo add the pipe to your code, we recommend using the built-in keyboard shortcut Ctrl/Cmd + Shift + M.\n要将管道添加到你的代码中，我们建议使用内置的键盘快捷键 Ctrl/Cmd + Shift + M。\nYou’ll need to make one change to your RStudio options to use |&gt; instead of %&gt;% as shown in Figure 3.1; more on %&gt;% shortly.\n你需要对 RStudio 选项进行一项更改，以使用 |&gt; 而不是 %&gt;%，如 Figure 3.1 所示；稍后将详细介绍 %&gt;%。\n\n\n\n\n\n\n\nFigure 3.1: To insert |&gt;, make sure the “Use native pipe operator” option is checked.  要插入 |&gt;，请确保选中“使用原生管道运算符” (Use native pipe operator) 选项。\n\n\n\n\n\n\n\n\n\n\nmagrittr\n\n\n\nIf you’ve been using the tidyverse for a while, you might be familiar with the %&gt;% pipe provided by the magrittr package.\n如果你已经使用 tidyverse 一段时间了，你可能熟悉 magrittr 包提供的 %&gt;% 管道。\nThe magrittr package is included in the core tidyverse, so you can use %&gt;% whenever you load the tidyverse:\nmagrittr 包包含在 tidyverse 核心中，因此每当你加载 tidyverse 时都可以使用 %&gt;%：\n\nlibrary(tidyverse)\n\nmtcars %&gt;% \n  group_by(cyl) %&gt;%\n  summarize(n = n())\n\nFor simple cases, |&gt; and %&gt;% behave identically.\n在简单的情况下，|&gt; 和 %&gt;% 的行为完全相同。\nSo why do we recommend the base pipe?\n那么我们为什么推荐基础管道呢？\nFirstly, because it’s part of base R, it’s always available for you to use, even when you’re not using the tidyverse.\n首先，因为它是基础 R 的一部分，所以即使你不使用 tidyverse，它也始终可用。\nSecondly, |&gt; is quite a bit simpler than %&gt;%: in the time between the invention of %&gt;% in 2014 and the inclusion of |&gt; in R 4.1.0 in 2021, we gained a better understanding of the pipe.\n其次，|&gt; 比 %&gt;% 简单得多：在 2014 年 %&gt;% 发明和 2021 年 R 4.1.0 包含 |&gt; 之间的时间里，我们对管道有了更好的理解。\nThis allowed the base implementation to jettison infrequently used and less important features.\n这使得基础实现可以摒弃不常用和不太重要的功能。",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data transformation</span>"
    ]
  },
  {
    "objectID": "data-transform.html#groups",
    "href": "data-transform.html#groups",
    "title": "3  Data transformation",
    "section": "\n3.5 Groups",
    "text": "3.5 Groups\nSo far you’ve learned about functions that work with rows and columns.\n到目前为止，你已经学习了处理行和列的函数。\ndplyr gets even more powerful when you add in the ability to work with groups.\n当你加入处理分组的能力时，dplyr 会变得更加强大。\nIn this section, we’ll focus on the most important functions: group_by(), summarize(), and the slice family of functions.\n在本节中，我们将重点介绍最重要的函数：group_by()、summarize() 和 slice 系列函数。\n\n3.5.1 group_by()\n\nUse group_by() to divide your dataset into groups meaningful for your analysis:\n使用 group_by() 将你的数据集划分为对你的分析有意义的组：\n\nflights |&gt; \n  group_by(month)\n#&gt; # A tibble: 336,776 × 19\n#&gt; # Groups:   month [12]\n#&gt;    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n#&gt; 1  2013     1     1      517            515         2      830            819\n#&gt; 2  2013     1     1      533            529         4      850            830\n#&gt; 3  2013     1     1      542            540         2      923            850\n#&gt; 4  2013     1     1      544            545        -1     1004           1022\n#&gt; 5  2013     1     1      554            600        -6      812            837\n#&gt; 6  2013     1     1      554            558        -4      740            728\n#&gt; # ℹ 336,770 more rows\n#&gt; # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, …\n\ngroup_by() doesn’t change the data but, if you look closely at the output, you’ll notice that the output indicates that it is “grouped by” month (Groups: month [12]).group_by() 不会改变数据，但是，如果你仔细观察输出，你会注意到输出表明它已按月份“分组” (Groups: month [12])。\nThis means subsequent operations will now work “by month”.\n这意味着后续操作现在将“按月”工作。\ngroup_by() adds this grouped feature (referred to as class) to the data frame, which changes the behavior of the subsequent verbs applied to the data.group_by() 将此分组特征 (称为类) 添加到数据框中，这会改变应用于数据的后续动词的行为。\n\n3.5.2 summarize()\n\nThe most important grouped operation is a summary, which, if being used to calculate a single summary statistic, reduces the data frame to have a single row for each group.\n最重要的分组操作是摘要，如果用于计算单个摘要统计量，它会将数据框减少为每个组只有一行。\nIn dplyr, this operation is performed by summarize()3, as shown by the following example, which computes the average departure delay by month:\n在 dplyr 中，此操作由 summarize()3 执行，如下例所示，该示例计算按月平均出发延误：\n\nflights |&gt; \n  group_by(month) |&gt; \n  summarize(\n    avg_delay = mean(dep_delay)\n  )\n#&gt; # A tibble: 12 × 2\n#&gt;   month avg_delay\n#&gt;   &lt;int&gt;     &lt;dbl&gt;\n#&gt; 1     1        NA\n#&gt; 2     2        NA\n#&gt; 3     3        NA\n#&gt; 4     4        NA\n#&gt; 5     5        NA\n#&gt; 6     6        NA\n#&gt; # ℹ 6 more rows\n\nUh-oh!\n噢！\nSomething has gone wrong, and all of our results are NAs (pronounced “N-A”), R’s symbol for missing value.\n出问题了，我们所有的结果都是 NA (读作“N-A”)，这是 R 中表示缺失值的符号。\nThis happened because some of the observed flights had missing data in the delay column, and so when we calculated the mean including those values, we got an NA result.\n发生这种情况是因为一些观察到的航班在延误列中有缺失数据，因此当我们计算包括这些值的平均值时，我们得到了一个 NA 结果。\nWe’ll come back to discuss missing values in detail in Chapter 18, but for now, we’ll tell the mean() function to ignore all missing values by setting the argument na.rm to TRUE:\n我们将在 Chapter 18 中详细讨论缺失值，但现在，我们将通过将参数 na.rm 设置为 TRUE 来告诉 mean() 函数忽略所有缺失值：\n\nflights |&gt; \n  group_by(month) |&gt; \n  summarize(\n    avg_delay = mean(dep_delay, na.rm = TRUE)\n  )\n#&gt; # A tibble: 12 × 2\n#&gt;   month avg_delay\n#&gt;   &lt;int&gt;     &lt;dbl&gt;\n#&gt; 1     1      10.0\n#&gt; 2     2      10.8\n#&gt; 3     3      13.2\n#&gt; 4     4      13.9\n#&gt; 5     5      13.0\n#&gt; 6     6      20.8\n#&gt; # ℹ 6 more rows\n\nYou can create any number of summaries in a single call to summarize().\n你可以在一次 summarize() 调用中创建任意数量的摘要。\nYou’ll learn various useful summaries in the upcoming chapters, but one very useful summary is n(), which returns the number of rows in each group:\n你将在接下来的章节中学到各种有用的摘要，但一个非常有用的摘要是 n()，它返回每个组中的行数：\n\nflights |&gt; \n  group_by(month) |&gt; \n  summarize(\n    avg_delay = mean(dep_delay, na.rm = TRUE), \n    n = n()\n  )\n#&gt; # A tibble: 12 × 3\n#&gt;   month avg_delay     n\n#&gt;   &lt;int&gt;     &lt;dbl&gt; &lt;int&gt;\n#&gt; 1     1      10.0 27004\n#&gt; 2     2      10.8 24951\n#&gt; 3     3      13.2 28834\n#&gt; 4     4      13.9 28330\n#&gt; 5     5      13.0 28796\n#&gt; 6     6      20.8 28243\n#&gt; # ℹ 6 more rows\n\nMeans and counts can get you a surprisingly long way in data science!\n均值和计数可以让你在数据科学领域走得很远！\n\n3.5.3 The slice_ functions\nThere are five handy functions that allow you to extract specific rows within each group:\n有五个方便的函数，可以让你提取每个组中的特定行：\n\ndf |&gt; slice_head(n = 1) takes the first row from each group.\n从每个组中取第一行。\ndf |&gt; slice_tail(n = 1) takes the last row in each group.\n从每个组中取最后一行。\ndf |&gt; slice_min(x, n = 1) takes the row with the smallest value of column x.\n取 x 列值最小的行。\ndf |&gt; slice_max(x, n = 1) takes the row with the largest value of column x.\n取 x 列值最大的行。\ndf |&gt; slice_sample(n = 1) takes one random row.\n取一个随机行。\n\nYou can vary n to select more than one row, or instead of n =, you can use prop = 0.1 to select (e.g.) 10% of the rows in each group.\n你可以改变 n 来选择多于一行，或者用 prop = 0.1 来代替 n =，以选择 (例如) 每个组中 10% 的行。\nFor example, the following code finds the flights that are most delayed upon arrival at each destination:\n例如，以下代码查找在每个目的地到达时延误最严重的航班：\n\nflights |&gt; \n  group_by(dest) |&gt; \n  slice_max(arr_delay, n = 1) |&gt;\n  relocate(dest)\n#&gt; # A tibble: 108 × 19\n#&gt; # Groups:   dest [105]\n#&gt;   dest   year month   day dep_time sched_dep_time dep_delay arr_time\n#&gt;   &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n#&gt; 1 ABQ    2013     7    22     2145           2007        98      132\n#&gt; 2 ACK    2013     7    23     1139            800       219     1250\n#&gt; 3 ALB    2013     1    25      123           2000       323      229\n#&gt; 4 ANC    2013     8    17     1740           1625        75     2042\n#&gt; 5 ATL    2013     7    22     2257            759       898      121\n#&gt; 6 AUS    2013     7    10     2056           1505       351     2347\n#&gt; # ℹ 102 more rows\n#&gt; # ℹ 11 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, …\n\nNote that there are 105 destinations but we get 108 rows here.\n请注意，这里有 105 个目的地，但我们得到了 108 行。\nWhat’s up?\n怎么回事？\nslice_min() and slice_max() keep tied values so n = 1 means give us all rows with the highest value.slice_min() 和 slice_max() 会保留并列值，所以 n = 1 意味着给我们所有具有最高值的行。\nIf you want exactly one row per group you can set with_ties = FALSE.\n如果你想每个组只保留一行，你可以设置 with_ties = FALSE。\nThis is similar to computing the max delay with summarize(), but you get the whole corresponding row (or rows if there’s a tie) instead of the single summary statistic.\n这类似于用 summarize() 计算最大延迟，但你会得到整个对应的行 (如果有并列，则为多行)，而不是单个摘要统计量。\n\n3.5.4 Grouping by multiple variables\nYou can create groups using more than one variable.\n你可以使用多个变量创建分组。\nFor example, we could make a group for each date.\n例如，我们可以为每个日期创建一个分组。\n\ndaily &lt;- flights |&gt;  \n  group_by(year, month, day)\ndaily\n#&gt; # A tibble: 336,776 × 19\n#&gt; # Groups:   year, month, day [365]\n#&gt;    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n#&gt; 1  2013     1     1      517            515         2      830            819\n#&gt; 2  2013     1     1      533            529         4      850            830\n#&gt; 3  2013     1     1      542            540         2      923            850\n#&gt; 4  2013     1     1      544            545        -1     1004           1022\n#&gt; 5  2013     1     1      554            600        -6      812            837\n#&gt; 6  2013     1     1      554            558        -4      740            728\n#&gt; # ℹ 336,770 more rows\n#&gt; # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, …\n\nWhen you summarize a tibble grouped by more than one variable, each summary peels off the last group.\n当你对按多个变量分组的 tibble 进行汇总时，每个汇总都会剥离最后一个分组。\nIn hindsight, this wasn’t a great way to make this function work, but it’s difficult to change without breaking existing code.\n事后看来，这不是一个让这个函数工作的好方法，但很难在不破坏现有代码的情况下进行更改。\nTo make it obvious what’s happening, dplyr displays a message that tells you how you can change this behavior:\n为了清楚地说明发生了什么，dplyr 会显示一条消息，告诉你如何改变这种行为：\n\ndaily_flights &lt;- daily |&gt; \n  summarize(n = n())\n#&gt; `summarise()` has grouped output by 'year', 'month'. You can override using\n#&gt; the `.groups` argument.\n\nIf you’re happy with this behavior, you can explicitly request it in order to suppress the message:\n如果你对这种行为感到满意，可以明确请求它以抑制消息：\n\ndaily_flights &lt;- daily |&gt; \n  summarize(\n    n = n(), \n    .groups = \"drop_last\"\n  )\n\nAlternatively, change the default behavior by setting a different value, e.g., \"drop\" to drop all grouping or \"keep\" to preserve the same groups.\n或者，通过设置不同的值来更改默认行为，例如，\"drop\" 用于删除所有分组，\"keep\" 用于保留相同的分组。\n\n3.5.5 Ungrouping\nYou might also want to remove grouping from a data frame without using summarize().\n你可能还想在不使用 summarize() 的情况下从数据框中删除分组。\nYou can do this with ungroup().\n你可以使用 ungroup() 来做到这一点。\n\ndaily |&gt; \n  ungroup()\n#&gt; # A tibble: 336,776 × 19\n#&gt;    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n#&gt; 1  2013     1     1      517            515         2      830            819\n#&gt; 2  2013     1     1      533            529         4      850            830\n#&gt; 3  2013     1     1      542            540         2      923            850\n#&gt; 4  2013     1     1      544            545        -1     1004           1022\n#&gt; 5  2013     1     1      554            600        -6      812            837\n#&gt; 6  2013     1     1      554            558        -4      740            728\n#&gt; # ℹ 336,770 more rows\n#&gt; # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, …\n\nNow let’s see what happens when you summarize an ungrouped data frame.\n现在让我们看看当你汇总一个未分组的数据框时会发生什么。\n\ndaily |&gt; \n  ungroup() |&gt;\n  summarize(\n    avg_delay = mean(dep_delay, na.rm = TRUE), \n    flights = n()\n  )\n#&gt; # A tibble: 1 × 2\n#&gt;   avg_delay flights\n#&gt;       &lt;dbl&gt;   &lt;int&gt;\n#&gt; 1      12.6  336776\n\nYou get a single row back because dplyr treats all the rows in an ungrouped data frame as belonging to one group.\n你只会得到一行，因为 dplyr 将未分组数据框中的所有行都视为属于一个组。\n\n3.5.6 .by\n\ndplyr 1.1.0 includes a new, experimental, syntax for per-operation grouping, the .by argument.\ndplyr 1.1.0 包含了一个新的、实验性的、用于按操作分组的语法，即 .by 参数。\ngroup_by() and ungroup() aren’t going away, but you can now also use the .by argument to group within a single operation:group_by() 和 ungroup() 不会消失，但你现在也可以使用 .by 参数在单个操作内进行分组：\n\nflights |&gt; \n  summarize(\n    delay = mean(dep_delay, na.rm = TRUE), \n    n = n(),\n    .by = month\n  )\n\nOr if you want to group by multiple variables:\n或者如果你想按多个变量分组：\n\nflights |&gt; \n  summarize(\n    delay = mean(dep_delay, na.rm = TRUE), \n    n = n(),\n    .by = c(origin, dest)\n  )\n\n.by works with all verbs and has the advantage that you don’t need to use the .groups argument to suppress the grouping message or ungroup() when you’re done..by 适用于所有动词，其优点是当你完成操作后，无需使用 .groups 参数来抑制分组消息或使用 ungroup()。\nWe didn’t focus on this syntax in this chapter because it was very new when we wrote the book.\n我们在本章中没有重点介绍这种语法，因为在我们写书时它还是非常新的。\nWe did want to mention it because we think it has a lot of promise and it’s likely to be quite popular.\n我们确实想提一下它，因为我们认为它有很大的潜力，而且很可能会非常流行。\nYou can learn more about it in the dplyr 1.1.0 blog post.\n你可以在 dplyr 1.1.0 博客文章 中了解更多相关信息。\n\n3.5.7 Exercises\n\nWhich carrier has the worst average delays? Challenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not? (Hint: think about flights |&gt; group_by(carrier, dest) |&gt; summarize(n()))\nFind the flights that are most delayed upon departure from each destination.\nHow do delays vary over the course of the day? Illustrate your answer with a plot.\nWhat happens if you supply a negative n to slice_min() and friends?\nExplain what count() does in terms of the dplyr verbs you just learned. What does the sort argument to count() do?\n\nSuppose we have the following tiny data frame:\n\ndf &lt;- tibble(\n  x = 1:5,\n  y = c(\"a\", \"b\", \"a\", \"a\", \"b\"),\n  z = c(\"K\", \"K\", \"L\", \"L\", \"K\")\n)\n\n\n\nWrite down what you think the output will look like, then check if you were correct, and describe what group_by() does.\n\ndf |&gt;\n  group_by(y)\n\n\n\nWrite down what you think the output will look like, then check if you were correct, and describe what arrange() does. Also, comment on how it’s different from the group_by() in part (a).\n\ndf |&gt;\n  arrange(y)\n\n\n\nWrite down what you think the output will look like, then check if you were correct, and describe what the pipeline does.\n\ndf |&gt;\n  group_by(y) |&gt;\n  summarize(mean_x = mean(x))\n\n\n\nWrite down what you think the output will look like, then check if you were correct, and describe what the pipeline does. Then, comment on what the message says.\n\ndf |&gt;\n  group_by(y, z) |&gt;\n  summarize(mean_x = mean(x))\n\n\n\nWrite down what you think the output will look like, then check if you were correct, and describe what the pipeline does. How is the output different from the one in part (d)?\n\ndf |&gt;\n  group_by(y, z) |&gt;\n  summarize(mean_x = mean(x), .groups = \"drop\")\n\n\n\nWrite down what you think the outputs will look like, then check if you were correct, and describe what each pipeline does. How are the outputs of the two pipelines different?\n\ndf |&gt;\n  group_by(y, z) |&gt;\n  summarize(mean_x = mean(x))\n\ndf |&gt;\n  group_by(y, z) |&gt;\n  mutate(mean_x = mean(x))",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data transformation</span>"
    ]
  },
  {
    "objectID": "data-transform.html#sec-sample-size",
    "href": "data-transform.html#sec-sample-size",
    "title": "3  Data transformation",
    "section": "\n3.6 Case study: aggregates and sample size",
    "text": "3.6 Case study: aggregates and sample size\nWhenever you do any aggregation, it’s always a good idea to include a count (n()).\n每当你进行任何聚合时，包含一个计数 (n()) 总是好主意。\nThat way, you can ensure that you’re not drawing conclusions based on very small amounts of data.\n这样，你可以确保你不是基于非常少量的数据得出结论。\nWe’ll demonstrate this with some baseball data from the Lahman package.\n我们将使用 Lahman 包中的一些棒球数据来演示这一点。\nSpecifically, we will compare what proportion of times a player gets a hit (H) vs. the number of times they try to put the ball in play (AB):\n具体来说，我们将比较球员击中球的次数 (H) 与他们尝试将球击入场内的次数 (AB) 的比例：\n\nbatters &lt;- Lahman::Batting |&gt; \n  group_by(playerID) |&gt; \n  summarize(\n    performance = sum(H, na.rm = TRUE) / sum(AB, na.rm = TRUE),\n    n = sum(AB, na.rm = TRUE)\n  )\nbatters\n#&gt; # A tibble: 20,730 × 3\n#&gt;   playerID  performance     n\n#&gt;   &lt;chr&gt;           &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 aardsda01      0          4\n#&gt; 2 aaronha01      0.305  12364\n#&gt; 3 aaronto01      0.229    944\n#&gt; 4 aasedo01       0          5\n#&gt; 5 abadan01       0.0952    21\n#&gt; 6 abadfe01       0.111      9\n#&gt; # ℹ 20,724 more rows\n\nWhen we plot the skill of the batter (measured by the batting average, performance) against the number of opportunities to hit the ball (measured by times at bat, n), you see two patterns:\n当我们将击球手的技术 (用击球率 performance 衡量) 与击球机会数 (用击球次数 n 衡量) 绘制成图时，你会看到两种模式：\n\n\nThe variation in performance is larger among players with fewer at-bats.\n击球次数较少的球员的 performance 变化更大。\nThe shape of this plot is very characteristic: whenever you plot a mean (or other summary statistics) vs. group size, you’ll see that the variation decreases as the sample size increases4.\n这张图的形状非常有特点：每当你绘制均值 (或其他摘要统计量) 与组大小的关系图时，你都会看到随着样本量的增加，变异会减小4。\n\nThere’s a positive correlation between skill (performance) and opportunities to hit the ball (n) because teams want to give their best batters the most opportunities to hit the ball.\n技术 (performance) 与击球机会 (n) 之间存在正相关关系，因为球队希望给他们最好的击球手最多的击球机会。\n\n\nbatters |&gt; \n  filter(n &gt; 100) |&gt; \n  ggplot(aes(x = n, y = performance)) +\n  geom_point(alpha = 1 / 10) + \n  geom_smooth(se = FALSE)\n\n\n\n\n\n\n\nNote the handy pattern for combining ggplot2 and dplyr.\n注意结合 ggplot2 和 dplyr 的便捷模式。\nYou just have to remember to switch from |&gt;, for dataset processing, to + for adding layers to your plot.\n你只需要记住从用于数据集处理的 |&gt; 切换到用于向绘图添加图层的 +。\nThis also has important implications for ranking.\n这对排名也有重要影响。\nIf you naively sort on desc(performance), the people with the best batting averages are clearly the ones who tried to put the ball in play very few times and happened to get a hit, they’re not necessarily the most skilled players:\n如果你天真地按 desc(performance) 排序，那么击球率最高的人显然是那些尝试将球击入场内次数很少但碰巧击中了的人，他们不一定是最有技术的球员：\n\nbatters |&gt; \n  arrange(desc(performance))\n#&gt; # A tibble: 20,730 × 3\n#&gt;   playerID  performance     n\n#&gt;   &lt;chr&gt;           &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 abramge01           1     1\n#&gt; 2 alberan01           1     1\n#&gt; 3 banisje01           1     1\n#&gt; 4 bartocl01           1     1\n#&gt; 5 bassdo01            1     1\n#&gt; 6 birasst01           1     2\n#&gt; # ℹ 20,724 more rows\n\nYou can find a good explanation of this problem and how to overcome it at http://varianceexplained.org/r/empirical_bayes_baseball/ and https://www.evanmiller.org/how-not-to-sort-by-average-rating.html.\n你可以在 http://varianceexplained.org/r/empirical_bayes_baseball/ 和 https://www.evanmiller.org/how-not-to-sort-by-average-rating.html 找到对这个问题以及如何克服它的很好解释。",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data transformation</span>"
    ]
  },
  {
    "objectID": "data-transform.html#summary",
    "href": "data-transform.html#summary",
    "title": "3  Data transformation",
    "section": "\n3.7 Summary",
    "text": "3.7 Summary\nIn this chapter, you’ve learned the tools that dplyr provides for working with data frames.\n在本章中，你学习了 dplyr 为处理数据框提供的工具。\nThe tools are roughly grouped into three categories: those that manipulate the rows (like filter() and arrange()), those that manipulate the columns (like select() and mutate()) and those that manipulate groups (like group_by() and summarize()).\n这些工具大致分为三类：操作行的工具 (如 filter() 和 arrange())、操作列的工具 (如 select() 和 mutate()) 以及操作组的工具 (如 group_by() 和 summarize())。\nIn this chapter, we’ve focused on these “whole data frame” tools, but you haven’t yet learned much about what you can do with the individual variable.\n在本章中，我们重点介绍了这些“整个数据框”的工具，但你还没有学到太多关于如何处理单个变量的知识。\nWe’ll return to that in the Transform part of the book, where each chapter provides tools for a specific type of variable.\n我们将在本书的“转换”部分回到这个问题，其中每一章都为特定类型的变量提供了工具。\nIn the next chapter, we’ll pivot back to workflow to discuss the importance of code style and keeping your code well organized to make it easy for you and others to read and understand.\n在下一章中，我们将转回工作流程，讨论代码风格的重要性以及保持代码井井有条，以便你和其他人易于阅读和理解。",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data transformation</span>"
    ]
  },
  {
    "objectID": "data-transform.html#footnotes",
    "href": "data-transform.html#footnotes",
    "title": "3  Data transformation",
    "section": "",
    "text": "Later, you’ll learn about the slice_*() family, which allows you to choose rows based on their positions.↩︎\nRemember that in RStudio, the easiest way to see a dataset with many columns is View().↩︎\nOr summarise(), if you prefer British English.↩︎\n*cough* the law of large numbers *cough*.↩︎",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data transformation</span>"
    ]
  },
  {
    "objectID": "workflow-style.html",
    "href": "workflow-style.html",
    "title": "4  Workflow: code style",
    "section": "",
    "text": "4.1 Names\nGood coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread. Even as a very new programmer, it’s a good idea to work on your code style. Using a consistent style makes it easier for others (including future-you!) to read your work and is particularly important if you need to get help from someone else. This chapter will introduce the most important points of the tidyverse style guide, which is used throughout this book.\n良好的编码风格就像正确的标点符号：没有它也能行，但它确实能让事情更容易阅读。即使你是一个编程新手，养成良好的代码风格也是一个好主意。使用一致的风格可以让他人（包括未来的你！）更容易阅读你的作品，在你需要他人帮助时尤其重要。本章将介绍 tidyverse 风格指南 中最重要的几点，本书通篇都使用了该指南。\nStyling your code will feel a bit tedious to start with, but if you practice it, it will soon become second nature. Additionally, there are some great tools to quickly restyle existing code, like the styler package by Lorenz Walthert. Once you’ve installed it with install.packages(\"styler\"), an easy way to use it is via RStudio’s command palette. The command palette lets you use any built-in RStudio command and many addins provided by packages. Open the palette by pressing Cmd/Ctrl + Shift + P, then type “styler” to see all the shortcuts offered by styler. Figure 4.1 shows the results.\n一开始，你可能会觉得代码风格化有点乏味，但如果你勤加练习，它很快就会成为你的第二天性。此外，还有一些很棒的工具可以快速重塑现有代码的风格，比如 Lorenz Walthert 开发的 styler 包。通过 install.packages(\"styler\") 安装后，一个简便的使用方法是通过 RStudio 的 命令面板 (command palette)。命令面板让你能使用任何 RStudio 内置命令以及包提供的许多插件。按 Cmd/Ctrl + Shift + P 打开面板，然后输入 “styler” 就可以看到 styler 提供的所有快捷方式。Figure 4.1 展示了结果。\nWe’ll use the tidyverse and nycflights13 packages for code examples in this chapter.\n在本章的代码示例中，我们将使用 tidyverse 和 nycflights13 包。\nWe talked briefly about names in Section 2.3. Remember that variable names (those created by &lt;- and those created by mutate()) should use only lowercase letters, numbers, and _. Use _ to separate words within a name.\n我们在 Section 2.3 中简要讨论了命名。请记住，变量名（由 &lt;- 创建的和由 mutate() 创建的）应仅使用小写字母、数字和 _。使用 _ 来分隔名称中的单词。\n# Strive for:\nshort_flights &lt;- flights |&gt; filter(air_time &lt; 60)\n\n# Avoid:\nSHORTFLIGHTS &lt;- flights |&gt; filter(air_time &lt; 60)\nAs a general rule of thumb, it’s better to prefer long, descriptive names that are easy to understand rather than concise names that are fast to type. Short names save relatively little time when writing code (especially since autocomplete will help you finish typing them), but it can be time-consuming when you come back to old code and are forced to puzzle out a cryptic abbreviation.\n根据经验，最好选择易于理解的长描述性名称，而不是输入快的简洁名称。短名称在编写代码时节省的时间相对较少（特别是因为自动补全会帮你完成输入），但当你回头看旧代码并被迫琢磨一个神秘的缩写时，可能会非常耗时。\nIf you have a bunch of names for related things, do your best to be consistent. It’s easy for inconsistencies to arise when you forget a previous convention, so don’t feel bad if you have to go back and rename things. In general, if you have a bunch of variables that are a variation on a theme, you’re better off giving them a common prefix rather than a common suffix because autocomplete works best on the start of a variable.\n如果你有一堆相关事物的名称，请尽量保持一致。当你忘记之前的约定而导致不一致时，这是很容易发生的，所以如果你需要回去重命名，也不必感到难过。总的来说，如果你有一组主题相近的变量，最好给它们一个共同的前缀而不是后缀，因为自动补全在变量开头效果最好。",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Workflow: code style</span>"
    ]
  },
  {
    "objectID": "workflow-style.html#spaces",
    "href": "workflow-style.html#spaces",
    "title": "4  Workflow: code style",
    "section": "\n4.2 Spaces",
    "text": "4.2 Spaces\nPut spaces on either side of mathematical operators apart from ^ (i.e. +, -, ==, &lt;, …), and around the assignment operator (&lt;-).\n在数学运算符（除了 ^，即 +、-、==、&lt; 等）的两侧以及赋值运算符 (&lt;-) 的周围加上空格。\n\n# Strive for\nz &lt;- (a + b)^2 / d\n\n# Avoid\nz&lt;-( a + b ) ^ 2/d\n\nDon’t put spaces inside or outside parentheses for regular function calls. Always put a space after a comma, just like in standard English.\n在常规函数调用的括号内外不要加空格。在逗号后面一定要加一个空格，就像标准英语一样。\n\n# Strive for\nmean(x, na.rm = TRUE)\n\n# Avoid\nmean (x ,na.rm=TRUE)\n\nIt’s OK to add extra spaces if it improves alignment. For example, if you’re creating multiple variables in mutate(), you might want to add spaces so that all the = line up.1 This makes it easier to skim the code.\n如果能改善对齐，可以添加额外的空格。例如，当你在 mutate() 中创建多个变量时，你可能想添加空格以使所有的 = 对齐。1 这能让代码更容易浏览。\n\nflights |&gt; \n  mutate(\n    speed      = distance / air_time,\n    dep_hour   = dep_time %/% 100,\n    dep_minute = dep_time %%  100\n  )",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Workflow: code style</span>"
    ]
  },
  {
    "objectID": "workflow-style.html#sec-pipes",
    "href": "workflow-style.html#sec-pipes",
    "title": "4  Workflow: code style",
    "section": "\n4.3 Pipes",
    "text": "4.3 Pipes\n|&gt; should always have a space before it and should typically be the last thing on a line. This makes it easier to add new steps, rearrange existing steps, modify elements within a step, and get a 10,000 ft view by skimming the verbs on the left-hand side.|&gt; 前面应该总有一个空格，并且通常应该是一行的最后一个元素。这使得添加新步骤、重新排列现有步骤、修改步骤中的元素以及通过浏览左侧的动词来获得宏观视角变得更加容易。\n\n# Strive for \nflights |&gt;  \n  filter(!is.na(arr_delay), !is.na(tailnum)) |&gt; \n  count(dest)\n\n# Avoid\nflights|&gt;filter(!is.na(arr_delay), !is.na(tailnum))|&gt;count(dest)\n\nIf the function you’re piping into has named arguments (like mutate() or summarize()), put each argument on a new line. If the function doesn’t have named arguments (like select() or filter()), keep everything on one line unless it doesn’t fit, in which case you should put each argument on its own line.\n如果你正在管道连接的函数有命名参数（如 mutate() 或 summarize()），请将每个参数放在新的一行。如果函数没有命名参数（如 select() 或 filter()），则将所有内容保持在一行，除非一行写不下，这种情况下你应该将每个参数放在单独的一行。\n\n# Strive for\nflights |&gt;  \n  group_by(tailnum) |&gt; \n  summarize(\n    delay = mean(arr_delay, na.rm = TRUE),\n    n = n()\n  )\n\n# Avoid\nflights |&gt;\n  group_by(\n    tailnum\n  ) |&gt; \n  summarize(delay = mean(arr_delay, na.rm = TRUE), n = n())\n\nAfter the first step of the pipeline, indent each line by two spaces. RStudio will automatically put the spaces in for you after a line break following a |&gt; . If you’re putting each argument on its own line, indent by an extra two spaces. Make sure ) is on its own line, and un-indented to match the horizontal position of the function name.\n在管道的第一步之后，每行缩进两个空格。在 |&gt; 后面换行时，RStudio 会自动为你添加空格。如果你将每个参数放在单独的一行，则额外缩进两个空格。确保 ) 单独占一行，并且不缩进，以与函数名的水平位置对齐。\n\n# Strive for \nflights |&gt;  \n  group_by(tailnum) |&gt; \n  summarize(\n    delay = mean(arr_delay, na.rm = TRUE),\n    n = n()\n  )\n\n# Avoid\nflights|&gt;\n  group_by(tailnum) |&gt; \n  summarize(\n             delay = mean(arr_delay, na.rm = TRUE), \n             n = n()\n           )\n\n# Avoid\nflights|&gt;\n  group_by(tailnum) |&gt; \n  summarize(\n  delay = mean(arr_delay, na.rm = TRUE), \n  n = n()\n  )\n\nIt’s OK to shirk some of these rules if your pipeline fits easily on one line. But in our collective experience, it’s common for short snippets to grow longer, so you’ll usually save time in the long run by starting with all the vertical space you need.\n如果你的管道可以轻松地放在一行上，可以不遵守这些规则。但根据我们的集体经验，短代码片段通常会变得越来越长，所以从一开始就使用所需的垂直空间，从长远来看通常会节省时间。\n\n# This fits compactly on one line\ndf |&gt; mutate(y = x + 1)\n\n# While this takes up 4x as many lines, it's easily extended to \n# more variables and more steps in the future\ndf |&gt; \n  mutate(\n    y = x + 1\n  )\n\nFinally, be wary of writing very long pipes, say longer than 10-15 lines. Try to break them up into smaller sub-tasks, giving each task an informative name. The names will help cue the reader into what’s happening and makes it easier to check that intermediate results are as expected. Whenever you can give something an informative name, you should give it an informative name, for example when you fundamentally change the structure of the data, e.g., after pivoting or summarizing. Don’t expect to get it right the first time! This means breaking up long pipelines if there are intermediate states that can get good names.\n最后，要警惕编写过长的管道，比如超过 10-15 行。试着将它们分解成更小的子任务，并给每个任务一个信息丰富的名称。这些名称将有助于提示读者正在发生什么，并使其更容易检查中间结果是否符合预期。只要你能给某个东西一个信息丰富的名称，你就应该这样做，例如，当你从根本上改变数据结构时（例如，在数据透视或汇总之后）。不要期望第一次就做对！这意味着如果存在可以获得好名称的中间状态，就应该拆分长管道。",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Workflow: code style</span>"
    ]
  },
  {
    "objectID": "workflow-style.html#ggplot2",
    "href": "workflow-style.html#ggplot2",
    "title": "4  Workflow: code style",
    "section": "\n4.4 ggplot2",
    "text": "4.4 ggplot2\nThe same basic rules that apply to the pipe also apply to ggplot2; just treat + the same way as |&gt;.\n适用于管道的基本规则同样适用于 ggplot2；只需将 + 视为 |&gt; 即可。\n\nflights |&gt; \n  group_by(month) |&gt; \n  summarize(\n    delay = mean(arr_delay, na.rm = TRUE)\n  ) |&gt; \n  ggplot(aes(x = month, y = delay)) +\n  geom_point() + \n  geom_line()\n\nAgain, if you can’t fit all of the arguments to a function on to a single line, put each argument on its own line:\n同样，如果一个函数的所有参数无法放在一行内，就将每个参数单独放在一行：\n\nflights |&gt; \n  group_by(dest) |&gt; \n  summarize(\n    distance = mean(distance),\n    speed = mean(distance / air_time, na.rm = TRUE)\n  ) |&gt; \n  ggplot(aes(x = distance, y = speed)) +\n  geom_smooth(\n    method = \"loess\",\n    span = 0.5,\n    se = FALSE, \n    color = \"white\", \n    linewidth = 4\n  ) +\n  geom_point()\n\nWatch for the transition from |&gt; to +. We wish this transition wasn’t necessary, but unfortunately, ggplot2 was written before the pipe was discovered.\n注意从 |&gt; 到 + 的转换。我们希望这种转换是不必要的，但遗憾的是，ggplot2 是在管道被发现之前编写的。",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Workflow: code style</span>"
    ]
  },
  {
    "objectID": "workflow-style.html#sectioning-comments",
    "href": "workflow-style.html#sectioning-comments",
    "title": "4  Workflow: code style",
    "section": "\n4.5 Sectioning comments",
    "text": "4.5 Sectioning comments\nAs your scripts get longer, you can use sectioning comments to break up your file into manageable pieces:\n当你的脚本越来越长时，你可以使用 分节 注释将文件分解成易于管理的部分：\n\n# Load data --------------------------------------\n\n# Plot data --------------------------------------\n\nRStudio provides a keyboard shortcut to create these headers (Cmd/Ctrl + Shift + R), and will display them in the code navigation drop-down at the bottom-left of the editor, as shown in Figure 4.2.\nRStudio 提供了创建这些标题的键盘快捷键 (Cmd/Ctrl + Shift + R)，并会在编辑器左下角的代码导航下拉菜单中显示它们，如 Figure 4.2 所示。\n\n\n\n\n\n\n\nFigure 4.2: After adding sectioning comments to your script, you can easily navigate to them using the code navigation tool in the bottom-left of the script editor.",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Workflow: code style</span>"
    ]
  },
  {
    "objectID": "workflow-style.html#exercises",
    "href": "workflow-style.html#exercises",
    "title": "4  Workflow: code style",
    "section": "\n4.6 Exercises",
    "text": "4.6 Exercises\n\n\nRestyle the following pipelines following the guidelines above.\n\nflights|&gt;filter(dest==\"IAH\")|&gt;group_by(year,month,day)|&gt;summarize(n=n(),\ndelay=mean(arr_delay,na.rm=TRUE))|&gt;filter(n&gt;10)\n\nflights|&gt;filter(carrier==\"UA\",dest%in%c(\"IAH\",\"HOU\"),sched_dep_time&gt;\n0900,sched_arr_time&lt;2000)|&gt;group_by(flight)|&gt;summarize(delay=mean(\narr_delay,na.rm=TRUE),cancelled=sum(is.na(arr_delay)),n=n())|&gt;filter(n&gt;10)",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Workflow: code style</span>"
    ]
  },
  {
    "objectID": "workflow-style.html#summary",
    "href": "workflow-style.html#summary",
    "title": "4  Workflow: code style",
    "section": "\n4.7 Summary",
    "text": "4.7 Summary\nIn this chapter, you’ve learned the most important principles of code style. These may feel like a set of arbitrary rules to start with (because they are!) but over time, as you write more code, and share code with more people, you’ll see how important a consistent style is. And don’t forget about the styler package: it’s a great way to quickly improve the quality of poorly styled code.\n在本章中，你学习了代码风格最重要的原则。一开始，这些可能感觉像是一套武断的规则（因为它们确实是！），但随着时间的推移，当你编写更多代码，并与更多人共享代码时，你会看到一致的风格是多么重要。别忘了 styler 包：它是一个快速提高差风格代码质量的好方法。\nIn the next chapter, we switch back to data science tools, learning about tidy data. Tidy data is a consistent way of organizing your data frames that is used throughout the tidyverse. This consistency makes your life easier because once you have tidy data, it just works with the vast majority of tidyverse functions. Of course, life is never easy, and most datasets you encounter in the wild will not already be tidy. So we’ll also teach you how to use the tidyr package to tidy your untidy data.\n在下一章中，我们将回到数据科学工具，学习整洁数据 (tidy data)。整洁数据是组织数据框的一种一致方式，在整个 tidyverse 中都在使用。这种一致性使你的工作更轻松，因为一旦你拥有了整洁数据，它就能与绝大多数 tidyverse 函数协同工作。当然，生活从不轻松，你在野外遇到的大多数数据集都不会是整洁的。因此，我们还将教你如何使用 tidyr 包来整理你的非整洁数据。",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Workflow: code style</span>"
    ]
  },
  {
    "objectID": "workflow-style.html#footnotes",
    "href": "workflow-style.html#footnotes",
    "title": "4  Workflow: code style",
    "section": "",
    "text": "Since dep_time is in HMM or HHMM format, we use integer division (%/%) to get hour and remainder (also known as modulo, %%) to get minute.↩︎",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Workflow: code style</span>"
    ]
  },
  {
    "objectID": "data-tidy.html",
    "href": "data-tidy.html",
    "title": "5  Data tidying",
    "section": "",
    "text": "5.1 Introduction\nIn this chapter, you will learn a consistent way to organize your data in R using a system called tidy data. Getting your data into this format requires some work up front, but that work pays off in the long term. Once you have tidy data and the tidy tools provided by packages in the tidyverse, you will spend much less time munging data from one representation to another, allowing you to spend more time on the data questions you care about.\n在本章中，你将学习一种在 R 中组织数据的一致性方法，这个系统被称为整洁数据 (tidy data)。 将数据整理成这种格式需要一些前期工作，但从长远来看，这些工作是值得的。 一旦你拥有了整洁的数据以及 tidyverse 中各个包提供的整洁工具，你将花费更少的时间在不同数据表示形式之间进行转换，从而可以投入更多时间来关注你所关心的数据问题。\nIn this chapter, you’ll first learn the definition of tidy data and see it applied to a simple toy dataset. Then we’ll dive into the primary tool you’ll use for tidying data: pivoting. Pivoting allows you to change the form of your data without changing any of the values.\n在本章中，你将首先学习整洁数据的定义，并看到它如何应用于一个简单的示例数据集。 然后，我们将深入探讨用于整理数据的主要工具：透视 (pivoting)。 透视允许你在不改变任何值的情况下改变数据的形态。",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data tidying</span>"
    ]
  },
  {
    "objectID": "data-tidy.html#introduction",
    "href": "data-tidy.html#introduction",
    "title": "5  Data tidying",
    "section": "",
    "text": "“Happy families are all alike; every unhappy family is unhappy in its own way.”\n— Leo Tolstoy\n“幸福的家庭都是相似的；不幸的家庭各有各的不幸。”\n— 列夫·托尔斯泰\n\n\n“Tidy datasets are all alike, but every messy dataset is messy in its own way.”\n— Hadley Wickham\n“整洁的数据集都是相似的，但每个凌乱的数据集各有各的凌乱之处。”\n— Hadley Wickham\n\n\n\n\n5.1.1 Prerequisites\nIn this chapter, we’ll focus on tidyr, a package that provides a bunch of tools to help tidy up your messy datasets. tidyr is a member of the core tidyverse.\n在本章中，我们将重点介绍 tidyr，这个包提供了一系列工具来帮助你整理凌乱的数据集。 tidyr 是核心 tidyverse 的成员之一。\n\nlibrary(tidyverse)\n\nFrom this chapter on, we’ll suppress the loading message from library(tidyverse).\n从本章开始，我们将抑制 library(tidyverse) 的加载消息。",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data tidying</span>"
    ]
  },
  {
    "objectID": "data-tidy.html#sec-tidy-data",
    "href": "data-tidy.html#sec-tidy-data",
    "title": "5  Data tidying",
    "section": "\n5.2 Tidy data",
    "text": "5.2 Tidy data\nYou can represent the same underlying data in multiple ways. The example below shows the same data organized in three different ways. Each dataset shows the same values of four variables: country, year, population, and number of documented cases of TB (tuberculosis), but each dataset organizes the values in a different way.\n你可以用多种方式表示相同的基础数据。 下面的示例展示了以三种不同方式组织的相同数据。 每个数据集都显示了四个变量的相同值：country (国家)、year (年份)、population (人口) 和记录在案的 cases (结核病病例数)，但每个数据集以不同的方式组织这些值。\n\ntable1\n#&gt; # A tibble: 6 × 4\n#&gt;   country      year  cases population\n#&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1 Afghanistan  1999    745   19987071\n#&gt; 2 Afghanistan  2000   2666   20595360\n#&gt; 3 Brazil       1999  37737  172006362\n#&gt; 4 Brazil       2000  80488  174504898\n#&gt; 5 China        1999 212258 1272915272\n#&gt; 6 China        2000 213766 1280428583\n\ntable2\n#&gt; # A tibble: 12 × 4\n#&gt;   country      year type           count\n#&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 Afghanistan  1999 cases            745\n#&gt; 2 Afghanistan  1999 population  19987071\n#&gt; 3 Afghanistan  2000 cases           2666\n#&gt; 4 Afghanistan  2000 population  20595360\n#&gt; 5 Brazil       1999 cases          37737\n#&gt; 6 Brazil       1999 population 172006362\n#&gt; # ℹ 6 more rows\n\ntable3\n#&gt; # A tibble: 6 × 3\n#&gt;   country      year rate             \n#&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n#&gt; 1 Afghanistan  1999 745/19987071     \n#&gt; 2 Afghanistan  2000 2666/20595360    \n#&gt; 3 Brazil       1999 37737/172006362  \n#&gt; 4 Brazil       2000 80488/174504898  \n#&gt; 5 China        1999 212258/1272915272\n#&gt; 6 China        2000 213766/1280428583\n\nThese are all representations of the same underlying data, but they are not equally easy to use. One of them, table1, will be much easier to work with inside the tidyverse because it’s tidy.\n这些都是相同基础数据的不同表示形式，但它们的使用便利性并不相同。 其中之一，table1，在 tidyverse 中使用起来会容易得多，因为它是整洁的。\nThere are three interrelated rules that make a dataset tidy:\n有三条相互关联的规则可以使数据集变得整洁：\n\nEach variable is a column; each column is a variable.\nEach observation is a row; each row is an observation.\nEach value is a cell; each cell is a single value.\n每个变量是一列；每列是一个变量。\n每个观测是一行；每行是一个观测。\n每个值是一个单元格；每个单元格是一个值。\n\nFigure 5.1 shows the rules visually.Figure 5.1 直观地展示了这些规则。\n\n\n\n\n\n\n\nFigure 5.1: The following three rules make a dataset tidy: variables are columns, observations are rows, and values are cells.\n\n\n\n\nWhy ensure that your data is tidy? There are two main advantages:\n为什么要确保你的数据是整洁的？ 主要有两个优点：\n\nThere’s a general advantage to picking one consistent way of storing data. If you have a consistent data structure, it’s easier to learn the tools that work with it because they have an underlying uniformity.\nThere’s a specific advantage to placing variables in columns because it allows R’s vectorized nature to shine. As you learned in Section 3.3.1 and Section 3.5.2, most built-in R functions work with vectors of values. That makes transforming tidy data feel particularly natural.\n选择一种一致的数据存储方式具有普遍的优势。 如果你有一个一致的数据结构，学习与之配合的工具会更容易，因为它们具有内在的一致性。\n将变量放在列中有一个特定的优势，因为这能让 R 的向量化特性大放异彩。 正如你在 Section 3.3.1 和 Section 3.5.2 中学到的，大多数内置的 R 函数都处理值的向量。 这使得转换整洁的数据感觉特别自然。\n\ndplyr, ggplot2, and all the other packages in the tidyverse are designed to work with tidy data. Here are a few small examples showing how you might work with table1.\ndplyr、ggplot2 以及 tidyverse 中的所有其他包都是为处理整洁数据而设计的。 这里有几个小例子，展示了你如何使用 table1。\n\n# Compute rate per 10,000\ntable1 |&gt;\n  mutate(rate = cases / population * 10000)\n#&gt; # A tibble: 6 × 5\n#&gt;   country      year  cases population  rate\n#&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Afghanistan  1999    745   19987071 0.373\n#&gt; 2 Afghanistan  2000   2666   20595360 1.29 \n#&gt; 3 Brazil       1999  37737  172006362 2.19 \n#&gt; 4 Brazil       2000  80488  174504898 4.61 \n#&gt; 5 China        1999 212258 1272915272 1.67 \n#&gt; 6 China        2000 213766 1280428583 1.67\n\n# Compute total cases per year\ntable1 |&gt; \n  group_by(year) |&gt; \n  summarize(total_cases = sum(cases))\n#&gt; # A tibble: 2 × 2\n#&gt;    year total_cases\n#&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1  1999      250740\n#&gt; 2  2000      296920\n\n# Visualize changes over time\nggplot(table1, aes(x = year, y = cases)) +\n  geom_line(aes(group = country), color = \"grey50\") +\n  geom_point(aes(color = country, shape = country)) +\n  scale_x_continuous(breaks = c(1999, 2000)) # x-axis breaks at 1999 and 2000\n\n\n\n\n\n\n\n\n5.2.1 Exercises\n\nFor each of the sample tables, describe what each observation and each column represents.\n\nSketch out the process you’d use to calculate the rate for table2 and table3. You will need to perform four operations:\n\nExtract the number of TB cases per country per year.\nExtract the matching population per country per year.\nDivide cases by population, and multiply by 10000.\nStore back in the appropriate place.\n\nYou haven’t yet learned all the functions you’d need to actually perform these operations, but you should still be able to think through the transformations you’d need.",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data tidying</span>"
    ]
  },
  {
    "objectID": "data-tidy.html#sec-pivoting",
    "href": "data-tidy.html#sec-pivoting",
    "title": "5  Data tidying",
    "section": "\n5.3 Lengthening data",
    "text": "5.3 Lengthening data\nThe principles of tidy data might seem so obvious that you wonder if you’ll ever encounter a dataset that isn’t tidy. Unfortunately, however, most real data is untidy. There are two main reasons:\n整洁数据的原则可能看起来如此显而易见，以至于你可能会怀疑是否会遇到不整洁的数据集。 然而，不幸的是，大多数真实数据都是不整洁的。 主要有两个原因：\n\nData is often organized to facilitate some goal other than analysis. For example, it’s common for data to be structured to make data entry, not analysis, easy. 数据的组织方式通常是为了促进除分析之外的其他目标。 例如，为了方便数据录入而非分析而构建数据结构是很常见的。\nMost people aren’t familiar with the principles of tidy data, and it’s hard to derive them yourself unless you spend a lot of time working with data.\n大多数人并不熟悉整洁数据的原则，除非你花大量时间处理数据，否则很难自己推导出这些原则。\n\nThis means that most real analyses will require at least a little tidying. You’ll begin by figuring out what the underlying variables and observations are. Sometimes this is easy; other times you’ll need to consult with the people who originally generated the data. Next, you’ll pivot your data into a tidy form, with variables in the columns and observations in the rows.\n这意味着大多数实际分析至少需要一些整理工作。 你将首先弄清楚潜在的变量和观测值是什么。 有时这很简单；其他时候你可能需要咨询最初生成数据的人。 接下来，你将透视 (pivot) 你的数据，使其成为变量在列、观测在行的整洁形式。\ntidyr provides two functions for pivoting data: pivot_longer() and pivot_wider(). We’ll first start with pivot_longer() because it’s the most common case. Let’s dive into some examples.\ntidyr 提供了两个用于数据透视的函数：pivot_longer() 和 pivot_wider()。 我们将首先从 pivot_longer() 开始，因为这是最常见的情况。 让我们来看一些例子。\n\n5.3.1 Data in column names\nThe billboard dataset records the billboard rank of songs in the year 2000:billboard 数据集记录了 2000 年歌曲的 billboard 排行榜排名：\n\nbillboard\n#&gt; # A tibble: 317 × 79\n#&gt;   artist       track               date.entered   wk1   wk2   wk3   wk4   wk5\n#&gt;   &lt;chr&gt;        &lt;chr&gt;               &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 2 Pac        Baby Don't Cry (Ke… 2000-02-26      87    82    72    77    87\n#&gt; 2 2Ge+her      The Hardest Part O… 2000-09-02      91    87    92    NA    NA\n#&gt; 3 3 Doors Down Kryptonite          2000-04-08      81    70    68    67    66\n#&gt; 4 3 Doors Down Loser               2000-10-21      76    76    72    69    67\n#&gt; 5 504 Boyz     Wobble Wobble       2000-04-15      57    34    25    17    17\n#&gt; 6 98^0         Give Me Just One N… 2000-08-19      51    39    34    26    26\n#&gt; # ℹ 311 more rows\n#&gt; # ℹ 71 more variables: wk6 &lt;dbl&gt;, wk7 &lt;dbl&gt;, wk8 &lt;dbl&gt;, wk9 &lt;dbl&gt;, …\n\nIn this dataset, each observation is a song. The first three columns (artist, track and date.entered) are variables that describe the song. Then we have 76 columns (wk1-wk76) that describe the rank of the song in each week1. Here, the column names are one variable (the week) and the cell values are another (the rank).\n在这个数据集中，每个观测是一首歌曲。 前三列（artist、track 和 date.entered）是描述歌曲的变量。 然后我们有 76 列（wk1 到 wk76），描述了歌曲在每周的排名1。 在这里，列名是一个变量（week），而单元格的值是另一个变量（rank）。\nTo tidy this data, we’ll use pivot_longer():\n为了整理这个数据，我们将使用 pivot_longer()：\n\nbillboard |&gt; \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\"\n  )\n#&gt; # A tibble: 24,092 × 5\n#&gt;    artist track                   date.entered week   rank\n#&gt;    &lt;chr&gt;  &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt; &lt;dbl&gt;\n#&gt;  1 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk1      87\n#&gt;  2 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk2      82\n#&gt;  3 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk3      72\n#&gt;  4 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk4      77\n#&gt;  5 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk5      87\n#&gt;  6 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk6      94\n#&gt;  7 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk7      99\n#&gt;  8 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk8      NA\n#&gt;  9 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk9      NA\n#&gt; 10 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk10     NA\n#&gt; # ℹ 24,082 more rows\n\nAfter the data, there are three key arguments:\n在数据之后，有三个关键参数：\n\ncols specifies which columns need to be pivoted, i.e. which columns aren’t variables. This argument uses the same syntax as select() so here we could use !c(artist, track, date.entered) or starts_with(\"wk\").cols 指定哪些列需要进行透视，即哪些列不是变量。此参数使用与 select() 相同的语法，所以这里我们可以用 !c(artist, track, date.entered) 或 starts_with(\"wk\")。\nnames_to names the variable stored in the column names, we named that variable week. names_to 为存储在列名中的变量命名，我们将其命名为 week。\nvalues_to names the variable stored in the cell values, we named that variable rank.values_to 为存储在单元格值中的变量命名，我们将其命名为 rank。\n\nNote that in the code \"week\" and \"rank\" are quoted because those are new variables we’re creating, they don’t yet exist in the data when we run the pivot_longer() call.\n请注意，在代码中 \"week\" 和 \"rank\" 是带引号的，因为它们是我们正在创建的新变量，在我们运行 pivot_longer() 调用时，它们还不存在于数据中。\nNow let’s turn our attention to the resulting, longer data frame. What happens if a song is in the top 100 for less than 76 weeks? Take 2 Pac’s “Baby Don’t Cry”, for example. The above output suggests that it was only in the top 100 for 7 weeks, and all the remaining weeks are filled in with missing values. These NAs don’t really represent unknown observations; they were forced to exist by the structure of the dataset2, so we can ask pivot_longer() to get rid of them by setting values_drop_na = TRUE:\n现在让我们把注意力转向生成的更长的数据框。 如果一首歌进入前 100 名的时间少于 76 周会发生什么？ 以 2 Pac 的 “Baby Don’t Cry” 为例。 上面的输出表明它只在前 100 名中待了 7 周，而所有剩余的周数都被填充了缺失值。 这些 NA 并不真正代表未知的观测值；它们是由于数据集的结构而被强制存在的2，所以我们可以通过设置 values_drop_na = TRUE 来让 pivot_longer() 移除它们：\n\nbillboard |&gt; \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\",\n    values_drop_na = TRUE\n  )\n#&gt; # A tibble: 5,307 × 5\n#&gt;   artist track                   date.entered week   rank\n#&gt;   &lt;chr&gt;  &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt; &lt;dbl&gt;\n#&gt; 1 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk1      87\n#&gt; 2 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk2      82\n#&gt; 3 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk3      72\n#&gt; 4 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk4      77\n#&gt; 5 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk5      87\n#&gt; 6 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk6      94\n#&gt; # ℹ 5,301 more rows\n\nThe number of rows is now much lower, indicating that many rows with NAs were dropped.\n现在行数大大减少了，这表明许多带有 NA 的行被删除了。\nYou might also wonder what happens if a song is in the top 100 for more than 76 weeks? We can’t tell from this data, but you might guess that additional columns wk77, wk78, … would be added to the dataset.\n你可能还会想，如果一首歌在前 100 名中停留超过 76 周会发生什么？ 我们无法从这个数据中得知，但你可能会猜测数据集中会添加额外的列 wk77、wk78 等。\nThis data is now tidy, but we could make future computation a bit easier by converting values of week from character strings to numbers using mutate() and readr::parse_number(). parse_number() is a handy function that will extract the first number from a string, ignoring all other text.\n这个数据现在是整洁的，但我们可以通过使用 mutate() 和 readr::parse_number() 将 week 的值从字符串转换为数字，来使未来的计算更容易一些。 parse_number() 是一个方便的函数，它会从字符串中提取第一个数字，并忽略所有其他文本。\n\nbillboard_longer &lt;- billboard |&gt; \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\",\n    values_drop_na = TRUE\n  ) |&gt; \n  mutate(\n    week = parse_number(week)\n  )\nbillboard_longer\n#&gt; # A tibble: 5,307 × 5\n#&gt;   artist track                   date.entered  week  rank\n#&gt;   &lt;chr&gt;  &lt;chr&gt;                   &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 2 Pac  Baby Don't Cry (Keep... 2000-02-26       1    87\n#&gt; 2 2 Pac  Baby Don't Cry (Keep... 2000-02-26       2    82\n#&gt; 3 2 Pac  Baby Don't Cry (Keep... 2000-02-26       3    72\n#&gt; 4 2 Pac  Baby Don't Cry (Keep... 2000-02-26       4    77\n#&gt; 5 2 Pac  Baby Don't Cry (Keep... 2000-02-26       5    87\n#&gt; 6 2 Pac  Baby Don't Cry (Keep... 2000-02-26       6    94\n#&gt; # ℹ 5,301 more rows\n\nNow that we have all the week numbers in one variable and all the rank values in another, we’re in a good position to visualize how song ranks vary over time. The code is shown below and the result is in Figure 5.2. We can see that very few songs stay in the top 100 for more than 20 weeks.\n现在我们把所有的周数放在一个变量中，所有的排名值放在另一个变量中，这为我们可视化歌曲排名随时间变化的情况创造了良好条件。 代码如下所示，结果见 Figure 5.2。 我们可以看到，很少有歌曲能停留在前 100 名超过 20 周。\n\nbillboard_longer |&gt; \n  ggplot(aes(x = week, y = rank, group = track)) + \n  geom_line(alpha = 0.25) + \n  scale_y_reverse()\n\n\n\n\n\n\nFigure 5.2: A line plot showing how the rank of a song changes over time.\n\n\n\n\n\n5.3.2 How does pivoting work?\nNow that you’ve seen how we can use pivoting to reshape our data, let’s take a little time to gain some intuition about what pivoting does to the data. Let’s start with a very simple dataset to make it easier to see what’s happening. Suppose we have three patients with ids A, B, and C, and we take two blood pressure measurements on each patient. We’ll create the data with tribble(), a handy function for constructing small tibbles by hand:\n既然你已经看到了如何使用透视来重塑数据，让我们花点时间来直观地了解一下透视对数据的作用。 让我们从一个非常简单的数据集开始，以便更容易地看清发生了什么。 假设我们有三个病人，他们的 id 分别是 A、B 和 C，我们对每个病人进行了两次血压测量。 我们将使用 tribble() 创建数据，这是一个方便手动构建小型 tibble 的函数：\n\ndf &lt;- tribble(\n  ~id,  ~bp1, ~bp2,\n   \"A\",  100,  120,\n   \"B\",  140,  115,\n   \"C\",  120,  125\n)\n\nWe want our new dataset to have three variables: id (already exists), measurement (the column names), and value (the cell values). To achieve this, we need to pivot df longer:\n我们希望新数据集有三个变量：id（已存在）、measurement（列名）和 value（单元格值）。 为了实现这一点，我们需要将 df 透视得更长：\n\ndf |&gt; \n  pivot_longer(\n    cols = bp1:bp2,\n    names_to = \"measurement\",\n    values_to = \"value\"\n  )\n#&gt; # A tibble: 6 × 3\n#&gt;   id    measurement value\n#&gt;   &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;\n#&gt; 1 A     bp1           100\n#&gt; 2 A     bp2           120\n#&gt; 3 B     bp1           140\n#&gt; 4 B     bp2           115\n#&gt; 5 C     bp1           120\n#&gt; 6 C     bp2           125\n\nHow does the reshaping work? It’s easier to see if we think about it column by column. As shown in Figure 5.3, the values in a column that was already a variable in the original dataset (id) need to be repeated, once for each column that is pivoted.\n重塑是如何工作的？ 如果我们逐列思考，会更容易理解。 如 Figure 5.3 所示，原始数据集中已经是变量的列（id）中的值需要重复，每个被透视的列重复一次。\n\n\n\n\n\n\n\nFigure 5.3: Columns that are already variables need to be repeated, once for each column that is pivoted.\n\n\n\n\nThe column names become values in a new variable, whose name is defined by names_to, as shown in Figure 5.4. They need to be repeated once for each row in the original dataset.\n列名成为一个新变量中的值，该新变量的名称由 names_to 定义，如 Figure 5.4 所示。 它们需要为原始数据集中的每一行重复一次。\n\n\n\n\n\n\n\nFigure 5.4: The column names of pivoted columns become values in a new column. The values need to be repeated once for each row of the original dataset.\n\n\n\n\nThe cell values also become values in a new variable, with a name defined by values_to. They are unwound row by row. Figure 5.5 illustrates the process.\n单元格的值也成为一个新变量中的值，其名称由 values_to 定义。 它们是逐行展开的。 Figure 5.5 说明了这个过程。\n\n\n\n\n\n\n\nFigure 5.5: The number of values is preserved (not repeated), but unwound row-by-row.\n\n\n\n\n\n5.3.3 Many variables in column names\nA more challenging situation occurs when you have multiple pieces of information crammed into the column names, and you would like to store these in separate new variables. For example, take the who2 dataset, the source of table1 and friends that you saw above:\n一个更具挑战性的情况是，当你有多条信息挤在列名中，而你希望将它们存储在不同的新变量中时。 例如，以 who2 数据集为例，这是你之前看到的 table1 及相关表格的来源：\n\nwho2\n#&gt; # A tibble: 7,240 × 58\n#&gt;   country      year sp_m_014 sp_m_1524 sp_m_2534 sp_m_3544 sp_m_4554\n#&gt;   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 Afghanistan  1980       NA        NA        NA        NA        NA\n#&gt; 2 Afghanistan  1981       NA        NA        NA        NA        NA\n#&gt; 3 Afghanistan  1982       NA        NA        NA        NA        NA\n#&gt; 4 Afghanistan  1983       NA        NA        NA        NA        NA\n#&gt; 5 Afghanistan  1984       NA        NA        NA        NA        NA\n#&gt; 6 Afghanistan  1985       NA        NA        NA        NA        NA\n#&gt; # ℹ 7,234 more rows\n#&gt; # ℹ 51 more variables: sp_m_5564 &lt;dbl&gt;, sp_m_65 &lt;dbl&gt;, sp_f_014 &lt;dbl&gt;, …\n\nThis dataset, collected by the World Health Organisation, records information about tuberculosis diagnoses. There are two columns that are already variables and are easy to interpret: country and year. They are followed by 56 columns like sp_m_014, ep_m_4554, and rel_m_3544. If you stare at these columns for long enough, you’ll notice there’s a pattern. Each column name is made up of three pieces separated by _. The first piece, sp/rel/ep, describes the method used for the diagnosis, the second piece, m/f is the gender (coded as a binary variable in this dataset), and the third piece, 014/1524/2534/3544/4554/5564/65 is the age range (014 represents 0-14, for example).\n这个由世界卫生组织收集的数据集记录了关于结核病诊断的信息。 有两列已经是变量且易于解释：country 和 year。 紧随其后的是 56 个类似 sp_m_014、ep_m_4554 和 rel_m_3544 的列。 如果你盯着这些列足够长的时间，你会注意到一个模式。 每个列名由三个部分组成，用 _ 分隔。 第一部分 sp/rel/ep 描述了诊断方法，第二部分 m/f 是 gender（性别，在此数据集中编码为二元变量），第三部分 014/1524/2534/3544/4554/5564/65 是 age（年龄）范围（例如，014 代表 0-14 岁）。\nSo in this case we have six pieces of information recorded in who2: the country and the year (already columns); the method of diagnosis, the gender category, and the age range category (contained in the other column names); and the count of patients in that category (cell values). To organize these six pieces of information in six separate columns, we use pivot_longer() with a vector of column names for names_to and instructors for splitting the original variable names into pieces for names_sep as well as a column name for values_to:\n因此，在这种情况下，我们在 who2 中记录了六条信息：国家和年份（已是列）；诊断方法、性别类别和年龄范围类别（包含在其他列名中）；以及该类别中的患者计数（单元格值）。 为了将这六条信息组织在六个独立的列中，我们使用 pivot_longer()，为 names_to 提供一个列名向量，为 names_sep 提供将原始变量名拆分成片段的指令，并为 values_to 提供一个列名：\n\nwho2 |&gt; \n  pivot_longer(\n    cols = !(country:year),\n    names_to = c(\"diagnosis\", \"gender\", \"age\"), \n    names_sep = \"_\",\n    values_to = \"count\"\n  )\n#&gt; # A tibble: 405,440 × 6\n#&gt;   country      year diagnosis gender age   count\n#&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n#&gt; 1 Afghanistan  1980 sp        m      014      NA\n#&gt; 2 Afghanistan  1980 sp        m      1524     NA\n#&gt; 3 Afghanistan  1980 sp        m      2534     NA\n#&gt; 4 Afghanistan  1980 sp        m      3544     NA\n#&gt; 5 Afghanistan  1980 sp        m      4554     NA\n#&gt; 6 Afghanistan  1980 sp        m      5564     NA\n#&gt; # ℹ 405,434 more rows\n\nAn alternative to names_sep is names_pattern, which you can use to extract variables from more complicated naming scenarios, once you’ve learned about regular expressions in Chapter 15.names_sep 的一个替代方案是 names_pattern，在你学习了 Chapter 15 中的正则表达式后，可以用它从更复杂的命名场景中提取变量。\nConceptually, this is only a minor variation on the simpler case you’ve already seen. Figure 5.6 shows the basic idea: now, instead of the column names pivoting into a single column, they pivot into multiple columns. You can imagine this happening in two steps (first pivoting and then separating) but under the hood it happens in a single step because that’s faster.\n从概念上讲，这只是你已经看过的简单情况的一个微小变体。 Figure 5.6 展示了基本思想：现在，列名不是透视到单个列中，而是透视到多个列中。 你可以想象这分两步发生（先透视再分离），但在底层它是一步完成的，因为这样更快。\n\n\n\n\n\n\n\nFigure 5.6: Pivoting columns with multiple pieces of information in the names means that each column name now fills in values in multiple output columns.\n\n\n\n\n\n5.3.4 Data and variable names in the column headers\nThe next step up in complexity is when the column names include a mix of variable values and variable names. For example, take the household dataset:\n下一个复杂程度的提升是当列名中混合了变量值和变量名。 例如，以 household 数据集为例：\n\nhousehold\n#&gt; # A tibble: 5 × 5\n#&gt;   family dob_child1 dob_child2 name_child1 name_child2\n#&gt;    &lt;int&gt; &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;       &lt;chr&gt;      \n#&gt; 1      1 1998-11-26 2000-01-29 Susan       Jose       \n#&gt; 2      2 1996-06-22 NA         Mark        &lt;NA&gt;       \n#&gt; 3      3 2002-07-11 2004-04-05 Sam         Seth       \n#&gt; 4      4 2004-10-10 2009-08-27 Craig       Khai       \n#&gt; 5      5 2000-12-05 2005-02-28 Parker      Gracie\n\nThis dataset contains data about five families, with the names and dates of birth of up to two children. The new challenge in this dataset is that the column names contain the names of two variables (dob, name) and the values of another (child, with values 1 or 2). To solve this problem we again need to supply a vector to names_to but this time we use the special \".value\" sentinel; this isn’t the name of a variable but a unique value that tells pivot_longer() to do something different. This overrides the usual values_to argument to use the first component of the pivoted column name as a variable name in the output.\n该数据集包含五个家庭的数据，其中包含最多两个孩子的姓名和出生日期。 这个数据集中的新挑战是，列名包含了两个变量的名称（dob、name）和另一个变量的值（child，值为 1 或 2）。 要解决这个问题，我们再次需要向 names_to 提供一个向量，但这次我们使用特殊的 \".value\" 标记；这不是一个变量名，而是一个特殊的值，告诉 pivot_longer() 做一些不同的事情。 这会覆盖通常的 values_to 参数，转而使用被透视列名的第一个组成部分作为输出中的变量名。\n\nhousehold |&gt; \n  pivot_longer(\n    cols = !family, \n    names_to = c(\".value\", \"child\"), \n    names_sep = \"_\", \n    values_drop_na = TRUE\n  )\n#&gt; # A tibble: 9 × 4\n#&gt;   family child  dob        name \n#&gt;    &lt;int&gt; &lt;chr&gt;  &lt;date&gt;     &lt;chr&gt;\n#&gt; 1      1 child1 1998-11-26 Susan\n#&gt; 2      1 child2 2000-01-29 Jose \n#&gt; 3      2 child1 1996-06-22 Mark \n#&gt; 4      3 child1 2002-07-11 Sam  \n#&gt; 5      3 child2 2004-04-05 Seth \n#&gt; 6      4 child1 2004-10-10 Craig\n#&gt; # ℹ 3 more rows\n\nWe again use values_drop_na = TRUE, since the shape of the input forces the creation of explicit missing variables (e.g., for families with only one child).\n我们再次使用 values_drop_na = TRUE，因为输入的形状强制创建了显式的缺失变量（例如，对于只有一个孩子的家庭）。\nFigure 5.7 illustrates the basic idea with a simpler example. When you use \".value\" in names_to, the column names in the input contribute to both values and variable names in the output.Figure 5.7 用一个更简单的例子阐述了基本思想。 当你在 names_to 中使用 \".value\" 时，输入中的列名同时贡献于输出中的值和变量名。\n\n\n\n\n\n\n\nFigure 5.7: Pivoting with names_to = c(\".value\", \"num\") splits the column names into two components: the first part determines the output column name (x or y), and the second part determines the value of the num column.",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data tidying</span>"
    ]
  },
  {
    "objectID": "data-tidy.html#widening-data",
    "href": "data-tidy.html#widening-data",
    "title": "5  Data tidying",
    "section": "\n5.4 Widening data",
    "text": "5.4 Widening data\nSo far we’ve used pivot_longer() to solve the common class of problems where values have ended up in column names. Next we’ll pivot (HA HA) to pivot_wider(), which makes datasets wider by increasing columns and reducing rows and helps when one observation is spread across multiple rows. This seems to arise less commonly in the wild, but it does seem to crop up a lot when dealing with governmental data.\n到目前为止，我们已经使用 pivot_longer() 解决了值最终出现在列名中的一类常见问题。 接下来，我们将转向（哈哈，一语双关）pivot_wider()，它通过增加列数和减少行数使数据集变宽，并在一个观测分布在多行时提供帮助。 这种情况在现实世界中似乎不那么常见，但在处理政府数据时却经常出现。\nWe’ll start by looking at cms_patient_experience, a dataset from the Centers of Medicare and Medicaid services that collects data about patient experiences:\n我们将从 cms_patient_experience 数据集开始，这是一个来自医疗保险和医疗补助服务中心 (Centers of Medicare and Medicaid services) 的数据集，收集了关于患者体验的数据：\n\ncms_patient_experience\n#&gt; # A tibble: 500 × 5\n#&gt;   org_pac_id org_nm                     measure_cd   measure_title   prf_rate\n#&gt;   &lt;chr&gt;      &lt;chr&gt;                      &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt;\n#&gt; 1 0446157747 USC CARE MEDICAL GROUP INC CAHPS_GRP_1  CAHPS for MIPS…       63\n#&gt; 2 0446157747 USC CARE MEDICAL GROUP INC CAHPS_GRP_2  CAHPS for MIPS…       87\n#&gt; 3 0446157747 USC CARE MEDICAL GROUP INC CAHPS_GRP_3  CAHPS for MIPS…       86\n#&gt; 4 0446157747 USC CARE MEDICAL GROUP INC CAHPS_GRP_5  CAHPS for MIPS…       57\n#&gt; 5 0446157747 USC CARE MEDICAL GROUP INC CAHPS_GRP_8  CAHPS for MIPS…       85\n#&gt; 6 0446157747 USC CARE MEDICAL GROUP INC CAHPS_GRP_12 CAHPS for MIPS…       24\n#&gt; # ℹ 494 more rows\n\nThe core unit being studied is an organization, but each organization is spread across six rows, with one row for each measurement taken in the survey organization. We can see the complete set of values for measure_cd and measure_title by using distinct():\n研究的核心单位是一个组织，但每个组织的数据分布在六行中，每一行对应于该调查组织进行的一项测量。 我们可以使用 distinct() 查看 measure_cd 和 measure_title 的完整值集合：\n\ncms_patient_experience |&gt; \n  distinct(measure_cd, measure_title)\n#&gt; # A tibble: 6 × 2\n#&gt;   measure_cd   measure_title                                                 \n#&gt;   &lt;chr&gt;        &lt;chr&gt;                                                         \n#&gt; 1 CAHPS_GRP_1  CAHPS for MIPS SSM: Getting Timely Care, Appointments, and In…\n#&gt; 2 CAHPS_GRP_2  CAHPS for MIPS SSM: How Well Providers Communicate            \n#&gt; 3 CAHPS_GRP_3  CAHPS for MIPS SSM: Patient's Rating of Provider              \n#&gt; 4 CAHPS_GRP_5  CAHPS for MIPS SSM: Health Promotion and Education            \n#&gt; 5 CAHPS_GRP_8  CAHPS for MIPS SSM: Courteous and Helpful Office Staff        \n#&gt; 6 CAHPS_GRP_12 CAHPS for MIPS SSM: Stewardship of Patient Resources\n\nNeither of these columns will make particularly great variable names: measure_cd doesn’t hint at the meaning of the variable and measure_title is a long sentence containing spaces. We’ll use measure_cd as the source for our new column names for now, but in a real analysis you might want to create your own variable names that are both short and meaningful.\n这两列都不能成为特别好的变量名：measure_cd 没有暗示变量的含义，而 measure_title 是一个包含空格的长句子。 我们暂时使用 measure_cd 作为新列名的来源，但在实际分析中，你可能希望创建既简短又有意义的自己的变量名。\npivot_wider() has the opposite interface to pivot_longer(): instead of choosing new column names, we need to provide the existing columns that define the values (values_from) and the column name (names_from):pivot_wider() 的接口与 pivot_longer() 相反：我们不是选择新的列名，而是需要提供定义值的现有列 (values_from) 和定义列名的列 (names_from)：\n\ncms_patient_experience |&gt; \n  pivot_wider(\n    names_from = measure_cd,\n    values_from = prf_rate\n  )\n#&gt; # A tibble: 500 × 9\n#&gt;   org_pac_id org_nm                   measure_title   CAHPS_GRP_1 CAHPS_GRP_2\n#&gt;   &lt;chr&gt;      &lt;chr&gt;                    &lt;chr&gt;                 &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1 0446157747 USC CARE MEDICAL GROUP … CAHPS for MIPS…          63          NA\n#&gt; 2 0446157747 USC CARE MEDICAL GROUP … CAHPS for MIPS…          NA          87\n#&gt; 3 0446157747 USC CARE MEDICAL GROUP … CAHPS for MIPS…          NA          NA\n#&gt; 4 0446157747 USC CARE MEDICAL GROUP … CAHPS for MIPS…          NA          NA\n#&gt; 5 0446157747 USC CARE MEDICAL GROUP … CAHPS for MIPS…          NA          NA\n#&gt; 6 0446157747 USC CARE MEDICAL GROUP … CAHPS for MIPS…          NA          NA\n#&gt; # ℹ 494 more rows\n#&gt; # ℹ 4 more variables: CAHPS_GRP_3 &lt;dbl&gt;, CAHPS_GRP_5 &lt;dbl&gt;, …\n\nThe output doesn’t look quite right; we still seem to have multiple rows for each organization. That’s because, we also need to tell pivot_wider() which column or columns have values that uniquely identify each row; in this case those are the variables starting with \"org\":\n输出看起来不太对；我们似乎每个组织仍然有多行。 这是因为，我们还需要告诉 pivot_wider() 哪个或哪些列的值可以唯一标识每一行；在这种情况下，这些是以 \"org\" 开头的变量：\n\ncms_patient_experience |&gt; \n  pivot_wider(\n    id_cols = starts_with(\"org\"),\n    names_from = measure_cd,\n    values_from = prf_rate\n  )\n#&gt; # A tibble: 95 × 8\n#&gt;   org_pac_id org_nm           CAHPS_GRP_1 CAHPS_GRP_2 CAHPS_GRP_3 CAHPS_GRP_5\n#&gt;   &lt;chr&gt;      &lt;chr&gt;                  &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1 0446157747 USC CARE MEDICA…          63          87          86          57\n#&gt; 2 0446162697 ASSOCIATION OF …          59          85          83          63\n#&gt; 3 0547164295 BEAVER MEDICAL …          49          NA          75          44\n#&gt; 4 0749333730 CAPE PHYSICIANS…          67          84          85          65\n#&gt; 5 0840104360 ALLIANCE PHYSIC…          66          87          87          64\n#&gt; 6 0840109864 REX HOSPITAL INC          73          87          84          67\n#&gt; # ℹ 89 more rows\n#&gt; # ℹ 2 more variables: CAHPS_GRP_8 &lt;dbl&gt;, CAHPS_GRP_12 &lt;dbl&gt;\n\nThis gives us the output that we’re looking for.\n这就得到了我们想要的结果。\n\n5.4.1 How does pivot_wider() work?\nTo understand how pivot_wider() works, let’s again start with a very simple dataset. This time we have two patients with ids A and B, we have three blood pressure measurements on patient A and two on patient B:\n为了理解 pivot_wider() 的工作原理，我们再次从一个非常简单的数据集开始。 这次我们有两个病人，ID 分别为 A 和 B，我们对病人 A 进行了三次血压测量，对病人 B 进行了两次：\n\ndf &lt;- tribble(\n  ~id, ~measurement, ~value,\n  \"A\",        \"bp1\",    100,\n  \"B\",        \"bp1\",    140,\n  \"B\",        \"bp2\",    115, \n  \"A\",        \"bp2\",    120,\n  \"A\",        \"bp3\",    105\n)\n\nWe’ll take the values from the value column and the names from the measurement column:\n我们将从 value 列中获取值，从 measurement 列中获取名称：\n\ndf |&gt; \n  pivot_wider(\n    names_from = measurement,\n    values_from = value\n  )\n#&gt; # A tibble: 2 × 4\n#&gt;   id      bp1   bp2   bp3\n#&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 A       100   120   105\n#&gt; 2 B       140   115    NA\n\nTo begin the process pivot_wider() needs to first figure out what will go in the rows and columns. The new column names will be the unique values of measurement.\n要开始这个过程，pivot_wider() 首先需要确定行和列的内容。 新的列名将是 measurement 的唯一值。\n\ndf |&gt; \n  distinct(measurement) |&gt; \n  pull()\n#&gt; [1] \"bp1\" \"bp2\" \"bp3\"\n\nBy default, the rows in the output are determined by all the variables that aren’t going into the new names or values. These are called the id_cols. Here there is only one column, but in general there can be any number.\n默认情况下，输出中的行由所有不用于新名称或值的变量决定。 这些被称为 id_cols。 这里只有一个列，但通常可以有任意数量的列。\n\ndf |&gt; \n  select(-measurement, -value) |&gt; \n  distinct()\n#&gt; # A tibble: 2 × 1\n#&gt;   id   \n#&gt;   &lt;chr&gt;\n#&gt; 1 A    \n#&gt; 2 B\n\npivot_wider() then combines these results to generate an empty data frame:\n然后，pivot_wider() 结合这些结果生成一个空的数据框：\n\ndf |&gt; \n  select(-measurement, -value) |&gt; \n  distinct() |&gt; \n  mutate(x = NA, y = NA, z = NA)\n#&gt; # A tibble: 2 × 4\n#&gt;   id    x     y     z    \n#&gt;   &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt;\n#&gt; 1 A     NA    NA    NA   \n#&gt; 2 B     NA    NA    NA\n\nIt then fills in all the missing values using the data in the input. In this case, not every cell in the output has a corresponding value in the input as there’s no third blood pressure measurement for patient B, so that cell remains missing. We’ll come back to this idea that pivot_wider() can “make” missing values in Chapter 18.\n然后，它使用输入中的数据填充所有缺失值。 在这种情况下，输出中的并非每个单元格在输入中都有对应的值，因为病人 B 没有第三次血压测量值，所以该单元格保持缺失状态。 我们将在 Chapter 18 中再次讨论 pivot_wider() 可以“制造”缺失值的这个概念。\nYou might also wonder what happens if there are multiple rows in the input that correspond to one cell in the output. The example below has two rows that correspond to id “A” and measurement “bp1”:\n你可能还会想，如果输入中有多个行对应于输出中的一个单元格，会发生什么。 下面的例子中有两行对应 id “A” 和 measurement “bp1”：\n\ndf &lt;- tribble(\n  ~id, ~measurement, ~value,\n  \"A\",        \"bp1\",    100,\n  \"A\",        \"bp1\",    102,\n  \"A\",        \"bp2\",    120,\n  \"B\",        \"bp1\",    140, \n  \"B\",        \"bp2\",    115\n)\n\nIf we attempt to pivot this we get an output that contains list-columns, which you’ll learn more about in Chapter 23:\n如果我们尝试对此进行透视，我们会得到一个包含列表列（list-columns）的输出，你将在 Chapter 23 中学到更多相关内容：\n\ndf |&gt;\n  pivot_wider(\n    names_from = measurement,\n    values_from = value\n  )\n#&gt; Warning: Values from `value` are not uniquely identified; output will contain\n#&gt; list-cols.\n#&gt; • Use `values_fn = list` to suppress this warning.\n#&gt; • Use `values_fn = {summary_fun}` to summarise duplicates.\n#&gt; • Use the following dplyr code to identify duplicates.\n#&gt;   {data} |&gt;\n#&gt;   dplyr::summarise(n = dplyr::n(), .by = c(id, measurement)) |&gt;\n#&gt;   dplyr::filter(n &gt; 1L)\n#&gt; # A tibble: 2 × 3\n#&gt;   id    bp1       bp2      \n#&gt;   &lt;chr&gt; &lt;list&gt;    &lt;list&gt;   \n#&gt; 1 A     &lt;dbl [2]&gt; &lt;dbl [1]&gt;\n#&gt; 2 B     &lt;dbl [1]&gt; &lt;dbl [1]&gt;\n\nSince you don’t know how to work with this sort of data yet, you’ll want to follow the hint in the warning to figure out where the problem is:\n由于你还不知道如何处理这类数据，你可能需要遵循警告中的提示来找出问题所在：\n\ndf |&gt; \n  group_by(id, measurement) |&gt; \n  summarize(n = n(), .groups = \"drop\") |&gt; \n  filter(n &gt; 1)\n#&gt; # A tibble: 1 × 3\n#&gt;   id    measurement     n\n#&gt;   &lt;chr&gt; &lt;chr&gt;       &lt;int&gt;\n#&gt; 1 A     bp1             2\n\nIt’s then up to you to figure out what’s gone wrong with your data and either repair the underlying damage or use your grouping and summarizing skills to ensure that each combination of row and column values only has a single row.\n接下来就由你来找出数据中出了什么问题，要么修复潜在的损坏，要么利用你的分组和汇总技能，确保行和列值的每个组合只有一个单行。",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data tidying</span>"
    ]
  },
  {
    "objectID": "data-tidy.html#summary",
    "href": "data-tidy.html#summary",
    "title": "5  Data tidying",
    "section": "\n5.5 Summary",
    "text": "5.5 Summary\nIn this chapter you learned about tidy data: data that has variables in columns and observations in rows. Tidy data makes working in the tidyverse easier, because it’s a consistent structure understood by most functions, the main challenge is transforming the data from whatever structure you receive it in to a tidy format. To that end, you learned about pivot_longer() and pivot_wider() which allow you to tidy up many untidy datasets. The examples we presented here are a selection of those from vignette(\"pivot\", package = \"tidyr\"), so if you encounter a problem that this chapter doesn’t help you with, that vignette is a good place to try next.\n在本章中，你学习了整洁数据：即变量在列、观测在行的数据。 整洁数据使得在 tidyverse 中工作更加容易，因为它是一个被大多数函数所理解的一致性结构，主要的挑战在于将你收到的任何结构的数据转换为整洁格式。 为此，你学习了 pivot_longer() 和 pivot_wider()，它们可以让你整理许多不整洁的数据集。 我们在这里展示的例子选自 vignette(\"pivot\", package = \"tidyr\")，所以如果你遇到本章无法解决的问题，那篇小品文是一个很好的下一步尝试。\nAnother challenge is that, for a given dataset, it can be impossible to label the longer or the wider version as the “tidy” one. This is partly a reflection of our definition of tidy data, where we said tidy data has one variable in each column, but we didn’t actually define what a variable is (and it’s surprisingly hard to do so). It’s totally fine to be pragmatic and to say a variable is whatever makes your analysis easiest. So if you’re stuck figuring out how to do some computation, consider switching up the organisation of your data; don’t be afraid to untidy, transform, and re-tidy as needed!\n另一个挑战是，对于给定的数据集，可能无法将长格式或宽格式版本标记为“整洁”的版本。 这部分反映了我们对整洁数据的定义，我们说整洁数据每列有一个变量，但我们实际上没有定义什么是变量（而且令人惊讶的是，这很难做到）。 采取务实的态度是完全可以的，可以说变量就是任何使你的分析最容易的东西。 所以，如果你在计算上遇到困难，可以考虑改变数据的组织方式；不要害怕按需进行非整洁化、转换和重新整理！\nIf you enjoyed this chapter and want to learn more about the underlying theory, you can learn more about the history and theoretical underpinnings in the Tidy Data paper published in the Journal of Statistical Software.\n如果你喜欢这一章并想了解更多关于其背后理论的知识，你可以在发表于《统计软件杂志》(Journal of Statistical Software) 的 Tidy Data 论文中了解更多关于其历史和理论基础的内容。\nNow that you’re writing a substantial amount of R code, it’s time to learn more about organizing your code into files and directories. In the next chapter, you’ll learn all about the advantages of scripts and projects, and some of the many tools that they provide to make your life easier.\n现在你已经编写了大量的 R 代码，是时候学习更多关于将代码组织到文件和目录中的知识了。 在下一章中，你将学习脚本和项目的所有优点，以及它们为使你的生活更轻松而提供的许多工具。",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data tidying</span>"
    ]
  },
  {
    "objectID": "data-tidy.html#footnotes",
    "href": "data-tidy.html#footnotes",
    "title": "5  Data tidying",
    "section": "",
    "text": "The song will be included as long as it was in the top 100 at some point in 2000, and is tracked for up to 72 weeks after it appears.↩︎\nWe’ll come back to this idea in Chapter 18.↩︎",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data tidying</span>"
    ]
  },
  {
    "objectID": "workflow-scripts.html",
    "href": "workflow-scripts.html",
    "title": "6  Workflow: scripts and projects",
    "section": "",
    "text": "6.1 Scripts\nThis chapter will introduce you to two essential tools for organizing your code: scripts and projects.\n本章将向你介绍两个组织代码的基本工具：脚本和项目。\nSo far, you have used the console to run code. That’s a great place to start, but you’ll find it gets cramped pretty quickly as you create more complex ggplot2 graphics and longer dplyr pipelines. To give yourself more room to work, use the script editor. Open it up by clicking the File menu, selecting New File, then R script, or using the keyboard shortcut Cmd/Ctrl + Shift + N. Now you’ll see four panes, as in Figure 6.1. The script editor is a great place to experiment with your code. When you want to change something, you don’t have to re-type the whole thing, you can just edit the script and re-run it. And once you have written code that works and does what you want, you can save it as a script file to easily return to later.\n到目前为止，你一直在使用控制台 (console) 运行代码。 这是一个很好的起点，但当你创建更复杂的 ggplot2 图形和更长的 dplyr 管道时，你会发现它很快就会变得拥挤不堪。 为了给自己更多的工作空间，请使用脚本编辑器。 通过点击文件菜单，选择新建文件，然后选择 R 脚本来打开它，或者使用键盘快捷键 Cmd/Ctrl + Shift + N。 现在你会看到四个窗格，如 Figure 6.1 所示。 脚本编辑器是试验代码的绝佳场所。 当你想改变某些东西时，你不必重新输入全部内容，只需编辑脚本并重新运行即可。 而且，一旦你编写了能正常工作并实现你想要的功能的代码，你可以将其保存为脚本文件，以便日后轻松返回。\nFigure 6.1: Opening the script editor adds a new pane at the top-left of the IDE.",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Workflow: scripts and projects</span>"
    ]
  },
  {
    "objectID": "workflow-scripts.html#scripts",
    "href": "workflow-scripts.html#scripts",
    "title": "6  Workflow: scripts and projects",
    "section": "",
    "text": "6.1.1 Running code\nThe script editor is an excellent place for building complex ggplot2 plots or long sequences of dplyr manipulations. The key to using the script editor effectively is to memorize one of the most important keyboard shortcuts: Cmd/Ctrl + Enter. This executes the current R expression in the console. For example, take the code below.\n脚本编辑器是构建复杂 ggplot2 图或长序列 dplyr 操作的绝佳场所。 有效使用脚本编辑器的关键是记住一个最重要的键盘快捷键：Cmd/Ctrl + Enter。 这会在控制台中执行当前的 R 表达式。 例如，看下面的代码。\n\nlibrary(dplyr)\nlibrary(nycflights13)\n\nnot_cancelled &lt;- flights |&gt; \n  filter(!is.na(dep_delay)█, !is.na(arr_delay))\n\nnot_cancelled |&gt; \n  group_by(year, month, day) |&gt; \n  summarize(mean = mean(dep_delay))\n\nIf your cursor is at █, pressing Cmd/Ctrl + Enter will run the complete command that generates not_cancelled. It will also move the cursor to the following statement (beginning with not_cancelled |&gt;). That makes it easy to step through your complete script by repeatedly pressing Cmd/Ctrl + Enter.\n如果你的光标在 █ 处，按下 Cmd/Ctrl + Enter 将运行生成 not_cancelled 的完整命令。 它还会将光标移动到下一个语句（以 not_cancelled |&gt; 开头）。 这样，通过重复按 Cmd/Ctrl + Enter，你就可以轻松地逐步执行整个脚本。\nInstead of running your code expression-by-expression, you can also execute the complete script in one step with Cmd/Ctrl + Shift + S. Doing this regularly is a great way to ensure that you’ve captured all the important parts of your code in the script.\n除了逐个表达式运行代码，你还可以通过 Cmd/Ctrl + Shift + S 一步执行整个脚本。 定期这样做是确保你已将所有重要代码部分保存在脚本中的好方法。\nWe recommend you always start your script with the packages you need. That way, if you share your code with others, they can easily see which packages they need to install. Note, however, that you should never include install.packages() in a script you share. It’s inconsiderate to hand off a script that will change something on their computer if they’re not being careful!\n我们建议你始终在脚本的开头声明所需的包。 这样，如果你与他人共享代码，他们可以轻松看到需要安装哪些包。 但是请注意，你永远不应该在你分享的脚本中包含 install.packages()。 如果别人不小心，你的脚本可能会在他们的计算机上做出更改，递交这样的脚本是不体贴的行为！\nWhen working through future chapters, we highly recommend starting in the script editor and practicing your keyboard shortcuts. Over time, sending code to the console in this way will become so natural that you won’t even think about it.\n在学习后续章节时，我们强烈建议你从脚本编辑器开始，并练习你的键盘快捷键。 随着时间的推移，以这种方式向控制台发送代码将变得如此自然，以至于你甚至不会去想它。\n\n6.1.2 RStudio diagnostics\nIn the script editor, RStudio will highlight syntax errors with a red squiggly line and a cross in the sidebar:\n在脚本编辑器中，RStudio 会用红色波浪线和侧边栏中的叉号来高亮显示语法错误：\n\n\n\n\n\n\n\n\nHover over the cross to see what the problem is:\n将鼠标悬停在叉号上查看问题所在：\n\n\n\n\n\n\n\n\nRStudio will also let you know about potential problems:\nRStudio 也会让你知道潜在的问题：\n\n\n\n\n\n\n\n\n\n6.1.3 Saving and naming\nRStudio automatically saves the contents of the script editor when you quit, and automatically reloads it when you re-open. Nevertheless, it’s a good idea to avoid Untitled1, Untitled2, Untitled3, and so on and instead save your scripts and to give them informative names.\n当你退出 RStudio 时，它会自动保存脚本编辑器的内容，并在你重新打开时自动重新加载。 尽管如此，最好还是避免使用 Untitled1、Untitled2、Untitled3 等名称，而是保存你的脚本并给它们起一些信息丰富的名字。\nIt might be tempting to name your files code.R or myscript.R, but you should think a bit harder before choosing a name for your file. Three important principles for file naming are as follows:\n你可能会想把你的文件命名为 code.R 或 myscript.R，但在选择文件名之前，你应该再多想一想。 文件命名的三个重要原则如下：\n\nFile names should be machine readable: avoid spaces, symbols, and special characters. Don’t rely on case sensitivity to distinguish files.\n文件名应便于机器读取：避免使用空格、符号和特殊字符。不要依赖大小写区分文件。\nFile names should be human readable: use file names to describe what’s in the file.\n文件名应便于人类阅读：用文件名描述文件内容。\nFile names should play well with default ordering: start file names with numbers so that alphabetical sorting puts them in the order they get used.\n文件名应便于默认排序：用数字开头，这样按字母排序时文件会按使用顺序排列。\n\nFor example, suppose you have the following files in a project folder.\n例如，假设你的项目文件夹中有以下文件。\nalternative model.R\ncode for exploratory analysis.r\nfinalreport.qmd\nFinalReport.qmd\nfig 1.png\nFigure_02.png\nmodel_first_try.R\nrun-first.r\ntemp.txt\nThere are a variety of problems here: it’s hard to find which file to run first, file names contain spaces, there are two files with the same name but different capitalization (finalreport vs. FinalReport1), and some names don’t describe their contents (run-first and temp).\n这里存在各种问题：很难找到首先要运行哪个文件，文件名包含空格，有两个名称相同但大小写不同的文件（finalreport vs. FinalReport1），而且有些名称没有描述其内容（run-first 和 temp）。\nHere’s a better way of naming and organizing the same set of files:\n这里有一种更好的命名和组织同一组文件的方式：\n01-load-data.R\n02-exploratory-analysis.R\n03-model-approach-1.R\n04-model-approach-2.R\nfig-01.png\nfig-02.png\nreport-2022-03-20.qmd\nreport-2022-04-02.qmd\nreport-draft-notes.txt\nNumbering the key scripts makes it obvious in which order to run them and a consistent naming scheme makes it easier to see what varies. Additionally, the figures are labelled similarly, the reports are distinguished by dates included in the file names, and temp is renamed to report-draft-notes to better describe its contents. If you have a lot of files in a directory, taking organization one step further and placing different types of files (scripts, figures, etc.) in different directories is recommended.\n对关键脚本进行编号可以清楚地表明运行它们的顺序，而一致的命名方案也更容易看出不同之处。 此外，图形的标签也类似，报告通过文件名中包含的日期来区分，temp 被重命名为 report-draft-notes 以更好地描述其内容。 如果一个目录中有很多文件，建议将组织工作更进一步，将不同类型的文件（脚本、图形等）放在不同的目录中。",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Workflow: scripts and projects</span>"
    ]
  },
  {
    "objectID": "workflow-scripts.html#projects",
    "href": "workflow-scripts.html#projects",
    "title": "6  Workflow: scripts and projects",
    "section": "\n6.2 Projects",
    "text": "6.2 Projects\nOne day, you will need to quit R, go do something else, and return to your analysis later. One day, you will be working on multiple analyses simultaneously and you want to keep them separate. One day, you will need to bring data from the outside world into R and send numerical results and figures from R back out into the world.\n总有一天，你需要退出 R，去做别的事情，然后再回到你的分析中。 总有一天，你会同时进行多个分析，并且你希望将它们分开。 总有一天，你需要将外部世界的数据导入 R，并将数值结果和图表从 R 导出到外部世界。\nTo handle these real life situations, you need to make two decisions:\n为了处理这些现实生活中的情况，你需要做出两个决定：\n\nWhat is the source of truth? What will you save as your lasting record of what happened?\n事实来源是什么？ 你会保存什么作为你所做事情的永久记录？\nWhere does your analysis live?\n你的分析存放在哪里？\n\n\n6.2.1 What is the source of truth?\nAs a beginner, it’s okay to rely on your current Environment to contain all the objects you have created throughout your analysis. However, to make it easier to work on larger projects or collaborate with others, your source of truth should be the R scripts. With your R scripts (and your data files), you can recreate the environment. With only your environment, it’s much harder to recreate your R scripts: you’ll either have to retype a lot of code from memory (inevitably making mistakes along the way) or you’ll have to carefully mine your R history.\n作为初学者，依赖你当前的环境 (Environment) 来包含你在整个分析过程中创建的所有对象是可以的。 然而，为了更容易地处理大型项目或与他人协作，你的事实来源应该是 R 脚本。 有了你的 R 脚本（和你的数据文件），你就可以重现环境。 如果只有你的环境，要重现你的 R 脚本就困难得多：你要么必须凭记忆重新输入大量代码（这不可避免地会出错），要么就得仔细挖掘你的 R 历史记录。\nTo help keep your R scripts as the source of truth for your analysis, we highly recommend that you instruct RStudio not to preserve your workspace between sessions. You can do this either by running usethis::use_blank_slate()2 or by mimicking the options shown in Figure 6.2. This will cause you some short-term pain, because now when you restart RStudio, it will no longer remember the code that you ran last time nor will the objects you created or the datasets you read be available to use. But this short-term pain saves you long-term agony because it forces you to capture all important procedures in your code. There’s nothing worse than discovering three months after the fact that you’ve only stored the results of an important calculation in your environment, not the calculation itself in your code.\n为了帮助保持你的 R 脚本作为你分析的事实来源，我们强烈建议你指示 RStudio 在会话之间不要保存你的工作区。 你可以通过运行 usethis::use_blank_slate()1 或模仿 Figure 6.2 中显示的选项来做到这一点。这会给你带来一些短期的痛苦，因为现在当你重新启动 RStudio 时，它将不再记得你上次运行的代码，你创建的对象或读取的数据集也无法使用。 但这种短期的痛苦可以为你省去长期的折磨，因为它迫使你将所有重要的过程都记录在你的代码中。 没有什么比在三个月后发现你只在环境中存储了重要计算的结果，而没有在代码中存储计算本身更糟糕的了。\n\n\n\n\n\n\n\nFigure 6.2: Copy these options in your RStudio options to always start your RStudio session with a clean slate.\n\n\n\n\nThere is a great pair of keyboard shortcuts that will work together to make sure you’ve captured the important parts of your code in the editor:\n有一对很棒的键盘快捷键可以协同工作，确保你已经将代码的重要部分保存在编辑器中：\n\nPress Cmd/Ctrl + Shift + 0/F10 to restart R.\n按下 Cmd/Ctrl + Shift + 0/F10 重启 R。\nPress Cmd/Ctrl + Shift + S to re-run the current script.\n按下 Cmd/Ctrl + Shift + S 重新运行当前脚本。\n\nWe collectively use this pattern hundreds of times a week.\n我们每周会集体使用这个模式数百次。\nAlternatively, if you don’t use keyboard shortcuts, you can go to Session &gt; Restart R and then highlight and re-run your current script.\n或者，如果你不使用键盘快捷键，你可以转到 Session &gt; Restart R，然后高亮并重新运行你当前的脚本。\n\n\n\n\n\n\nRStudio server\n\n\n\nIf you’re using RStudio server, your R session is never restarted by default. When you close your RStudio server tab, it might feel like you’re closing R, but the server actually keeps it running in the background. The next time you return, you’ll be in exactly the same place you left. This makes it even more important to regularly restart R so that you’re starting with a clean slate.\n如果你正在使用 RStudio 服务器，你的 R 会话默认情况下永远不会重启。 当你关闭 RStudio 服务器的标签页时，可能感觉像是关闭了 R，但服务器实际上在后台保持它运行。 下次你回来时，你将正好在你离开的地方。 这使得定期重启 R 以便从一个干净的状态开始变得更加重要。\n\n\n\n6.2.2 Where does your analysis live?\nR has a powerful notion of the working directory. This is where R looks for files that you ask it to load, and where it will put any files that you ask it to save. RStudio shows your current working directory at the top of the console:\nR 有一个强大的概念，叫做工作目录 (working directory)。 这是 R 寻找你要求它加载的文件的位置，也是它存放你要求它保存的任何文件的位置。 RStudio 在控制台的顶部显示你当前的工作目录：\n\n\n\n\n\n\n\n\nAnd you can print this out in R code by running getwd():\n你也可以在 R 代码中通过运行 getwd() 来打印出这个路径：\n\ngetwd()\n#&gt; [1] \"/Users/hadley/Documents/r4ds\"\n\nIn this R session, the current working directory (think of it as “home”) is in hadley’s Documents folder, in a subfolder called r4ds. This code will return a different result when you run it, because your computer has a different directory structure than Hadley’s!\n在这个 R 会话中，当前的工作目录（可以把它想象成“家”）在 hadley 的 Documents 文件夹下的一个名为 r4ds 的子文件夹中。 当你运行这段代码时，会返回一个不同的结果，因为你的计算机目录结构与 Hadley 的不同！\nAs a beginning R user, it’s OK to let your working directory be your home directory, documents directory, or any other weird directory on your computer. But you’re more than a handful of chapters into this book, and you’re no longer a beginner. Very soon now you should evolve to organizing your projects into directories and, when working on a project, set R’s working directory to the associated directory.\n作为 R 的初学者，让你的工作目录是你的主目录、文档目录或你电脑上的任何其他奇怪目录都是可以的。 但你已经读了这本书好几章了，你不再是初学者了。 很快你就应该进化到将你的项目组织到目录中，并且在处理一个项目时，将 R 的工作目录设置为相关的目录。\nYou can set the working directory from within R but we do not recommend it:\n你可以在 R 内部设置工作目录，但我们不推荐这样做：\n\nsetwd(\"/path/to/my/CoolProject\")\n\nThere’s a better way; a way that also puts you on the path to managing your R work like an expert. That way is the RStudio project.\n有更好的方法；一种能让你像专家一样管理你的 R 工作的途径。 那就是 RStudio 项目 (project)。\n\n6.2.3 RStudio projects\nKeeping all the files associated with a given project (input data, R scripts, analytical results, and figures) together in one directory is such a wise and common practice that RStudio has built-in support for this via projects. Let’s make a project for you to use while you’re working through the rest of this book. Click File &gt; New Project, then follow the steps shown in Figure 6.3.\n将与特定项目相关的所有文件（输入数据、R 脚本、分析结果和图表）都放在一个目录中是一种非常明智和普遍的做法，RStudio 通过项目为这种做法提供了内置支持。 让我们为你创建一个项目，以便你在学习本书其余部分时使用。 点击 File &gt; New Project，然后按照 Figure 6.3 中显示的步骤操作。\n\n\n\n\n\n\n\nFigure 6.3: To create new project: (top) first click New Directory, then (middle) click New Project, then (bottom) fill in the directory (project) name, choose a good subdirectory for its home and click Create Project.\n\n\n\n\nCall your project r4ds and think carefully about which subdirectory you put the project in. If you don’t store it somewhere sensible, it will be hard to find it in the future!\n将你的项目命名为 r4ds，并仔细考虑将项目放在哪个子目录中。 如果你不把它存放在一个合理的地方，将来会很难找到它！\nOnce this process is complete, you’ll get a new RStudio project just for this book. Check that the “home” of your project is the current working directory:\n一旦这个过程完成，你将为这本书得到一个新的 RStudio 项目。 检查一下你项目的“家”是否是当前的工作目录：\n\ngetwd()\n#&gt; [1] /Users/hadley/Documents/r4ds\n\nNow enter the following commands in the script editor, and save the file, calling it “diamonds.R”. Then, create a new folder called “data”. You can do this by clicking on the “New Folder” button in the Files pane in RStudio. Finally, run the complete script which will save a PNG and CSV file into your project directory. Don’t worry about the details, you’ll learn them later in the book.\n现在在脚本编辑器中输入以下命令，并保存文件，命名为“diamonds.R”。 然后，创建一个名为“data”的新文件夹。 你可以通过点击 RStudio 文件窗格中的“New Folder”按钮来完成此操作。 最后，运行整个脚本，这会将一个 PNG 和一个 CSV 文件保存到你的项目目录中。 别担心细节，你将在书的后面学习它们。\n\nlibrary(tidyverse)\n\nggplot(diamonds, aes(x = carat, y = price)) + \n  geom_hex()\nggsave(\"diamonds.png\")\n\nwrite_csv(diamonds, \"data/diamonds.csv\")\n\nQuit RStudio. Inspect the folder associated with your project — notice the .Rproj file. Double-click that file to re-open the project. Notice you get back to where you left off: it’s the same working directory and command history, and all the files you were working on are still open. Because you followed our instructions above, you will, however, have a completely fresh environment, guaranteeing that you’re starting with a clean slate.\n退出 RStudio。 检查与你的项目关联的文件夹——注意那个 .Rproj 文件。 双击该文件以重新打开项目。 你会发现你回到了离开时的地方：工作目录和命令历史记录都相同，你正在处理的所有文件仍然是打开的。 然而，因为你遵循了我们上面的指示，你将拥有一个全新的环境，保证你是从一个干净的状态开始。\nIn your favorite OS-specific way, search your computer for diamonds.png and you will find the PNG (no surprise) but also the script that created it (diamonds.R). This is a huge win! One day, you will want to remake a figure or just understand where it came from. If you rigorously save figures to files with R code and never with the mouse or the clipboard, you will be able to reproduce old work with ease!\n用你喜欢的特定于操作系统的方式，在你的电脑上搜索 diamonds.png，你会找到这个 PNG 文件（不奇怪），但同时也会找到创建它的脚本 (diamonds.R)。 这是一个巨大的胜利！ 总有一天，你会想要重做一个图表，或者只是想了解它是怎么来的。 如果你严格地用 R 代码将图表保存到文件，而不是用鼠标或剪贴板，你将能够轻松地重现旧的工作！\n\n6.2.4 Relative and absolute paths\nOnce you’re inside a project, you should only ever use relative paths not absolute paths. What’s the difference? A relative path is relative to the working directory, i.e. the project’s home. When Hadley wrote data/diamonds.csv above it was a shortcut for /Users/hadley/Documents/r4ds/data/diamonds.csv. But importantly, if Mine ran this code on her computer, it would point to /Users/Mine/Documents/r4ds/data/diamonds.csv. This is why relative paths are important: they’ll work regardless of where the R project folder ends up.\n一旦你进入一个项目中，你应该只使用相对路径 (relative paths)，而不是绝对路径 (absolute paths)。 有什么区别呢？ 相对路径是相对于工作目录的，也就是项目的主目录。 当 Hadley 在上面写 data/diamonds.csv 时，它是 /Users/hadley/Documents/r4ds/data/diamonds.csv 的一个快捷方式。 但重要的是，如果 Mine 在她的电脑上运行这段代码，它将指向 /Users/Mine/Documents/r4ds/data/diamonds.csv。 这就是为什么相对路径很重要：无论 R 项目文件夹最终在哪里，它们都能工作。\nAbsolute paths point to the same place regardless of your working directory. They look a little different depending on your operating system. On Windows they start with a drive letter (e.g., C:) or two backslashes (e.g., \\\\servername) and on Mac/Linux they start with a slash “/” (e.g., /users/hadley). You should never use absolute paths in your scripts, because they hinder sharing: no one else will have exactly the same directory configuration as you.\n无论你的工作目录是什么，绝对路径都指向同一个地方。 它们根据你的操作系统看起来有些不同。 在 Windows 上，它们以驱动器号（例如 C:）或两个反斜杠（例如 \\\\servername）开头，而在 Mac/Linux 上，它们以斜杠“/”开头（例如 /users/hadley）。 你永远不应该在你的脚本中使用绝对路径，因为它们会妨碍共享：没有其他人会拥有与你完全相同的目录配置。\nThere’s another important difference between operating systems: how you separate the components of the path. Mac and Linux uses slashes (e.g., data/diamonds.csv) and Windows uses backslashes (e.g., data\\diamonds.csv). R can work with either type (no matter what platform you’re currently using), but unfortunately, backslashes mean something special to R, and to get a single backslash in the path, you need to type two backslashes! That makes life frustrating, so we recommend always using the Linux/Mac style with forward slashes.\n操作系统之间还有另一个重要的区别：你如何分隔路径的组成部分。 Mac 和 Linux 使用正斜杠（例如 data/diamonds.csv），而 Windows 使用反斜杠（例如 data\\diamonds.csv）。 R 可以处理这两种类型（无论你当前使用的是哪个平台），但不幸的是，反斜杠对 R 来说有特殊含义，要在路径中得到一个反斜杠，你需要输入两个反斜杠！ 这让生活变得令人沮丧，所以我们建议始终使用 Linux/Mac 风格的正斜杠。",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Workflow: scripts and projects</span>"
    ]
  },
  {
    "objectID": "workflow-scripts.html#exercises",
    "href": "workflow-scripts.html#exercises",
    "title": "6  Workflow: scripts and projects",
    "section": "\n6.3 Exercises",
    "text": "6.3 Exercises\n\nGo to the RStudio Tips Twitter account, https://twitter.com/rstudiotips and find one tip that looks interesting. Practice using it!\nWhat other common mistakes will RStudio diagnostics report? Read https://support.posit.co/hc/en-us/articles/205753617-Code-Diagnostics to find out.",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Workflow: scripts and projects</span>"
    ]
  },
  {
    "objectID": "workflow-scripts.html#summary",
    "href": "workflow-scripts.html#summary",
    "title": "6  Workflow: scripts and projects",
    "section": "\n6.4 Summary",
    "text": "6.4 Summary\nIn this chapter, you’ve learned how to organize your R code in scripts (files) and projects (directories). Much like code style, this may feel like busywork at first. But as you accumulate more code across multiple projects, you’ll learn to appreciate how a little up front organisation can save you a bunch of time down the road.\n在本章中，你学习了如何在脚本（文件）和项目（目录）中组织你的 R 代码。 就像代码风格一样，这起初可能感觉像是琐碎的工作。 但随着你在多个项目中积累了越来越多的代码，你会逐渐体会到，一点点前期的组织工作能在未来为你节省大量时间。\nIn summary, scripts and projects give you a solid workflow that will serve you well in the future:\n总而言之，脚本和项目为你提供了一个坚实的工作流程，这将在未来对你大有裨益：\n\nCreate one RStudio project for each data analysis project.\n为每个数据分析项目创建一个 RStudio 项目。\nSave your scripts (with informative names) in the project, edit them, run them in bits or as a whole. Restart R frequently to make sure you’ve captured everything in your scripts.\n在项目中保存你的脚本（并使用信息丰富的名称），编辑它们，分段或整体运行它们。频繁重启 R 以确保你已将所有内容都记录在脚本中。\nOnly ever use relative paths, not absolute paths.\n只使用相对路径，不使用绝对路径。\n\nThen everything you need is in one place and cleanly separated from all the other projects that you are working on.\n然后，你需要的一切都在一个地方，并与你正在进行的所有其他项目清晰地分离开来。\nSo far, we’ve worked with datasets bundled inside of R packages. This makes it easier to get some practice on pre-prepared data, but obviously your data won’t be available in this way. So in the next chapter, you’re going to learn how load data from disk into your R session using the readr package.\n到目前为止，我们一直使用 R 包中捆绑的数据集。 这使得在预先准备好的数据上进行练习变得更容易，但显然你的数据不会以这种方式提供。 因此，在下一章中，你将学习如何使用 readr 包将数据从磁盘加载到你的 R 会话中。",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Workflow: scripts and projects</span>"
    ]
  },
  {
    "objectID": "workflow-scripts.html#footnotes",
    "href": "workflow-scripts.html#footnotes",
    "title": "6  Workflow: scripts and projects",
    "section": "",
    "text": "Not to mention that you’re tempting fate by using “final” in the name 😆 The comic Piled Higher and Deeper has a fun strip on this.↩︎\nIf you don’t have usethis installed, you can install it with install.packages(\"usethis\").↩︎",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Workflow: scripts and projects</span>"
    ]
  },
  {
    "objectID": "data-import.html",
    "href": "data-import.html",
    "title": "7  Data import",
    "section": "",
    "text": "7.1 Introduction\nWorking with data provided by R packages is a great way to learn data science tools, but you want to apply what you’ve learned to your own data at some point.\n使用 R 包中提供的数据是学习数据科学工具的好方法，但在某些时候，你会希望将所学知识应用到自己的数据上。\nIn this chapter, you’ll learn the basics of reading data files into R.\n在本章中，你将学习将数据文件读入 R 的基础知识。\nSpecifically, this chapter will focus on reading plain-text rectangular files.\n具体来说，本章将重点介绍如何读取纯文本矩形文件。\nWe’ll start with practical advice for handling features like column names, types, and missing data.\n我们将从处理列名、类型和缺失数据等特性的实用建议开始。\nYou will then learn about reading data from multiple files at once and writing data from R to a file.\n然后，你将学习如何一次性从多个文件中读取数据，以及如何将数据从 R 写入文件。\nFinally, you’ll learn how to handcraft data frames in R.\n最后，你将学习如何在 R 中手动创建数据框。",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data import</span>"
    ]
  },
  {
    "objectID": "data-import.html#introduction",
    "href": "data-import.html#introduction",
    "title": "7  Data import",
    "section": "",
    "text": "7.1.1 Prerequisites\nIn this chapter, you’ll learn how to load flat files in R with the readr package, which is part of the core tidyverse.\n在本章中，你将学习如何使用 readr 包在 R 中加载平面文件，该包是核心 tidyverse 的一部分。\n\nlibrary(tidyverse)",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data import</span>"
    ]
  },
  {
    "objectID": "data-import.html#reading-data-from-a-file",
    "href": "data-import.html#reading-data-from-a-file",
    "title": "7  Data import",
    "section": "\n7.2 Reading data from a file",
    "text": "7.2 Reading data from a file\nTo begin, we’ll focus on the most common rectangular data file type: CSV, which is short for comma-separated values.\n首先，我们将重点关注最常见的矩形数据文件类型：CSV，即逗号分隔值 (comma-separated values) 的缩写。\nHere is what a simple CSV file looks like.\n下面是一个简单的 CSV 文件的样子。\nThe first row, commonly called the header row, gives the column names, and the following six rows provide the data.\n第一行，通常称为标题行 (header row)，给出了列名，接下来的六行提供了数据。\nThe columns are separated, aka delimited, by commas.\n列之间由逗号分隔，也称为定界 (delimited)。\n\nStudent ID,Full Name,favourite.food,mealPlan,AGE\n1,Sunil Huffmann,Strawberry yoghurt,Lunch only,4\n2,Barclay Lynn,French fries,Lunch only,5\n3,Jayendra Lyne,N/A,Breakfast and lunch,7\n4,Leon Rossini,Anchovies,Lunch only,\n5,Chidiegwu Dunkel,Pizza,Breakfast and lunch,five\n6,Güvenç Attila,Ice cream,Lunch only,6\n\nTable 7.1 shows a representation of the same data as a table.Table 7.1 以表格形式展示了相同的数据。\n\n\n\nTable 7.1: Data from the students.csv file as a table.\n\n\n\n\n\n\n\n\n\n\n\nStudent ID\nFull Name\nfavourite.food\nmealPlan\nAGE\n\n\n\n1\nSunil Huffmann\nStrawberry yoghurt\nLunch only\n4\n\n\n2\nBarclay Lynn\nFrench fries\nLunch only\n5\n\n\n3\nJayendra Lyne\nN/A\nBreakfast and lunch\n7\n\n\n4\nLeon Rossini\nAnchovies\nLunch only\nNA\n\n\n5\nChidiegwu Dunkel\nPizza\nBreakfast and lunch\nfive\n\n\n6\nGüvenç Attila\nIce cream\nLunch only\n6\n\n\n\n\n\n\n\n\nWe can read this file into R using read_csv().\n我们可以使用 read_csv() 将这个文件读入 R。\nThe first argument is the most important: the path to the file.\n第一个参数是最重要的：文件的路径。\nYou can think about the path as the address of the file: the file is called students.csv and it lives in the data folder.\n你可以将路径看作是文件的地址：文件名为 students.csv，它位于 data 文件夹中。\n\nstudents &lt;- read_csv(\"data/students.csv\")\n#&gt; Rows: 6 Columns: 5\n#&gt; ── Column specification ─────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr (4): Full Name, favourite.food, mealPlan, AGE\n#&gt; dbl (1): Student ID\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nThe code above will work if you have the students.csv file in a data folder in your project.\n如果你的项目中有一个 data 文件夹，并且其中包含 students.csv 文件，那么上面的代码将会正常工作。\nYou can download the students.csv file from https://pos.it/r4ds-students-csv or you can read it directly from that URL with:\n你可以从 https://pos.it/r4ds-students-csv 下载 students.csv 文件，或者使用以下代码直接从该 URL 读取：\n\nstudents &lt;- read_csv(\"https://pos.it/r4ds-students-csv\")\n\nWhen you run read_csv(), it prints out a message telling you the number of rows and columns of data, the delimiter that was used, and the column specifications (names of columns organized by the type of data the column contains).\n当你运行 read_csv() 时，它会打印出一条消息，告诉你数据的行数和列数、使用的分隔符以及列的规格 (column specifications)（按列所含数据类型组织的列名）。\nIt also prints out some information about retrieving the full column specification and how to quiet this message.\n它还会打印一些关于检索完整列规格以及如何静默此消息的信息。\nThis message is an integral part of readr, and we’ll return to it in Section 7.3.\n这条消息是 readr 不可或缺的一部分，我们将在 Section 7.3 中再次讨论它。\n\n7.2.1 Practical advice\nOnce you read data in, the first step usually involves transforming it in some way to make it easier to work with in the rest of your analysis.\n一旦你读入数据，第一步通常是将其以某种方式进行转换，使其在后续分析中更易于使用。\nLet’s take another look at the students data with that in mind.\n让我们带着这个想法再看一下 students 数据。\n\nstudents\n#&gt; # A tibble: 6 × 5\n#&gt;   `Student ID` `Full Name`      favourite.food     mealPlan            AGE  \n#&gt;          &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n#&gt; 1            1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n#&gt; 2            2 Barclay Lynn     French fries       Lunch only          5    \n#&gt; 3            3 Jayendra Lyne    N/A                Breakfast and lunch 7    \n#&gt; 4            4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n#&gt; 5            5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n#&gt; 6            6 Güvenç Attila    Ice cream          Lunch only          6\n\nIn the favourite.food column, there are a bunch of food items, and then the character string N/A, which should have been a real NA that R will recognize as “not available”.\n在 favourite.food 列中，有一堆食物条目，然后是字符串 N/A，它本应是一个真正的 NA，R 会将其识别为“不可用”(not available)。\nThis is something we can address using the na argument.\n这是我们可以使用 na 参数来解决的问题。\nBy default, read_csv() only recognizes empty strings (\"\") in this dataset as NAs, and we want it to also recognize the character string \"N/A\".\n默认情况下，read_csv() 在这个数据集中只将空字符串 (\"\") 识别为 NA，我们希望它也能识别字符串 \"N/A\"。\n\nstudents &lt;- read_csv(\"data/students.csv\", na = c(\"N/A\", \"\"))\n\nstudents\n#&gt; # A tibble: 6 × 5\n#&gt;   `Student ID` `Full Name`      favourite.food     mealPlan            AGE  \n#&gt;          &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n#&gt; 1            1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n#&gt; 2            2 Barclay Lynn     French fries       Lunch only          5    \n#&gt; 3            3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch 7    \n#&gt; 4            4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n#&gt; 5            5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n#&gt; 6            6 Güvenç Attila    Ice cream          Lunch only          6\n\nYou might also notice that the Student ID and Full Name columns are surrounded by backticks.\n你可能还会注意到 Student ID 和 Full Name 列被反引号包围。\nThat’s because they contain spaces, breaking R’s usual rules for variable names; they’re non-syntactic names.\n这是因为它们包含空格，破坏了 R 通常的变量命名规则；它们是非语法名称 (non-syntactic names)。\nTo refer to these variables, you need to surround them with backticks, `:\n要引用这些变量，你需要用反引号 ` 将它们括起来：\n\nstudents |&gt; \n  rename(\n    student_id = `Student ID`,\n    full_name = `Full Name`\n  )\n#&gt; # A tibble: 6 × 5\n#&gt;   student_id full_name        favourite.food     mealPlan            AGE  \n#&gt;        &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n#&gt; 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n#&gt; 2          2 Barclay Lynn     French fries       Lunch only          5    \n#&gt; 3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch 7    \n#&gt; 4          4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n#&gt; 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n#&gt; 6          6 Güvenç Attila    Ice cream          Lunch only          6\n\nAn alternative approach is to use janitor::clean_names() to use some heuristics to turn them all into snake case at once1.\n另一种方法是使用 janitor::clean_names()，它会运用一些启发式方法将所有列名一次性转换为蛇形命名法 (snake case)1。\n\nstudents |&gt; janitor::clean_names()\n#&gt; # A tibble: 6 × 5\n#&gt;   student_id full_name        favourite_food     meal_plan           age  \n#&gt;        &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n#&gt; 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n#&gt; 2          2 Barclay Lynn     French fries       Lunch only          5    \n#&gt; 3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch 7    \n#&gt; 4          4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n#&gt; 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n#&gt; 6          6 Güvenç Attila    Ice cream          Lunch only          6\n\nAnother common task after reading in data is to consider variable types.\n读入数据后的另一个常见任务是考虑变量类型。\nFor example, meal_plan is a categorical variable with a known set of possible values, which in R should be represented as a factor:\n例如，meal_plan 是一个分类变量，其可能值的集合是已知的，在 R 中应该表示为一个因子 (factor)：\n\nstudents |&gt;\n  janitor::clean_names() |&gt;\n  mutate(meal_plan = factor(meal_plan))\n#&gt; # A tibble: 6 × 5\n#&gt;   student_id full_name        favourite_food     meal_plan           age  \n#&gt;        &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;fct&gt;               &lt;chr&gt;\n#&gt; 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n#&gt; 2          2 Barclay Lynn     French fries       Lunch only          5    \n#&gt; 3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch 7    \n#&gt; 4          4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n#&gt; 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n#&gt; 6          6 Güvenç Attila    Ice cream          Lunch only          6\n\nNote that the values in the meal_plan variable have stayed the same, but the type of variable denoted underneath the variable name has changed from character (&lt;chr&gt;) to factor (&lt;fct&gt;).\n请注意，meal_plan 变量中的值保持不变，但变量名下方表示的变量类型已从字符 (&lt;chr&gt;) 变为了因子 (&lt;fct&gt;)。\nYou’ll learn more about factors in Chapter 16.\n你将在 Chapter 16 中学到更多关于因子的知识。\nBefore you analyze these data, you’ll probably want to fix the age column.\n在分析这些数据之前，你可能需要修正 age 列。\nCurrently, age is a character variable because one of the observations is typed out as five instead of a numeric 5.\n目前，age 是一个字符变量，因为其中一个观测值被输入为 five 而不是数字 5。\nWe discuss the details of fixing this issue in Chapter 20.\n我们将在 Chapter 20 中讨论修复这个问题的细节。\n\nstudents &lt;- students |&gt;\n  janitor::clean_names() |&gt;\n  mutate(\n    meal_plan = factor(meal_plan),\n    age = parse_number(if_else(age == \"five\", \"5\", age))\n  )\n\nstudents\n#&gt; # A tibble: 6 × 5\n#&gt;   student_id full_name        favourite_food     meal_plan             age\n#&gt;        &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;fct&gt;               &lt;dbl&gt;\n#&gt; 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4\n#&gt; 2          2 Barclay Lynn     French fries       Lunch only              5\n#&gt; 3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch     7\n#&gt; 4          4 Leon Rossini     Anchovies          Lunch only             NA\n#&gt; 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch     5\n#&gt; 6          6 Güvenç Attila    Ice cream          Lunch only              6\n\nA new function here is if_else(), which has three arguments.\n这里有一个新函数 if_else()，它有三个参数。\nThe first argument test should be a logical vector.\n第一个参数 test 应该是一个逻辑向量。\nThe result will contain the value of the second argument, yes, when test is TRUE, and the value of the third argument, no, when it is FALSE.\n当 test 为 TRUE 时，结果将包含第二个参数 yes 的值；当 test 为 FALSE 时，结果将包含第三个参数 no 的值。\nHere we’re saying if age is the character string \"five\", make it \"5\", and if not leave it as age.\n这里我们是说，如果 age 是字符串 \"five\"，就把它变成 \"5\"，否则保持 age 不变。\nYou will learn more about if_else() and logical vectors in Chapter 12.\n你将在 Chapter 12 中学习更多关于 if_else() 和逻辑向量的知识。\n\n7.2.2 Other arguments\nThere are a couple of other important arguments that we need to mention, and they’ll be easier to demonstrate if we first show you a handy trick: read_csv() can read text strings that you’ve created and formatted like a CSV file:\n还有几个其他重要的参数需要我们提及，如果我们先向你展示一个方便的技巧，它们会更容易演示：read_csv() 可以读取你创建并格式化为 CSV 文件形式的文本字符串：\n\nread_csv(\n  \"a,b,c\n  1,2,3\n  4,5,6\"\n)\n#&gt; # A tibble: 2 × 3\n#&gt;       a     b     c\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1     2     3\n#&gt; 2     4     5     6\n\nUsually, read_csv() uses the first line of the data for the column names, which is a very common convention.\n通常，read_csv() 使用数据的第一行作为列名，这是一个非常普遍的惯例。\nBut it’s not uncommon for a few lines of metadata to be included at the top of the file.\n但是在文件顶部包含几行元数据的情况也并不少见。\nYou can use skip = n to skip the first n lines or use comment = \"#\" to drop all lines that start with (e.g.) #:\n你可以使用 skip = n 来跳过前 n 行，或者使用 comment = \"#\" 来删除所有以（例如）# 开头的行：\n\nread_csv(\n  \"The first line of metadata\n  The second line of metadata\n  x,y,z\n  1,2,3\",\n  skip = 2\n)\n#&gt; # A tibble: 1 × 3\n#&gt;       x     y     z\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1     2     3\n\nread_csv(\n  \"# A comment I want to skip\n  x,y,z\n  1,2,3\",\n  comment = \"#\"\n)\n#&gt; # A tibble: 1 × 3\n#&gt;       x     y     z\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1     2     3\n\nIn other cases, the data might not have column names.\n在其他情况下，数据可能没有列名。\nYou can use col_names = FALSE to tell read_csv() not to treat the first row as headings and instead label them sequentially from X1 to Xn:\n你可以使用 col_names = FALSE 来告诉 read_csv() 不要将第一行作为标题，而是将它们从 X1 到 Xn 顺序标记：\n\nread_csv(\n  \"1,2,3\n  4,5,6\",\n  col_names = FALSE\n)\n#&gt; # A tibble: 2 × 3\n#&gt;      X1    X2    X3\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1     2     3\n#&gt; 2     4     5     6\n\nAlternatively, you can pass col_names a character vector which will be used as the column names:\n或者，你可以给 col_names 传递一个字符向量，它将被用作列名：\n\nread_csv(\n  \"1,2,3\n  4,5,6\",\n  col_names = c(\"x\", \"y\", \"z\")\n)\n#&gt; # A tibble: 2 × 3\n#&gt;       x     y     z\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1     2     3\n#&gt; 2     4     5     6\n\nThese arguments are all you need to know to read the majority of CSV files that you’ll encounter in practice.\n这些参数就是你在实践中读取绝大多数 CSV 文件所需要知道的全部内容。\n(For the rest, you’ll need to carefully inspect your .csv file and read the documentation for read_csv()’s many other arguments.)\n（对于其余情况，你需要仔细检查你的 .csv 文件，并阅读 read_csv() 许多其他参数的文档。）\n\n7.2.3 Other file types\nOnce you’ve mastered read_csv(), using readr’s other functions is straightforward; it’s just a matter of knowing which function to reach for:\n一旦你掌握了 read_csv()，使用 readr 的其他函数就变得很简单了；关键在于知道该使用哪个函数：\n\nread_csv2() reads semicolon-separated files. These use ; instead of , to separate fields and are common in countries that use , as the decimal marker.read_csv2() 读取以分号分隔的文件。这些文件使用 ; 而不是 , 来分隔字段，在那些使用 , 作为小数点的国家很常见。\nread_tsv() reads tab-delimited files.read_tsv() 读取制表符分隔的文件。\nread_delim() reads in files with any delimiter, attempting to automatically guess the delimiter if you don’t specify it.read_delim() 读取任何分隔符的文件，如果你不指定分隔符，它会尝试自动猜测。\nread_fwf() reads fixed-width files. You can specify fields by their widths with fwf_widths() or by their positions with fwf_positions().read_fwf() 读取固定宽度文件。你可以使用 fwf_widths() 按宽度指定字段，或使用 fwf_positions() 按位置指定。\nread_table() reads a common variation of fixed-width files where columns are separated by white space.read_table() 读取固定宽度文件的一种常见变体，其中列由空白分隔。\nread_log() reads Apache-style log files.read_log() 读取 Apache 风格的日志文件。\n\n7.2.4 Exercises\n\nWhat function would you use to read a file where fields were separated with “|”?\nApart from file, skip, and comment, what other arguments do read_csv() and read_tsv() have in common?\nWhat are the most important arguments to read_fwf()?\n\nSometimes strings in a CSV file contain commas. To prevent them from causing problems, they need to be surrounded by a quoting character, like \" or '. By default, read_csv() assumes that the quoting character will be \". To read the following text into a data frame, what argument to read_csv() do you need to specify?\n\n\"x,y\\n1,'a,b'\"\n\n\n\nIdentify what is wrong with each of the following inline CSV files. What happens when you run the code?\n\nread_csv(\"a,b\\n1,2,3\\n4,5,6\")\nread_csv(\"a,b,c\\n1,2\\n1,2,3,4\")\nread_csv(\"a,b\\n\\\"1\")\nread_csv(\"a,b\\n1,2\\na,b\")\nread_csv(\"a;b\\n1;3\")\n\n\n\nPractice referring to non-syntactic names in the following data frame by:\n\nExtracting the variable called 1.\nPlotting a scatterplot of 1 vs. 2.\nCreating a new column called 3, which is 2 divided by 1.\nRenaming the columns to one, two, and three.\n\n\nannoying &lt;- tibble(\n  `1` = 1:10,\n  `2` = `1` * 2 + rnorm(length(`1`))\n)",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data import</span>"
    ]
  },
  {
    "objectID": "data-import.html#sec-col-types",
    "href": "data-import.html#sec-col-types",
    "title": "7  Data import",
    "section": "\n7.3 Controlling column types",
    "text": "7.3 Controlling column types\nA CSV file doesn’t contain any information about the type of each variable (i.e. whether it’s a logical, number, string, etc.), so readr will try to guess the type.\nCSV 文件不包含任何关于每个变量类型的信息（例如，它是一个逻辑值、数字、字符串等），所以 readr 会尝试猜测其类型。\nThis section describes how the guessing process works, how to resolve some common problems that cause it to fail, and, if needed, how to supply the column types yourself.\n本节描述了猜测过程是如何工作的，如何解决一些导致它失败的常见问题，以及在需要时如何自己提供列类型。\nFinally, we’ll mention a few general strategies that are useful if readr is failing catastrophically and you need to get more insight into the structure of your file.\n最后，我们将提及一些通用的策略，如果 readr 发生灾难性故障，而你需要更深入地了解文件结构，这些策略会很有用。\n\n7.3.1 Guessing types\nreadr uses a heuristic to figure out the column types.\nreadr 使用一种启发式方法 (heuristic) 来确定列类型。\nFor each column, it pulls the values of 1,0002 rows spaced evenly from the first row to the last, ignoring missing values.\n对于每一列，它会从第一行到最后一行均匀地抽取 1000 行 2 的值，并忽略缺失值。\nIt then works through the following questions:\n然后它会按顺序考虑以下问题：\n\nDoes it contain only F, T, FALSE, or TRUE (ignoring case)? If so, it’s a logical.\n它是否只包含 F、T、FALSE 或 TRUE（忽略大小写）？如果是，那么它是一个逻辑值。\nDoes it contain only numbers (e.g., 1, -4.5, 5e6, Inf)? If so, it’s a number.\n它是否只包含数字（例如 1, -4.5, 5e6, Inf）？如果是，那么它是一个数字。\nDoes it match the ISO8601 standard? If so, it’s a date or date-time. (We’ll return to date-times in more detail in Section 17.2).\n它是否符合 ISO8601 标准？如果是，那么它是一个日期或日期时间。（我们将在 Section 17.2 中更详细地回到日期时间。）\nOtherwise, it must be a string.\n否则，它必须是一个字符串。\n\nYou can see that behavior in action in this simple example:\n你可以在这个简单的例子中看到这种行为：\n\nread_csv(\"\n  logical,numeric,date,string\n  TRUE,1,2021-01-15,abc\n  false,4.5,2021-02-15,def\n  T,Inf,2021-02-16,ghi\n\")\n#&gt; # A tibble: 3 × 4\n#&gt;   logical numeric date       string\n#&gt;   &lt;lgl&gt;     &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt; \n#&gt; 1 TRUE        1   2021-01-15 abc   \n#&gt; 2 FALSE       4.5 2021-02-15 def   \n#&gt; 3 TRUE      Inf   2021-02-16 ghi\n\nThis heuristic works well if you have a clean dataset, but in real life, you’ll encounter a selection of weird and beautiful failures.\n如果你有一个干净的数据集，这种启发式方法会工作得很好，但在现实生活中，你会遇到各种奇怪而美妙的失败情况。\n\n7.3.2 Missing values, column types, and problems\nThe most common way column detection fails is that a column contains unexpected values, and you get a character column instead of a more specific type.\n列检测失败最常见的方式是列中包含意外值，导致你得到一个字符列而不是更具体的类型。\nOne of the most common causes for this is a missing value, recorded using something other than the NA that readr expects.\n其中一个最常见的原因是缺失值，它使用了 readr 期望的 NA 之外的其它方式进行记录。\nTake this simple 1 column CSV file as an example:\n以这个简单的单列 CSV 文件为例：\n\nsimple_csv &lt;- \"\n  x\n  10\n  .\n  20\n  30\"\n\nIf we read it without any additional arguments, x becomes a character column:\n如果我们不带任何附加参数读取它，x 会变成一个字符列：\n\nread_csv(simple_csv)\n#&gt; # A tibble: 4 × 1\n#&gt;   x    \n#&gt;   &lt;chr&gt;\n#&gt; 1 10   \n#&gt; 2 .    \n#&gt; 3 20   \n#&gt; 4 30\n\nIn this very small case, you can easily see the missing value ..\n在这个非常小的情况下，你可以很容易地看到缺失值 .。\nBut what happens if you have thousands of rows with only a few missing values represented by .s sprinkled among them?\n但是，如果你有成千上万行数据，其中只散布着少数用 . 表示的缺失值，会发生什么呢？\nOne approach is to tell readr that x is a numeric column, and then see where it fails.\n一种方法是告诉 readr x 是一个数值列，然后看它在哪里失败。\nYou can do that with the col_types argument, which takes a named list where the names match the column names in the CSV file:\n你可以通过 col_types 参数来做到这一点，它接受一个命名列表，其中名称与 CSV 文件中的列名匹配：\n\ndf &lt;- read_csv(\n  simple_csv, \n  col_types = list(x = col_double())\n)\n#&gt; Warning: One or more parsing issues, call `problems()` on your data frame for\n#&gt; details, e.g.:\n#&gt;   dat &lt;- vroom(...)\n#&gt;   problems(dat)\n\nNow read_csv() reports that there was a problem, and tells us we can find out more with problems():\n现在 read_csv() 报告说有问题，并告诉我们可以用 problems() 了解更多信息：\n\nproblems(df)\n#&gt; # A tibble: 1 × 5\n#&gt;     row   col expected actual file                                           \n#&gt;   &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;                                          \n#&gt; 1     3     1 a double .      C:/Users/14913/AppData/Local/Temp/Rtmp2j43zp/f…\n\nThis tells us that there was a problem in row 3, col 1 where readr expected a double but got a ..\n这告诉我们在第 3 行第 1 列存在一个问题，readr 期望一个双精度浮点数 (double)，但得到了一个 .。\nThat suggests this dataset uses . for missing values.\n这表明该数据集使用 . 表示缺失值。\nSo then we set na = \".\", the automatic guessing succeeds, giving us the numeric column that we want:\n所以我们设置 na = \".\"，自动猜测成功了，给了我们想要的数值列：\n\nread_csv(simple_csv, na = \".\")\n#&gt; # A tibble: 4 × 1\n#&gt;       x\n#&gt;   &lt;dbl&gt;\n#&gt; 1    10\n#&gt; 2    NA\n#&gt; 3    20\n#&gt; 4    30\n\n\n7.3.3 Column types\nreadr provides a total of nine column types for you to use:\nreadr 共提供了九种列类型供你使用：\n\ncol_logical() and col_double() read logicals and real numbers. They’re relatively rarely needed (except as above), since readr will usually guess them for you.col_logical() 和 col_double() 读取逻辑值和实数。它们相对较少需要（除非像上面那样），因为 readr 通常会为你猜出它们。\ncol_integer() reads integers. We seldom distinguish integers and doubles in this book because they’re functionally equivalent, but reading integers explicitly can occasionally be useful because they occupy half the memory of doubles.col_integer() 读取整数。在本书中，我们很少区分整数和双精度浮点数，因为它们在功能上是等效的，但明确读取为整数有时可能很有用，因为它们只占用双精度浮点数一半的内存。\ncol_character() reads strings. This can be useful to specify explicitly when you have a column that is a numeric identifier, i.e., long series of digits that identifies an object but doesn’t make sense to apply mathematical operations to. Examples include phone numbers, social security numbers, credit card numbers, etc.col_character() 读取字符串。当你有一个作为数字标识符的列时，明确指定它会很有用，即，一长串标识对象的数字，但对其应用数学运算没有意义。例如电话号码、社会安全号码、信用卡号等。\ncol_factor(), col_date(), and col_datetime() create factors, dates, and date-times respectively; you’ll learn more about those when we get to those data types in Chapter 16 and Chapter 17.col_factor(), col_date() 和 col_datetime() 分别创建因子、日期和日期时间；当我们在 Chapter 16 和 Chapter 17 中讲到这些数据类型时，你将学到更多相关知识。\ncol_number() is a permissive numeric parser that will ignore non-numeric components, and is particularly useful for currencies. You’ll learn more about it in Chapter 13.col_number() 是一个宽容的数字解析器，它会忽略非数字部分，对于货币特别有用。你将在 Chapter 13 中学到更多相关知识。\ncol_skip() skips a column so it’s not included in the result, which can be useful for speeding up reading the data if you have a large CSV file and you only want to use some of the columns.col_skip() 会跳过一列，使其不包含在结果中，这在处理大型 CSV 文件且你只想使用部分列时，可以加快数据读取速度。\n\nIt’s also possible to override the default column by switching from list() to cols() and specifying .default:\n也可以通过从 list() 切换到 cols() 并指定 .default 来覆盖默认的列类型：\n\nanother_csv &lt;- \"\nx,y,z\n1,2,3\"\n\nread_csv(\n  another_csv, \n  col_types = cols(.default = col_character())\n)\n#&gt; # A tibble: 1 × 3\n#&gt;   x     y     z    \n#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n#&gt; 1 1     2     3\n\nAnother useful helper is cols_only() which will read in only the columns you specify:\n另一个有用的辅助函数是 cols_only()，它将只读入你指定的列：\n\nread_csv(\n  another_csv,\n  col_types = cols_only(x = col_character())\n)\n#&gt; # A tibble: 1 × 1\n#&gt;   x    \n#&gt;   &lt;chr&gt;\n#&gt; 1 1",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data import</span>"
    ]
  },
  {
    "objectID": "data-import.html#sec-readr-directory",
    "href": "data-import.html#sec-readr-directory",
    "title": "7  Data import",
    "section": "\n7.4 Reading data from multiple files",
    "text": "7.4 Reading data from multiple files\nSometimes your data is split across multiple files instead of being contained in a single file.\n有时你的数据分散在多个文件中，而不是包含在单个文件中。\nFor example, you might have sales data for multiple months, with each month’s data in a separate file: 01-sales.csv for January, 02-sales.csv for February, and 03-sales.csv for March.\n例如，你可能有多個月的銷售數據，每個月的數據都存放在一個單獨的文件中：一月是 01-sales.csv，二月是 02-sales.csv，三月是 03-sales.csv。\nWith read_csv() you can read these data in at once and stack them on top of each other in a single data frame.\n使用 read_csv()，你可以一次性读取这些数据，并将它们堆叠在一个数据框中。\n\nsales_files &lt;- c(\"data/01-sales.csv\", \"data/02-sales.csv\", \"data/03-sales.csv\")\nread_csv(sales_files, id = \"file\")\n#&gt; # A tibble: 19 × 6\n#&gt;   file              month    year brand  item     n\n#&gt;   &lt;chr&gt;             &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 data/01-sales.csv January  2019     1  1234     3\n#&gt; 2 data/01-sales.csv January  2019     1  8721     9\n#&gt; 3 data/01-sales.csv January  2019     1  1822     2\n#&gt; 4 data/01-sales.csv January  2019     2  3333     1\n#&gt; 5 data/01-sales.csv January  2019     2  2156     9\n#&gt; 6 data/01-sales.csv January  2019     2  3987     6\n#&gt; # ℹ 13 more rows\n\nOnce again, the code above will work if you have the CSV files in a data folder in your project.\n同样，如果你的项目中有一个 data 文件夹，并且其中包含这些 CSV 文件，那么上面的代码将会正常工作。\nYou can download these files from https://pos.it/r4ds-01-sales, https://pos.it/r4ds-02-sales, and https://pos.it/r4ds-03-sales or you can read them directly with:\n你可以从 https://pos.it/r4ds-01-sales、https://pos.it/r4ds-02-sales 和 https://pos.it/r4ds-03-sales 下载这些文件，或者使用以下代码直接读取它们：\n\nsales_files &lt;- c(\n  \"https://pos.it/r4ds-01-sales\",\n  \"https://pos.it/r4ds-02-sales\",\n  \"https://pos.it/r4ds-03-sales\"\n)\nread_csv(sales_files, id = \"file\")\n\nThe id argument adds a new column called file to the resulting data frame that identifies the file the data come from.id 参数会向结果数据框中添加一个名为 file 的新列，用于标识数据来自哪个文件。\nThis is especially helpful in circumstances where the files you’re reading in do not have an identifying column that can help you trace the observations back to their original sources.\n当 你读入的文件中没有一个可用于将观测值追溯到其原始来源的标识列时，这尤其有用。\nIf you have many files you want to read in, it can get cumbersome to write out their names as a list.\n如果你有许多文件要读入，将它们的名字写成一个列表可能会很麻烦。\nInstead, you can use the base list.files() function to find the files for you by matching a pattern in the file names.\n相反，你可以使用基础函数 list.files()，通过匹配文件名中的模式来为你查找文件。\nYou’ll learn more about these patterns in Chapter 15.\n你将在 Chapter 15 中学到更多关于这些模式的知识。\n\nsales_files &lt;- list.files(\"data\", pattern = \"sales\\\\.csv$\", full.names = TRUE)\nsales_files\n#&gt; [1] \"data/01-sales.csv\" \"data/02-sales.csv\" \"data/03-sales.csv\"",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data import</span>"
    ]
  },
  {
    "objectID": "data-import.html#sec-writing-to-a-file",
    "href": "data-import.html#sec-writing-to-a-file",
    "title": "7  Data import",
    "section": "\n7.5 Writing to a file",
    "text": "7.5 Writing to a file\nreadr also comes with two useful functions for writing data back to disk: write_csv() and write_tsv().\nreadr 还附带了两个用于将数据写回磁盘的有用函数：write_csv() 和 write_tsv()。\nThe most important arguments to these functions are x (the data frame to save) and file (the location to save it).\n这些函数最重要的参数是 x（要保存的数据框）和 file（保存它的位置）。\nYou can also specify how missing values are written with na, and if you want to append to an existing file.\n你还可以用 na 指定缺失值的写入方式，以及是否要 append（追加）到现有文件中。\n\nwrite_csv(students, \"students.csv\")\n\nNow let’s read that csv file back in.\n现在让我们把那个 csv 文件再读回来。\nNote that the variable type information that you just set up is lost when you save to CSV because you’re starting over with reading from a plain text file again:\n请注意，当你保存为 CSV 格式时，刚刚设置的变量类型信息会丢失，因为你又从纯文本文件重新开始读取了：\n\nstudents\n#&gt; # A tibble: 6 × 5\n#&gt;   student_id full_name        favourite_food     meal_plan             age\n#&gt;        &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;fct&gt;               &lt;dbl&gt;\n#&gt; 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4\n#&gt; 2          2 Barclay Lynn     French fries       Lunch only              5\n#&gt; 3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch     7\n#&gt; 4          4 Leon Rossini     Anchovies          Lunch only             NA\n#&gt; 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch     5\n#&gt; 6          6 Güvenç Attila    Ice cream          Lunch only              6\nwrite_csv(students, \"students-2.csv\")\nread_csv(\"students-2.csv\")\n#&gt; # A tibble: 6 × 5\n#&gt;   student_id full_name        favourite_food     meal_plan             age\n#&gt;        &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;dbl&gt;\n#&gt; 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4\n#&gt; 2          2 Barclay Lynn     French fries       Lunch only              5\n#&gt; 3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch     7\n#&gt; 4          4 Leon Rossini     Anchovies          Lunch only             NA\n#&gt; 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch     5\n#&gt; 6          6 Güvenç Attila    Ice cream          Lunch only              6\n\nThis makes CSVs a little unreliable for caching interim results—you need to recreate the column specification every time you load in.\n这使得 CSV 对于缓存中间结果有些不可靠——每次加载时你都需要重新创建列的规格。\nThere are two main alternatives:\n有两个主要的替代方案：\n\n\nwrite_rds() and read_rds() are uniform wrappers around the base functions readRDS() and saveRDS(). These store data in R’s custom binary format called RDS. This means that when you reload the object, you are loading the exact same R object that you stored.write_rds() 和 read_rds() 是对基础函数 readRDS() 和 saveRDS() 的统一封装。它们以 R 的自定义二进制格式（称为 RDS）存储数据。这意味着当你重新加载对象时，你加载的是与你存储的完全相同的 R 对象。\n\nwrite_rds(students, \"students.rds\")\nread_rds(\"students.rds\")\n#&gt; # A tibble: 6 × 5\n#&gt;   student_id full_name        favourite_food     meal_plan             age\n#&gt;        &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;fct&gt;               &lt;dbl&gt;\n#&gt; 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4\n#&gt; 2          2 Barclay Lynn     French fries       Lunch only              5\n#&gt; 3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch     7\n#&gt; 4          4 Leon Rossini     Anchovies          Lunch only             NA\n#&gt; 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch     5\n#&gt; 6          6 Güvenç Attila    Ice cream          Lunch only              6\n\n\n\nThe arrow package allows you to read and write parquet files, a fast binary file format that can be shared across programming languages. We’ll return to arrow in more depth in Chapter 22.\narrow 包允许你读写 parquet 文件，这是一种快速的二进制文件格式，可以在不同编程语言之间共享。我们将在 Chapter 22 中更深入地探讨 arrow。\n\nlibrary(arrow)\nwrite_parquet(students, \"students.parquet\")\nread_parquet(\"students.parquet\")\n#&gt; # A tibble: 6 × 5\n#&gt;   student_id full_name          favourite_food     meal_plan               age\n#&gt;        &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;              &lt;fct&gt;                 &lt;dbl&gt;\n#&gt; 1          1 Sunil Huffmann     Strawberry yoghurt Lunch only                4\n#&gt; 2          2 Barclay Lynn       French fries       Lunch only                5\n#&gt; 3          3 Jayendra Lyne      NA                 Breakfast and lunch       7\n#&gt; 4          4 Leon Rossini       Anchovies          Lunch only               NA\n#&gt; 5          5 Chidiegwu Dunkel   Pizza              Breakfast and lunch       5\n#&gt; 6          6 Güvenç Attila      Ice cream          Lunch only                6\n\n\n\nParquet tends to be much faster than RDS and is usable outside of R, but does require the arrow package.\nParquet 格式通常比 RDS 快得多，并且可以在 R 之外使用，但需要 arrow 包。",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data import</span>"
    ]
  },
  {
    "objectID": "data-import.html#data-entry",
    "href": "data-import.html#data-entry",
    "title": "7  Data import",
    "section": "\n7.6 Data entry",
    "text": "7.6 Data entry\nSometimes you’ll need to assemble a tibble “by hand” doing a little data entry in your R script.\n有时你需要在 R 脚本中通过少量的数据录入来“手动”组建一个 tibble。\nThere are two useful functions to help you do this which differ in whether you layout the tibble by columns or by rows.\n有两个有用的函数可以帮助你做到这一点，它们的区别在于你是按列还是按行来布局 tibble。\ntibble() works by column:tibble() 按列工作：\n\ntibble(\n  x = c(1, 2, 5), \n  y = c(\"h\", \"m\", \"g\"),\n  z = c(0.08, 0.83, 0.60)\n)\n#&gt; # A tibble: 3 × 3\n#&gt;       x y         z\n#&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n#&gt; 1     1 h      0.08\n#&gt; 2     2 m      0.83\n#&gt; 3     5 g      0.6\n\nLaying out the data by column can make it hard to see how the rows are related, so an alternative is tribble(), short for transposed tibble, which lets you lay out your data row by row.\n按列布局数据可能很难看清行与行之间的关系，所以一个替代方案是 tribble()，即转置 tibble (transposed tibble) 的缩写，它让你能够逐行布局数据。\ntribble() is customized for data entry in code: column headings start with ~ and entries are separated by commas.tribble() 是为在代码中进行数据录入而定制的：列标题以 ~ 开头，条目之间用逗号分隔。\nThis makes it possible to lay out small amounts of data in an easy to read form:\n这使得以易于阅读的形式布置少量数据成为可能：\n\ntribble(\n  ~x, ~y, ~z,\n  1, \"h\", 0.08,\n  2, \"m\", 0.83,\n  5, \"g\", 0.60\n)\n#&gt; # A tibble: 3 × 3\n#&gt;       x y         z\n#&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n#&gt; 1     1 h      0.08\n#&gt; 2     2 m      0.83\n#&gt; 3     5 g      0.6",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data import</span>"
    ]
  },
  {
    "objectID": "data-import.html#summary",
    "href": "data-import.html#summary",
    "title": "7  Data import",
    "section": "\n7.7 Summary",
    "text": "7.7 Summary\nIn this chapter, you’ve learned how to load CSV files with read_csv() and to do your own data entry with tibble() and tribble().\n在本章中，你学会了如何使用 read_csv() 加载 CSV 文件，以及如何使用 tibble() 和 tribble() 进行自己的数据录入。\nYou’ve learned how csv files work, some of the problems you might encounter, and how to overcome them.\n你已经了解了 csv 文件的工作原理，可能会遇到的一些问题，以及如何克服它们。\nWe’ll come to data import a few times in this book: Chapter 20 from Excel and Google Sheets, Chapter 21 will show you how to load data from databases, Chapter 22 from parquet files, Chapter 23 from JSON, and Chapter 24 from websites.\n在本书中，我们会多次涉及数据导入：Chapter 20 介绍从 Excel 和 Google Sheets 导入，Chapter 21 将向你展示如何从数据库加载数据，Chapter 22 从 parquet 文件导入，Chapter 23 从 JSON 导入，以及 Chapter 24 从网站导入。\nWe’re just about at the end of this section of the book, but there’s one important last topic to cover: how to get help.\n我们即将结束本书的这一部分，但还有一个重要的最后主题需要涵盖：如何获取帮助。\nSo in the next chapter, you’ll learn some good places to look for help, how to create a reprex to maximize your chances of getting good help, and some general advice on keeping up with the world of R.\n因此，在下一章中，你将学到一些寻求帮助的好地方，如何创建一个可复现的例子 (reprex) 来最大化你获得有效帮助的机会，以及一些关于跟上 R 世界发展步伐的通用建议。",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data import</span>"
    ]
  },
  {
    "objectID": "data-import.html#footnotes",
    "href": "data-import.html#footnotes",
    "title": "7  Data import",
    "section": "",
    "text": "The janitor package is not part of the tidyverse, but it offers handy functions for data cleaning and works well within data pipelines that use |&gt;.↩︎\nYou can override the default of 1000 with the guess_max argument.↩︎",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data import</span>"
    ]
  },
  {
    "objectID": "workflow-help.html",
    "href": "workflow-help.html",
    "title": "8  Workflow: getting help",
    "section": "",
    "text": "8.1 Google is your friend\nThis book is not an island; there is no single resource that will allow you to master R. As you begin to apply the techniques described in this book to your own data, you will soon find questions that we do not answer. This section describes a few tips on how to get help and to help you keep learning.\n本书不是一座孤岛；没有任何单一资源能让你精通 R。 当你开始将本书中描述的技术应用于你自己的数据时，你很快就会发现我们没有回答的问题。 本节将介绍一些获取帮助以及帮助你持续学习的技巧。\nIf you get stuck, start with Google. Typically adding “R” to a query is enough to restrict it to relevant results: if the search isn’t useful, it often means that there aren’t any R-specific results available. Additionally, adding package names like “tidyverse” or “ggplot2” will help narrow down the results to code that will feel more familiar to you as well, e.g., “how to make a boxplot in R” vs. “how to make a boxplot in R with ggplot2”. Google is particularly useful for error messages. If you get an error message and you have no idea what it means, try googling it! Chances are that someone else has been confused by it in the past, and there will be help somewhere on the web. (If the error message isn’t in English, run Sys.setenv(LANGUAGE = \"en\") and re-run the code; you’re more likely to find help for English error messages.)\n如果你遇到困难，从 Google 开始。 通常，在查询中添加 “R” 就足以将其限制在相关的结果中：如果搜索结果不理想，这通常意味着没有 R 特定的结果可用。 此外，添加像 “tidyverse” 或 “ggplot2” 这样的包名也会帮助你将结果缩小到你更熟悉的代码，例如，“how to make a boxplot in R” 对比 “how to make a boxplot in R with ggplot2”。 Google 对于错误信息尤其有用。 如果你收到一条错误信息，并且不知道它是什么意思，试试用 Google 搜索它！ 很可能过去有其他人也对此感到困惑，网上某个地方会有帮助。 （如果错误信息不是英文的，运行 Sys.setenv(LANGUAGE = \"en\") 并重新运行代码；你更有可能找到英文错误信息的帮助。）\nIf Google doesn’t help, try Stack Overflow. Start by spending a little time searching for an existing answer, including [R], to restrict your search to questions and answers that use R.\n如果 Google 帮不了你，试试 Stack Overflow。 首先花点时间搜索现有的答案，记得加上 [R]，将你的搜索限制在使用 R 的问题和答案上。",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Workflow: getting help</span>"
    ]
  },
  {
    "objectID": "workflow-help.html#making-a-reprex",
    "href": "workflow-help.html#making-a-reprex",
    "title": "8  Workflow: getting help",
    "section": "\n8.2 Making a reprex",
    "text": "8.2 Making a reprex\nIf your googling doesn’t find anything useful, it’s a really good idea to prepare a reprex, short for minimal reproducible example. A good reprex makes it easier for other people to help you, and often you’ll figure out the problem yourself in the course of making it. There are two parts to creating a reprex:\n如果你的 Google 搜索没有找到任何有用的东西，那么准备一个 reprex (最小可复现示例的缩写) 是个非常好的主意。 一个好的 reprex 能让其他人更容易帮助你，而且通常在制作 reprex 的过程中你就能自己找出问题所在。 创建 reprex 有两个部分：\n\nFirst, you need to make your code reproducible. This means that you need to capture everything, i.e. include any library() calls and create all necessary objects. The easiest way to make sure you’ve done this is using the reprex package.\n首先，你需要让你的代码可复现。 这意味着你需要捕获所有东西，即包含任何 library() 调用并创建所有必要的对象。 确保做到这一点最简单的方法是使用 reprex 包。\nSecond, you need to make it minimal. Strip away everything that is not directly related to your problem. This usually involves creating a much smaller and simpler R object than the one you’re facing in real life or even using built-in data.\n其次，你需要让它最小化。 剔除所有与你的问题不直接相关的东西。 这通常涉及创建一个比你现实生活中面临的 R 对象小得多、简单得多的对象，甚至使用内置数据。\n\nThat sounds like a lot of work! And it can be, but it has a great payoff:\n这听起来工作量很大！ 事实可能如此，但它有巨大的回报：\n\n80% of the time, creating an excellent reprex reveals the source of your problem. It’s amazing how often the process of writing up a self-contained and minimal example allows you to answer your own question.\n80% 的情况下，创建一个出色的 reprex 会揭示你问题的根源。 令人惊讶的是，编写一个独立的最小示例的过程常常能让你自己回答自己的问题。\nThe other 20% of the time, you will have captured the essence of your problem in a way that is easy for others to play with. This substantially improves your chances of getting help!\n另外 20% 的情况下，你将以一种便于他人操作的方式抓住了问题的本质。 这大大提高了你获得帮助的机会！\n\nWhen creating a reprex by hand, it’s easy to accidentally miss something, meaning your code can’t be run on someone else’s computer. Avoid this problem by using the reprex package, which is installed as part of the tidyverse. Let’s say you copy this code onto your clipboard (or, on RStudio Server or Cloud, select it):\n当手动创建 reprex 时，很容易不小心漏掉某些东西，这意味着你的代码无法在别人的电脑上运行。 通过使用 reprex 包可以避免这个问题，该包是作为 tidyverse 的一部分安装的。 假设你将这段代码复制到剪贴板（或者，在 RStudio Server 或 Cloud 上，选中它）：\n\ny &lt;- 1:4\nmean(y)\n\nThen call reprex(), where the default output is formatted for GitHub:\n然后调用 reprex()，其默认输出格式是为 GitHub 准备的：\nreprex::reprex()\nA nicely rendered HTML preview will display in RStudio’s Viewer (if you’re in RStudio) or your default browser otherwise. The reprex is automatically copied to your clipboard (on RStudio Server or Cloud, you will need to copy this yourself):\n一个渲染精美的 HTML 预览将显示在 RStudio 的 Viewer 窗格中（如果你在 RStudio 中）或者你的默认浏览器中。 reprex 会自动复制到你的剪贴板（在 RStudio Server 或 Cloud 上，你需要自己复制）：\n``` r\ny &lt;- 1:4\nmean(y)\n#&gt; [1] 2.5\n```\nThis text is formatted in a special way, called Markdown, which can be pasted to sites like StackOverflow or Github and they will automatically render it to look like code. Here’s what that Markdown would look like rendered on GitHub:\n这段文本以一种特殊的方式格式化，称为 Markdown，可以粘贴到像 StackOverflow 或 Github 这样的网站上，它们会自动将其渲染成代码的样子。 以下是该 Markdown 在 GitHub 上渲染后的样子：\n\ny &lt;- 1:4\nmean(y)\n#&gt; [1] 2.5\n\nAnyone else can copy, paste, and run this immediately.\n任何其他人都可以立即复制、粘贴并运行这段代码。\nThere are three things you need to include to make your example reproducible: required packages, data, and code.\n要使你的示例可复现，你需要包含三样东西：所需的包、数据和代码。\n\nPackages should be loaded at the top of the script so it’s easy to see which ones the example needs. This is a good time to check that you’re using the latest version of each package; you may have discovered a bug that’s been fixed since you installed or last updated the package. For packages in the tidyverse, the easiest way to check is to run tidyverse_update().包 (Packages) 应该在脚本的顶部加载，这样可以很容易地看到示例需要哪些包。 这是一个检查你是否正在使用每个包最新版本的好时机；你可能发现了一个在你安装或上次更新包之后已经被修复的 bug。 对于 tidyverse 中的包，最简单的检查方法是运行 tidyverse_update()。\n\nThe easiest way to include data is to use dput() to generate the R code needed to recreate it. For example, to recreate the mtcars dataset in R, perform the following steps:\n包含 数据 (data) 最简单的方法是使用 dput() 生成重现数据所需的 R 代码。 例如，要在 R 中重现 mtcars 数据集，请执行以下步骤：\n\nRun dput(mtcars) in R\nCopy the output\nIn reprex, type mtcars &lt;-, then paste.\n\nTry to use the smallest subset of your data that still reveals the problem.\n尽量使用仍能揭示问题的最小数据子集。\n\n\nSpend a little bit of time ensuring that your code is easy for others to read:\n花一点时间确保你的 代码 (code) 便于他人阅读：\n\nMake sure you’ve used spaces and your variable names are concise yet informative.\n确保你使用了空格，并且你的变量名简洁而信息丰富。\nUse comments to indicate where your problem lies.\n使用注释来指出你的问题所在。\nDo your best to remove everything that is not related to the problem.\n尽力删除所有与问题无关的内容。\n\nThe shorter your code is, the easier it is to understand and the easier it is to fix.\n你的代码越短，就越容易理解，也越容易修复。\n\n\nFinish by checking that you have actually made a reproducible example by starting a fresh R session and copying and pasting your script.\n最后，通过启动一个新的 R 会话并复制粘贴你的脚本，来检查你是否确实创建了一个可复现的示例。\nCreating reprexes is not trivial, and it will take some practice to learn to create good, truly minimal reprexes. However, learning to ask questions that include the code, and investing the time to make it reproducible will continue to pay off as you learn and master R.\n创建 reprex 并非易事，需要一些练习才能学会创建好的、真正最小化的 reprex。 然而，学会提出包含代码的问题，并投入时间使其可复现，将在你学习和掌握 R 的过程中持续带来回报。",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Workflow: getting help</span>"
    ]
  },
  {
    "objectID": "workflow-help.html#investing-in-yourself",
    "href": "workflow-help.html#investing-in-yourself",
    "title": "8  Workflow: getting help",
    "section": "\n8.3 Investing in yourself",
    "text": "8.3 Investing in yourself\nYou should also spend some time preparing yourself to solve problems before they occur. Investing a little time in learning R each day will pay off handsomely in the long run. One way is to follow what the tidyverse team is doing on the tidyverse blog. To keep up with the R community more broadly, we recommend reading R Weekly: it’s a community effort to aggregate the most interesting news in the R community each week.\n你也应该花一些时间为自己做好准备，以便在问题发生前就能解决它们。 每天投入一点时间学习 R，从长远来看将获得丰厚的回报。 一种方法是在 tidyverse 博客 上关注 tidyverse 团队的动态。 为了更广泛地了解 R 社区的动态，我们推荐阅读 R Weekly：这是一个社区项目，每周汇总 R 社区最有趣的新闻。",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Workflow: getting help</span>"
    ]
  },
  {
    "objectID": "workflow-help.html#summary",
    "href": "workflow-help.html#summary",
    "title": "8  Workflow: getting help",
    "section": "\n8.4 Summary",
    "text": "8.4 Summary\nThis chapter concludes the Whole Game part of the book. You’ve now seen the most important parts of the data science process: visualization, transformation, tidying and importing. Now you’ve got a holistic view of the whole process, and we start to get into the details of small pieces.\n本章结束了本书的“全局概览”部分。 你现在已经看到了数据科学流程中最重要的部分：可视化、转换、整理和导入。 现在你对整个流程有了全面的了解，我们将开始深入探讨各个小部分的细节。\nThe next part of the book, Visualize, does a deeper dive into the grammar of graphics and creating data visualizations with ggplot2, showcases how to use the tools you’ve learned so far to conduct exploratory data analysis, and introduces good practices for creating plots for communication.\n本书的下一部分“可视化”，将更深入地探讨图形语法和使用 ggplot2 创建数据可视化，展示如何使用你目前学到的工具进行探索性数据分析，并介绍创建用于交流的图表的良好实践。",
    "crumbs": [
      "Whole game",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Workflow: getting help</span>"
    ]
  },
  {
    "objectID": "visualize.html",
    "href": "visualize.html",
    "title": "Visualize",
    "section": "",
    "text": "After reading the first part of the book, you understand (at least superficially) the most important tools for doing data science. Now it’s time to start diving into the details. In this part of the book, you’ll learn about visualizing data in further depth.\n读完本书的第一部分，你已经（至少表面上）了解了数据科学中最重要的工具。 现在是时候开始深入细节了。 在本书的这一部分，你将更深入地学习数据可视化。\n\n\n\n\n\n\n\nFigure 1: Data visualization is often the first step in data exploration.  数据可视化通常是数据探索的第一步。\n\n\n\n\nEach chapter addresses one to a few aspects of creating a data visualization.\n每一章都会讨论创建数据可视化的一到几个方面。\n\nIn 9  Layers you will learn about the layered grammar of graphics.\n在 9  Layers 中，你将学习分层图形语法。\nIn 10  Exploratory data analysis, you’ll combine visualization with your curiosity and skepticism to ask and answer interesting questions about data.\n在 10  Exploratory data analysis 中，你将把可视化与你的好奇心和怀疑精神结合起来，对数据提出并回答有趣的问题。\nFinally, in 11  Communication you will learn how to take your exploratory graphics, elevate them, and turn them into expository graphics, graphics that help the newcomer to your analysis understand what’s going on as quickly and easily as possible.\n最后，在 11  Communication 中，你将学习如何将你的探索性图形进行升华，将它们转变为解释性图形，即能帮助初次接触你分析的人尽可能快速、轻松地理解分析内容的图形。\n\nThese three chapters get you started in the world of visualization, but there is much more to learn. The absolute best place to learn more is the ggplot2 book: ggplot2: Elegant graphics for data analysis. It goes into much more depth about the underlying theory, and has many more examples of how to combine the individual pieces to solve practical problems. Another great resource is the ggplot2 extensions gallery https://exts.ggplot2.tidyverse.org/gallery/. This site lists many of the packages that extend ggplot2 with new geoms and scales. It’s a great place to start if you’re trying to do something that seems hard with ggplot2.\n这三章为你开启了可视化世界的大门，但还有很多东西需要学习。 学习更多知识的最佳去处是 ggplot2 的书籍：ggplot2: Elegant graphics for data analysis。 它更深入地探讨了底层理论，并提供了更多关于如何组合各个部分来解决实际问题的例子。 另一个很棒的资源是 ggplot2 扩展包的展廊 https://exts.ggplot2.tidyverse.org/gallery/。 这个网站列出了许多用新的几何对象 (geoms) 和标度 (scales) 扩展 ggplot2 的包。 如果你想用 ggplot2 做一些看起来很困难的事情，这是一个很好的起点。",
    "crumbs": [
      "Visualize"
    ]
  },
  {
    "objectID": "layers.html",
    "href": "layers.html",
    "title": "9  Layers",
    "section": "",
    "text": "9.1 Introduction\nIn Chapter 1, you learned much more than just how to make scatterplots, bar charts, and boxplots. You learned a foundation that you can use to make any type of plot with ggplot2.\n在 Chapter 1 中，你学到的远不止如何制作散点图、条形图和箱线图。你学到了一个可以用 ggplot2 制作任何类型图表的基础。\nIn this chapter, you’ll expand on that foundation as you learn about the layered grammar of graphics. We’ll start with a deeper dive into aesthetic mappings, geometric objects, and facets. Then, you will learn about statistical transformations ggplot2 makes under the hood when creating a plot. These transformations are used to calculate new values to plot, such as the heights of bars in a bar plot or medians in a box plot. You will also learn about position adjustments, which modify how geoms are displayed in your plots. Finally, we’ll briefly introduce coordinate systems.\n在本章中，你将通过学习分层图形语法来扩展这一基础。我们将从深入探讨美学映射 (aesthetic mappings)、几何对象 (geometric objects) 和分面 (facets) 开始。然后，你将学习 ggplot2 在创建图表时在底层进行的统计变换 (statistical transformations)。这些变换用于计算要绘制的新值，例如条形图中条形的高度或箱线图中的中位数。你还将学习位置调整 (position adjustments)，它可以修改几何对象在图表中的显示方式。最后，我们将简要介绍坐标系 (coordinate systems)。\nWe will not cover every single function and option for each of these layers, but we will walk you through the most important and commonly used functionality provided by ggplot2 as well as introduce you to packages that extend ggplot2.\n我们不会涵盖这些图层中的每一个函数和选项，但我们会引导你了解 ggplot2 提供的最重要和最常用的功能，并向你介绍扩展 ggplot2 的包。",
    "crumbs": [
      "Visualize",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Layers</span>"
    ]
  },
  {
    "objectID": "layers.html#introduction",
    "href": "layers.html#introduction",
    "title": "9  Layers",
    "section": "",
    "text": "9.1.1 Prerequisites\nThis chapter focuses on ggplot2. To access the datasets, help pages, and functions used in this chapter, load the tidyverse by running this code:\n本章重点介绍 ggplot2。要访问本章中使用的数据集、帮助页面和函数，请运行以下代码加载 tidyverse：\n\nlibrary(tidyverse)",
    "crumbs": [
      "Visualize",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Layers</span>"
    ]
  },
  {
    "objectID": "layers.html#aesthetic-mappings",
    "href": "layers.html#aesthetic-mappings",
    "title": "9  Layers",
    "section": "\n9.2 Aesthetic mappings",
    "text": "9.2 Aesthetic mappings\n\n“The greatest value of a picture is when it forces us to notice what we never expected to see.” — John Tukey\n“一图胜千言，尤其是在它迫使我们注意到我们从未预料到的事物时。” — 约翰·图基 (John Tukey)\n\nRemember that the mpg data frame bundled with the ggplot2 package contains 234 observations on 38 car models.\n请记住，ggplot2 包中附带的 mpg 数据框包含 234 条关于 38 种车型的观测数据。\n\nmpg\n#&gt; # A tibble: 234 × 11\n#&gt;   manufacturer model displ  year   cyl trans      drv     cty   hwy fl   \n#&gt;   &lt;chr&gt;        &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;\n#&gt; 1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p    \n#&gt; 2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p    \n#&gt; 3 audi         a4      2    2008     4 manual(m6) f        20    31 p    \n#&gt; 4 audi         a4      2    2008     4 auto(av)   f        21    30 p    \n#&gt; 5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p    \n#&gt; 6 audi         a4      2.8  1999     6 manual(m5) f        18    26 p    \n#&gt; # ℹ 228 more rows\n#&gt; # ℹ 1 more variable: class &lt;chr&gt;\n\nAmong the variables in mpg are:mpg 数据集中的变量包括：\n\ndispl: A car’s engine size, in liters. A numerical variable.displ：汽车的发动机尺寸，单位为升。\n这是一个数值变量。\nhwy: A car’s fuel efficiency on the highway, in miles per gallon (mpg). A car with a low fuel efficiency consumes more fuel than a car with a high fuel efficiency when they travel the same distance. A numerical variable.hwy：汽车在高速公路上的燃油效率，单位为英里/加仑 (mpg)。\n当行驶相同距离时，燃油效率低的汽车比燃油效率高的汽车消耗更多的燃料。\n这是一个数值变量。\nclass: Type of car. A categorical variable.class：汽车类型。\n这是一个分类变量。\n\nLet’s start by visualizing the relationship between displ and hwy for various classes of cars. We can do this with a scatterplot where the numerical variables are mapped to the x and y aesthetics and the categorical variable is mapped to an aesthetic like color or shape.\n让我们从可视化不同 class 类型汽车的 displ 和 hwy 之间的关系开始。我们可以通过散点图来实现，其中数值变量映射到 x 和 y 美学属性，而分类变量映射到像 color 或 shape 这样的美学属性。\n# Left\nggplot(mpg, aes(x = displ, y = hwy, color = class)) +\n  geom_point()\n\n# Right\nggplot(mpg, aes(x = displ, y = hwy, shape = class)) +\n  geom_point()\n#&gt; Warning: The shape palette can deal with a maximum of 6 discrete values because more\n#&gt; than 6 becomes difficult to discriminate\n#&gt; ℹ you have requested 7 values. Consider specifying shapes manually if you\n#&gt;   need that many of them.\n#&gt; Warning: Removed 62 rows containing missing values or values outside the scale range\n#&gt; (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nWhen class is mapped to shape, we get two warnings:\n当 class 映射到 shape 时，我们会收到两个警告：\n\n1: The shape palette can deal with a maximum of 6 discrete values because more than 6 becomes difficult to discriminate; you have 7. Consider specifying shapes manually if you must have them.\n形状调色板最多只能处理 6 个离散值，因为超过 6 个就很难区分了；而你有 7 个。如果必须使用它们，请考虑手动指定形状。\n2: Removed 62 rows containing missing values (geom_point()).\n移除了 62 行包含缺失值的数据 (geom_point())。\n\nSince ggplot2 will only use six shapes at a time, by default, additional groups will go unplotted when you use the shape aesthetic. The second warning is related – there are 62 SUVs in the dataset and they’re not plotted.\n由于 ggplot2 默认一次只使用六种形状，因此当你使用形状美学时，额外的分组将不会被绘制出来。第二个警告与此相关——数据集中有 62 辆 SUV 没有被绘制。\nSimilarly, we can map class to size or alpha aesthetics as well, which control the size and the transparency of the points, respectively.\n同样，我们也可以将 class 映射到 size 或 alpha 美学，它们分别控制点的大小和透明度。\n# Left\nggplot(mpg, aes(x = displ, y = hwy, size = class)) +\n  geom_point()\n#&gt; Warning: Using size for a discrete variable is not advised.\n\n# Right\nggplot(mpg, aes(x = displ, y = hwy, alpha = class)) +\n  geom_point()\n#&gt; Warning: Using alpha for a discrete variable is not advised.\n\n\n\n\n\n\n\n\n\n\nBoth of these produce warnings as well:\n这两者也都会产生警告：\n\nUsing alpha for a discrete variable is not advised. 不建议对离散变量使用 alpha。\n\nMapping an unordered discrete (categorical) variable (class) to an ordered aesthetic (size or alpha) is generally not a good idea because it implies a ranking that does not in fact exist.\n将一个无序的离散（分类）变量（class）映射到一个有序的美学属性（size 或 alpha）通常不是一个好主意，因为它暗示了一个实际上不存在的排序。\nOnce you map an aesthetic, ggplot2 takes care of the rest. It selects a reasonable scale to use with the aesthetic, and it constructs a legend that explains the mapping between levels and values. For x and y aesthetics, ggplot2 does not create a legend, but it creates an axis line with tick marks and a label. The axis line provides the same information as a legend; it explains the mapping between locations and values.\n一旦你映射了一个美学属性，ggplot2 会处理剩下的事情。它会选择一个合理的标度与该美学属性一起使用，并构建一个图例来解释级别和值之间的映射关系。对于 x 和 y 美学属性，ggplot2 不会创建图例，但会创建带有刻度线和标签的坐标轴。坐标轴线提供了与图例相同的信息；它解释了位置和值之间的映射。\nYou can also set the visual properties of your geom manually as an argument of your geom function (outside of aes()) instead of relying on a variable mapping to determine the appearance. For example, we can make all of the points in our plot blue:\n你也可以在几何对象函数中手动设置其视觉属性作为参数（在 aes() 之外），而不是依赖变量映射来决定外观。例如，我们可以将图中的所有点都设置为蓝色：\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point(color = \"blue\")\n\n\n\n\n\n\n\nHere, the color doesn’t convey information about a variable, but only changes the appearance of the plot. You’ll need to pick a value that makes sense for that aesthetic:\n在这里，颜色并不传达关于变量的信息，而只是改变了图表的外观。你需要为该美学选择一个有意义的值：\n\nThe name of a color as a character string, e.g., color = \"blue\"\n作为字符​​串的颜色名称，例如 color = \"blue\"\nThe size of a point in mm, e.g., size = 1 以毫米为单位的点的大小，例如 size = 1\nThe shape of a point as a number, e.g, shape = 1, as shown in Figure 9.1.\n点的形状，以数字表示，例如 shape = 1，如 Figure 9.1 所示。\n\n\n\n\n\n\n\n\nFigure 9.1: R has 26 built-in shapes that are identified by numbers. There are some seeming duplicates: for example, 0, 15, and 22 are all squares. The difference comes from the interaction of the color and fill aesthetics. The hollow shapes (0–14) have a border determined by color; the solid shapes (15–20) are filled with color; the filled shapes (21–25) have a border of color and are filled with fill. Shapes are arranged to keep similar shapes next to each other.\n\n\n\nR 有 26 种内置形状，用数字标识。有些形状看起来重复了：例如，0、15 和 22 都是正方形。区别在于 color 和 fill 美学的交互作用。空心形状 (0–14) 的边框由 color 决定；实心形状 (15–20) 由 color 填充；填充形状 (21–25) 的边框是 color，填充是 fill。形状的排列是为了让相似的形状彼此相邻。\nSo far we have discussed aesthetics that we can map or set in a scatterplot, when using a point geom. You can learn more about all possible aesthetic mappings in the aesthetic specifications vignette at https://ggplot2.tidyverse.org/articles/ggplot2-specs.html.\n到目前为止，我们已经讨论了在使用点几何对象时，可以在散点图中映射或设置的美学。你可以在美学规范小品文 https://ggplot2.tidyverse.org/articles/ggplot2-specs.html 中了解更多关于所有可能的美学映射。\nThe specific aesthetics you can use for a plot depend on the geom you use to represent the data. In the next section we dive deeper into geoms.\n你可以用于绘图的特定美学取决于你用来表示数据的几何对象。在下一节中，我们将更深入地探讨几何对象。\n\n9.2.1 Exercises\n\nCreate a scatterplot of hwy vs. displ where the points are pink filled in triangles.\n\nWhy did the following code not result in a plot with blue points?\n\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy, color = \"blue\"))\n\n\nWhat does the stroke aesthetic do? What shapes does it work with? (Hint: use ?geom_point)\nWhat happens if you map an aesthetic to something other than a variable name, like aes(color = displ &lt; 5)? Note, you’ll also need to specify x and y.",
    "crumbs": [
      "Visualize",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Layers</span>"
    ]
  },
  {
    "objectID": "layers.html#sec-geometric-objects",
    "href": "layers.html#sec-geometric-objects",
    "title": "9  Layers",
    "section": "\n9.3 Geometric objects",
    "text": "9.3 Geometric objects\nHow are these two plots similar?\n这两幅图有何相似之处？\n\n\n\n\n\n\n\n\n\n\nBoth plots contain the same x variable, the same y variable, and both describe the same data. But the plots are not identical. Each plot uses a different geometric object, geom, to represent the data. The plot on the left uses the point geom, and the plot on the right uses the smooth geom, a smooth line fitted to the data.\n两张图都包含相同的 x 变量，相同的 y 变量，并且都描述了相同的数据。但这两张图并不完全相同。每张图都使用不同的几何对象（geom）来表示数据。左边的图使用了点几何对象（point geom），右边的图使用了平滑几何对象（smooth geom），即一条拟合数据的平滑曲线。\nTo change the geom in your plot, change the geom function that you add to ggplot(). For instance, to make the plots above, you can use the following code:\n要更改绘图中的几何对象，请更改添加到 ggplot() 的几何函数。例如，要制作上面的图，您可以使用以下代码：\n\n# Left\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point()\n\n# Right\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_smooth()\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\nEvery geom function in ggplot2 takes a mapping argument, either defined locally in the geom layer or globally in the ggplot() layer. However, not every aesthetic works with every geom. You could set the shape of a point, but you couldn’t set the “shape” of a line. If you try, ggplot2 will silently ignore that aesthetic mapping. On the other hand, you could set the linetype of a line. geom_smooth() will draw a different line, with a different linetype, for each unique value of the variable that you map to linetype.\nggplot2 中的每个几何函数都接受一个 mapping 参数，该参数可以在几何层中局部定义，也可以在 ggplot() 层中全局定义。然而，并非每个美学都适用于每个几何对象。你可以设置点的形状，但不能设置线的“形状”。如果你尝试这样做，ggplot2 会默默地忽略该美学映射。另一方面，你可以设置线的线型。geom_smooth() 会为映射到线型的变量的每个唯一值绘制一条不同的线，并具有不同的线型。\n# Left\nggplot(mpg, aes(x = displ, y = hwy, shape = drv)) + \n  geom_smooth()\n\n# Right\nggplot(mpg, aes(x = displ, y = hwy, linetype = drv)) + \n  geom_smooth()\n\n\n\n\n\n\n\n\n\n\nHere, geom_smooth() separates the cars into three lines based on their drv value, which describes a car’s drive train. One line describes all of the points that have a 4 value, one line describes all of the points that have an f value, and one line describes all of the points that have an r value. Here, 4 stands for four-wheel drive, f for front-wheel drive, and r for rear-wheel drive.\n在这里，geom_smooth() 根据汽车的 drv 值（描述汽车的驱动系统）将汽车分为三条线。一条线描述所有 drv 值为 4 的点，一条线描述所有值为 f 的点，另一条线描述所有值为 r 的点。这里，4 代表四轮驱动 (four-wheel drive)，f 代表前轮驱动 (front-wheel drive)，r 代表后轮驱动 (rear-wheel drive)。\nIf this sounds strange, we can make it clearer by overlaying the lines on top of the raw data and then coloring everything according to drv.\n如果这听起来有些奇怪，我们可以通过将线条叠加在原始数据之上，然后根据 drv 对所有内容进行着色，使其更加清晰。\n\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) + \n  geom_point() +\n  geom_smooth(aes(linetype = drv))\n\n\n\n\n\n\n\nNotice that this plot contains two geoms in the same graph.\n注意，此图在同一张图表中包含了两种几何对象。\nMany geoms, like geom_smooth(), use a single geometric object to display multiple rows of data. For these geoms, you can set the group aesthetic to a categorical variable to draw multiple objects. ggplot2 will draw a separate object for each unique value of the grouping variable. In practice, ggplot2 will automatically group the data for these geoms whenever you map an aesthetic to a discrete variable (as in the linetype example). It is convenient to rely on this feature because the group aesthetic by itself does not add a legend or distinguishing features to the geoms.\n许多几何对象（geoms），如 geom_smooth()，使用单个几何对象来显示多行数据。对于这些几何对象，您可以将 group 美学设置为分类变量以绘制多个对象。ggplot2 将为分组变量的每个唯一值绘制一个单独的对象。在实践中，只要您将美学映射到离散变量（如 linetype 示例中），ggplot2 就会自动为这些几何对象分组数据。依赖此功能很方便，因为 group 美学本身不会为几何对象添加图例或区分特征。\n# Left\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_smooth()\n\n# Middle\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_smooth(aes(group = drv))\n\n# Right\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_smooth(aes(color = drv), show.legend = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you place mappings in a geom function, ggplot2 will treat them as local mappings for the layer. It will use these mappings to extend or overwrite the global mappings for that layer only. This makes it possible to display different aesthetics in different layers.\n如果你将映射放置在 geom 函数中，ggplot2 会将它们视为该图层的局部映射。它将使用这些映射来扩展或覆盖仅该图层的全局映射。这使得在不同图层中显示不同的美学成为可能。\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point(aes(color = class)) + \n  geom_smooth()\n\n\n\n\n\n\n\nYou can use the same idea to specify different data for each layer. Here, we use red points as well as open circles to highlight two-seater cars. The local data argument in geom_point() overrides the global data argument in ggplot() for that layer only.\n您可以使用相同的思路为每个图层指定不同的 data。在这里，我们使用红点和空心圆来突出显示双座汽车。geom_point() 中的局部数据参数仅覆盖 ggplot() 中该图层的全局数据参数。\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  geom_point(\n    data = mpg |&gt; filter(class == \"2seater\"), \n    color = \"red\"\n  ) +\n  geom_point(\n    data = mpg |&gt; filter(class == \"2seater\"), \n    shape = \"circle open\", size = 3, color = \"red\"\n  )\n\n\n\n\n\n\n\nGeoms are the fundamental building blocks of ggplot2. You can completely transform the look of your plot by changing its geom, and different geoms can reveal different features of your data. For example, the histogram and density plot below reveal that the distribution of highway mileage is bimodal and right skewed while the boxplot reveals two potential outliers.\nGeoms 是 ggplot2 的基本构建块。您可以通过更改其 geom 来完全改变绘图的外观，不同的 geoms 可以揭示您数据的不同特征。例如，下面的直方图和密度图显示高速公路里程的分布是双峰和右偏的，而箱线图则揭示了两个潜在的异常值。\n# Left\nggplot(mpg, aes(x = hwy)) +\n  geom_histogram(binwidth = 2)\n\n# Middle\nggplot(mpg, aes(x = hwy)) +\n  geom_density()\n\n# Right\nggplot(mpg, aes(x = hwy)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2 provides more than 40 geoms but these don’t cover all possible plots one could make. If you need a different geom, we recommend looking into extension packages first to see if someone else has already implemented it (see https://exts.ggplot2.tidyverse.org/gallery/ for a sampling). For example, the ggridges package (https://wilkelab.org/ggridges) is useful for making ridgeline plots, which can be useful for visualizing the density of a numerical variable for different levels of a categorical variable. In the following plot not only did we use a new geom (geom_density_ridges()), but we have also mapped the same variable to multiple aesthetics (drv to y, fill, and color) as well as set an aesthetic (alpha = 0.5) to make the density curves transparent.\nggplot2 提供了 40 多种几何对象，但这些并不能涵盖所有可能制作的图表。如果您需要一种不同的几何对象，我们建议首先查看扩展包，看是否有人已经实现了它（参见 https://exts.ggplot2.tidyverse.org/gallery/ 以获取示例）。例如，ggridges 包（https://wilkelab.org/ggridges）对于制作山脊线图非常有用，这对于可视化一个数值变量在不同分类变量水平下的密度非常有用。在下面的图表中，我们不仅使用了一个新的几何对象（geom_density_ridges()），还将同一个变量映射到了多个美学属性（将 drv 映射到 y、fill 和 color），并且设置了一个美学属性（alpha = 0.5）来使密度曲线透明。\n\nlibrary(ggridges)\n\nggplot(mpg, aes(x = hwy, y = drv, fill = drv, color = drv)) +\n  geom_density_ridges(alpha = 0.5, show.legend = FALSE)\n#&gt; Picking joint bandwidth of 1.28\n\n\n\n\n\n\n\nThe best place to get a comprehensive overview of all of the geoms ggplot2 offers, as well as all functions in the package, is the reference page: https://ggplot2.tidyverse.org/reference. To learn more about any single geom, use the help (e.g., ?geom_smooth).\n要全面了解 ggplot2 提供的所有几何对象以及包中的所有函数，最好的地方是参考页面：https://ggplot2.tidyverse.org/reference。要了解任何单个几何对象，请使用帮助（例如，?geom_smooth）。\n\n9.3.1 Exercises\n\nWhat geom would you use to draw a line chart? A boxplot? A histogram? An area chart?\n\nEarlier in this chapter we used show.legend without explaining it:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_smooth(aes(color = drv), show.legend = FALSE)\n\nWhat does show.legend = FALSE do here? What happens if you remove it? Why do you think we used it earlier?\n\nWhat does the se argument to geom_smooth() do?\n\nRecreate the R code necessary to generate the following graphs. Note that wherever a categorical variable is used in the plot, it’s drv.",
    "crumbs": [
      "Visualize",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Layers</span>"
    ]
  },
  {
    "objectID": "layers.html#facets",
    "href": "layers.html#facets",
    "title": "9  Layers",
    "section": "\n9.4 Facets",
    "text": "9.4 Facets\nIn Chapter 1 you learned about faceting with facet_wrap(), which splits a plot into subplots that each display one subset of the data based on a categorical variable.\n在 Chapter 1 中，你学习了使用 facet_wrap() 进行分面，该函数根据一个分类变量将图表分割成多个子图，每个子图显示数据的一个子集。\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  facet_wrap(~cyl)\n\n\n\n\n\n\n\nTo facet your plot with the combination of two variables, switch from facet_wrap() to facet_grid(). The first argument of facet_grid() is also a formula, but now it’s a double sided formula: rows ~ cols.\n要使用两个变量的组合对图表进行分面，请从 facet_wrap() 切换到 facet_grid()。facet_grid() 的第一个参数也是一个公式，但现在它是一个双边公式：rows ~ cols。\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  facet_grid(drv ~ cyl)\n\n\n\n\n\n\n\nBy default each of the facets share the same scale and range for x and y axes. This is useful when you want to compare data across facets but it can be limiting when you want to visualize the relationship within each facet better. Setting the scales argument in a faceting function to \"free_x\" will allow for different scales of x-axis across columns, \"free_y\" will allow for different scales on y-axis across rows, and \"free\" will allow both.\n默认情况下，每个分面共享相同的 x 轴和 y 轴刻度及范围。当您想在不同分面之间比较数据时，这很有用，但当您想更好地可视化每个分面内部的关系时，这可能会有限制。在分面函数中将 scales 参数设置为 \"free_x\" 将允许跨列使用不同的 x 轴刻度，\"free_y\" 将允许跨行使用不同的 y 轴刻度，而 \"free\" 将同时允许两者。\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  facet_grid(drv ~ cyl, scales = \"free\")\n\n\n\n\n\n\n\n\n9.4.1 Exercises\n\nWhat happens if you facet on a continuous variable?\n\nWhat do the empty cells in the plot above with facet_grid(drv ~ cyl) mean? Run the following code. How do they relate to the resulting plot?\n\nggplot(mpg) + \n  geom_point(aes(x = drv, y = cyl))\n\n\n\nWhat plots does the following code make? What does . do?\n\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) +\n  facet_grid(drv ~ .)\n\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) +\n  facet_grid(. ~ cyl)\n\n\n\nTake the first faceted plot in this section:\n\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) + \n  facet_wrap(~ cyl, nrow = 2)\n\nWhat are the advantages to using faceting instead of the color aesthetic? What are the disadvantages? How might the balance change if you had a larger dataset?\n\nRead ?facet_wrap. What does nrow do? What does ncol do? What other options control the layout of the individual panels? Why doesn’t facet_grid() have nrow and ncol arguments?\n\nWhich of the following plots makes it easier to compare engine size (displ) across cars with different drive trains? What does this say about when to place a faceting variable across rows or columns?\n\nggplot(mpg, aes(x = displ)) + \n  geom_histogram() + \n  facet_grid(drv ~ .)\n\nggplot(mpg, aes(x = displ)) + \n  geom_histogram() +\n  facet_grid(. ~ drv)\n\n\n\nRecreate the following plot using facet_wrap() instead of facet_grid(). How do the positions of the facet labels change?\n\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) +\n  facet_grid(drv ~ .)",
    "crumbs": [
      "Visualize",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Layers</span>"
    ]
  },
  {
    "objectID": "layers.html#statistical-transformations",
    "href": "layers.html#statistical-transformations",
    "title": "9  Layers",
    "section": "\n9.5 Statistical transformations",
    "text": "9.5 Statistical transformations\nConsider a basic bar chart, drawn with geom_bar() or geom_col(). The following chart displays the total number of diamonds in the diamonds dataset, grouped by cut. The diamonds dataset is in the ggplot2 package and contains information on ~54,000 diamonds, including the price, carat, color, clarity, and cut of each diamond. The chart shows that more diamonds are available with high quality cuts than with low quality cuts.\n考虑一个用 geom_bar() 或 geom_col() 绘制的基本条形图。下图显示了 diamonds 数据集中按 cut 分组的钻石总数。diamonds 数据集位于 ggplot2 包中，包含约 54,000 颗钻石的信息，包括每颗钻石的 price、carat、color、clarity 和 cut。该图表显示，高质量切工的钻石比低质量切工的钻石更多。\n\nggplot(diamonds, aes(x = cut)) + \n  geom_bar()\n\n\n\n\n\n\n\nOn the x-axis, the chart displays cut, a variable from diamonds. On the y-axis, it displays count, but count is not a variable in diamonds! Where does count come from? Many graphs, like scatterplots, plot the raw values of your dataset. Other graphs, like bar charts, calculate new values to plot:\n在 x 轴上，图表显示 cut，这是 diamonds 数据集中的一个变量。在 y 轴上，它显示计数 (count)，但计数并不是 diamonds 数据集中的变量！计数从何而来？许多图表，如散点图，会绘制数据集的原始值。而其他图表，如条形图，则会计算新的值来进行绘制：\n\nBar charts, histograms, and frequency polygons bin your data and then plot bin counts, the number of points that fall in each bin.\n条形图、直方图和频率多边形将您的数据分箱，然后绘制每个箱中的计数，即落入每个箱中的点的数量。\nSmoothers fit a model to your data and then plot predictions from the model.\n平滑器 (Smoothers) 会对你的数据拟合一个模型，然后绘制出模型的预测值。\nBoxplots compute the five-number summary of the distribution and then display that summary as a specially formatted box.\n箱线图计算分布的五数概括（five-number summary），然后将该概括显示为特殊格式的箱形。\n\nThe algorithm used to calculate new values for a graph is called a stat, short for statistical transformation. Figure 9.2 shows how this process works with geom_bar().\n用于为图形计算新值的算法称为 stat，即统计变换 (statistical transformation) 的缩写。Figure 9.2 展示了此过程如何与 geom_bar() 一起工作。\n\n\n\n\n\n\n\nFigure 9.2: When creating a bar chart we first start with the raw data, then aggregate it to count the number of observations in each bar, and finally map those computed variables to plot aesthetics.\n\n\n\n\n在创建条形图时，我们首先从原始数据开始，然后对其进行聚合以计算每个条形中的观测数量，最后将这些计算出的变量映射到绘图美学上。\nYou can learn which stat a geom uses by inspecting the default value for the stat argument. For example, ?geom_bar shows that the default value for stat is “count”, which means that geom_bar() uses stat_count(). stat_count() is documented on the same page as geom_bar(). If you scroll down, the section called “Computed variables” explains that it computes two new variables: count and prop.\n你可以通过检查 stat 参数的默认值来了解一个几何对象（geom）使用了哪个统计变换（stat）。例如，?geom_bar 显示 stat 的默认值是 “count”，这意味着 geom_bar() 使用了 stat_count()。stat_count() 和 geom_bar() 在同一个帮助页面上有文档说明。如果你向下滚动，名为“计算变量”的部分会解释它计算了两个新变量：count 和 prop。\nEvery geom has a default stat; and every stat has a default geom. This means that you can typically use geoms without worrying about the underlying statistical transformation. However, there are three reasons why you might need to use a stat explicitly:\n每个 geom 都有一个默认的 stat；每个 stat 也有一个默认的 geom。这意味着你通常可以使用 geom 而不必担心底层的统计转换。然而，有三个原因可能让你需要明确地使用 stat：\n\n\nYou might want to override the default stat. In the code below, we change the stat of geom_bar() from count (the default) to identity. This lets us map the height of the bars to the raw values of a y variable.\n你可能想要覆盖默认的统计变换。在下面的代码中，我们将 geom_bar() 的统计变换从 count（默认值）更改为 identity。这使我们可以将条形的高度映射到 y 变量的原始值。\n\ndiamonds |&gt;\n  count(cut) |&gt;\n  ggplot(aes(x = cut, y = n)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\n\nYou might want to override the default mapping from transformed variables to aesthetics. For example, you might want to display a bar chart of proportions, rather than counts:\n你可能想要覆盖从转换后变量到美学的默认映射。例如，你可能想显示一个比例条形图，而不是计数条形图：\n\nggplot(diamonds, aes(x = cut, y = after_stat(prop), group = 1)) + \n  geom_bar()\n\n\n\n\n\n\n\nTo find the possible variables that can be computed by the stat, look for the section titled “computed variables” in the help for geom_bar().\n要查找可由统计变换计算的可能变量，请在 geom_bar() 的帮助文档中查找标题为“计算变量”的部分。\n\n\nYou might want to draw greater attention to the statistical transformation in your code. For example, you might use stat_summary(), which summarizes the y values for each unique x value, to draw attention to the summary that you’re computing:\n你可能想在代码中更加突出统计变换。例如，你可以使用 stat_summary()，它为每个唯一的 x 值汇总 y 值，以突出你正在计算的摘要：\n\nggplot(diamonds) + \n  stat_summary(\n    aes(x = cut, y = depth),\n    fun.min = min,\n    fun.max = max,\n    fun = median\n  )\n\n\n\n\n\n\n\n\n\nggplot2 provides more than 20 stats for you to use. Each stat is a function, so you can get help in the usual way, e.g., ?stat_bin.\nggplot2 提供了超过 20 种统计变换供您使用。每种统计变换都是一个函数，因此您可以通过常规方式获取帮助，例如 ?stat_bin。\n\n9.5.1 Exercises\n\nWhat is the default geom associated with stat_summary()? How could you rewrite the previous plot to use that geom function instead of the stat function?\nWhat does geom_col() do? How is it different from geom_bar()?\nMost geoms and stats come in pairs that are almost always used in concert. Make a list of all the pairs. What do they have in common? (Hint: Read through the documentation.)\nWhat variables does stat_smooth() compute? What arguments control its behavior?\n\nIn our proportion bar chart, we needed to set group = 1. Why? In other words, what is the problem with these two graphs?\n\nggplot(diamonds, aes(x = cut, y = after_stat(prop))) + \n  geom_bar()\nggplot(diamonds, aes(x = cut, fill = color, y = after_stat(prop))) + \n  geom_bar()",
    "crumbs": [
      "Visualize",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Layers</span>"
    ]
  },
  {
    "objectID": "layers.html#position-adjustments",
    "href": "layers.html#position-adjustments",
    "title": "9  Layers",
    "section": "\n9.6 Position adjustments",
    "text": "9.6 Position adjustments\nThere’s one more piece of magic associated with bar charts. You can color a bar chart using either the color aesthetic, or, more usefully, the fill aesthetic:\n条形图还有一个神奇之处。你可以使用 color 美学，或者更有用的 fill 美学来为条形图上色：\n# Left\nggplot(mpg, aes(x = drv, color = drv)) + \n  geom_bar()\n\n# Right\nggplot(mpg, aes(x = drv, fill = drv)) + \n  geom_bar()\n\n\n\n\n\n\n\n\n\n\nNote what happens if you map the fill aesthetic to another variable, like class: the bars are automatically stacked. Each colored rectangle represents a combination of drv and class.\n请注意，如果将填充美学映射到另一个变量（如 class），会发生什么：条形会自动堆叠。每个彩色矩形代表 drv 和 class 的组合。\n\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar()\n\n\n\n\n\n\n\nThe stacking is performed automatically using the position adjustment specified by the position argument. If you don’t want a stacked bar chart, you can use one of three other options: \"identity\", \"dodge\" or \"fill\".\n堆叠是通过 position 参数指定的 位置调整 (position adjustment) 自动执行的。如果你不想要堆叠条形图，可以使用其他三个选项之一：\"identity\"、\"dodge\" 或 \"fill\"。\n\n\nposition = \"identity\" will place each object exactly where it falls in the context of the graph. This is not very useful for bars, because it overlaps them. To see that overlapping we either need to make the bars slightly transparent by setting alpha to a small value, or completely transparent by setting fill = NA.\n# Left\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar(alpha = 1/5, position = \"identity\")\n\n# Right\nggplot(mpg, aes(x = drv, color = class)) + \n  geom_bar(fill = NA, position = \"identity\")\n\n\n\n\n\n\n\n\n\n\n\n\nposition = \"identity\" 会将每个对象精确地放置在它在图表上下文中的位置。对于条形图来说，这不太有用，因为它会使它们重叠。为了看到这种重叠，我们需要通过将 alpha 设置为一个较小的值来使条形图略微透明，或者通过设置 fill = NA 来使其完全透明。\nThe identity position adjustment is more useful for 2d geoms, like points, where it is the default.identity 位置调整对于二维几何对象（如点）更有用，它是默认设置。\n\nposition = \"fill\" works like stacking, but makes each set of stacked bars the same height. This makes it easier to compare proportions across groups.\nposition = \"fill\" 的作用类似于堆叠，但会使每组堆叠的条形图具有相同的高度。这使得跨组比较比例变得更加容易。\n\nposition = \"dodge\" places overlapping objects directly beside one another. This makes it easier to compare individual values.\n# Left\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar(position = \"fill\")\n\n# Right\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\n\nposition = \"dodge\" 将重叠的对象直接并排放置。这使得比较单个值变得更容易。\n\nThere’s one other type of adjustment that’s not useful for bar charts, but can be very useful for scatterplots. Recall our first scatterplot. Did you notice that the plot displays only 126 points, even though there are 234 observations in the dataset?\n还有一种调整对条形图没什么用，但对散点图却非常有用。回想一下我们的第一个散点图。你有没有注意到，尽管数据集中有 234 个观测值，但该图只显示了 126 个点？\n\n\n\n\n\n\n\n\nThe underlying values of hwy and displ are rounded so the points appear on a grid and many points overlap each other. This problem is known as overplotting. This arrangement makes it difficult to see the distribution of the data. Are the data points spread equally throughout the graph, or is there one special combination of hwy and displ that contains 109 values?hwy 和 displ 的基础值是四舍五入的，所以点出现在一个网格上，许多点相互重叠。这个问题被称为 过绘 (overplotting)。这种排列方式使得很难看出数据的分布情况。数据点是均匀地分布在整个图表中，还是存在一个包含 109 个值的特殊 hwy 和 displ 组合？\nYou can avoid this gridding by setting the position adjustment to “jitter”. position = \"jitter\" adds a small amount of random noise to each point. This spreads the points out because no two points are likely to receive the same amount of random noise.\n您可以通过将位置调整设置为“jitter”来避免这种网格化。position = \"jitter\" 会为每个点添加少量随机噪声。这会将点散开，因为不太可能有两点会接收到相同量的随机噪声。\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point(position = \"jitter\")\n\n\n\n\n\n\n\nAdding randomness seems like a strange way to improve your plot, but while it makes your graph less accurate at small scales, it makes your graph more revealing at large scales. Because this is such a useful operation, ggplot2 comes with a shorthand for geom_point(position = \"jitter\"): geom_jitter().\n增加随机性似乎是一种奇怪的改善图表的方式，但虽然它在小尺度上使你的图表不那么精确，但在大尺度上却使你的图表更具揭示性。因为这是一个非常有用的操作，ggplot2 为 geom_point(position = \"jitter\") 提供了一个简写：geom_jitter()。\nTo learn more about a position adjustment, look up the help page associated with each adjustment: ?position_dodge, ?position_fill, ?position_identity, ?position_jitter, and ?position_stack.\n要了解有关位置调整的更多信息，请查阅与每个调整相关的帮助页面：?position_dodge、?position_fill、?position_identity、?position_jitter 和 ?position_stack。\n\n9.6.1 Exercises\n\n\nWhat is the problem with the following plot? How could you improve it?\n\nggplot(mpg, aes(x = cty, y = hwy)) + \n  geom_point()\n\n\n\nWhat, if anything, is the difference between the two plots? Why?\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point()\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(position = \"identity\")\n\n\nWhat parameters to geom_jitter() control the amount of jittering?\nCompare and contrast geom_jitter() with geom_count().\nWhat’s the default position adjustment for geom_boxplot()? Create a visualization of the mpg dataset that demonstrates it.",
    "crumbs": [
      "Visualize",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Layers</span>"
    ]
  },
  {
    "objectID": "layers.html#coordinate-systems",
    "href": "layers.html#coordinate-systems",
    "title": "9  Layers",
    "section": "\n9.7 Coordinate systems",
    "text": "9.7 Coordinate systems\nCoordinate systems are probably the most complicated part of ggplot2. The default coordinate system is the Cartesian coordinate system where the x and y positions act independently to determine the location of each point. There are two other coordinate systems that are occasionally helpful.\n坐标系可能是 ggplot2 中最复杂的部分。默认的坐标系是笛卡尔坐标系，其中 x 和 y 的位置独立地决定每个点的位置。还有另外两个偶尔有用的坐标系。\n\n\ncoord_quickmap() sets the aspect ratio correctly for geographic maps. This is very important if you’re plotting spatial data with ggplot2. We don’t have the space to discuss maps in this book, but you can learn more in the Maps chapter of ggplot2: Elegant graphics for data analysis.\nnz &lt;- map_data(\"nz\")\n\nggplot(nz, aes(x = long, y = lat, group = group)) +\n  geom_polygon(fill = \"white\", color = \"black\")\n\nggplot(nz, aes(x = long, y = lat, group = group)) +\n  geom_polygon(fill = \"white\", color = \"black\") +\n  coord_quickmap()\n\n\n\n\n\n\n\n\n\n\n\ncoord_quickmap() 为地理地图正确设置长宽比。如果您正在使用 ggplot2 绘制空间数据，这一点非常重要。我们在这本书中没有篇幅讨论地图，但您可以在 ggplot2: Elegant graphics for data analysis 的地图章节中了解更多信息。\n\ncoord_polar() uses polar coordinates. Polar coordinates reveal an interesting connection between a bar chart and a Coxcomb chart.\nbar &lt;- ggplot(data = diamonds) + \n  geom_bar(\n    mapping = aes(x = clarity, fill = clarity), \n    show.legend = FALSE,\n    width = 1\n  ) + \n  theme(aspect.ratio = 1)\n\nbar + coord_flip()\nbar + coord_polar()\n\n\n\n\n\n\n\n\n\n\n\ncoord_polar() 使用极坐标。极坐标揭示了条形图和南丁格尔玫瑰图 (Coxcomb chart) 之间一个有趣的联系。\n\n\n9.7.1 Exercises\n\nTurn a stacked bar chart into a pie chart using coord_polar().\nWhat’s the difference between coord_quickmap() and coord_map()?\n\nWhat does the following plot tell you about the relationship between city and highway mpg? Why is coord_fixed() important? What does geom_abline() do?\n\nggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +\n  geom_point() + \n  geom_abline() +\n  coord_fixed()",
    "crumbs": [
      "Visualize",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Layers</span>"
    ]
  },
  {
    "objectID": "layers.html#the-layered-grammar-of-graphics",
    "href": "layers.html#the-layered-grammar-of-graphics",
    "title": "9  Layers",
    "section": "\n9.8 The layered grammar of graphics",
    "text": "9.8 The layered grammar of graphics\nWe can expand on the graphing template you learned in Section 1.3 by adding position adjustments, stats, coordinate systems, and faceting:\n我们可以通过添加位置调整、统计变换、坐标系和分面，来扩展您在 Section 1.3 中学到的绘图模板：\nggplot(data = &lt;DATA&gt;) + \n  &lt;GEOM_FUNCTION&gt;(\n     mapping = aes(&lt;MAPPINGS&gt;),\n     stat = &lt;STAT&gt;, \n     position = &lt;POSITION&gt;\n  ) +\n  &lt;COORDINATE_FUNCTION&gt; +\n  &lt;FACET_FUNCTION&gt;\nOur new template takes seven parameters, the bracketed words that appear in the template. In practice, you rarely need to supply all seven parameters to make a graph because ggplot2 will provide useful defaults for everything except the data, the mappings, and the geom function.\n我们的新模板有七个参数，即模板中出现的方括号内的词。在实践中，您很少需要提供所有七个参数来制作图表，因为 ggplot2 会为除了数据、映射和几何函数之外的所有内容提供有用的默认值。\nThe seven parameters in the template compose the grammar of graphics, a formal system for building plots. The grammar of graphics is based on the insight that you can uniquely describe any plot as a combination of a dataset, a geom, a set of mappings, a stat, a position adjustment, a coordinate system, a faceting scheme, and a theme.\n模板中的七个参数构成了图形语法，这是一个用于构建绘图的正式系统。图形语法基于这样一个洞见：你可以将任何绘图唯一地描述为一个数据集、一个几何对象、一组映射、一个统计变换、一个位置调整、一个坐标系、一个分面方案和一个主题的组合。\nTo see how this works, consider how you could build a basic plot from scratch: you could start with a dataset and then transform it into the information that you want to display (with a stat). Next, you could choose a geometric object to represent each observation in the transformed data. You could then use the aesthetic properties of the geoms to represent variables in the data. You would map the values of each variable to the levels of an aesthetic. These steps are illustrated in Figure 9.3. You’d then select a coordinate system to place the geoms into, using the location of the objects (which is itself an aesthetic property) to display the values of the x and y variables.\n为了理解这是如何工作的，可以考虑如何从头开始构建一个基本图表：你可以从一个数据集开始，然后（通过一个统计变换 stat）将其转换为你想要显示的信息。接下来，你可以选择一个几何对象来表示转换后数据中的每个观测值。然后，你可以使用几何对象的美学属性来表示数据中的变量。你会将每个变量的值映射到美学的一个层次上。这些步骤在 Figure 9.3 中有所说明。然后，你会选择一个坐标系来放置这些几何对象，利用对象的位置（其本身也是一个美学属性）来显示 x 和 y 变量的值。\n\n\n\n\n\n\n\nFigure 9.3: Steps for going from raw data to a table of frequencies to a bar plot where the heights of the bar represent the frequencies.\n\n\n\n\n从原始数据到频率表，再到条形图的步骤，其中条形的高度代表频率。\nAt this point, you would have a complete graph, but you could further adjust the positions of the geoms within the coordinate system (a position adjustment) or split the graph into subplots (faceting). You could also extend the plot by adding one or more additional layers, where each additional layer uses a dataset, a geom, a set of mappings, a stat, and a position adjustment.\n至此，你将得到一个完整的图形，但你可以进一步调整坐标系内几何对象的位置（位置调整），或将图形分割成子图（分面）。你还可以通过添加一个或多个附加图层来扩展该图，其中每个附加图层都使用一个数据集、一个几何对象、一组映射、一个统计变换和一个位置调整。\nYou could use this method to build any plot that you imagine. In other words, you can use the code template that you’ve learned in this chapter to build hundreds of thousands of unique plots.\n你可以用这种方法构建你所能想象的任何图表。换句话说，你可以使用本章学到的代码模板来构建成千上万个独特的图表。\nIf you’d like to learn more about the theoretical underpinnings of ggplot2, you might enjoy reading “The Layered Grammar of Graphics”, the scientific paper that describes the theory of ggplot2 in detail.\n如果你想深入了解 ggplot2 的理论基础，你可能会喜欢阅读《分层图形语法》，这篇科学论文详细描述了 ggplot2 的理论。",
    "crumbs": [
      "Visualize",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Layers</span>"
    ]
  },
  {
    "objectID": "layers.html#summary",
    "href": "layers.html#summary",
    "title": "9  Layers",
    "section": "\n9.9 Summary",
    "text": "9.9 Summary\nIn this chapter you learned about the layered grammar of graphics starting with aesthetics and geometries to build a simple plot, facets for splitting the plot into subsets, statistics for understanding how geoms are calculated, position adjustments for controlling the fine details of position when geoms might otherwise overlap, and coordinate systems which allow you to fundamentally change what x and y mean. One layer we have not yet touched on is theme, which we will introduce in Section 11.5.\n在本章中，你学习了分层图形语法，从美学和几何学开始构建一个简单的图，用分面将图分割成子集，用统计来理解几何对象的计算方式，用位置调整来控制几何对象可能重叠时的位置细节，以及用坐标系来从根本上改变 x 和 y 的含义。我们尚未涉及的一个图层是主题，我们将在 Section 11.5 中介绍它。\nTwo very useful resources for getting an overview of the complete ggplot2 functionality are the ggplot2 cheatsheet (which you can find at https://posit.co/resources/cheatsheets) and the ggplot2 package website (https://ggplot2.tidyverse.org).\n要想全面了解 ggplot2 的功能，有两个非常有用的资源：ggplot2 速查表（你可以在 https://posit.co/resources/cheatsheets 找到）和 ggplot2 包的网站（https://ggplot2.tidyverse.org）。\nAn important lesson you should take from this chapter is that when you feel the need for a geom that is not provided by ggplot2, it’s always a good idea to look into whether someone else has already solved your problem by creating a ggplot2 extension package that offers that geom.\n你应该从本章中学到的一个重要教训是，当你觉得需要一个 ggplot2 未提供的几何对象时，最好先去看看是否已经有人通过创建提供该几何对象的 ggplot2 扩展包解决了你的问题。",
    "crumbs": [
      "Visualize",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Layers</span>"
    ]
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "10  Exploratory data analysis",
    "section": "",
    "text": "10.1 Introduction\nThis chapter will show you how to use visualization and transformation to explore your data in a systematic way, a task that statisticians call exploratory data analysis, or EDA for short.\n本章将向你展示如何使用可视化和转换来系统地探索数据，统计学家称之为探索性数据分析（exploratory data analysis，简称 EDA）。\nEDA is an iterative cycle. You:\nEDA 是一个迭代的过程。你需要：\nEDA is not a formal process with a strict set of rules.\nEDA 并非一个有着严格规则的正式流程。\nMore than anything, EDA is a state of mind.\n更重要的是，EDA 是一种思维状态。\nDuring the initial phases of EDA you should feel free to investigate every idea that occurs to you.\n在 EDA 的初始阶段，你应该自由地去探究你脑海中出现的每一个想法。\nSome of these ideas will pan out, and some will be dead ends.\n其中一些想法会成功，而另一些则会是死胡同。\nAs your exploration continues, you will home in on a few particularly productive insights that you’ll eventually write up and communicate to others.\n随着你探索的深入，你会逐渐聚焦于一些特别富有成效的见解，并最终将它们整理成文，与他人交流。\nEDA is an important part of any data analysis, even if the primary research questions are handed to you on a platter, because you always need to investigate the quality of your data.\nEDA 是任何数据分析中都至关重要的一环，即使主要的研究问题已经现成地摆在你面前，因为你总是需要考察数据的质量。\nData cleaning is just one application of EDA: you ask questions about whether your data meets your expectations or not.\n数据清洗只是 EDA 的一种应用：你需要提出问题，判断你的数据是否符合预期。\nTo do data cleaning, you’ll need to deploy all the tools of EDA: visualization, transformation, and modelling.\n要进行数据清洗，你需要运用 EDA 的所有工具：可视化、转换和建模。",
    "crumbs": [
      "Visualize",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "EDA.html#introduction",
    "href": "EDA.html#introduction",
    "title": "10  Exploratory data analysis",
    "section": "",
    "text": "Generate questions about your data.\n生成关于数据的问题。\nSearch for answers by visualizing, transforming, and modelling your data.\n通过可视化、转换和建模来寻找答案。\nUse what you learn to refine your questions and/or generate new questions.\n利用你所学到的知识来完善你的问题和/或生成新的问题。\n\n\n\n\n\n\n\n\n\n\n10.1.1 Prerequisites\nIn this chapter we’ll combine what you’ve learned about dplyr and ggplot2 to interactively ask questions, answer them with data, and then ask new questions.\n在本章中，我们将结合你所学的 dplyr 和 ggplot2 知识，以交互方式提出问题，用数据回答问题，然后再提出新的问题。\n\nlibrary(tidyverse)",
    "crumbs": [
      "Visualize",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "EDA.html#questions",
    "href": "EDA.html#questions",
    "title": "10  Exploratory data analysis",
    "section": "\n10.2 Questions",
    "text": "10.2 Questions\n\n“There are no routine statistical questions, only questionable statistical routines.” — Sir David Cox\n\n&gt; “没有常规的统计问题，只有值得怀疑的统计套路。” — David Cox 爵士\n\n“Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.” — John Tukey\n\n&gt; “对正确问题（通常是模糊的）的近似回答，远胜于对错误问题（总能精确化）的精确回答。” — John Tukey\nYour goal during EDA is to develop an understanding of your data.\n你在 EDA 期间的目标是建立对数据的理解。\nThe easiest way to do this is to use questions as tools to guide your investigation.\n最简单的方法是使用问题作为工具来引导你的调查。\nWhen you ask a question, the question focuses your attention on a specific part of your dataset and helps you decide which graphs, models, or transformations to make.\n当你提出一个问题时，这个问题会将你的注意力集中到数据集的特定部分，并帮助你决定制作哪些图表、模型或进行哪些转换。\nEDA is fundamentally a creative process.\nEDA 本质上是一个创造性的过程。\nAnd like most creative processes, the key to asking quality questions is to generate a large quantity of questions.\n和大多数创造性过程一样，提出 高质量 问题的关键在于生成 大量 的问题。\nIt is difficult to ask revealing questions at the start of your analysis because you do not know what insights can be gleaned from your dataset.\n在分析之初很难提出有启发性的问题，因为你不知道可以从数据集中获得哪些见解。\nOn the other hand, each new question that you ask will expose you to a new aspect of your data and increase your chance of making a discovery.\n另一方面，你每提出一个新问题，都会让你接触到数据的一个新方面，并增加你做出发现的机会。\nYou can quickly drill down into the most interesting parts of your data—and develop a set of thought-provoking questions—if you follow up each question with a new question based on what you find.\n如果你在每个问题之后都根据你的发现提出一个新问题，你就可以迅速深入到数据最有趣的部分，并形成一系列发人深省的问题。\nThere is no rule about which questions you should ask to guide your research.\n关于应该提出哪些问题来指导你的研究，并没有固定的规则。\nHowever, two types of questions will always be useful for making discoveries within your data.\n然而，有两类问题对于在数据中进行发现总是很有用的。\nYou can loosely word these questions as:\n你可以将这些问题大致表述为：\n\nWhat type of variation occurs within my variables?\n我的变量内部存在哪种类型的变异？\nWhat type of covariation occurs between my variables?\n我的变量之间存在哪种类型的协变？\n\nThe rest of this chapter will look at these two questions.\n本章的其余部分将探讨这两个问题。\nWe’ll explain what variation and covariation are, and we’ll show you several ways to answer each question.\n我们将解释什么是变异和协变，并向你展示几种回答每个问题的方法。",
    "crumbs": [
      "Visualize",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "EDA.html#variation",
    "href": "EDA.html#variation",
    "title": "10  Exploratory data analysis",
    "section": "\n10.3 Variation",
    "text": "10.3 Variation\nVariation is the tendency of the values of a variable to change from measurement to measurement.\n**变异（Variation）**是指一个变量的值在每次测量之间发生变化的趋势。\nYou can see variation easily in real life; if you measure any continuous variable twice, you will get two different results.\n你在现实生活中很容易看到变异；如果你对任何连续变量进行两次测量，你会得到两个不同的结果。\nThis is true even if you measure quantities that are constant, like the speed of light.\n即使你测量的是像光速这样的恒定量，情况也是如此。\nEach of your measurements will include a small amount of error that varies from measurement to measurement.\n你的每次测量都会包含少量的误差，这个误差在每次测量之间都会有所不同。\nVariables can also vary if you measure across different subjects (e.g., the eye colors of different people) or at different times (e.g., the energy levels of an electron at different moments).\n如果你测量不同的主体（例如，不同人的眼睛颜色）或在不同的时间（例如，一个电子在不同时刻的能级），变量也会发生变化。\nEvery variable has its own pattern of variation, which can reveal interesting information about how it varies between measurements on the same observation as well as across observations.\n每个变量都有其自身的变异模式，这可以揭示关于它在同一次观测的测量之间以及不同观测之间如何变化的有趣信息。\nThe best way to understand that pattern is to visualize the distribution of the variable’s values, which you’ve learned about in Chapter 1.\n理解这种模式的最佳方法是可视化变量值的分布，你已经在 Chapter 1 中学习过相关内容。\nWe’ll start our exploration by visualizing the distribution of weights (carat) of ~54,000 diamonds from the diamonds dataset.\n我们将通过可视化 diamonds 数据集中约 54,000 颗钻石的重量（carat）分布来开始我们的探索。\nSince carat is a numerical variable, we can use a histogram:\n由于 carat 是一个数值变量，我们可以使用直方图：\n\nggplot(diamonds, aes(x = carat)) +\n  geom_histogram(binwidth = 0.5)\n\n\n\n\n\n\n\nNow that you can visualize variation, what should you look for in your plots?\n既然你能够可视化变异，那么你应该在图表中寻找什么呢？\nAnd what type of follow-up questions should you ask?\n你应该提出什么样的后续问题呢？\nWe’ve put together a list below of the most useful types of information that you will find in your graphs, along with some follow-up questions for each type of information.\n我们在下面整理了一个列表，列出了你可以在图表中找到的最有用的信息类型，以及针对每种信息类型的一些后续问题。\nThe key to asking good follow-up questions will be to rely on your curiosity (What do you want to learn more about?) as well as your skepticism (How could this be misleading?).\n提出好的后续问题的关键在于依赖你的好奇心（你想了解更多关于什么？）和你的怀疑精神（这怎么可能会误导人？）。\n\n10.3.1 Typical values\nIn both bar charts and histograms, tall bars show the common values of a variable, and shorter bars show less-common values.\n在条形图和直方图中，高条显示变量的常见值，短条显示不那么常见的值。\nPlaces that do not have bars reveal values that were not seen in your data.\n没有条形的地方揭示了你的数据中未曾出现的值。\nTo turn this information into useful questions, look for anything unexpected:\n要将这些信息转化为有用的问题，请寻找任何意想不到的地方：\n\nWhich values are the most common? Why?\n哪些值最常见？为什么？\nWhich values are rare? Why? Does that match your expectations?\n哪些值很罕见？为什么？这符合你的预期吗？\nCan you see any unusual patterns? What might explain them?\n你能看到任何不寻常的模式吗？可能的原因是什么？\n\nLet’s take a look at the distribution of carat for smaller diamonds.\n让我们来看一下较小钻石的 carat 分布。\n\nsmaller &lt;- diamonds |&gt; \n  filter(carat &lt; 3)\n\nggplot(smaller, aes(x = carat)) +\n  geom_histogram(binwidth = 0.01)\n\n\n\n\n\n\n\nThis histogram suggests several interesting questions:\n这个直方图引出了几个有趣的问题：\n\nWhy are there more diamonds at whole carats and common fractions of carats?\n为什么在整数克拉和常见的克拉分数处有更多的钻石？\nWhy are there more diamonds slightly to the right of each peak than there are slightly to the left of each peak?\n为什么每个峰值右侧的钻石数量比左侧的要多？\n\nVisualizations can also reveal clusters, which suggest that subgroups exist in your data.\n可视化还可以揭示聚类，这表明你的数据中存在子群。\nTo understand the subgroups, ask:\n要理解这些子群，可以问：\n\nHow are the observations within each subgroup similar to each other?\n每个子群内的观测值彼此之间有何相似之处？\nHow are the observations in separate clusters different from each other?\n不同聚类中的观测值彼此之间有何不同之处？\nHow can you explain or describe the clusters?\n你如何解释或描述这些聚类？\nWhy might the appearance of clusters be misleading?\n为什么聚类的出现可能会产生误导？\n\nSome of these questions can be answered with the data while some will require domain expertise about the data.\n其中一些问题可以用数据来回答，而另一些则需要关于该数据的领域专业知识。\nMany of them will prompt you to explore a relationship between variables, for example, to see if the values of one variable can explain the behavior of another variable.\n其中许多问题会促使你探索变量之间的关系，例如，看看一个变量的值是否能解释另一个变量的行为。\nWe’ll get to that shortly.\n我们很快就会谈到这一点。\n\n10.3.2 Unusual values\nOutliers are observations that are unusual; data points that don’t seem to fit the pattern.\n离群值是异常的观测值；即那些似乎不符合模式的数据点。\nSometimes outliers are data entry errors, sometimes they are simply values at the extremes that happened to be observed in this data collection, and other times they suggest important new discoveries.\n有时离群值是数据录入错误，有时它们仅仅是在这次数据收集中碰巧观测到的极端值，而其他时候它们则可能预示着重要的新发现。\nWhen you have a lot of data, outliers are sometimes difficult to see in a histogram.\n当你有大量数据时，离群值有时在直方图中很难看出来。\nFor example, take the distribution of the y variable from the diamonds dataset.\n例如，以钻石数据集中的 y 变量分布为例。\nThe only evidence of outliers is the unusually wide limits on the x-axis.\n离群值的唯一证据是 x 轴上异常宽的范围。\n\nggplot(diamonds, aes(x = y)) + \n  geom_histogram(binwidth = 0.5)\n\n\n\n\n\n\n\nThere are so many observations in the common bins that the rare bins are very short, making it very difficult to see them (although maybe if you stare intently at 0 you’ll spot something).\n在常见的箱（bin）中有如此多的观测值，以至于罕见的箱非常短，使得它们很难被看到（尽管如果你仔细盯着 0 的位置，或许能发现点什么）。\nTo make it easy to see the unusual values, we need to zoom to small values of the y-axis with coord_cartesian():\n为了更容易地看到异常值，我们需要使用 coord_cartesian() 来放大 y 轴的较小值部分：\n\nggplot(diamonds, aes(x = y)) + \n  geom_histogram(binwidth = 0.5) +\n  coord_cartesian(ylim = c(0, 50))\n\n\n\n\n\n\n\ncoord_cartesian() also has an xlim() argument for when you need to zoom into the x-axis.coord_cartesian() 也有一个 xlim() 参数，用于当你需要放大 x 轴时。\nggplot2 also has xlim() and ylim() functions that work slightly differently: they throw away the data outside the limits.\nggplot2 也有 xlim() 和 ylim() 函数，它们的工作方式略有不同：它们会丢弃超出范围的数据。\nThis allows us to see that there are three unusual values: 0, ~30, and ~60.\n这让我们能看到有三个异常值：0、约 30 和约 60。\nWe pluck them out with dplyr:\n我们用 dplyr 把它们挑选出来：\n\nunusual &lt;- diamonds |&gt; \n  filter(y &lt; 3 | y &gt; 20) |&gt; \n  select(price, x, y, z) |&gt;\n  arrange(y)\nunusual\n#&gt; # A tibble: 9 × 4\n#&gt;   price     x     y     z\n#&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  5139  0      0    0   \n#&gt; 2  6381  0      0    0   \n#&gt; 3 12800  0      0    0   \n#&gt; 4 15686  0      0    0   \n#&gt; 5 18034  0      0    0   \n#&gt; 6  2130  0      0    0   \n#&gt; 7  2130  0      0    0   \n#&gt; 8  2075  5.15  31.8  5.12\n#&gt; 9 12210  8.09  58.9  8.06\n\nThe y variable measures one of the three dimensions of these diamonds, in mm.y 变量以毫米（mm）为单位，测量了这些钻石的三个维度之一。\nWe know that diamonds can’t have a width of 0mm, so these values must be incorrect.\n我们知道钻石的宽度不可能是 0 毫米，所以这些值肯定是错误的。\nBy doing EDA, we have discovered missing data that was coded as 0, which we never would have found by simply searching for NAs.\n通过进行 EDA，我们发现了被编码为 0 的缺失数据，这是我们仅通过搜索 NA 永远无法发现的。\nGoing forward we might choose to re-code these values as NAs in order to prevent misleading calculations.\n接下来，我们可能会选择将这些值重新编码为 NA，以防止产生误导性的计算结果。\nWe might also suspect that measurements of 32mm and 59mm are implausible: those diamonds are over an inch long, but don’t cost hundreds of thousands of dollars!\n我们可能还会怀疑 32 毫米和 59 毫米的测量值是不可信的：那些钻石超过一英寸长，但价格却没有达到数十万美元！\nIt’s good practice to repeat your analysis with and without the outliers.\n一个好的做法是，在包含和不包含离群值的情况下重复你的分析。\nIf they have minimal effect on the results, and you can’t figure out why they’re there, it’s reasonable to omit them, and move on.\n如果它们对结果的影响微乎其微，而且你无法找出它们存在的原因，那么省略它们并继续分析是合理的。\nHowever, if they have a substantial effect on your results, you shouldn’t drop them without justification.\n然而，如果它们对你的结果有重大影响，你就不应该在没有正当理由的情况下丢弃它们。\nYou’ll need to figure out what caused them (e.g., a data entry error) and disclose that you removed them in your write-up.\n你需要弄清楚是什么导致了它们（例如，数据录入错误），并在你的报告中披露你移除了它们。\n\n10.3.3 Exercises\n\nExplore the distribution of each of the x, y, and z variables in diamonds. What do you learn? Think about a diamond and how you might decide which dimension is the length, width, and depth.\nExplore the distribution of price. Do you discover anything unusual or surprising? (Hint: Carefully think about the binwidth and make sure you try a wide range of values.)\nHow many diamonds are 0.99 carat? How many are 1 carat? What do you think is the cause of the difference?\nCompare and contrast coord_cartesian() vs. xlim() or ylim() when zooming in on a histogram. What happens if you leave binwidth unset? What happens if you try and zoom so only half a bar shows?",
    "crumbs": [
      "Visualize",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "EDA.html#sec-unusual-values-eda",
    "href": "EDA.html#sec-unusual-values-eda",
    "title": "10  Exploratory data analysis",
    "section": "\n10.4 Unusual values",
    "text": "10.4 Unusual values\nIf you’ve encountered unusual values in your dataset, and simply want to move on to the rest of your analysis, you have two options.\n如果你在数据集中遇到了异常值，并且只想继续进行其余的分析，你有两个选择。\n\n\nDrop the entire row with the strange values:\n删除包含异常值的整行：\n\ndiamonds2 &lt;- diamonds |&gt; \n  filter(between(y, 3, 20))\n\nWe don’t recommend this option because one invalid value doesn’t imply that all the other values for that observation are also invalid.\n我们不推荐这个选项，因为一个无效值并不意味着该观测的所有其他值也都无效。\nAdditionally, if you have low quality data, by the time that you’ve applied this approach to every variable you might find that you don’t have any data left!\n此外，如果你的数据质量很低，当你对每个变量都采用这种方法后，你可能会发现你已经没有任何数据了！\n\n\nInstead, we recommend replacing the unusual values with missing values.\n相反，我们建议用缺失值替换异常值。\nThe easiest way to do this is to use mutate() to replace the variable with a modified copy.\n最简单的方法是使用 mutate() 将变量替换为其修改后的副本。\nYou can use the if_else() function to replace unusual values with NA:\n你可以使用 if_else() 函数将异常值替换为 NA：\n\ndiamonds2 &lt;- diamonds |&gt; \n  mutate(y = if_else(y &lt; 3 | y &gt; 20, NA, y))\n\n\n\nIt’s not obvious where you should plot missing values, so ggplot2 doesn’t include them in the plot, but it does warn that they’ve been removed:\n由于不清楚应该在哪里绘制缺失值，ggplot2 在绘图时不会包含它们，但会发出警告，提示它们已被移除：\n\nggplot(diamonds2, aes(x = x, y = y)) + \n  geom_point()\n#&gt; Warning: Removed 9 rows containing missing values or values outside the scale range\n#&gt; (`geom_point()`).\n\n\n\n\n\n\n\nTo suppress that warning, set na.rm = TRUE:\n要抑制该警告，请设置 na.rm = TRUE：\n\nggplot(diamonds2, aes(x = x, y = y)) + \n  geom_point(na.rm = TRUE)\n\nOther times you want to understand what makes observations with missing values different to observations with recorded values.\n其他时候，你想要理解具有缺失值的观测值与具有记录值的观测值有何不同。\nFor example, in nycflights13::flights1, missing values in the dep_time variable indicate that the flight was cancelled.\n例如，在 nycflights13::flights1 中，dep_time 变量中的缺失值表示航班被取消了。\nSo you might want to compare the scheduled departure times for cancelled and non-cancelled times.\n因此，你可能想比较已取消航班和未取消航班的计划起飞时间。\nYou can do this by making a new variable, using is.na() to check if dep_time is missing.\n你可以通过创建一个新变量来实现这一点，使用 is.na() 来检查 dep_time 是否缺失。\n\nnycflights13::flights |&gt; \n  mutate(\n    cancelled = is.na(dep_time),\n    sched_hour = sched_dep_time %/% 100,\n    sched_min = sched_dep_time %% 100,\n    sched_dep_time = sched_hour + (sched_min / 60)\n  ) |&gt; \n  ggplot(aes(x = sched_dep_time)) + \n  geom_freqpoly(aes(color = cancelled), binwidth = 1/4)\n\n\n\n\n\n\n\nHowever this plot isn’t great because there are many more non-cancelled flights than cancelled flights.\n然而，这张图并不理想，因为未取消的航班数量远远多于已取消的航班。\nIn the next section we’ll explore some techniques for improving this comparison.\n在下一节中，我们将探讨一些改进这种比较的技巧。\n\n10.4.1 Exercises\n\nWhat happens to missing values in a histogram? What happens to missing values in a bar chart? Why is there a difference in how missing values are handled in histograms and bar charts?\nWhat does na.rm = TRUE do in mean() and sum()?\nRecreate the frequency plot of scheduled_dep_time colored by whether the flight was cancelled or not. Also facet by the cancelled variable. Experiment with different values of the scales variable in the faceting function to mitigate the effect of more non-cancelled flights than cancelled flights.",
    "crumbs": [
      "Visualize",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "EDA.html#covariation",
    "href": "EDA.html#covariation",
    "title": "10  Exploratory data analysis",
    "section": "\n10.5 Covariation",
    "text": "10.5 Covariation\nIf variation describes the behavior within a variable, covariation describes the behavior between variables.\n如果说变异描述的是单个变量内部的行为，那么协变则描述的是变量之间的行为。\nCovariation is the tendency for the values of two or more variables to vary together in a related way.\n**协变（Covariation）**是两个或多个变量的值以相关的方式一同变化的趋势。\nThe best way to spot covariation is to visualize the relationship between two or more variables.\n发现协变的最好方法是可视化两个或多个变量之间的关系。\n\n10.5.1 A categorical and a numerical variable\nFor example, let’s explore how the price of a diamond varies with its quality (measured by cut) using geom_freqpoly():\n例如，让我们使用 geom_freqpoly() 来探究钻石价格如何随其质量（以 cut 衡量）而变化：\n\nggplot(diamonds, aes(x = price)) + \n  geom_freqpoly(aes(color = cut), binwidth = 500, linewidth = 0.75)\n\n\n\n\n\n\n\nNote that ggplot2 uses an ordered color scale for cut because it’s defined as an ordered factor variable in the data.\n请注意，ggplot2 对 cut 使用了有序颜色标度，因为它在数据中被定义为一个有序因子变量。\nYou’ll learn more about these in Section 16.6.\n你将在 Section 16.6 章节中学到更多关于这些的内容。\nThe default appearance of geom_freqpoly() is not that useful here because the height, determined by the overall count, differs so much across cuts, making it hard to see the differences in the shapes of their distributions.\n在这里，geom_freqpoly() 的默认外观并不十分有用，因为由总计数决定的高度在不同的 cut（切工）之间差异巨大，这使得我们很难看出它们分布形状的差异。\nTo make the comparison easier we need to swap what is displayed on the y-axis.\n为了让比较更容易，我们需要更换 Y 轴上显示的内容。\nInstead of displaying count, we’ll display the density, which is the count standardized so that the area under each frequency polygon is one.\n我们将不再显示计数，而是显示密度，密度是经过标准化的计数，使得每个频率多边形下的面积为一。\n\nggplot(diamonds, aes(x = price, y = after_stat(density))) + \n  geom_freqpoly(aes(color = cut), binwidth = 500, linewidth = 0.75)\n\n\n\n\n\n\n\nNote that we’re mapping the density to y, but since density is not a variable in the diamonds dataset, we need to first calculate it.\n请注意，我们将密度映射到 y，但由于 density 不是 diamonds 数据集中的变量，我们需要先计算它。\nWe use the after_stat() function to do so.\n我们使用 after_stat() 函数来完成这个计算。\nThere’s something rather surprising about this plot - it appears that fair diamonds (the lowest quality) have the highest average price!\n这张图有些出人意料——似乎 “Fair”（最差质量）等级的钻石平均价格最高！\nBut maybe that’s because frequency polygons are a little hard to interpret - there’s a lot going on in this plot.\n但这也许是因为频率多边形有点难以解读——这张图中信息量太大了。\nA visually simpler plot for exploring this relationship is using side-by-side boxplots.\n一个在视觉上更简洁、用于探索这种关系的图是并排箱线图。\n\nggplot(diamonds, aes(x = cut, y = price)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nWe see much less information about the distribution, but the boxplots are much more compact so we can more easily compare them (and fit more on one plot).\n我们看到的关于分布的信息少了很多，但箱线图更加紧凑，因此我们可以更容易地比较它们（并且可以在一张图上容纳更多内容）。\nIt supports the counter-intuitive finding that better quality diamonds are typically cheaper!\n它支持了一个与直觉相悖的发现：质量更好的钻石通常更便宜！\nIn the exercises, you’ll be challenged to figure out why.\n在练习中，你将需要挑战去弄清楚为什么会这样。\ncut is an ordered factor: fair is worse than good, which is worse than very good and so on.cut 是一个有序因子：fair 比 good 差，good 又比 very good 差，以此类推。\nMany categorical variables don’t have such an intrinsic order, so you might want to reorder them to make a more informative display.\n许多分类变量没有这种内在的顺序，所以你可能想要重新排序它们以获得信息更丰富的展示。\nOne way to do that is with fct_reorder().\n一种方法是使用 fct_reorder()。\nYou’ll learn more about that function in Section 16.4, but we want to give you a quick preview here because it’s so useful.\n你将在 Section 16.4 中学到更多关于该函数的内容，但我们想在这里给你一个快速预览，因为它非常有用。\nFor example, take the class variable in the mpg dataset.\n例如，以 mpg 数据集中的 class 变量为例。\nYou might be interested to know how highway mileage varies across classes:\n你可能想知道不同车辆类别的高速公路里程数有何不同：\n\nggplot(mpg, aes(x = class, y = hwy)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nTo make the trend easier to see, we can reorder class based on the median value of hwy:\n为了使趋势更易于观察，我们可以根据 hwy 的中位数对 class 进行重新排序：\n\nggplot(mpg, aes(x = fct_reorder(class, hwy, median), y = hwy)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nIf you have long variable names, geom_boxplot() will work better if you flip it 90°.\n如果你的变量名很长，将 geom_boxplot() 翻转 90° 会有更好的效果。\nYou can do that by exchanging the x and y aesthetic mappings.\n你可以通过交换 x 和 y 的美学映射来实现这一点。\n\nggplot(mpg, aes(x = hwy, y = fct_reorder(class, hwy, median))) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n10.5.1.1 Exercises\n\nUse what you’ve learned to improve the visualization of the departure times of cancelled vs. non-cancelled flights.\nBased on EDA, what variable in the diamonds dataset appears to be most important for predicting the price of a diamond? How is that variable correlated with cut? Why does the combination of those two relationships lead to lower quality diamonds being more expensive?\nInstead of exchanging the x and y variables, add coord_flip() as a new layer to the vertical boxplot to create a horizontal one. How does this compare to exchanging the variables?\nOne problem with boxplots is that they were developed in an era of much smaller datasets and tend to display a prohibitively large number of “outlying values”. One approach to remedy this problem is the letter value plot. Install the lvplot package, and try using geom_lv() to display the distribution of price vs. cut. What do you learn? How do you interpret the plots?\nCreate a visualization of diamond prices vs. a categorical variable from the diamonds dataset using geom_violin(), then a faceted geom_histogram(), then a colored geom_freqpoly(), and then a colored geom_density(). Compare and contrast the four plots. What are the pros and cons of each method of visualizing the distribution of a numerical variable based on the levels of a categorical variable?\nIf you have a small dataset, it’s sometimes useful to use geom_jitter() to avoid overplotting to more easily see the relationship between a continuous and categorical variable. The ggbeeswarm package provides a number of methods similar to geom_jitter(). List them and briefly describe what each one does.\n\n10.5.2 Two categorical variables\nTo visualize the covariation between categorical variables, you’ll need to count the number of observations for each combination of levels of these categorical variables.\n要可视化分类变量之间的协变，你需要计算这些分类变量各水平组合的观测数量。\nOne way to do that is to rely on the built-in geom_count():\n一种方法是依赖于内置的 geom_count() 函数：\n\nggplot(diamonds, aes(x = cut, y = color)) +\n  geom_count()\n\n\n\n\n\n\n\nThe size of each circle in the plot displays how many observations occurred at each combination of values.\n图中每个圆圈的大小显示了在每个值组合处发生的观测次数。\nCovariation will appear as a strong correlation between specific x values and specific y values.\n协变将表现为特定 x 值和特定 y 值之间的强相关性。\nAnother approach for exploring the relationship between these variables is computing the counts with dplyr:\n探索这些变量之间关系的另一种方法是使用 dplyr 计算计数：\n\ndiamonds |&gt; \n  count(color, cut)\n#&gt; # A tibble: 35 × 3\n#&gt;   color cut           n\n#&gt;   &lt;ord&gt; &lt;ord&gt;     &lt;int&gt;\n#&gt; 1 D     Fair        163\n#&gt; 2 D     Good        662\n#&gt; 3 D     Very Good  1513\n#&gt; 4 D     Premium    1603\n#&gt; 5 D     Ideal      2834\n#&gt; 6 E     Fair        224\n#&gt; # ℹ 29 more rows\n\nThen visualize with geom_tile() and the fill aesthetic:\n然后使用 geom_tile() 和填充美学进行可视化：\n\ndiamonds |&gt; \n  count(color, cut) |&gt;  \n  ggplot(aes(x = color, y = cut)) +\n  geom_tile(aes(fill = n))\n\n\n\n\n\n\n\nIf the categorical variables are unordered, you might want to use the seriation package to simultaneously reorder the rows and columns in order to more clearly reveal interesting patterns.\n如果分类变量是无序的，你可能需要使用 seriation 包来同时对行和列进行重新排序，以便更清晰地揭示有趣的模式。\nFor larger plots, you might want to try the heatmaply package, which creates interactive plots.\n对于较大的图表，你可能想尝试 heatmaply 包，它可以创建交互式图表。\n\n10.5.2.1 Exercises\n\nHow could you rescale the count dataset above to more clearly show the distribution of cut within color, or color within cut?\nWhat different data insights do you get with a segmented bar chart if color is mapped to the x aesthetic and cut is mapped to the fill aesthetic? Calculate the counts that fall into each of the segments.\nUse geom_tile() together with dplyr to explore how average flight departure delays vary by destination and month of year. What makes the plot difficult to read? How could you improve it?\n\n10.5.3 Two numerical variables\nYou’ve already seen one great way to visualize the covariation between two numerical variables: draw a scatterplot with geom_point().\n你已经见过一种可视化两个数值变量协变的好方法：用 geom_point() 绘制散点图。\nYou can see covariation as a pattern in the points.\n你可以将协变看作是数据点中的一种模式。\nFor example, you can see a positive relationship between the carat size and price of a diamond: diamonds with more carats have a higher price.\n例如，你可以看到钻石的克拉大小和价格之间存在正相关关系：克拉数越大的钻石价格越高。\nThe relationship is exponential.\n这种关系是指数级的。\n\nggplot(smaller, aes(x = carat, y = price)) +\n  geom_point()\n\n\n\n\n\n\n\n(In this section we’ll use the smaller dataset to stay focused on the bulk of the diamonds that are smaller than 3 carats)\n（在本节中，我们将使用 smaller 数据集，以专注于小于 3 克拉的大部分钻石）\nScatterplots become less useful as the size of your dataset grows, because points begin to overplot, and pile up into areas of uniform black, making it hard to judge differences in the density of the data across the 2-dimensional space as well as making it hard to spot the trend.\n随着数据集规模的增长，散点图的用处会减小，因为数据点开始重叠绘图（overplot），堆积成一片纯黑的区域，这使得判断数据在二维空间中的密度差异以及发现趋势都变得困难。\nYou’ve already seen one way to fix the problem: using the alpha aesthetic to add transparency.\n你已经看到一种解决这个问题的方法：使用 alpha 美学来增加透明度。\n\nggplot(smaller, aes(x = carat, y = price)) + \n  geom_point(alpha = 1 / 100)\n\n\n\n\n\n\n\nBut using transparency can be challenging for very large datasets.\n但是对于非常大的数据集，使用透明度可能具有挑战性。\nAnother solution is to use bin.\n另一个解决方案是使用分箱（bin）。\nPreviously you used geom_histogram() and geom_freqpoly() to bin in one dimension.\n之前你使用 geom_histogram() 和 geom_freqpoly() 在一维空间中进行分箱。\nNow you’ll learn how to use geom_bin2d() and geom_hex() to bin in two dimensions.\n现在你将学习如何使用 geom_bin2d() 和 geom_hex() 在二维空间中进行分箱。\ngeom_bin2d() and geom_hex() divide the coordinate plane into 2d bins and then use a fill color to display how many points fall into each bin.geom_bin2d() 和 geom_hex() 将坐标平面划分为二维的箱子，然后使用填充颜色来显示每个箱子中有多少个数据点。\ngeom_bin2d() creates rectangular bins.geom_bin2d() 创建矩形箱。\ngeom_hex() creates hexagonal bins.geom_hex() 创建六边形箱。\nYou will need to install the hexbin package to use geom_hex().\n你需要安装 hexbin 包才能使用 geom_hex()。\nggplot(smaller, aes(x = carat, y = price)) +\n  geom_bin2d()\n\n# install.packages(\"hexbin\")\nggplot(smaller, aes(x = carat, y = price)) +\n  geom_hex()\n\n\n\n\n\n\n\n\n\n\nAnother option is to bin one continuous variable so it acts like a categorical variable.\n另一种选择是对一个连续变量进行分箱，使其表现得像一个分类变量。\nThen you can use one of the techniques for visualizing the combination of a categorical and a continuous variable that you learned about.\n然后，你可以使用你学过的可视化分类变量和连续变量组合的技巧之一。\nFor example, you could bin carat and then for each group, display a boxplot:\n例如，你可以对 carat 进行分箱，然后为每个组显示一个箱线图：\n\nggplot(smaller, aes(x = carat, y = price)) + \n  geom_boxplot(aes(group = cut_width(carat, 0.1)))\n\n\n\n\n\n\n\ncut_width(x, width), as used above, divides x into bins of width width.\n如上所示，cut_width(x, width) 将 x 分割成宽度为 width 的区间。\nBy default, boxplots look roughly the same (apart from number of outliers) regardless of how many observations there are, so it’s difficult to tell that each boxplot summarizes a different number of points.\n默认情况下，无论观测数量多少，箱线图看起来都大致相同（除了离群点的数量），因此很难判断每个箱线图所概括的数据点数量是不同的。\nOne way to show that is to make the width of the boxplot proportional to the number of points with varwidth = TRUE.\n一种显示这一点的方法是，通过设置 varwidth = TRUE，使箱线图的宽度与数据点的数量成正比。\n\n10.5.3.1 Exercises\n\nInstead of summarizing the conditional distribution with a boxplot, you could use a frequency polygon. What do you need to consider when using cut_width() vs. cut_number()? How does that impact a visualization of the 2d distribution of carat and price?\nVisualize the distribution of carat, partitioned by price.\nHow does the price distribution of very large diamonds compare to small diamonds? Is it as you expect, or does it surprise you?\nCombine two of the techniques you’ve learned to visualize the combined distribution of cut, carat, and price.\n\nTwo dimensional plots reveal outliers that are not visible in one dimensional plots. For example, some points in the following plot have an unusual combination of x and y values, which makes the points outliers even though their x and y values appear normal when examined separately. Why is a scatterplot a better display than a binned plot for this case?\n\ndiamonds |&gt; \n  filter(x &gt;= 4) |&gt; \n  ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  coord_cartesian(xlim = c(4, 11), ylim = c(4, 11))\n\n\n\nInstead of creating boxes of equal width with cut_width(), we could create boxes that contain roughly equal number of points with cut_number(). What are the advantages and disadvantages of this approach?\n\nggplot(smaller, aes(x = carat, y = price)) + \n  geom_boxplot(aes(group = cut_number(carat, 20)))",
    "crumbs": [
      "Visualize",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "EDA.html#patterns-and-models",
    "href": "EDA.html#patterns-and-models",
    "title": "10  Exploratory data analysis",
    "section": "\n10.6 Patterns and models",
    "text": "10.6 Patterns and models\nIf a systematic relationship exists between two variables it will appear as a pattern in the data.\n如果两个变量之间存在系统性关系，它将以模式的形式出现在数据中。\nIf you spot a pattern, ask yourself:\n如果你发现一个模式，问问自己：\n\nCould this pattern be due to coincidence (i.e. random chance)?\n这个模式可能是由于巧合（即随机机会）吗？\nHow can you describe the relationship implied by the pattern?\n你如何描述这个模式所暗示的关系？\nHow strong is the relationship implied by the pattern?\n这个模式所暗示的关系有多强？\nWhat other variables might affect the relationship?\n还有哪些其他变量可能会影响这种关系？\nDoes the relationship change if you look at individual subgroups of the data?\n如果观察数据的单个子群，这种关系会改变吗？\n\nPatterns in your data provide clues about relationships, i.e., they reveal covariation.\n数据中的模式为我们提供了关于关系的线索，也就是说，它们揭示了协变关系。\nIf you think of variation as a phenomenon that creates uncertainty, covariation is a phenomenon that reduces it.\n如果你将变异视为一种产生不确定性的现象，那么协变就是一种减少不确定性的现象。\nIf two variables covary, you can use the values of one variable to make better predictions about the values of the second.\n如果两个变量协变，你可以利用一个变量的值来更好地预测另一个变量的值。\nIf the covariation is due to a causal relationship (a special case), then you can use the value of one variable to control the value of the second.\n如果协变是由因果关系（一种特殊情况）引起的，那么你可以利用一个变量的值来控制另一个变量的值。\nModels are a tool for extracting patterns out of data.\n模型是从数据中提取模式的工具。\nFor example, consider the diamonds data.\n例如，考虑钻石数据。\nIt’s hard to understand the relationship between cut and price, because cut and carat, and carat and price are tightly related.\n很难理解切工和价格之间的关系，因为切工和克拉，以及克拉和价格是紧密相关的。\nIt’s possible to use a model to remove the very strong relationship between price and carat so we can explore the subtleties that remain.\n我们可以使用一个模型来移除价格和克拉之间非常强的关系，这样我们就可以探索其中剩下的细微之处。\nThe following code fits a model that predicts price from carat and then computes the residuals (the difference between the predicted value and the actual value).\n以下代码拟合了一个模型，该模型根据 carat 预测 price，然后计算残差（预测值与实际值之间的差异）。\nThe residuals give us a view of the price of the diamond, once the effect of carat has been removed.\n残差让我们能够看到在移除了克拉效应后钻石的价格情况。\nNote that instead of using the raw values of price and carat, we log transform them first, and fit a model to the log-transformed values.\n请注意，我们没有使用 price 和 carat 的原始值，而是先对它们进行对数转换，然后对对数转换后的值拟合模型。\nThen, we exponentiate the residuals to put them back in the scale of raw prices.\n然后，我们对残差进行指数化，以将它们恢复到原始价格的尺度。\n\nlibrary(tidymodels)\n\ndiamonds &lt;- diamonds |&gt;\n  mutate(\n    log_price = log(price),\n    log_carat = log(carat)\n  )\n\ndiamonds_fit &lt;- linear_reg() |&gt;\n  fit(log_price ~ log_carat, data = diamonds)\n\ndiamonds_aug &lt;- augment(diamonds_fit, new_data = diamonds) |&gt;\n  mutate(.resid = exp(.resid))\n\nggplot(diamonds_aug, aes(x = carat, y = .resid)) + \n  geom_point()\n\n\n\n\n\n\n\nOnce you’ve removed the strong relationship between carat and price, you can see what you expect in the relationship between cut and price: relative to their size, better quality diamonds are more expensive.\n一旦你移除了克拉和价格之间的强相关性，你就能在切工和价格的关系中看到你所预期的结果：相对于它们的大小而言，质量更好的钻石更昂贵。\n\nggplot(diamonds_aug, aes(x = cut, y = .resid)) + \n  geom_boxplot()\n\n\n\n\n\n\n\nWe’re not discussing modelling in this book because understanding what models are and how they work is easiest once you have tools of data wrangling and programming in hand.\n我们在这本书中不讨论建模，因为一旦你掌握了数据整理和编程的工具，理解模型是什么以及它们如何工作就会变得最容易。",
    "crumbs": [
      "Visualize",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "EDA.html#summary",
    "href": "EDA.html#summary",
    "title": "10  Exploratory data analysis",
    "section": "\n10.7 Summary",
    "text": "10.7 Summary\nIn this chapter you’ve learned a variety of tools to help you understand the variation within your data.\n在本章中，你学习了各种工具来帮助你理解数据中的变异。\nYou’ve seen techniques that work with a single variable at a time and with a pair of variables.\n你已经看到了处理单个变量以及一对变量的技巧。\nThis might seem painfully restrictive if you have tens or hundreds of variables in your data, but they’re the foundation upon which all other techniques are built.\n如果你的数据中有数十或数百个变量，这可能看起来限制性很强，但它们是所有其他技术构建的基础。\nIn the next chapter, we’ll focus on the tools we can use to communicate our results.\n在下一章中，我们将专注于可以用来交流我们结果的工具。",
    "crumbs": [
      "Visualize",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "EDA.html#footnotes",
    "href": "EDA.html#footnotes",
    "title": "10  Exploratory data analysis",
    "section": "",
    "text": "Remember that when we need to be explicit about where a function (or dataset) comes from, we’ll use the special form package::function() or package::dataset.↩︎",
    "crumbs": [
      "Visualize",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "communication.html",
    "href": "communication.html",
    "title": "11  Communication",
    "section": "",
    "text": "11.1 Introduction\nIn Chapter 10, you learned how to use plots as tools for exploration. When you make exploratory plots, you know—even before looking—which variables the plot will display. You made each plot for a purpose, could quickly look at it, and then move on to the next plot. In the course of most analyses, you’ll produce tens or hundreds of plots, most of which are immediately thrown away.\n在 Chapter 10 中，你学习了如何使用图形作为探索的工具。当你制作探索性图形时，你甚至在看图之前就知道它将显示哪些变量。你制作的每个图形都有其目的，可以快速浏览一下，然后继续制作下一个。在大多数分析过程中，你会生成数十甚至数百个图形，其中大部分都会被立即丢弃。\nNow that you understand your data, you need to communicate your understanding to others. Your audience will likely not share your background knowledge and will not be deeply invested in the data. To help others quickly build up a good mental model of the data, you will need to invest considerable effort in making your plots as self-explanatory as possible. In this chapter, you’ll learn some of the tools that ggplot2 provides to do so.\n现在你已经理解了你的数据，你需要将你的理解传达给他人。你的受众很可能不具备你的背景知识，也不会对数据投入太多精力。为了帮助他人快速建立起对数据的良好心智模型，你需要投入大量精力使你的图形尽可能地不言自明。在本章中，你将学习 ggplot2 为此提供的一些工具。\nThis chapter focuses on the tools you need to create good graphics. We assume that you know what you want, and just need to know how to do it. For that reason, we highly recommend pairing this chapter with a good general visualization book. We particularly like The Truthful Art, by Albert Cairo. It doesn’t teach the mechanics of creating visualizations, but instead focuses on what you need to think about in order to create effective graphics.\n本章重点介绍创建优秀图形所需的工具。我们假设你已经知道自己想要什么，只需要知道如何实现它。因此，我们强烈建议将本章与一本优秀的通用可视化书籍结合阅读。我们特别推荐 Albert Cairo 的 The Truthful Art。这本书不教你创建可视化的具体操作，而是专注于为了创作有效的图形你需要思考些什么。",
    "crumbs": [
      "Visualize",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Communication</span>"
    ]
  },
  {
    "objectID": "communication.html#introduction",
    "href": "communication.html#introduction",
    "title": "11  Communication",
    "section": "",
    "text": "11.1.1 Prerequisites\nIn this chapter, we’ll focus once again on ggplot2. We’ll also use a little dplyr for data manipulation, scales to override the default breaks, labels, transformations and palettes, and a few ggplot2 extension packages, including ggrepel (https://ggrepel.slowkow.com) by Kamil Slowikowski and patchwork (https://patchwork.data-imaginist.com) by Thomas Lin Pedersen. Don’t forget that you’ll need to install those packages with install.packages() if you don’t already have them.\n在本章中，我们将再次聚焦于 ggplot2。我们还会使用一些 dplyr 进行数据操作，使用 scales 包来覆盖默认的刻度、标签、变换和调色板，以及一些 ggplot2 扩展包，包括 Kamil Slowikowski 开发的 ggrepel (https://ggrepel.slowkow.com) 和 Thomas Lin Pedersen 开发的 patchwork (https://patchwork.data-imaginist.com)。如果你还没有安装这些包，别忘了用 install.packages() 来安装它们。\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(ggrepel)\nlibrary(patchwork)",
    "crumbs": [
      "Visualize",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Communication</span>"
    ]
  },
  {
    "objectID": "communication.html#labels",
    "href": "communication.html#labels",
    "title": "11  Communication",
    "section": "\n11.2 Labels",
    "text": "11.2 Labels\nThe easiest place to start when turning an exploratory graphic into an expository graphic is with good labels. You add labels with the labs() function.\n将探索性图形转变为解释性图形，最简单的入手点就是添加良好的标签。你可以使用 labs() 函数来添加标签。\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(se = FALSE) +\n  labs(\n    x = \"Engine displacement (L)\",\n    y = \"Highway fuel economy (mpg)\",\n    color = \"Car type\",\n    title = \"Fuel efficiency generally decreases with engine size\",\n    subtitle = \"Two seaters (sports cars) are an exception because of their light weight\",\n    caption = \"Data from fueleconomy.gov\"\n  )\n\n\n\n\n\n\n\nThe purpose of a plot title is to summarize the main finding. Avoid titles that just describe what the plot is, e.g., “A scatterplot of engine displacement vs. fuel economy”.\n图形标题的目的是总结主要发现。应避免使用仅仅描述图形内容的标题，例如“发动机排量与燃油经济性的散点图”。\nIf you need to add more text, there are two other useful labels: subtitle adds additional detail in a smaller font beneath the title and caption adds text at the bottom right of the plot, often used to describe the source of the data. You can also use labs() to replace the axis and legend titles. It’s usually a good idea to replace short variable names with more detailed descriptions, and to include the units.\n如果你需要添加更多文字，还有两个有用的标签：subtitle (副标题) 会在主标题下方以较小字体添加额外细节，而 caption (说明文字) 会在图形右下角添加文字，通常用来描述数据来源。你也可以使用 labs() 来替换坐标轴和图例的标题。通常，用更详细的描述替换简短的变量名，并包含单位，是一个好主意。\nIt’s possible to use mathematical equations instead of text strings. Just switch \"\" out for quote() and read about the available options in ?plotmath:\n你也可以使用数学公式代替文本字符串。只需将 \"\" 换成 quote()，并在 ?plotmath 中阅读有关可用选项的信息：\n\ndf &lt;- tibble(\n  x = 1:10,\n  y = cumsum(x^2)\n)\n\nggplot(df, aes(x, y)) +\n  geom_point() +\n  labs(\n    x = quote(x[i]),\n    y = quote(sum(x[i] ^ 2, i == 1, n))\n  )\n\n\n\n\n\n\n\n\n11.2.1 Exercises\n\nCreate one plot on the fuel economy data with customized title, subtitle, caption, x, y, and color labels.\n\nRecreate the following plot using the fuel economy data. Note that both the colors and shapes of points vary by type of drive train.\n\n\n\n\n\n\n\n\n\nTake an exploratory graphic that you’ve created in the last month, and add informative titles to make it easier for others to understand.",
    "crumbs": [
      "Visualize",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Communication</span>"
    ]
  },
  {
    "objectID": "communication.html#annotations",
    "href": "communication.html#annotations",
    "title": "11  Communication",
    "section": "\n11.3 Annotations",
    "text": "11.3 Annotations\nIn addition to labelling major components of your plot, it’s often useful to label individual observations or groups of observations. The first tool you have at your disposal is geom_text(). geom_text() is similar to geom_point(), but it has an additional aesthetic: label. This makes it possible to add textual labels to your plots.\n除了为图形的主要组件添加标签外，为单个观测或观测组添加标签也常常很有用。你可以使用的第一个工具是 geom_text()。geom_text() 类似于 geom_point()，但它有一个额外的美学属性：label。这使得你可以在图形中添加文本标签。\nThere are two possible sources of labels. First, you might have a tibble that provides labels. In the following plot we pull out the cars with the highest engine size in each drive type and save their information as a new data frame called label_info.\n标签有两个可能的来源。首先，你可能有一个提供标签的 tibble。在下面的图形中，我们提取了每种驱动类型中发动机尺寸最大的汽车，并将其信息保存为一个名为 label_info 的新数据框。\n\nlabel_info &lt;- mpg |&gt;\n  group_by(drv) |&gt;\n  arrange(desc(displ)) |&gt;\n  slice_head(n = 1) |&gt;\n  mutate(\n    drive_type = case_when(\n      drv == \"f\" ~ \"front-wheel drive\",\n      drv == \"r\" ~ \"rear-wheel drive\",\n      drv == \"4\" ~ \"4-wheel drive\"\n    )\n  ) |&gt;\n  select(displ, hwy, drv, drive_type)\n\nlabel_info\n#&gt; # A tibble: 3 × 4\n#&gt; # Groups:   drv [3]\n#&gt;   displ   hwy drv   drive_type       \n#&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;            \n#&gt; 1   6.5    17 4     4-wheel drive    \n#&gt; 2   5.3    25 f     front-wheel drive\n#&gt; 3   7      24 r     rear-wheel drive\n\nThen, we use this new data frame to directly label the three groups to replace the legend with labels placed directly on the plot. Using the fontface and size arguments we can customize the look of the text labels. They’re larger than the rest of the text on the plot and bolded. (theme(legend.position = \"none\") turns all the legends off — we’ll talk about it more shortly.)\n然后，我们使用这个新的数据框来直接标记这三个组，用直接放置在图上的标签取代图例。通过使用 fontface 和 size 参数，我们可以自定义文本标签的外观。它们比图上其他文本更大并且加粗了。(theme(legend.position = \"none\") 会关闭所有图例——我们稍后会更详细地讨论它。)\n\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(se = FALSE) +\n  geom_text(\n    data = label_info, \n    aes(x = displ, y = hwy, label = drive_type),\n    fontface = \"bold\", size = 5, hjust = \"right\", vjust = \"bottom\"\n  ) +\n  theme(legend.position = \"none\")\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\nNote the use of hjust (horizontal justification) and vjust (vertical justification) to control the alignment of the label.\n注意使用 hjust (水平对齐) 和 vjust (垂直对齐) 来控制标签的对齐方式。\nHowever the annotated plot we made above is hard to read because the labels overlap with each other, and with the points. We can use the geom_label_repel() function from the ggrepel package to address both of these issues. This useful package will automatically adjust labels so that they don’t overlap:\n然而，我们上面制作的带注释的图很难阅读，因为标签之间以及标签与数据点之间存在重叠。我们可以使用 ggrepel 包中的 geom_label_repel() 函数来解决这两个问题。这个有用的包会自动调整标签，使其不重叠：\n\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(se = FALSE) +\n  geom_label_repel(\n    data = label_info, \n    aes(x = displ, y = hwy, label = drive_type),\n    fontface = \"bold\", size = 5, nudge_y = 2\n  ) +\n  theme(legend.position = \"none\")\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\nYou can also use the same idea to highlight certain points on a plot with geom_text_repel() from the ggrepel package. Note another handy technique used here: we added a second layer of large, hollow points to further highlight the labelled points.\n你也可以用同样的方法，使用 ggrepel 包中的 geom_text_repel() 来高亮图上的某些点。注意这里使用的另一个便捷技巧：我们添加了第二层大的空心点，以进一步突出显示被标记的点。\n\npotential_outliers &lt;- mpg |&gt;\n  filter(hwy &gt; 40 | (hwy &gt; 20 & displ &gt; 5))\n  \nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  geom_text_repel(data = potential_outliers, aes(label = model)) +\n  geom_point(data = potential_outliers, color = \"red\") +\n  geom_point(\n    data = potential_outliers,\n    color = \"red\", size = 3, shape = \"circle open\"\n  )\n\n\n\n\n\n\n\nRemember, in addition to geom_text() and geom_label(), you have many other geoms in ggplot2 available to help annotate your plot. A couple ideas:\n请记住，除了 geom_text() 和 geom_label() 之外，ggplot2 中还有许多其他几何对象 (geom) 可以帮助你注释图形。有几个想法：\n\nUse geom_hline() and geom_vline() to add reference lines. We often make them thick (linewidth = 2) and white (color = white), and draw them underneath the primary data layer. That makes them easy to see, without drawing attention away from the data.\n我们通常将它们设置得较粗 (linewidth = 2) 且为白色 (color = white)，并将其绘制在主数据层的下方。&lt;br&gt;这样既容易看到，又不会分散对数据的注意力。\nUse geom_rect() to draw a rectangle around points of interest. The boundaries of the rectangle are defined by aesthetics xmin, xmax, ymin, ymax. Alternatively, look into the ggforce package, specifically geom_mark_hull(), which allows you to annotate subsets of points with hulls.\n矩形的边界由美学属性 xmin、xmax、ymin、ymax 定义。&lt;br&gt;或者，可以研究一下 ggforce 包，特别是 geom_mark_hull()，它允许你用凸包来注释点的子集。\nUse geom_segment() with the arrow argument to draw attention to a point with an arrow. Use aesthetics x and y to define the starting location, and xend and yend to define the end location.\n使用美学属性 x 和 y 定义起始位置，xend 和 yend 定义结束位置。\n\nAnother handy function for adding annotations to plots is annotate(). As a rule of thumb, geoms are generally useful for highlighting a subset of the data while annotate() is useful for adding one or few annotation elements to a plot.\n另一个为图形添加注释的便捷函数是 annotate()。根据经验，几何对象 (geom) 通常用于高亮显示数据的子集，而 annotate() 则适用于向图形中添加一个或几个注释元素。\nTo demonstrate using annotate(), let’s create some text to add to our plot. The text is a bit long, so we’ll use stringr::str_wrap() to automatically add line breaks to it given the number of characters you want per line:\n为了演示 annotate() 的用法，让我们创建一些文本添加到图中。这段文本有点长，所以我们使用 stringr::str_wrap()，根据你希望每行显示的字符数来自动为其添加换行符：\n\ntrend_text &lt;- \"Larger engine sizes tend to have lower fuel economy.\" |&gt;\n  str_wrap(width = 30)\ntrend_text\n#&gt; [1] \"Larger engine sizes tend to\\nhave lower fuel economy.\"\n\nThen, we add two layers of annotation: one with a label geom and the other with a segment geom. The x and y aesthetics in both define where the annotation should start, and the xend and yend aesthetics in the segment annotation define the end location of the segment. Note also that the segment is styled as an arrow.\n然后，我们添加两层注释：一层是标签几何对象，另一层是线段几何对象。两者中的 x 和 y 美学属性定义了注释的起始位置，而线段注释中的 xend 和 yend 美学属性定义了线段的结束位置。还要注意，该线段被样式化为箭头。\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  annotate(\n    geom = \"label\", x = 3.5, y = 38,\n    label = trend_text,\n    hjust = \"left\", color = \"red\"\n  ) +\n  annotate(\n    geom = \"segment\",\n    x = 3, y = 35, xend = 5, yend = 25, color = \"red\",\n    arrow = arrow(type = \"closed\")\n  )\n\n\n\n\n\n\n\nAnnotation is a powerful tool for communicating main takeaways and interesting features of your visualizations. The only limit is your imagination (and your patience with positioning annotations to be aesthetically pleasing)!\n注释是传达可视化主要结论和有趣特征的强大工具。唯一的限制是你的想象力（以及你为美观地定位注释而付出的耐心）！\n\n11.3.1 Exercises\n\nUse geom_text() with infinite positions to place text at the four corners of the plot.\nUse annotate() to add a point geom in the middle of your last plot without having to create a tibble. Customize the shape, size, or color of the point.\nHow do labels with geom_text() interact with faceting? How can you add a label to a single facet? How can you put a different label in each facet? (Hint: Think about the dataset that is being passed to geom_text().)\nWhat arguments to geom_label() control the appearance of the background box?\nWhat are the four arguments to arrow()? How do they work? Create a series of plots that demonstrate the most important options.",
    "crumbs": [
      "Visualize",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Communication</span>"
    ]
  },
  {
    "objectID": "communication.html#scales",
    "href": "communication.html#scales",
    "title": "11  Communication",
    "section": "\n11.4 Scales",
    "text": "11.4 Scales\nThe third way you can make your plot better for communication is to adjust the scales. Scales control how the aesthetic mappings manifest visually.\n让你的图表更易于交流的第三种方法是调整标度（scales）。标度控制着美学映射（aesthetic mappings）在视觉上的表现方式。\n\n11.4.1 Default scales\nNormally, ggplot2 automatically adds scales for you. For example, when you type:\n通常情况下，ggplot2 会自动为你添加标度。例如，当你输入：\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class))\n\nggplot2 automatically adds default scales behind the scenes:\nggplot2 会在后台自动添加默认的标度：\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  scale_x_continuous() +\n  scale_y_continuous() +\n  scale_color_discrete()\n\nNote the naming scheme for scales: scale_ followed by the name of the aesthetic, then _, then the name of the scale. The default scales are named according to the type of variable they align with: continuous, discrete, datetime, or date. scale_x_continuous() puts the numeric values from displ on a continuous number line on the x-axis, scale_color_discrete() chooses colors for each of the class of car, etc. There are lots of non-default scales which you’ll learn about below.\n注意标度的命名方案：scale_ 后跟美学属性的名称，然后是 _，再后跟标度的名称。默认标度是根据它们所对应的变量类型来命名的：连续型 (continuous)、离散型 (discrete)、日期时间型 (datetime) 或日期型 (date)。scale_x_continuous() 将 displ 的数值放在 x 轴的连续数轴上，scale_color_discrete() 为每种汽车 class 选择颜色，等等。下面你将学习到许多非默认的标度。\nThe default scales have been carefully chosen to do a good job for a wide range of inputs. Nevertheless, you might want to override the defaults for two reasons:\n默认标度经过精心挑选，能在各种输入下都表现良好。尽管如此，你可能出于两个原因想要覆盖默认设置：\n\nYou might want to tweak some of the parameters of the default scale. This allows you to do things like change the breaks on the axes, or the key labels on the legend.\n这允许你做一些事情，比如更改坐标轴上的刻度，或图例上的键标签。\nYou might want to replace the scale altogether, and use a completely different algorithm. Often you can do better than the default because you know more about the data.\n通常你可以做得比默认更好，因为你对数据有更多的了解。\n\n11.4.2 Axis ticks and legend keys\nCollectively axes and legends are called guides. Axes are used for x and y aesthetics; legends are used for everything else.\n坐标轴和图例统称为引导元素 (guides)。坐标轴用于 x 和 y 美学属性；图例用于所有其他美学属性。\nThere are two primary arguments that affect the appearance of the ticks on the axes and the keys on the legend: breaks and labels. Breaks controls the position of the ticks, or the values associated with the keys. Labels controls the text label associated with each tick/key. The most common use of breaks is to override the default choice:\n有两个主要参数会影响坐标轴上的刻度线和图例上的键的外观：breaks 和 labels。Breaks 控制刻度线的位置，或与键相关联的值。Labels 控制与每个刻度线/键相关联的文本标签。breaks 最常见的用途是覆盖默认选项：\n\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  scale_y_continuous(breaks = seq(15, 40, by = 5)) \n\n\n\n\n\n\n\nYou can use labels in the same way (a character vector the same length as breaks), but you can also set it to NULL to suppress the labels altogether. This can be useful for maps, or for publishing plots where you can’t share the absolute numbers. You can also use breaks and labels to control the appearance of legends. For discrete scales for categorical variables, labels can be a named list of the existing level names and the desired labels for them.\n你可以用同样的方式使用 labels (一个与 breaks 长度相同的字符向量)，但你也可以将其设置为 NULL 来完全抑制标签。这对于地图或发布那些不能分享绝对数字的图表可能很有用。你还可以使用 breaks 和 labels 来控制图例的外观。对于分类变量的离散标度，labels 可以是一个命名列表，包含现有的水平名称和它们期望的标签。\n\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  scale_x_continuous(labels = NULL) +\n  scale_y_continuous(labels = NULL) +\n  scale_color_discrete(labels = c(\"4\" = \"4-wheel\", \"f\" = \"front\", \"r\" = \"rear\"))\n\n\n\n\n\n\n\nThe labels argument coupled with labelling functions from the scales package is also useful for formatting numbers as currency, percent, etc. The plot on the left shows default labelling with label_dollar(), which adds a dollar sign as well as a thousand separator comma. The plot on the right adds further customization by dividing dollar values by 1,000 and adding a suffix “K” (for “thousands”) as well as adding custom breaks. Note that breaks is in the original scale of the data.labels 参数与 scales 包中的标签函数相结合，对于格式化数字（如货币、百分比等）也很有用。左图展示了使用 label_dollar() 的默认标签，它会添加美元符号和千位分隔逗号。右图通过将美元值除以 1000 并添加后缀“K”（代表“千”），以及添加自定义断点，进行了进一步的定制。注意，breaks 是基于原始数据的标度。\n# Left\nggplot(diamonds, aes(x = price, y = cut)) +\n  geom_boxplot(alpha = 0.05) +\n  scale_x_continuous(labels = label_dollar())\n\n# Right\nggplot(diamonds, aes(x = price, y = cut)) +\n  geom_boxplot(alpha = 0.05) +\n  scale_x_continuous(\n    labels = label_dollar(scale = 1/1000, suffix = \"K\"), \n    breaks = seq(1000, 19000, by = 6000)\n  )\n\n\n\n\n\n\n\n\n\n\nAnother handy label function is label_percent():\n另一个方便的标签函数是 label_percent()：\n\nggplot(diamonds, aes(x = cut, fill = clarity)) +\n  geom_bar(position = \"fill\") +\n  scale_y_continuous(name = \"Percentage\", labels = label_percent())\n\n\n\n\n\n\n\nAnother use of breaks is when you have relatively few data points and want to highlight exactly where the observations occur. For example, take this plot that shows when each US president started and ended their term.breaks 的另一个用途是当你数据点相对较少，并希望精确地突出显示观测值出现的位置时。例如，看下面这张图，它显示了每位美国总统任期的起止时间。\n\npresidential |&gt;\n  mutate(id = 33 + row_number()) |&gt;\n  ggplot(aes(x = start, y = id)) +\n  geom_point() +\n  geom_segment(aes(xend = end, yend = id)) +\n  scale_x_date(name = NULL, breaks = presidential$start, date_labels = \"'%y\")\n\n\n\n\n\n\n\nNote that for the breaks argument we pulled out the start variable as a vector with presidential$start because we can’t do an aesthetic mapping for this argument. Also note that the specification of breaks and labels for date and datetime scales is a little different:\n请注意，对于 breaks 参数，我们使用 presidential$start 将 start 变量提取为一个向量，因为我们不能对这个参数进行美学映射。另请注意，日期和日期时间标度的断点和标签的规范略有不同：\n\ndate_labels takes a format specification, in the same form as parse_datetime().date_labels 接受一个格式规范，其形式与 parse_datetime() 相同。\ndate_breaks (not shown here), takes a string like “2 days” or “1 month”.date_breaks（此处未显示）接受一个字符串，如 “2 days” 或 “1 month”。\n\n11.4.3 Legend layout\nYou will most often use breaks and labels to tweak the axes. While they both also work for legends, there are a few other techniques you are more likely to use.\n你最常使用 breaks 和 labels 来调整坐标轴。虽然它们也适用于图例，但你更可能使用一些其他的技巧。\nTo control the overall position of the legend, you need to use a theme() setting. We’ll come back to themes at the end of the chapter, but in brief, they control the non-data parts of the plot. The theme setting legend.position controls where the legend is drawn:\n要控制图例的整体位置，你需要使用 theme() 设置。我们将在本章末尾再讨论主题 (themes)，但简而言之，它们控制着图表的非数据部分。主题设置 legend.position 控制图例的绘制位置：\nbase &lt;- ggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class))\n\nbase + theme(legend.position = \"right\") # the default\nbase + theme(legend.position = \"left\")\nbase + \n  theme(legend.position = \"top\") +\n  guides(color = guide_legend(nrow = 3))\nbase + \n  theme(legend.position = \"bottom\") +\n  guides(color = guide_legend(nrow = 3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf your plot is short and wide, place the legend at the top or bottom, and if it’s tall and narrow, place the legend at the left or right. You can also use legend.position = \"none\" to suppress the display of the legend altogether.\n如果你的图形又短又宽，就把图例放在顶部或底部；如果它又高又窄，就把图例放在左侧或右侧。你也可以使用 legend.position = \"none\" 来完全抑制图例的显示。\nTo control the display of individual legends, use guides() along with guide_legend() or guide_colorbar(). The following example shows two important settings: controlling the number of rows the legend uses with nrow, and overriding one of the aesthetics to make the points bigger. This is particularly useful if you have used a low alpha to display many points on a plot.\n要控制单个图例的显示，请使用 guides() 函数，并配合 guide_legend() 或 guide_colorbar()。下面的例子展示了两个重要的设置：使用 nrow 控制图例使用的行数，以及覆盖其中一个美学属性以使点变大。如果你在图上使用了较低的 alpha 值来显示许多点，这尤其有用。\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(se = FALSE) +\n  theme(legend.position = \"bottom\") +\n  guides(color = guide_legend(nrow = 2, override.aes = list(size = 4)))\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\nNote that the name of the argument in guides() matches the name of the aesthetic, just like in labs().\n注意，guides() 中的参数名称与美学属性的名称相匹配，就像在 labs() 中一样。\n\n11.4.4 Replacing a scale\nInstead of just tweaking the details a little, you can instead replace the scale altogether. There are two types of scales you’re mostly likely to want to switch out: continuous position scales and color scales. Fortunately, the same principles apply to all the other aesthetics, so once you’ve mastered position and color, you’ll be able to quickly pick up other scale replacements.\n你不仅可以微调细节，还可以完全替换整个标度。你最可能想要更换的两种标度是：连续位置标度和颜色标度。幸运的是，同样的原则也适用于所有其他美学属性，所以一旦你掌握了位置和颜色，你就能很快学会其他标度的替换。\nIt’s very useful to plot transformations of your variable. For example, it’s easier to see the precise relationship between carat and price if we log transform them:\n对你的变量进行变换后绘图非常有用。例如，如果我们对 carat 和 price 进行对数变换，就更容易看清它们之间的精确关系：\n# Left\nggplot(diamonds, aes(x = carat, y = price)) +\n  geom_bin2d()\n\n# Right\nggplot(diamonds, aes(x = log10(carat), y = log10(price))) +\n  geom_bin2d()\n\n\n\n\n\n\n\n\n\n\nHowever, the disadvantage of this transformation is that the axes are now labelled with the transformed values, making it hard to interpret the plot. Instead of doing the transformation in the aesthetic mapping, we can instead do it with the scale. This is visually identical, except the axes are labelled on the original data scale.\n然而，这种变换的缺点是坐标轴现在用变换后的值来标记，这使得图表难以解读。我们可以在标度中进行变换，而不是在美学映射中进行。这样在视觉上是相同的，但坐标轴会以原始数据标度进行标记。\n\nggplot(diamonds, aes(x = carat, y = price)) +\n  geom_bin2d() + \n  scale_x_log10() + \n  scale_y_log10()\n\n\n\n\n\n\n\nAnother scale that is frequently customized is color. The default categorical scale picks colors that are evenly spaced around the color wheel. Useful alternatives are the ColorBrewer scales which have been hand tuned to work better for people with common types of color blindness. The two plots below look similar, but there is enough difference in the shades of red and green that the dots on the right can be distinguished even by people with red-green color blindness.1\n另一个经常被定制的标度是颜色。默认的分类标度会选择在色轮上均匀分布的颜色。一些有用的替代方案是 ColorBrewer 标度，这些标度经过手工调整，对患有常见色盲类型的人更加友好。下面的两幅图看起来相似，但红色和绿色的色度有足够的差异，即使是红绿色盲的人也能区分右图中的点。1\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = drv))\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = drv)) +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\n\n\nDon’t forget simpler techniques for improving accessibility. If there are just a few colors, you can add a redundant shape mapping. This will also help ensure your plot is interpretable in black and white.\n不要忘记使用更简单的技术来提高可访问性。如果只有几种颜色，你可以添加一个冗余的形状映射。这也有助于确保你的图在黑白模式下也是可以理解的。\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = drv, shape = drv)) +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\nThe ColorBrewer scales are documented online at https://colorbrewer2.org/ and made available in R via the RColorBrewer package, by Erich Neuwirth. Figure 11.1 shows the complete list of all palettes. The sequential (top) and diverging (bottom) palettes are particularly useful if your categorical values are ordered, or have a “middle”. This often arises if you’ve used cut() to make a continuous variable into a categorical variable.\nColorBrewer 色阶的文档可以在线查看：https://colorbrewer2.org/，并通过 Erich Neuwirth 开发的 RColorBrewer 包在 R 中使用。Figure 11.1 展示了所有调色板的完整列表。如果你的分类值是有序的，或者有一个“中间值”，那么顺序（顶部）和发散（底部）调色板就特别有用。这种情况通常发生在你使用 cut() 函数将连续变量转换为分类变量时。\n\n\n\n\n\n\n\nFigure 11.1: All colorBrewer scales.\n\n\n\n\nWhen you have a predefined mapping between values and colors, use scale_color_manual(). For example, if we map presidential party to color, we want to use the standard mapping of red for Republicans and blue for Democrats. One approach for assigning these colors is using hex color codes:\n当你有一个预定义的值与颜色之间的映射时，请使用 scale_color_manual()。例如，如果我们将总统的党派映射到颜色，我们希望使用标准的映射：共和党为红色，民主党为蓝色。分配这些颜色的一种方法是使用十六进制颜色代码：\n\npresidential |&gt;\n  mutate(id = 33 + row_number()) |&gt;\n  ggplot(aes(x = start, y = id, color = party)) +\n  geom_point() +\n  geom_segment(aes(xend = end, yend = id)) +\n  scale_color_manual(values = c(Republican = \"#E81B23\", Democratic = \"#00AEF3\"))\n\n\n\n\n\n\n\nFor continuous color, you can use the built-in scale_color_gradient() or scale_fill_gradient(). If you have a diverging scale, you can use scale_color_gradient2(). That allows you to give, for example, positive and negative values different colors. That’s sometimes also useful if you want to distinguish points above or below the mean.\n对于连续颜色，你可以使用内置的 scale_color_gradient() 或 scale_fill_gradient()。如果你有一个发散型标度，你可以使用 scale_color_gradient2()。这允许你，例如，给正值和负值赋予不同的颜色。如果你想区分平均值以上或以下的点，这有时也很有用。\nAnother option is to use the viridis color scales. The designers, Nathaniel Smith and Stéfan van der Walt, carefully tailored continuous color schemes that are perceptible to people with various forms of color blindness as well as perceptually uniform in both color and black and white. These scales are available as continuous (c), discrete (d), and binned (b) palettes in ggplot2.\n另一个选择是使用 viridis 颜色标度。其设计者 Nathaniel Smith 和 Stéfan van der Walt 精心设计了连续的颜色方案，这些方案对于各种形式的色盲人士来说都是可感知的，并且在彩色和黑白模式下都具有感知上的均匀性。这些标度在 ggplot2 中以连续 (c)、离散 (d) 和分箱 (b) 调色板的形式提供。\ndf &lt;- tibble(\n  x = rnorm(10000),\n  y = rnorm(10000)\n)\n\nggplot(df, aes(x, y)) +\n  geom_hex() +\n  coord_fixed() +\n  labs(title = \"Default, continuous\", x = NULL, y = NULL)\n\nggplot(df, aes(x, y)) +\n  geom_hex() +\n  coord_fixed() +\n  scale_fill_viridis_c() +\n  labs(title = \"Viridis, continuous\", x = NULL, y = NULL)\n\nggplot(df, aes(x, y)) +\n  geom_hex() +\n  coord_fixed() +\n  scale_fill_viridis_b() +\n  labs(title = \"Viridis, binned\", x = NULL, y = NULL)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that all color scales come in two varieties: scale_color_*() and scale_fill_*() for the color and fill aesthetics respectively (the color scales are available in both UK and US spellings).\n请注意，所有颜色标度都有两种变体：scale_color_*() 和 scale_fill_*()，分别对应 color 和 fill 美学属性（颜色标度提供英式和美式两种拼写）。\n\n11.4.5 Zooming\nThere are three ways to control the plot limits:\n有三种方法可以控制图的界限：\n\nAdjusting what data are plotted.\n调整被绘制的数据。\nSetting the limits in each scale.\n在每个标度中设置范围（limits）。\nSetting xlim and ylim in coord_cartesian().\n在 coord_cartesian() 中设置 xlim 和 ylim。\n\nWe’ll demonstrate these options in a series of plots. The plot on the left shows the relationship between engine size and fuel efficiency, colored by type of drive train. The plot on the right shows the same variables, but subsets the data that are plotted. Subsetting the data has affected the x and y scales as well as the smooth curve.\n我们将通过一系列图表来演示这些选项。左边的图表显示了发动机尺寸和燃油效率之间的关系，并按驱动类型着色。右边的图表显示了相同的变量，但对绘制的数据进行了子集化。对数据进行子集化影响了 x 和 y 轴的标度以及平滑曲线。\n# Left\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = drv)) +\n  geom_smooth()\n\n# Right\nmpg |&gt;\n  filter(displ &gt;= 5 & displ &lt;= 6 & hwy &gt;= 10 & hwy &lt;= 25) |&gt;\n  ggplot(aes(x = displ, y = hwy)) +\n  geom_point(aes(color = drv)) +\n  geom_smooth()\n\n\n\n\n\n\n\n\n\n\nLet’s compare these to the two plots below where the plot on the left sets the limits on individual scales and the plot on the right sets them in coord_cartesian(). We can see that reducing the limits is equivalent to subsetting the data. Therefore, to zoom in on a region of the plot, it’s generally best to use coord_cartesian().\n让我们将这些与下面的两张图进行比较，其中左边的图在单个标度上设置了 limits，而右边的图在 coord_cartesian() 中设置了它们。我们可以看到，缩小限制等同于对数据进行子集化。因此，要放大图的某个区域，通常最好使用 coord_cartesian()。\n# Left\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = drv)) +\n  geom_smooth() +\n  scale_x_continuous(limits = c(5, 6)) +\n  scale_y_continuous(limits = c(10, 25))\n\n# Right\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = drv)) +\n  geom_smooth() +\n  coord_cartesian(xlim = c(5, 6), ylim = c(10, 25))\n\n\n\n\n\n\n\n\n\n\nOn the other hand, setting the limits on individual scales is generally more useful if you want to expand the limits, e.g., to match scales across different plots. For example, if we extract two classes of cars and plot them separately, it’s difficult to compare the plots because all three scales (the x-axis, the y-axis, and the color aesthetic) have different ranges.\n另一方面，如果你想扩大范围，例如，为了在不同图之间匹配标度，那么在单个标度上设置 limits 通常更有用。例如，如果我们提取两类汽车并分别绘制它们，那么比较这些图会很困难，因为所有三个标度（x轴、y轴和颜色美学）都有不同的范围。\nsuv &lt;- mpg |&gt; filter(class == \"suv\")\ncompact &lt;- mpg |&gt; filter(class == \"compact\")\n\n# Left\nggplot(suv, aes(x = displ, y = hwy, color = drv)) +\n  geom_point()\n\n# Right\nggplot(compact, aes(x = displ, y = hwy, color = drv)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nOne way to overcome this problem is to share scales across multiple plots, training the scales with the limits of the full data.\n克服这个问题的一种方法是在多个图之间共享标度，使用完整数据的 limits 来“训练”这些标度。\nx_scale &lt;- scale_x_continuous(limits = range(mpg$displ))\ny_scale &lt;- scale_y_continuous(limits = range(mpg$hwy))\ncol_scale &lt;- scale_color_discrete(limits = unique(mpg$drv))\n\n# Left\nggplot(suv, aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  x_scale +\n  y_scale +\n  col_scale\n\n# Right\nggplot(compact, aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  x_scale +\n  y_scale +\n  col_scale\n\n\n\n\n\n\n\n\n\n\nIn this particular case, you could have simply used faceting, but this technique is useful more generally, if for instance, you want to spread plots over multiple pages of a report.\n在这个特定的案例中，你本可以简单地使用分面，但这种技术在更普遍的情况下也很有用，例如，当你想将图表分布在报告的多个页面上时。\n\n11.4.6 Exercises\n\n\nWhy doesn’t the following code override the default scale?\n\ndf &lt;- tibble(\n  x = rnorm(10000),\n  y = rnorm(10000)\n)\n\nggplot(df, aes(x, y)) +\n  geom_hex() +\n  scale_color_gradient(low = \"white\", high = \"red\") +\n  coord_fixed()\n\n\nWhat is the first argument to every scale? How does it compare to labs()?\n\nChange the display of the presidential terms by:\n\nCombining the two variants that customize colors and x axis breaks.\nImproving the display of the y axis.\nLabelling each term with the name of the president.\nAdding informative plot labels.\nPlacing breaks every 4 years (this is trickier than it seems!).\n\n\n\nFirst, create the following plot. Then, modify the code using override.aes to make the legend easier to see.\n\nggplot(diamonds, aes(x = carat, y = price)) +\n  geom_point(aes(color = cut), alpha = 1/20)",
    "crumbs": [
      "Visualize",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Communication</span>"
    ]
  },
  {
    "objectID": "communication.html#sec-themes",
    "href": "communication.html#sec-themes",
    "title": "11  Communication",
    "section": "\n11.5 Themes",
    "text": "11.5 Themes\nFinally, you can customize the non-data elements of your plot with a theme:\n最后，你可以使用主题 (theme) 来自定义图形中的非数据元素：\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(se = FALSE) +\n  theme_bw()\n\n\n\n\n\n\n\nggplot2 includes the eight themes shown in Figure 11.2, with theme_gray() as the default.2 Many more are included in add-on packages like ggthemes (https://jrnold.github.io/ggthemes), by Jeffrey Arnold. You can also create your own themes, if you are trying to match a particular corporate or journal style.\nggplot2 包含了 Figure 11.2 中显示的八个主题，其中 theme_gray() 是默认主题。2 还有更多主题包含在附加包中，例如 Jeffrey Arnold 开发的 ggthemes (https://jrnold.github.io/ggthemes)。如果你想匹配特定的公司或期刊风格，也可以创建自己的主题。\n\n\n\n\n\n\n\nFigure 11.2: The eight themes built-in to ggplot2.\n\n\n\n\nIt’s also possible to control individual components of each theme, like the size and color of the font used for the y axis. We’ve already seen that legend.position controls where the legend is drawn. There are many other aspects of the legend that can be customized with theme(). For example, in the plot below we change the direction of the legend as well as put a black border around it. Note that customization of the legend box and plot title elements of the theme are done with element_*() functions. These functions specify the styling of non-data components, e.g., the title text is bolded in the face argument of element_text() and the legend border color is defined in the color argument of element_rect(). The theme elements that control the position of the title and the caption are plot.title.position and plot.caption.position, respectively. In the following plot these are set to \"plot\" to indicate these elements are aligned to the entire plot area, instead of the plot panel (the default). A few other helpful theme() components are used to change the placement for format of the title and caption text.\n也可以控制每个主题的单个组件，比如 y 轴使用的字体大小和颜色。我们已经看到 legend.position 控制图例的绘制位置。图例的许多其他方面也可以用 theme() 来定制。例如，在下面的图中，我们改变了图例的方向，并给它加上了黑色的边框。注意，图例框和图标题等主题元素的定制是通过 element_*() 函数完成的。这些函数指定了非数据组件的样式，例如，标题文本在 element_text() 的 face 参数中被加粗，图例边框颜色在 element_rect() 的 color 参数中被定义。控制标题和说明文字位置的主题元素分别是 plot.title.position 和 plot.caption.position。在下面的图中，这些被设置为 \"plot\"，表示这些元素与整个绘图区域对齐，而不是绘图面板（默认值）。还使用了一些其他有用的 theme() 组件来更改标题和说明文字的位置格式。\n\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  labs(\n    title = \"Larger engine sizes tend to have lower fuel economy\",\n    caption = \"Source: https://fueleconomy.gov.\"\n  ) +\n  theme(\n    legend.position = c(0.6, 0.7),\n    legend.direction = \"horizontal\",\n    legend.box.background = element_rect(color = \"black\"),\n    plot.title = element_text(face = \"bold\"),\n    plot.title.position = \"plot\",\n    plot.caption.position = \"plot\",\n    plot.caption = element_text(hjust = 0)\n  )\n#&gt; Warning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n#&gt; 3.5.0.\n#&gt; ℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\n\n\n\n\n\n\nFor an overview of all theme() components, see help with ?theme. The ggplot2 book is also a great place to go for the full details on theming.\n要了解所有 theme() 组件的概览，请查看 ?theme 的帮助文档。ggplot2 book 也是一个了解主题设置全部细节的好去处。\n\n11.5.1 Exercises\n\nPick a theme offered by the ggthemes package and apply it to the last plot you made.\nMake the axis labels of your plot blue and bolded.",
    "crumbs": [
      "Visualize",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Communication</span>"
    ]
  },
  {
    "objectID": "communication.html#layout",
    "href": "communication.html#layout",
    "title": "11  Communication",
    "section": "\n11.6 Layout",
    "text": "11.6 Layout\nSo far we talked about how to create and modify a single plot. What if you have multiple plots you want to lay out in a certain way? The patchwork package allows you to combine separate plots into the same graphic. We loaded this package earlier in the chapter.\n到目前为止，我们讨论了如何创建和修改单个图。但如果你有多个图，并希望以某种特定方式布局它们，该怎么办呢？patchwork 包允许你将多个独立的图组合成一个图形。我们在本章前面已经加载了这个包。\nTo place two plots next to each other, you can simply add them to each other. Note that you first need to create the plots and save them as objects (in the following example they’re called p1 and p2). Then, you place them next to each other with +.\n要将两个图并排放置，你只需将它们相加即可。请注意，你首先需要创建这些图并将它们保存为对象（在下面的示例中，它们被称为 p1 和 p2）。然后，你用 + 将它们并排放置。\n\np1 &lt;- ggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  labs(title = \"Plot 1\")\np2 &lt;- ggplot(mpg, aes(x = drv, y = hwy)) + \n  geom_boxplot() + \n  labs(title = \"Plot 2\")\np1 + p2\n\n\n\n\n\n\n\nIt’s important to note that in the above code chunk we did not use a new function from the patchwork package. Instead, the package added a new functionality to the + operator.\n需要注意的是，在上面的代码块中，我们并未使用 patchwork 包中的新函数。相反，该包为 + 运算符添加了新的功能。\nYou can also create complex plot layouts with patchwork. In the following, | places the p1 and p3 next to each other and / moves p2 to the next line.\n你还可以使用 patchwork 创建复杂的图形布局。在下文中，| 将 p1 和 p3 并排放置，而 / 将 p2 移到下一行。\n\np3 &lt;- ggplot(mpg, aes(x = cty, y = hwy)) + \n  geom_point() + \n  labs(title = \"Plot 3\")\n(p1 | p3) / p2\n\n\n\n\n\n\n\nAdditionally, patchwork allows you to collect legends from multiple plots into one common legend, customize the placement of the legend as well as dimensions of the plots, and add a common title, subtitle, caption, etc. to your plots. Below we create 5 plots. We have turned off the legends on the box plots and the scatterplot and collected the legends for the density plots at the top of the plot with & theme(legend.position = \"top\"). Note the use of the & operator here instead of the usual +. This is because we’re modifying the theme for the patchwork plot as opposed to the individual ggplots. The legend is placed on top, inside the guide_area(). Finally, we have also customized the heights of the various components of our patchwork – the guide has a height of 1, the box plots 3, density plots 2, and the faceted scatterplot 4. Patchwork divides up the area you have allotted for your plot using this scale and places the components accordingly.\n此外，patchwork 允许你将多个图的图例收集到一个公共图例中，自定义图例的位置以及图的尺寸，并为你的图添加公共的标题、副标题、说明等。下面我们创建 5 个图。我们关闭了箱线图和散点图的图例，并使用 & theme(legend.position = \"top\") 将密度图的图例收集到图的顶部。注意这里使用了 & 操作符而不是通常的 +。这是因为我们正在修改 patchwork 图的主题，而不是单个 ggplot。图例被放置在顶部，在 guide_area() 内部。最后，我们还自定义了 patchwork 各个组件的高度——引导区高度为 1，箱线图为 3，密度图为 2，分面散点图为 4。Patchwork 使用这个比例来划分你为图分配的区域，并相应地放置组件。\n\np1 &lt;- ggplot(mpg, aes(x = drv, y = cty, color = drv)) + \n  geom_boxplot(show.legend = FALSE) + \n  labs(title = \"Plot 1\")\n\np2 &lt;- ggplot(mpg, aes(x = drv, y = hwy, color = drv)) + \n  geom_boxplot(show.legend = FALSE) + \n  labs(title = \"Plot 2\")\n\np3 &lt;- ggplot(mpg, aes(x = cty, color = drv, fill = drv)) + \n  geom_density(alpha = 0.5) + \n  labs(title = \"Plot 3\")\n\np4 &lt;- ggplot(mpg, aes(x = hwy, color = drv, fill = drv)) + \n  geom_density(alpha = 0.5) + \n  labs(title = \"Plot 4\")\n\np5 &lt;- ggplot(mpg, aes(x = cty, y = hwy, color = drv)) + \n  geom_point(show.legend = FALSE) + \n  facet_wrap(~drv) +\n  labs(title = \"Plot 5\")\n\n(guide_area() / (p1 + p2) / (p3 + p4) / p5) +\n  plot_annotation(\n    title = \"City and highway mileage for cars with different drive trains\",\n    caption = \"Source: https://fueleconomy.gov.\"\n  ) +\n  plot_layout(\n    guides = \"collect\",\n    heights = c(1, 3, 2, 4)\n    ) &\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\nIf you’d like to learn more about combining and layout out multiple plots with patchwork, we recommend looking through the guides on the package website: https://patchwork.data-imaginist.com.\n如果你想了解更多关于使用 patchwork 组合和布局多个图的信息，我们建议你浏览该包网站上的指南：https://patchwork.data-imaginist.com。\n\n11.6.1 Exercises\n\n\nWhat happens if you omit the parentheses in the following plot layout. Can you explain why this happens?\n\np1 &lt;- ggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  labs(title = \"Plot 1\")\np2 &lt;- ggplot(mpg, aes(x = drv, y = hwy)) + \n  geom_boxplot() + \n  labs(title = \"Plot 2\")\np3 &lt;- ggplot(mpg, aes(x = cty, y = hwy)) + \n  geom_point() + \n  labs(title = \"Plot 3\")\n\n(p1 | p2) / p3\n\n\n\nUsing the three plots from the previous exercise, recreate the following patchwork.",
    "crumbs": [
      "Visualize",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Communication</span>"
    ]
  },
  {
    "objectID": "communication.html#summary",
    "href": "communication.html#summary",
    "title": "11  Communication",
    "section": "\n11.7 Summary",
    "text": "11.7 Summary\nIn this chapter you’ve learned about adding plot labels such as title, subtitle, caption as well as modifying default axis labels, using annotation to add informational text to your plot or to highlight specific data points, customizing the axis scales, and changing the theme of your plot. You’ve also learned about combining multiple plots in a single graph using both simple and complex plot layouts.\n在本章中，你学习了添加图形标签（如标题、副标题、说明文字）以及修改默认坐标轴标签、使用注释为图形添加信息性文本或突出显示特定数据点、自定义坐标轴标度和更改图形主题。你还学习了使用简单和复杂的图形布局将多个图形组合成一个图形。\nWhile you’ve so far learned about how to make many different types of plots and how to customize them using a variety of techniques, we’ve barely scratched the surface of what you can create with ggplot2. If you want to get a comprehensive understanding of ggplot2, we recommend reading the book, ggplot2: Elegant Graphics for Data Analysis. Other useful resources are the R Graphics Cookbook by Winston Chang and Fundamentals of Data Visualization by Claus Wilke.\n到目前为止，你已经学会了如何制作多种不同类型的图，以及如何使用各种技术对其进行自定义，但我们对 ggplot2 能创建的内容还只是浅尝辄止。如果你想全面了解 ggplot2，我们推荐阅读 ggplot2: Elegant Graphics for Data Analysis 这本书。其他有用的资源包括 Winston Chang 的 R Graphics Cookbook 和 Claus Wilke 的 Fundamentals of Data Visualization。",
    "crumbs": [
      "Visualize",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Communication</span>"
    ]
  },
  {
    "objectID": "communication.html#footnotes",
    "href": "communication.html#footnotes",
    "title": "11  Communication",
    "section": "",
    "text": "You can use a tool like SimDaltonism to simulate color blindness to test these images.↩︎\nMany people wonder why the default theme has a gray background. This was a deliberate choice because it puts the data forward while still making the grid lines visible. The white grid lines are visible (which is important because they significantly aid position judgments), but they have little visual impact and we can easily tune them out. The gray background gives the plot a similar typographic color to the text, ensuring that the graphics fit in with the flow of a document without jumping out with a bright white background. Finally, the gray background creates a continuous field of color which ensures that the plot is perceived as a single visual entity.↩︎",
    "crumbs": [
      "Visualize",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Communication</span>"
    ]
  },
  {
    "objectID": "transform.html",
    "href": "transform.html",
    "title": "Transform",
    "section": "",
    "text": "The second part of the book was a deep dive into data visualization. In this part of the book, you’ll learn about the most important types of variables that you’ll encounter inside a data frame and learn the tools you can use to work with them.\n本书的第二部分深入探讨了数据可视化。在本书的这一部分，你将学习到在数据框 (data frame) 中会遇到的最重要变量类型，以及可以用来处理它们的工具。\n\n\n\n\n\n\n\nFigure 1: The options for data transformation depend heavily on the type of data involved, the subject of this part of the book.\n\n\n\n\nYou can read these chapters as you need them; they’re designed to be largely standalone so that they can be read out of order.\n你可以根据需要阅读这些章节；它们的设计初衷是使其在很大程度上保持独立，因此可以不按顺序阅读。\n\n12  Logical vectors teaches you about logical vectors. These are the simplest types of vectors, but are extremely powerful. You’ll learn how to create them with numeric comparisons, how to combine them with Boolean algebra, how to use them in summaries, and how to use them for conditional transformations. 这是最简单的向量类型，但功能极其强大。你将学习如何通过数值比较来创建它们，如何用布尔代数 (Boolean algebra) 来组合它们，如何在汇总 (summaries) 中使用它们，以及如何将它们用于条件转换。\n13  Numbers dives into tools for vectors of numbers, the powerhouse of data science. You’ll learn more about counting and a bunch of important transformation and summary functions. 你将学习更多关于计数以及一系列重要的转换和汇总函数。\n14  Strings will give you the tools to work with strings: you’ll slice them, you’ll dice them, and you’ll stick them back together again. This chapter mostly focuses on the stringr package, but you’ll also learn some more tidyr functions devoted to extracting data from character strings. 本章主要关注 stringr 包，但你也将学习一些 tidyr 中专门用于从字符串中提取数据的函数。\n15  Regular expressions introduces you to regular expressions, a powerful tool for manipulating strings. This chapter will take you from thinking that a cat walked over your keyboard to reading and writing complex string patterns. 本章将带你从感觉像是猫走过了你的键盘，到能够读懂并编写复杂的字符串模式。\n16  Factors introduces factors: the data type that R uses to store categorical data. You use a factor when variable has a fixed set of possible values, or when you want to use a non-alphabetical ordering of a string. 当一个变量有一组固定的可能值，或者当你想对字符串使用非字母顺序排序时，你会使用因子。\n17  Dates and times will give you the key tools for working with dates and date-times. Unfortunately, the more you learn about date-times, the more complicated they seem to get, but with the help of the lubridate package, you’ll learn to how to overcome the most common challenges. 不幸的是，你对日期时间了解得越多，它们似乎就变得越复杂，但在 lubridate 包的帮助下，你将学会如何克服最常见的挑战。\n18  Missing values discusses missing values in depth. We’ve discussed them a couple of times in isolation, but now it’s time to discuss them holistically, helping you come to grips with the difference between implicit and explicit missing values, and how and why you might convert between them. 我们已经零星地讨论过几次，但现在是时候全面地讨论它们了，帮助你理解隐式 (implicit) 和显式 (explicit) 缺失值之间的区别，以及如何以及为何在它们之间进行转换。\n19  Joins finishes up this part of the book by giving you tools to join two (or more) data frames together. Learning about joins will force you to grapple with the idea of keys, and think about how you identify each row in a dataset. 学习连接 (joins) 将迫使你深入理解键 (keys) 的概念，并思考如何识别数据集中的每一行。",
    "crumbs": [
      "Transform"
    ]
  },
  {
    "objectID": "logicals.html",
    "href": "logicals.html",
    "title": "12  Logical vectors",
    "section": "",
    "text": "12.1 Introduction\nIn this chapter, you’ll learn tools for working with logical vectors. Logical vectors are the simplest type of vector because each element can only be one of three possible values: TRUE, FALSE, and NA. It’s relatively rare to find logical vectors in your raw data, but you’ll create and manipulate them in the course of almost every analysis.\n在本章中，你将学习使用逻辑向量的工具。逻辑向量是最简单的向量类型，因为每个元素只能是三个可能值之一：TRUE、FALSE 和 NA。在原始数据中相对较少见到逻辑向量，但你几乎在每一次分析过程中都会创建和操作它们。\nWe’ll begin by discussing the most common way of creating logical vectors: with numeric comparisons. Then you’ll learn about how you can use Boolean algebra to combine different logical vectors, as well as some useful summaries. We’ll finish off with if_else() and case_when(), two useful functions for making conditional changes powered by logical vectors.\n我们将从讨论创建逻辑向量最常见的方法开始：使用数值比较。然后，你将学习如何使用布尔代数来组合不同的逻辑向量，以及一些有用的汇总方法。最后，我们将介绍 if_else() 和 case_when()，这是两个由逻辑向量驱动、用于进行条件性变更的有用函数。",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Logical vectors</span>"
    ]
  },
  {
    "objectID": "logicals.html#introduction",
    "href": "logicals.html#introduction",
    "title": "12  Logical vectors",
    "section": "",
    "text": "12.1.1 Prerequisites\nMost of the functions you’ll learn about in this chapter are provided by base R, so we don’t need the tidyverse, but we’ll still load it so we can use mutate(), filter(), and friends to work with data frames. We’ll also continue to draw examples from the nycflights13::flights dataset.\n本章你将学到的大多数函数都由 R base 提供，所以我们不需要 tidyverse，但我们仍然会加载它，以便我们可以使用 mutate()、filter() 及其他相关函数来处理数据框。我们也将继续使用 nycflights13::flights 数据集中的例子。\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\nHowever, as we start to cover more tools, there won’t always be a perfect real example. So we’ll start making up some dummy data with c():\n然而，随着我们开始介绍更多的工具，并不总能找到一个完美的真实案例。因此，我们将开始使用 c() 创建一些虚拟数据：\n\nx &lt;- c(1, 2, 3, 5, 7, 11, 13)\nx * 2\n#&gt; [1]  2  4  6 10 14 22 26\n\nThis makes it easier to explain individual functions at the cost of making it harder to see how it might apply to your data problems. Just remember that any manipulation we do to a free-floating vector, you can do to a variable inside a data frame with mutate() and friends.\n这样做虽然更容易解释单个函数，但代价是更难看出它如何应用于你的数据问题。只需记住，我们对一个独立向量所做的任何操作，你都可以通过 mutate() 及相关函数对数据框内的变量进行同样的操作。\n\ndf &lt;- tibble(x)\ndf |&gt; \n  mutate(y = x * 2)\n#&gt; # A tibble: 7 × 2\n#&gt;       x     y\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1     2\n#&gt; 2     2     4\n#&gt; 3     3     6\n#&gt; 4     5    10\n#&gt; 5     7    14\n#&gt; 6    11    22\n#&gt; # ℹ 1 more row",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Logical vectors</span>"
    ]
  },
  {
    "objectID": "logicals.html#comparisons",
    "href": "logicals.html#comparisons",
    "title": "12  Logical vectors",
    "section": "\n12.2 Comparisons",
    "text": "12.2 Comparisons\nA very common way to create a logical vector is via a numeric comparison with &lt;, &lt;=, &gt;, &gt;=, !=, and ==. So far, we’ve mostly created logical variables transiently within filter() — they are computed, used, and then thrown away. For example, the following filter finds all daytime departures that arrive roughly on time:\n创建逻辑向量的一个非常常见的方法是通过数值比较，使用 &lt;, &lt;=, &gt;, &gt;=, !=, 和 ==。到目前为止，我们主要是在 filter() 中临时创建逻辑变量——它们被计算、使用，然后被丢弃。例如，下面的筛选器可以找出所有在白天出发且大致准时到达的航班：\n\nflights |&gt; \n  filter(dep_time &gt; 600 & dep_time &lt; 2000 & abs(arr_delay) &lt; 20)\n#&gt; # A tibble: 172,286 × 19\n#&gt;    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n#&gt; 1  2013     1     1      601            600         1      844            850\n#&gt; 2  2013     1     1      602            610        -8      812            820\n#&gt; 3  2013     1     1      602            605        -3      821            805\n#&gt; 4  2013     1     1      606            610        -4      858            910\n#&gt; 5  2013     1     1      606            610        -4      837            845\n#&gt; 6  2013     1     1      607            607         0      858            915\n#&gt; # ℹ 172,280 more rows\n#&gt; # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, …\n\nIt’s useful to know that this is a shortcut and you can explicitly create the underlying logical variables with mutate():\n了解这是一种快捷方式会很有用，你也可以使用 mutate() 显式地创建底层的逻辑变量：\n\nflights |&gt; \n  mutate(\n    daytime = dep_time &gt; 600 & dep_time &lt; 2000,\n    approx_ontime = abs(arr_delay) &lt; 20,\n    .keep = \"used\"\n  )\n#&gt; # A tibble: 336,776 × 4\n#&gt;   dep_time arr_delay daytime approx_ontime\n#&gt;      &lt;int&gt;     &lt;dbl&gt; &lt;lgl&gt;   &lt;lgl&gt;        \n#&gt; 1      517        11 FALSE   TRUE         \n#&gt; 2      533        20 FALSE   FALSE        \n#&gt; 3      542        33 FALSE   FALSE        \n#&gt; 4      544       -18 FALSE   TRUE         \n#&gt; 5      554       -25 FALSE   FALSE        \n#&gt; 6      554        12 FALSE   TRUE         \n#&gt; # ℹ 336,770 more rows\n\nThis is particularly useful for more complicated logic because naming the intermediate steps makes it easier to both read your code and check that each step has been computed correctly.\n这对于更复杂的逻辑尤其有用，因为为中间步骤命名可以使你的代码更易于阅读，也更容易检查每一步是否计算正确。\nAll up, the initial filter is equivalent to:\n总而言之，最初的筛选器等同于：\n\nflights |&gt; \n  mutate(\n    daytime = dep_time &gt; 600 & dep_time &lt; 2000,\n    approx_ontime = abs(arr_delay) &lt; 20,\n  ) |&gt; \n  filter(daytime & approx_ontime)\n\n\n12.2.1 Floating point comparison\nBeware of using == with numbers. For example, it looks like this vector contains the numbers 1 and 2:\n注意，当处理数字时要小心使用 ==。例如，下面这个向量看起来包含了数字 1 和 2：\n\nx &lt;- c(1 / 49 * 49, sqrt(2) ^ 2)\nx\n#&gt; [1] 1 2\n\nBut if you test them for equality, you get FALSE:\n但如果你测试它们是否相等，你会得到 FALSE：\n\nx == c(1, 2)\n#&gt; [1] FALSE FALSE\n\nWhat’s going on? Computers store numbers with a fixed number of decimal places so there’s no way to exactly represent 1/49 or sqrt(2) and subsequent computations will be very slightly off. We can see the exact values by calling print() with the digits1 argument:\n这是怎么回事？计算机以固定的小数位数存储数字，因此无法精确表示 1/49 或 sqrt(2)，后续计算会有些微偏差。我们可以通过调用 print() 并使用 digits1 参数来查看确切的值：\n\nprint(x, digits = 16)\n#&gt; [1] 0.9999999999999999 2.0000000000000004\n\nYou can see why R defaults to rounding these numbers; they really are very close to what you expect.\n你可以看到为什么 R 默认会四舍五入这些数字；它们确实非常接近你的预期。\nNow that you’ve seen why == is failing, what can you do about it? One option is to use dplyr::near() which ignores small differences:\n既然你已经看到了 == 失败的原因，那你该怎么办呢？一个选项是使用 dplyr::near()，它会忽略微小的差异：\n\nnear(x, c(1, 2))\n#&gt; [1] TRUE TRUE\n\n\n12.2.2 Missing values\nMissing values represent the unknown so they are “contagious”: almost any operation involving an unknown value will also be unknown:\n缺失值代表未知，所以它们是“会传染的”：几乎任何涉及未知值的操作结果也将是未知的：\n\nNA &gt; 5\n#&gt; [1] NA\n10 == NA\n#&gt; [1] NA\n\nThe most confusing result is this one:\n最令人困惑的结果是这个：\n\nNA == NA\n#&gt; [1] NA\n\nIt’s easiest to understand why this is true if we artificially supply a little more context:\n如果我们人为地提供一些上下文，就最容易理解为什么这是真的：\n\n# We don't know how old Mary is\nage_mary &lt;- NA\n\n# We don't know how old John is\nage_john &lt;- NA\n\n# Are Mary and John the same age?\nage_mary == age_john\n#&gt; [1] NA\n# We don't know!\n\nSo if you want to find all flights where dep_time is missing, the following code doesn’t work because dep_time == NA will yield NA for every single row, and filter() automatically drops missing values:\n所以，如果你想查找所有 dep_time 缺失的航班，下面的代码是行不通的，因为 dep_time == NA 会对每一行都产生 NA，而 filter() 会自动丢弃缺失值：\n\nflights |&gt; \n  filter(dep_time == NA)\n#&gt; # A tibble: 0 × 19\n#&gt; # ℹ 19 variables: year &lt;int&gt;, month &lt;int&gt;, day &lt;int&gt;, dep_time &lt;int&gt;,\n#&gt; #   sched_dep_time &lt;int&gt;, dep_delay &lt;dbl&gt;, arr_time &lt;int&gt;, …\n\nInstead we’ll need a new tool: is.na().\n因此，我们需要一个新工具：is.na()。\n\n12.2.3 is.na()\n\nis.na(x) works with any type of vector and returns TRUE for missing values and FALSE for everything else:is.na(x) 适用于任何类型的向量，对于缺失值返回 TRUE，对于其他所有值返回 FALSE：\n\nis.na(c(TRUE, NA, FALSE))\n#&gt; [1] FALSE  TRUE FALSE\nis.na(c(1, NA, 3))\n#&gt; [1] FALSE  TRUE FALSE\nis.na(c(\"a\", NA, \"b\"))\n#&gt; [1] FALSE  TRUE FALSE\n\nWe can use is.na() to find all the rows with a missing dep_time:\n我们可以使用 is.na() 来查找所有 dep_time 缺失的行：\n\nflights |&gt; \n  filter(is.na(dep_time))\n#&gt; # A tibble: 8,255 × 19\n#&gt;    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n#&gt; 1  2013     1     1       NA           1630        NA       NA           1815\n#&gt; 2  2013     1     1       NA           1935        NA       NA           2240\n#&gt; 3  2013     1     1       NA           1500        NA       NA           1825\n#&gt; 4  2013     1     1       NA            600        NA       NA            901\n#&gt; 5  2013     1     2       NA           1540        NA       NA           1747\n#&gt; 6  2013     1     2       NA           1620        NA       NA           1746\n#&gt; # ℹ 8,249 more rows\n#&gt; # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, …\n\nis.na() can also be useful in arrange(). arrange() usually puts all the missing values at the end but you can override this default by first sorting by is.na():is.na() 在 arrange() 中也很有用。arrange() 通常会将所有缺失值放在末尾，但你可以通过先按 is.na() 排序来覆盖这个默认行为：\n\nflights |&gt; \n  filter(month == 1, day == 1) |&gt; \n  arrange(dep_time)\n#&gt; # A tibble: 842 × 19\n#&gt;    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n#&gt; 1  2013     1     1      517            515         2      830            819\n#&gt; 2  2013     1     1      533            529         4      850            830\n#&gt; 3  2013     1     1      542            540         2      923            850\n#&gt; 4  2013     1     1      544            545        -1     1004           1022\n#&gt; 5  2013     1     1      554            600        -6      812            837\n#&gt; 6  2013     1     1      554            558        -4      740            728\n#&gt; # ℹ 836 more rows\n#&gt; # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, …\n\nflights |&gt; \n  filter(month == 1, day == 1) |&gt; \n  arrange(desc(is.na(dep_time)), dep_time)\n#&gt; # A tibble: 842 × 19\n#&gt;    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n#&gt; 1  2013     1     1       NA           1630        NA       NA           1815\n#&gt; 2  2013     1     1       NA           1935        NA       NA           2240\n#&gt; 3  2013     1     1       NA           1500        NA       NA           1825\n#&gt; 4  2013     1     1       NA            600        NA       NA            901\n#&gt; 5  2013     1     1      517            515         2      830            819\n#&gt; 6  2013     1     1      533            529         4      850            830\n#&gt; # ℹ 836 more rows\n#&gt; # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, …\n\nWe’ll come back to cover missing values in more depth in Chapter 18.\n我们将在 Chapter 18 中更深入地讨论缺失值。\n\n12.2.4 Exercises\n\nHow does dplyr::near() work? Type near to see the source code. Is sqrt(2)^2 near 2?\nUse mutate(), is.na(), and count() together to describe how the missing values in dep_time, sched_dep_time and dep_delay are connected.",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Logical vectors</span>"
    ]
  },
  {
    "objectID": "logicals.html#boolean-algebra",
    "href": "logicals.html#boolean-algebra",
    "title": "12  Logical vectors",
    "section": "\n12.3 Boolean algebra",
    "text": "12.3 Boolean algebra\nOnce you have multiple logical vectors, you can combine them together using Boolean algebra. In R, & is “and”, | is “or”, ! is “not”, and xor() is exclusive or2. For example, df |&gt; filter(!is.na(x)) finds all rows where x is not missing and df |&gt; filter(x &lt; -10 | x &gt; 0) finds all rows where x is smaller than -10 or bigger than 0. Figure 12.1 shows the complete set of Boolean operations and how they work.\n一旦你有了多个逻辑向量，就可以使用布尔代数将它们组合起来。在 R 中，& 是“与”，| 是“或”，! 是“非”，而 xor() 是异或（exclusive or）1。例如，df |&gt; filter(!is.na(x)) 会找到所有 x 不缺失的行，而 df |&gt; filter(x &lt; -10 | x &gt; 0) 会找到所有 x 小于 -10 或大于 0 的行。Figure 12.1 展示了完整的布尔运算集及其工作原理。\n\n\n\n\n\n\n\nFigure 12.1: The complete set of Boolean operations. x is the left-hand circle, y is the right-hand circle, and the shaded regions show which parts each operator selects.\n\n\n\n\nAs well as & and |, R also has && and ||. Don’t use them in dplyr functions! These are called short-circuiting operators and only ever return a single TRUE or FALSE. They’re important for programming, not data science.\n除了 & 和 |，R 还有 && 和 ||。不要在 dplyr 函数中使用它们！这些被称为短路运算符，它们只返回单个的 TRUE 或 FALSE。它们对于编程很重要，而不是数据科学。\n\n12.3.1 Missing values\nThe rules for missing values in Boolean algebra are a little tricky to explain because they seem inconsistent at first glance:\n布尔代数中关于缺失值的规则有点难以解释，因为它们乍一看似乎不一致：\n\ndf &lt;- tibble(x = c(TRUE, FALSE, NA))\n\ndf |&gt; \n  mutate(\n    and = x & NA,\n    or = x | NA\n  )\n#&gt; # A tibble: 3 × 3\n#&gt;   x     and   or   \n#&gt;   &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt;\n#&gt; 1 TRUE  NA    TRUE \n#&gt; 2 FALSE FALSE NA   \n#&gt; 3 NA    NA    NA\n\nTo understand what’s going on, think about NA | TRUE (NA or TRUE). A missing value in a logical vector means that the value could either be TRUE or FALSE. TRUE | TRUE and FALSE | TRUE are both TRUE because at least one of them is TRUE. NA | TRUE must also be TRUE because NA can either be TRUE or FALSE. However, NA | FALSE is NA because we don’t know if NA is TRUE or FALSE. Similar reasoning applies for & considering that both conditions must be fulfilled. Therefore NA & TRUE is NA because NA can either be TRUE or FALSE and NA & FALSE is FALSE because at least one of the conditions is FALSE.\n要理解发生了什么，可以思考一下 NA | TRUE（NA 或 TRUE）。逻辑向量中的缺失值意味着该值可能是 TRUE 或 FALSE。TRUE | TRUE 和 FALSE | TRUE 都是 TRUE，因为其中至少有一个是 TRUE。NA | TRUE 也必须是 TRUE，因为 NA 可能是 TRUE 或 FALSE。然而，NA | FALSE 的结果是 NA，因为我们不知道 NA 是 TRUE 还是 FALSE。类似的推理也适用于 &，考虑到两个条件都必须满足。因此，NA & TRUE 的结果是 NA，因为 NA 可能是 TRUE 或 FALSE；而 NA & FALSE 的结果是 FALSE，因为至少有一个条件是 FALSE。\n\n12.3.2 Order of operations\nNote that the order of operations doesn’t work like English. Take the following code that finds all flights that departed in November or December:\n注意，运算顺序不像英语那样。看下面这段查找所有在 11 月或 12 月起飞的航班的代码：\n\nflights |&gt; \n  filter(month == 11 | month == 12)\n\nYou might be tempted to write it like you’d say in English: “Find all flights that departed in November or December.”:\n你可能会想当然地像用英语说的那样写：“查找所有在 11 月或 12 月起飞的航班。”：\n\nflights |&gt; \n  filter(month == 11 | 12)\n#&gt; # A tibble: 336,776 × 19\n#&gt;    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n#&gt; 1  2013     1     1      517            515         2      830            819\n#&gt; 2  2013     1     1      533            529         4      850            830\n#&gt; 3  2013     1     1      542            540         2      923            850\n#&gt; 4  2013     1     1      544            545        -1     1004           1022\n#&gt; 5  2013     1     1      554            600        -6      812            837\n#&gt; 6  2013     1     1      554            558        -4      740            728\n#&gt; # ℹ 336,770 more rows\n#&gt; # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, …\n\nThis code doesn’t error but it also doesn’t seem to have worked. What’s going on? Here, R first evaluates month == 11 creating a logical vector, which we call nov. It computes nov | 12. When you use a number with a logical operator it converts everything apart from 0 to TRUE, so this is equivalent to nov | TRUE which will always be TRUE, so every row will be selected:\n这段代码没有报错，但似乎也没有起作用。这是怎么回事？在这里，R 首先评估 month == 11，创建了一个我们称之为 nov 的逻辑向量。然后它计算 nov | 12。当你将数字与逻辑运算符一起使用时，除了 0 之外的所有数字都会被转换为 TRUE，所以这等价于 nov | TRUE，结果将永远是 TRUE，因此所有行都会被选中：\n\nflights |&gt; \n  mutate(\n    nov = month == 11,\n    final = nov | 12,\n    .keep = \"used\"\n  )\n#&gt; # A tibble: 336,776 × 3\n#&gt;   month nov   final\n#&gt;   &lt;int&gt; &lt;lgl&gt; &lt;lgl&gt;\n#&gt; 1     1 FALSE TRUE \n#&gt; 2     1 FALSE TRUE \n#&gt; 3     1 FALSE TRUE \n#&gt; 4     1 FALSE TRUE \n#&gt; 5     1 FALSE TRUE \n#&gt; 6     1 FALSE TRUE \n#&gt; # ℹ 336,770 more rows\n\n\n12.3.3 %in%\n\nAn easy way to avoid the problem of getting your ==s and |s in the right order is to use %in%. x %in% y returns a logical vector the same length as x that is TRUE whenever a value in x is anywhere in y .\n一个避免 == 和 | 排序问题的简单方法是使用 %in%。x %in% y 会返回一个与 x 长度相同的逻辑向量，当 x 中的值存在于 y 中任何位置时，该向量对应元素为 TRUE。\n\n1:12 %in% c(1, 5, 11)\n#&gt;  [1]  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\nletters[1:10] %in% c(\"a\", \"e\", \"i\", \"o\", \"u\")\n#&gt;  [1]  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE\n\nSo to find all flights in November and December we could write:\n所以要查找所有在十一月和十二月的航班，我们可以这样写：\n\nflights |&gt; \n  filter(month %in% c(11, 12))\n\nNote that %in% obeys different rules for NA to ==, as NA %in% NA is TRUE.\n注意，对于 NA，%in% 遵循与 == 不同的规则，因为 NA %in% NA 的结果是 TRUE。\n\nc(1, 2, NA) == NA\n#&gt; [1] NA NA NA\nc(1, 2, NA) %in% NA\n#&gt; [1] FALSE FALSE  TRUE\n\nThis can make for a useful shortcut:\n这可以成为一个有用的快捷方式：\n\nflights |&gt; \n  filter(dep_time %in% c(NA, 0800))\n#&gt; # A tibble: 8,803 × 19\n#&gt;    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n#&gt; 1  2013     1     1      800            800         0     1022           1014\n#&gt; 2  2013     1     1      800            810       -10      949            955\n#&gt; 3  2013     1     1       NA           1630        NA       NA           1815\n#&gt; 4  2013     1     1       NA           1935        NA       NA           2240\n#&gt; 5  2013     1     1       NA           1500        NA       NA           1825\n#&gt; 6  2013     1     1       NA            600        NA       NA            901\n#&gt; # ℹ 8,797 more rows\n#&gt; # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, …\n\n\n12.3.4 Exercises\n\nFind all flights where arr_delay is missing but dep_delay is not. Find all flights where neither arr_time nor sched_arr_time are missing, but arr_delay is.\nHow many flights have a missing dep_time? What other variables are missing in these rows? What might these rows represent?\nAssuming that a missing dep_time implies that a flight is cancelled, look at the number of cancelled flights per day. Is there a pattern? Is there a connection between the proportion of cancelled flights and the average delay of non-cancelled flights?",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Logical vectors</span>"
    ]
  },
  {
    "objectID": "logicals.html#sec-logical-summaries",
    "href": "logicals.html#sec-logical-summaries",
    "title": "12  Logical vectors",
    "section": "\n12.4 Summaries",
    "text": "12.4 Summaries\nThe following sections describe some useful techniques for summarizing logical vectors. As well as functions that only work specifically with logical vectors, you can also use functions that work with numeric vectors.\n以下各节介绍了一些汇总逻辑向量的有用技巧。除了专门处理逻辑向量的函数外，你还可以使用处理数值向量的函数。\n\n12.4.1 Logical summaries\nThere are two main logical summaries: any() and all(). any(x) is the equivalent of |; it’ll return TRUE if there are any TRUE’s in x. all(x) is equivalent of &; it’ll return TRUE only if all values of x are TRUE’s. Like most summary functions, you can make the missing values go away with na.rm = TRUE.\n有两个主要的逻辑汇总函数：any() 和 all()。any(x) 相当于 |；如果 x 中有任何一个 TRUE，它就会返回 TRUE。all(x) 相当于 &；只有当 x 的所有值都为 TRUE 时，它才会返回 TRUE。和大多数汇总函数一样，你可以通过 na.rm = TRUE 来移除缺失值。\nFor example, we could use all() and any() to find out if every flight was delayed on departure by at most an hour or if any flights were delayed on arrival by five hours or more. And using group_by() allows us to do that by day:\n例如，我们可以使用 all() 和 any() 来查明是否每架航班的起飞延误都不超过一小时，或者是否有任何航班的到达延误达到五小时或更长。并且使用 group_by() 允许我们按天来做这个分析：\n\nflights |&gt; \n  group_by(year, month, day) |&gt; \n  summarize(\n    all_delayed = all(dep_delay &lt;= 60, na.rm = TRUE),\n    any_long_delay = any(arr_delay &gt;= 300, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n#&gt; # A tibble: 365 × 5\n#&gt;    year month   day all_delayed any_long_delay\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;lgl&gt;       &lt;lgl&gt;         \n#&gt; 1  2013     1     1 FALSE       TRUE          \n#&gt; 2  2013     1     2 FALSE       TRUE          \n#&gt; 3  2013     1     3 FALSE       FALSE         \n#&gt; 4  2013     1     4 FALSE       FALSE         \n#&gt; 5  2013     1     5 FALSE       TRUE          \n#&gt; 6  2013     1     6 FALSE       FALSE         \n#&gt; # ℹ 359 more rows\n\nIn most cases, however, any() and all() are a little too crude, and it would be nice to be able to get a little more detail about how many values are TRUE or FALSE. That leads us to the numeric summaries.\n然而，在大多数情况下，any() 和 all() 有点过于粗略，如果能更详细地了解有多少值是 TRUE 或 FALSE 会更好。这就引出了数值汇总。\n\n12.4.2 Numeric summaries of logical vectors\nWhen you use a logical vector in a numeric context, TRUE becomes 1 and FALSE becomes 0. This makes sum() and mean() very useful with logical vectors because sum(x) gives the number of TRUEs and mean(x) gives the proportion of TRUEs (because mean() is just sum() divided by length()).\n当你在数值上下文中使用逻辑向量时，TRUE 会变成 1，FALSE 会变成 0。这使得 sum() 和 mean() 在处理逻辑向量时非常有用，因为 sum(x) 给出了 TRUE 的数量，而 mean(x) 给出了 TRUE 的比例（因为 mean() 就是 sum() 除以 length()）。\nThat, for example, allows us to see the proportion of flights that were delayed on departure by at most an hour and the number of flights that were delayed on arrival by five hours or more:\n例如，这让我们可以查看起飞延误最多一小时的航班比例，以及到达延误五小时或以上的航班数量：\n\nflights |&gt; \n  group_by(year, month, day) |&gt; \n  summarize(\n    proportion_delayed = mean(dep_delay &lt;= 60, na.rm = TRUE),\n    count_long_delay = sum(arr_delay &gt;= 300, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n#&gt; # A tibble: 365 × 5\n#&gt;    year month   day proportion_delayed count_long_delay\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;              &lt;dbl&gt;            &lt;int&gt;\n#&gt; 1  2013     1     1              0.939                3\n#&gt; 2  2013     1     2              0.914                3\n#&gt; 3  2013     1     3              0.941                0\n#&gt; 4  2013     1     4              0.953                0\n#&gt; 5  2013     1     5              0.964                1\n#&gt; 6  2013     1     6              0.959                0\n#&gt; # ℹ 359 more rows\n\n\n12.4.3 Logical subsetting\nThere’s one final use for logical vectors in summaries: you can use a logical vector to filter a single variable to a subset of interest. This makes use of the base [ (pronounced subset) operator, which you’ll learn more about in Section 27.2.\n在汇总中，逻辑向量还有一个最终用途：你可以使用逻辑向量将单个变量筛选到感兴趣的子集。这利用了 R base 的 [（发音为 subset）运算符，你将在 Section 27.2 中学到更多相关内容。\nImagine we wanted to look at the average delay just for flights that were actually delayed. One way to do so would be to first filter the flights and then calculate the average delay:\n假设我们只想看实际延误航班的平均延误时间。一种方法是先筛选出这些航班，然后计算平均延误时间：\n\nflights |&gt; \n  filter(arr_delay &gt; 0) |&gt; \n  group_by(year, month, day) |&gt; \n  summarize(\n    behind = mean(arr_delay),\n    n = n(),\n    .groups = \"drop\"\n  )\n#&gt; # A tibble: 365 × 5\n#&gt;    year month   day behind     n\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;int&gt;\n#&gt; 1  2013     1     1   32.5   461\n#&gt; 2  2013     1     2   32.0   535\n#&gt; 3  2013     1     3   27.7   460\n#&gt; 4  2013     1     4   28.3   297\n#&gt; 5  2013     1     5   22.6   238\n#&gt; 6  2013     1     6   24.4   381\n#&gt; # ℹ 359 more rows\n\nThis works, but what if we wanted to also compute the average delay for flights that arrived early? We’d need to perform a separate filter step, and then figure out how to combine the two data frames together3. Instead you could use [ to perform an inline filtering: arr_delay[arr_delay &gt; 0] will yield only the positive arrival delays.\n这样做是可行的，但如果我们还想计算提早到达航班的平均延误时间呢？我们就需要执行一个单独的筛选步骤，然后想办法将两个数据框合并在一起3。相反，你可以使用 [ 来执行内联筛选：arr_delay[arr_delay &gt; 0] 将只产生正的到达延误时间。\nThis leads to:\n这会得到：\n\nflights |&gt; \n  group_by(year, month, day) |&gt; \n  summarize(\n    behind = mean(arr_delay[arr_delay &gt; 0], na.rm = TRUE),\n    ahead = mean(arr_delay[arr_delay &lt; 0], na.rm = TRUE),\n    n = n(),\n    .groups = \"drop\"\n  )\n#&gt; # A tibble: 365 × 6\n#&gt;    year month   day behind ahead     n\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n#&gt; 1  2013     1     1   32.5 -12.5   842\n#&gt; 2  2013     1     2   32.0 -14.3   943\n#&gt; 3  2013     1     3   27.7 -18.2   914\n#&gt; 4  2013     1     4   28.3 -17.0   915\n#&gt; 5  2013     1     5   22.6 -14.0   720\n#&gt; 6  2013     1     6   24.4 -13.6   832\n#&gt; # ℹ 359 more rows\n\nAlso note the difference in the group size: in the first chunk n() gives the number of delayed flights per day; in the second, n() gives the total number of flights.\n同时注意组大小的差异：在第一个代码块中，n() 给出的是每天延误的航班数量；在第二个代码块中，n() 给出的是总航班数量。\n\n12.4.4 Exercises\n\nWhat will sum(is.na(x)) tell you? How about mean(is.na(x))?\nWhat does prod() return when applied to a logical vector? What logical summary function is it equivalent to? What does min() return when applied to a logical vector? What logical summary function is it equivalent to? Read the documentation and perform a few experiments.",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Logical vectors</span>"
    ]
  },
  {
    "objectID": "logicals.html#conditional-transformations",
    "href": "logicals.html#conditional-transformations",
    "title": "12  Logical vectors",
    "section": "\n12.5 Conditional transformations",
    "text": "12.5 Conditional transformations\nOne of the most powerful features of logical vectors are their use for conditional transformations, i.e. doing one thing for condition x, and something different for condition y. There are two important tools for this: if_else() and case_when().\n逻辑向量最强大的特性之一是它们在条件转换中的应用，即针对条件 x 做一件事，针对条件 y 做另一件事。有两个重要的工具可以实现这一点：if_else() 和 case_when()。\n\n12.5.1 if_else()\n\nIf you want to use one value when a condition is TRUE and another value when it’s FALSE, you can use dplyr::if_else()4. You’ll always use the first three argument of if_else(). The first argument, condition, is a logical vector, the second, true, gives the output when the condition is true, and the third, false, gives the output if the condition is false.\n如果你想在条件为 TRUE 时使用一个值，而在条件为 FALSE 时使用另一个值，你可以使用 dplyr::if_else()4。你总是会使用 if_else() 的前三个参数。第一个参数 condition 是一个逻辑向量，第二个参数 true 给出条件为真时的输出，第三个参数 false 给出条件为假时的输出。\nLet’s begin with a simple example of labeling a numeric vector as either “+ve” (positive) or “-ve” (negative):\n让我们从一个简单的例子开始，将一个数值向量标记为“+ve”（正数）或“-ve”（负数）：\n\nx &lt;- c(-3:3, NA)\nif_else(x &gt; 0, \"+ve\", \"-ve\")\n#&gt; [1] \"-ve\" \"-ve\" \"-ve\" \"-ve\" \"+ve\" \"+ve\" \"+ve\" NA\n\nThere’s an optional fourth argument, missing which will be used if the input is NA:\n还有一个可选的第四个参数 missing，当输入为 NA 时会使用这个参数：\n\nif_else(x &gt; 0, \"+ve\", \"-ve\", \"???\")\n#&gt; [1] \"-ve\" \"-ve\" \"-ve\" \"-ve\" \"+ve\" \"+ve\" \"+ve\" \"???\"\n\nYou can also use vectors for the true and false arguments. For example, this allows us to create a minimal implementation of abs():\n你也可以为 true 和 false 参数使用向量。例如，这允许我们创建一个 abs() 的最小化实现：\n\nif_else(x &lt; 0, -x, x)\n#&gt; [1]  3  2  1  0  1  2  3 NA\n\nSo far all the arguments have used the same vectors, but you can of course mix and match. For example, you could implement a simple version of coalesce() like this:\n到目前为止，所有的参数都使用了相同的向量，但你当然可以混合搭配。例如，你可以像这样实现一个 coalesce() 的简单版本：\n\nx1 &lt;- c(NA, 1, 2, NA)\ny1 &lt;- c(3, NA, 4, 6)\nif_else(is.na(x1), y1, x1)\n#&gt; [1] 3 1 2 6\n\nYou might have noticed a small infelicity in our labeling example above: zero is neither positive nor negative. We could resolve this by adding an additional if_else():\n你可能已经注意到我们上面标签示例中的一个小瑕疵：零既不是正数也不是负数。我们可以通过添加一个额外的 if_else() 来解决这个问题：\n\nif_else(x == 0, \"0\", if_else(x &lt; 0, \"-ve\", \"+ve\"), \"???\")\n#&gt; [1] \"-ve\" \"-ve\" \"-ve\" \"0\"   \"+ve\" \"+ve\" \"+ve\" \"???\"\n\nThis is already a little hard to read, and you can imagine it would only get harder if you have more conditions. Instead, you can switch to dplyr::case_when().\n这已经有点难读了，你可以想象，如果你有更多的条件，情况只会变得更糟。因此，你可以转而使用 dplyr::case_when()。\n\n12.5.2 case_when()\n\ndplyr’s case_when() is inspired by SQL’s CASE statement and provides a flexible way of performing different computations for different conditions. It has a special syntax that unfortunately looks like nothing else you’ll use in the tidyverse. It takes pairs that look like condition ~ output. condition must be a logical vector; when it’s TRUE, output will be used.\ndplyr 的 case_when() 受到 SQL 的 CASE 语句的启发，提供了一种为不同条件执行不同计算的灵活方式。它有一种特殊的语法，不幸的是，这与你在 tidyverse 中使用的其他任何东西都不一样。它接受形如 condition ~ output 的配对。condition 必须是一个逻辑向量；当它为 TRUE 时，将使用 output。\nThis means we could recreate our previous nested if_else() as follows:\n这意味着我们可以像下面这样重新创建我们之前的嵌套 if_else()：\n\nx &lt;- c(-3:3, NA)\ncase_when(\n  x == 0   ~ \"0\",\n  x &lt; 0    ~ \"-ve\", \n  x &gt; 0    ~ \"+ve\",\n  is.na(x) ~ \"???\"\n)\n#&gt; [1] \"-ve\" \"-ve\" \"-ve\" \"0\"   \"+ve\" \"+ve\" \"+ve\" \"???\"\n\nThis is more code, but it’s also more explicit.\n这需要更多的代码，但它也更明确。\nTo explain how case_when() works, let’s explore some simpler cases. If none of the cases match, the output gets an NA:\n为了解释 case_when() 的工作原理，让我们探讨一些更简单的情况。如果没有一个条件匹配，输出将得到一个 NA：\n\ncase_when(\n  x &lt; 0 ~ \"-ve\",\n  x &gt; 0 ~ \"+ve\"\n)\n#&gt; [1] \"-ve\" \"-ve\" \"-ve\" NA    \"+ve\" \"+ve\" \"+ve\" NA\n\nUse .default if you want to create a “default”/catch all value:\n如果你想创建一个“默认”或“包罗万象”的值，请使用 .default：\n\ncase_when(\n  x &lt; 0 ~ \"-ve\",\n  x &gt; 0 ~ \"+ve\",\n  .default = \"???\"\n)\n#&gt; [1] \"-ve\" \"-ve\" \"-ve\" \"???\" \"+ve\" \"+ve\" \"+ve\" \"???\"\n\nAnd note that if multiple conditions match, only the first will be used:\n并且请注意，如果多个条件匹配，只有第一个会被使用：\n\ncase_when(\n  x &gt; 0 ~ \"+ve\",\n  x &gt; 2 ~ \"big\"\n)\n#&gt; [1] NA    NA    NA    NA    \"+ve\" \"+ve\" \"+ve\" NA\n\nJust like with if_else() you can use variables on both sides of the ~ and you can mix and match variables as needed for your problem. For example, we could use case_when() to provide some human readable labels for the arrival delay:\n就像 if_else() 一样，你可以在 ~ 的两边使用变量，并且可以根据你的问题需要混合和匹配变量。例如，我们可以使用 case_when() 为到达延迟提供一些人类可读的标签：\n\nflights |&gt; \n  mutate(\n    status = case_when(\n      is.na(arr_delay)     ~ \"cancelled\",\n      arr_delay &lt; -30      ~ \"very early\",\n      arr_delay &lt; -15      ~ \"early\",\n      abs(arr_delay) &lt;= 15 ~ \"on time\",\n      arr_delay &lt; 60       ~ \"late\",\n      arr_delay &lt; Inf      ~ \"very late\",\n    ),\n    .keep = \"used\"\n  )\n#&gt; # A tibble: 336,776 × 2\n#&gt;   arr_delay status \n#&gt;       &lt;dbl&gt; &lt;chr&gt;  \n#&gt; 1        11 on time\n#&gt; 2        20 late   \n#&gt; 3        33 late   \n#&gt; 4       -18 early  \n#&gt; 5       -25 early  \n#&gt; 6        12 on time\n#&gt; # ℹ 336,770 more rows\n\nBe wary when writing this sort of complex case_when() statement; my first two attempts used a mix of &lt; and &gt; and I kept accidentally creating overlapping conditions.\n在编写这类复杂的 case_when() 语句时要小心；我最初的两次尝试混合使用了 &lt; 和 &gt;，结果不小心创建了重叠的条件。\n\n12.5.3 Compatible types\nNote that both if_else() and case_when() require compatible types in the output. If they’re not compatible, you’ll see errors like this:\n请注意，if_else() 和 case_when() 都要求输出中的类型是兼容的。如果它们不兼容，你会看到类似这样的错误：\n\nif_else(TRUE, \"a\", 1)\n#&gt; Error in `if_else()`:\n#&gt; ! Can't combine `true` &lt;character&gt; and `false` &lt;double&gt;.\n\ncase_when(\n  x &lt; -1 ~ TRUE,  \n  x &gt; 0  ~ now()\n)\n#&gt; Error in `case_when()`:\n#&gt; ! Can't combine `..1 (right)` &lt;logical&gt; and `..2 (right)` &lt;datetime&lt;local&gt;&gt;.\n\nOverall, relatively few types are compatible, because automatically converting one type of vector to another is a common source of errors. Here are the most important cases that are compatible:\n总的来说，相对较少的类型是兼容的，因为自动将一种类型的向量转换为另一种类型是错误的常见来源。以下是兼容的最重要情况：\n\nNumeric and logical vectors are compatible, as we discussed in Section 12.4.2.\n数值向量和逻辑向量是兼容的，正如我们在 Section 12.4.2 中讨论的那样。\nStrings and factors (Chapter 16) are compatible, because you can think of a factor as a string with a restricted set of values.\n字符串和因子（Chapter 16）是兼容的，因为你可以把因子看作是一组受限制的字符串。\nDates and date-times, which we’ll discuss in Chapter 17, are compatible because you can think of a date as a special case of date-time.\n日期和日期时间（我们将在 Chapter 17 中讨论）是兼容的，因为你可以把日期看作是日期时间的一种特殊情况。\nNA, which is technically a logical vector, is compatible with everything because every vector has some way of representing a missing value.NA，严格来说是一个逻辑向量，它与所有类型都兼容，因为每个向量都有表示缺失值的方式。\n\nWe don’t expect you to memorize these rules, but they should become second nature over time because they are applied consistently throughout the tidyverse.\n我们不期望你记住这些规则，但随着时间的推移，它们应该会成为你的第二天性，因为它们在整个 tidyverse 中都是一致应用的。\n\n12.5.4 Exercises\n\nA number is even if it’s divisible by two, which in R you can find out with x %% 2 == 0. Use this fact and if_else() to determine whether each number between 0 and 20 is even or odd.\nGiven a vector of days like x &lt;- c(\"Monday\", \"Saturday\", \"Wednesday\"), use an if_else() statement to label them as weekends or weekdays.\nUse if_else() to compute the absolute value of a numeric vector called x.\nWrite a case_when() statement that uses the month and day columns from flights to label a selection of important US holidays (e.g., New Years Day, 4th of July, Thanksgiving, and Christmas). First create a logical column that is either TRUE or FALSE, and then create a character column that either gives the name of the holiday or is NA.",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Logical vectors</span>"
    ]
  },
  {
    "objectID": "logicals.html#summary",
    "href": "logicals.html#summary",
    "title": "12  Logical vectors",
    "section": "\n12.6 Summary",
    "text": "12.6 Summary\nThe definition of a logical vector is simple because each value must be either TRUE, FALSE, or NA. But logical vectors provide a huge amount of power. In this chapter, you learned how to create logical vectors with &gt;, &lt;, &lt;=, &gt;=, ==, !=, and is.na(), how to combine them with !, &, and |, and how to summarize them with any(), all(), sum(), and mean(). You also learned the powerful if_else() and case_when() functions that allow you to return values depending on the value of a logical vector.\n逻辑向量的定义很简单，因为每个值必须是 TRUE、FALSE 或 NA 之一。但逻辑向量提供了巨大的能力。在本章中，你学习了如何使用 &gt;、&lt;、&lt;=、&gt;=、==、!= 和 is.na() 创建逻辑向量，如何使用 !、& 和 | 组合它们，以及如何使用 any()、all()、sum() 和 mean() 对它们进行汇总。你还学习了强大的 if_else() 和 case_when() 函数，它们允许你根据逻辑向量的值返回不同的值。\nWe’ll see logical vectors again and again in the following chapters. For example in Chapter 14 you’ll learn about str_detect(x, pattern) which returns a logical vector that’s TRUE for the elements of x that match the pattern, and in Chapter 17 you’ll create logical vectors from the comparison of dates and times. But for now, we’re going to move onto the next most important type of vector: numeric vectors.\n在接下来的章节中，我们将反复看到逻辑向量。例如，在 Chapter 14 中，你将学习 str_detect(x, pattern)，它会返回一个逻辑向量，对于 x 中匹配 pattern 的元素，该向量为 TRUE；在 Chapter 17 中，你将通过比较日期和时间来创建逻辑向量。但现在，我们将转向下一种最重要的向量类型：数值向量。",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Logical vectors</span>"
    ]
  },
  {
    "objectID": "logicals.html#footnotes",
    "href": "logicals.html#footnotes",
    "title": "12  Logical vectors",
    "section": "",
    "text": "R normally calls print for you (i.e. x is a shortcut for print(x)), but calling it explicitly is useful if you want to provide other arguments.↩︎\nThat is, xor(x, y) is true if x is true, or y is true, but not both. This is how we usually use “or” In English. “Both” is not usually an acceptable answer to the question “would you like ice cream or cake?”.↩︎\nWe’ll cover this in Chapter 19.↩︎\ndplyr’s if_else() is very similar to base R’s ifelse(). There are two main advantages of if_else()over ifelse(): you can choose what should happen to missing values, and if_else() is much more likely to give you a meaningful error if your variables have incompatible types.↩︎",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Logical vectors</span>"
    ]
  },
  {
    "objectID": "numbers.html",
    "href": "numbers.html",
    "title": "13  Numbers",
    "section": "",
    "text": "13.1 Introduction\nNumeric vectors are the backbone of data science, and you’ve already used them a bunch of times earlier in the book.\n数值向量是数据科学的支柱，你已经在本书的前面部分多次使用过它们。\nNow it’s time to systematically survey what you can do with them in R, ensuring that you’re well situated to tackle any future problem involving numeric vectors.\n现在是时候系统地审视一下你可以用 R 对它们做什么，确保你能够很好地应对未来任何涉及数值向量的问题。\nWe’ll start by giving you a couple of tools to make numbers if you have strings, and then going into a little more detail of count().\n我们将从介绍几个在你拥有字符串时用来创建数字的工具开始，然后更详细地探讨 count() 函数。\nThen we’ll dive into various numeric transformations that pair well with mutate(), including more general transformations that can be applied to other types of vectors, but are often used with numeric vectors.\n接着，我们将深入探讨与 mutate() 搭配使用的各种数值转换，包括那些可以应用于其他类型向量，但常用于数值向量的更通用的转换。\nWe’ll finish off by covering the summary functions that pair well with summarize() and show you how they can also be used with mutate().\n最后，我们将介绍与 summarize() 搭配使用的摘要函数，并展示它们如何也能与 mutate() 一起使用。",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Numbers</span>"
    ]
  },
  {
    "objectID": "numbers.html#introduction",
    "href": "numbers.html#introduction",
    "title": "13  Numbers",
    "section": "",
    "text": "13.1.1 Prerequisites\nThis chapter mostly uses functions from base R, which are available without loading any packages.\n本章主要使用 base R 中的函数，这些函数无需加载任何包即可使用。\nBut we still need the tidyverse because we’ll use these base R functions inside of tidyverse functions like mutate() and filter().\n但我们仍然需要 tidyverse，因为我们将在 tidyverse 函数如 mutate() 和 filter() 中使用这些 base R 函数。\nLike in the last chapter, we’ll use real examples from nycflights13, as well as toy examples made with c() and tribble().\n与上一章一样，我们将使用来自 nycflights13 的真实示例，以及用 c() 和 tribble() 创建的玩具示例。\n\nlibrary(tidyverse)\nlibrary(nycflights13)",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Numbers</span>"
    ]
  },
  {
    "objectID": "numbers.html#making-numbers",
    "href": "numbers.html#making-numbers",
    "title": "13  Numbers",
    "section": "\n13.2 Making numbers",
    "text": "13.2 Making numbers\nIn most cases, you’ll get numbers already recorded in one of R’s numeric types: integer or double.\n在大多数情况下，你会得到已经以 R 的数值类型之一（整数或双精度浮点数）记录的数字。\nIn some cases, however, you’ll encounter them as strings, possibly because you’ve created them by pivoting from column headers or because something has gone wrong in your data import process.\n然而，在某些情况下，你会遇到以字符串形式出现的数字，这可能是因为你是通过将列标题转置而创建的它们，或者是因为你的数据导入过程中出现了问题。\nreadr provides two useful functions for parsing strings into numbers: parse_double() and parse_number().\nreadr 提供了两个有用的函数用于将字符串解析为数字：parse_double() 和 parse_number()。\nUse parse_double() when you have numbers that have been written as strings:\n当你拥有以字符串形式书写的数字时，请使用 parse_double()：\n\nx &lt;- c(\"1.2\", \"5.6\", \"1e3\")\nparse_double(x)\n#&gt; [1]    1.2    5.6 1000.0\n\nUse parse_number() when the string contains non-numeric text that you want to ignore.\n当字符串包含你想要忽略的非数字文本时，请使用 parse_number()。\nThis is particularly useful for currency data and percentages:\n这对于处理货币数据和百分比特别有用：\n\nx &lt;- c(\"$1,234\", \"USD 3,513\", \"59%\")\nparse_number(x)\n#&gt; [1] 1234 3513   59",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Numbers</span>"
    ]
  },
  {
    "objectID": "numbers.html#sec-counts",
    "href": "numbers.html#sec-counts",
    "title": "13  Numbers",
    "section": "\n13.3 Counts",
    "text": "13.3 Counts\nIt’s surprising how much data science you can do with just counts and a little basic arithmetic, so dplyr strives to make counting as easy as possible with count().\n令人惊讶的是，仅用计数和一点基本算术就能完成大量的数据科学工作，因此 dplyr 致力于通过 count() 函数使计数尽可能地简单。\nThis function is great for quick exploration and checks during analysis:\n这个函数非常适合在分析过程中进行快速探索和检查：\n\nflights |&gt; count(dest)\n#&gt; # A tibble: 105 × 2\n#&gt;   dest      n\n#&gt;   &lt;chr&gt; &lt;int&gt;\n#&gt; 1 ABQ     254\n#&gt; 2 ACK     265\n#&gt; 3 ALB     439\n#&gt; 4 ANC       8\n#&gt; 5 ATL   17215\n#&gt; 6 AUS    2439\n#&gt; # ℹ 99 more rows\n\n(Despite the advice in Chapter 4, we usually put count() on a single line because it’s usually used at the console for a quick check that a calculation is working as expected.)\n（尽管在 Chapter 4 中有相关建议，我们通常将 count() 写在单行上，因为它通常在控制台中用于快速检查计算是否按预期工作。）\nIf you want to see the most common values, add sort = TRUE:\n如果你想查看最常见的值，请添加 sort = TRUE：\n\nflights |&gt; count(dest, sort = TRUE)\n#&gt; # A tibble: 105 × 2\n#&gt;   dest      n\n#&gt;   &lt;chr&gt; &lt;int&gt;\n#&gt; 1 ORD   17283\n#&gt; 2 ATL   17215\n#&gt; 3 LAX   16174\n#&gt; 4 BOS   15508\n#&gt; 5 MCO   14082\n#&gt; 6 CLT   14064\n#&gt; # ℹ 99 more rows\n\nAnd remember that if you want to see all the values, you can use |&gt; View() or |&gt; print(n = Inf).\n并且记住，如果你想查看所有值，你可以使用 |&gt; View() 或 |&gt; print(n = Inf)。\nYou can perform the same computation “by hand” with group_by(), summarize() and n().\n你可以使用 group_by()、summarize() 和 n() “手动” 执行相同的计算。\nThis is useful because it allows you to compute other summaries at the same time:\n这很有用，因为它允许你同时计算其他摘要：\n\nflights |&gt; \n  group_by(dest) |&gt; \n  summarize(\n    n = n(),\n    delay = mean(arr_delay, na.rm = TRUE)\n  )\n#&gt; # A tibble: 105 × 3\n#&gt;   dest      n delay\n#&gt;   &lt;chr&gt; &lt;int&gt; &lt;dbl&gt;\n#&gt; 1 ABQ     254  4.38\n#&gt; 2 ACK     265  4.85\n#&gt; 3 ALB     439 14.4 \n#&gt; 4 ANC       8 -2.5 \n#&gt; 5 ATL   17215 11.3 \n#&gt; 6 AUS    2439  6.02\n#&gt; # ℹ 99 more rows\n\nn() is a special summary function that doesn’t take any arguments and instead accesses information about the “current” group.n() 是一个特殊的摘要函数，它不接受任何参数，而是访问关于 “当前” 组的信息。\nThis means that it only works inside dplyr verbs:\n这意味着它只能在 dplyr 动词内部工作：\n\nn()\n#&gt; Error in `n()`:\n#&gt; ! Must only be used inside data-masking verbs like `mutate()`,\n#&gt;   `filter()`, and `group_by()`.\n\nThere are a couple of variants of n() and count() that you might find useful:n() 和 count() 有几个你可能会觉得有用的变体：\n\n\nn_distinct(x) counts the number of distinct (unique) values of one or more variables.\nFor example, we could figure out which destinations are served by the most carriers:n_distinct(x) 计算一个或多个变量的不重复（唯一）值的数量。 例如，我们可以找出哪些目的地有最多的航空公司使用：\n\nflights |&gt; \n  group_by(dest) |&gt; \n  summarize(carriers = n_distinct(carrier)) |&gt; \n  arrange(desc(carriers))\n#&gt; # A tibble: 105 × 2\n#&gt;   dest  carriers\n#&gt;   &lt;chr&gt;    &lt;int&gt;\n#&gt; 1 ATL          7\n#&gt; 2 BOS          7\n#&gt; 3 CLT          7\n#&gt; 4 ORD          7\n#&gt; 5 TPA          7\n#&gt; 6 AUS          6\n#&gt; # ℹ 99 more rows\n\n\n\nA weighted count is a sum. For example you could “count” the number of miles each plane flew:\n加权计数是一个求和。 例如，你可以“计数”每架飞机飞行的英里数：\n\nflights |&gt; \n  group_by(tailnum) |&gt; \n  summarize(miles = sum(distance))\n#&gt; # A tibble: 4,044 × 2\n#&gt;   tailnum  miles\n#&gt;   &lt;chr&gt;    &lt;dbl&gt;\n#&gt; 1 D942DN    3418\n#&gt; 2 N0EGMQ  250866\n#&gt; 3 N10156  115966\n#&gt; 4 N102UW   25722\n#&gt; 5 N103US   24619\n#&gt; 6 N104UW   25157\n#&gt; # ℹ 4,038 more rows\n\n\n\nWeighted counts are a common problem so count() has a wt argument that does the same thing:\n加权计数是一个常见问题，因此 count() 有一个 wt 参数可以实现同样的功能：\n\nflights |&gt; count(tailnum, wt = distance)\n\n\n\nYou can count missing values by combining sum() and is.na(). In the flights dataset this represents flights that are cancelled:\n你可以通过结合 sum() 和 is.na() 来统计缺失值的数量。 在 flights 数据集中，这表示被取消的航班：\n\nflights |&gt; \n  group_by(dest) |&gt; \n  summarize(n_cancelled = sum(is.na(dep_time))) \n#&gt; # A tibble: 105 × 2\n#&gt;   dest  n_cancelled\n#&gt;   &lt;chr&gt;       &lt;int&gt;\n#&gt; 1 ABQ             0\n#&gt; 2 ACK             0\n#&gt; 3 ALB            20\n#&gt; 4 ANC             0\n#&gt; 5 ATL           317\n#&gt; 6 AUS            21\n#&gt; # ℹ 99 more rows\n\n\n\n\n13.3.1 Exercises\n\nHow can you use count() to count the number of rows with a missing value for a given variable?\nExpand the following calls to count() to instead use group_by(), summarize(), and arrange():\n\nflights |&gt; count(dest, sort = TRUE)\nflights |&gt; count(tailnum, wt = distance)",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Numbers</span>"
    ]
  },
  {
    "objectID": "numbers.html#numeric-transformations",
    "href": "numbers.html#numeric-transformations",
    "title": "13  Numbers",
    "section": "\n13.4 Numeric transformations",
    "text": "13.4 Numeric transformations\nTransformation functions work well with mutate() because their output is the same length as the input.\n转换函数与 mutate() 配合得很好，因为它们的输出长度与输入长度相同。\nThe vast majority of transformation functions are already built into base R.\n绝大多数的转换函数都已内置在 base R 中。\nIt’s impractical to list them all so this section will show the most useful ones.\n要列出所有这些函数是不切实际的，所以本节将展示最有用的那些。\nAs an example, while R provides all the trigonometric functions that you might dream of, we don’t list them here because they’re rarely needed for data science.\n举个例子，虽然 R 提供了你可能梦想到的所有三角函数，但我们在这里不列出它们，因为它们在数据科学中很少需要。\n\n13.4.1 Arithmetic and recycling rules\nWe introduced the basics of arithmetic (+, -, *, /, ^) in Chapter 2 and have used them a bunch since.\n我们在 Chapter 2 中介绍了算术（+、-、*、/、^）的基础知识，并且从那以后已经大量使用它们。\nThese functions don’t need a huge amount of explanation because they do what you learned in grade school.\n这些函数不需要大量的解释，因为它们做的就是你在小学学到的东西。\nBut we need to briefly talk about the recycling rules which determine what happens when the left and right hand sides have different lengths.\n但是我们需要简要讨论一下循环规则 (recycling rules)，它决定了当左右两侧长度不同时会发生什么。\nThis is important for operations like flights |&gt; mutate(air_time = air_time / 60) because there are 336,776 numbers on the left of / but only one on the right.\n这对于像 flights |&gt; mutate(air_time = air_time / 60) 这样的操作很重要，因为 / 的左边有 336,776 个数字，而右边只有一个。\nR handles mismatched lengths by recycling, or repeating, the short vector.\nR 通过循环 (recycling) 或重复较短的向量来处理长度不匹配的情况。\nWe can see this in operation more easily if we create some vectors outside of a data frame:\n如果我们创建一些数据框之外的向量，可以更容易地看到这个操作：\n\nx &lt;- c(1, 2, 10, 20)\nx / 5\n#&gt; [1] 0.2 0.4 2.0 4.0\n# is shorthand for\nx / c(5, 5, 5, 5)\n#&gt; [1] 0.2 0.4 2.0 4.0\n\nGenerally, you only want to recycle single numbers (i.e. vectors of length 1), but R will recycle any shorter length vector.\n通常，你只想循环利用单个数字（即长度为 1 的向量），但 R 会循环利用任何更短长度的向量。\nIt usually (but not always) gives you a warning if the longer vector isn’t a multiple of the shorter:\n如果较长的向量不是较短向量的倍数，它通常（但并非总是）会给你一个警告：\n\nx * c(1, 2)\n#&gt; [1]  1  4 10 40\nx * c(1, 2, 3)\n#&gt; Warning in x * c(1, 2, 3): longer object length is not a multiple of shorter\n#&gt; object length\n#&gt; [1]  1  4 30 20\n\nThese recycling rules are also applied to logical comparisons (==, &lt;, &lt;=, &gt;, &gt;=, !=) and can lead to a surprising result if you accidentally use == instead of %in% and the data frame has an unfortunate number of rows.\n这些循环规则也适用于逻辑比较（==、&lt;、&lt;=、&gt;、&gt;=、!=），如果你不小心使用了 == 而不是 %in%，并且数据框的行数不凑巧，可能会导致令人惊讶的结果。\nFor example, take this code which attempts to find all flights in January and February:\n例如，看这段试图查找一月和二月所有航班的代码：\n\nflights |&gt; \n  filter(month == c(1, 2))\n#&gt; # A tibble: 25,977 × 19\n#&gt;    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n#&gt; 1  2013     1     1      517            515         2      830            819\n#&gt; 2  2013     1     1      542            540         2      923            850\n#&gt; 3  2013     1     1      554            600        -6      812            837\n#&gt; 4  2013     1     1      555            600        -5      913            854\n#&gt; 5  2013     1     1      557            600        -3      838            846\n#&gt; 6  2013     1     1      558            600        -2      849            851\n#&gt; # ℹ 25,971 more rows\n#&gt; # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, …\n\nThe code runs without error, but it doesn’t return what you want.\n代码运行没有错误，但它没有返回你想要的结果。\nBecause of the recycling rules it finds flights in odd numbered rows that departed in January and flights in even numbered rows that departed in February.\n由于循环规则，它会查找奇数行中在一月出发的航班和偶数行中在二月出发的航班。\nAnd unfortunately there’s no warning because flights has an even number of rows.\n而且不幸的是，因为 flights 数据框有偶数行，所以没有警告。\nTo protect you from this type of silent failure, most tidyverse functions use a stricter form of recycling that only recycles single values.\n为了保护你免受这类静默失败的影响，大多数 tidyverse 函数使用一种更严格的循环形式，只循环单个值。\nUnfortunately that doesn’t help here, or in many other cases, because the key computation is performed by the base R function ==, not filter().\n不幸的是，这在这里或许多其他情况下没有帮助，因为关键的计算是由 base R 函数 == 执行的，而不是 filter()。\n\n13.4.2 Minimum and maximum\nThe arithmetic functions work with pairs of variables.\n算术函数处理成对的变量。\nTwo closely related functions are pmin() and pmax(), which when given two or more variables will return the smallest or largest value in each row:\n两个密切相关的函数是 pmin() 和 pmax()，当给定两个或多个变量时，它们将返回每行中的最小值或最大值：\n\ndf &lt;- tribble(\n  ~x, ~y,\n  1,  3,\n  5,  2,\n  7, NA,\n)\n\ndf |&gt; \n  mutate(\n    min = pmin(x, y, na.rm = TRUE),\n    max = pmax(x, y, na.rm = TRUE)\n  )\n#&gt; # A tibble: 3 × 4\n#&gt;       x     y   min   max\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1     3     1     3\n#&gt; 2     5     2     2     5\n#&gt; 3     7    NA     7     7\n\nNote that these are different to the summary functions min() and max() which take multiple observations and return a single value.\n请注意，这些函数与摘要函数 min() 和 max() 不同，后者接受多个观测值并返回单个值。\nYou can tell that you’ve used the wrong form when all the minimums and all the maximums have the same value:\n当所有的最小值和所有的最大值都具有相同的值时，你就可以判断出你使用了错误的形式：\n\ndf |&gt; \n  mutate(\n    min = min(x, y, na.rm = TRUE),\n    max = max(x, y, na.rm = TRUE)\n  )\n#&gt; # A tibble: 3 × 4\n#&gt;       x     y   min   max\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1     3     1     7\n#&gt; 2     5     2     1     7\n#&gt; 3     7    NA     1     7\n\n\n13.4.3 Modular arithmetic\nModular arithmetic is the technical name for the type of math you did before you learned about decimal places, i.e. division that yields a whole number and a remainder.\n模运算是你在学习小数之前所做的那种数学的专业术语，即产生整数和余数的除法。\nIn R, %/% does integer division and %% computes the remainder:\n在 R 中，%/% 执行整数除法，而 %% 计算余数：\n\n1:10 %/% 3\n#&gt;  [1] 0 0 1 1 1 2 2 2 3 3\n1:10 %% 3\n#&gt;  [1] 1 2 0 1 2 0 1 2 0 1\n\nModular arithmetic is handy for the flights dataset, because we can use it to unpack the sched_dep_time variable into hour and minute:\n模运算对于 flights 数据集非常方便，因为我们可以用它将 sched_dep_time 变量分解为 hour 和 minute：\n\nflights |&gt; \n  mutate(\n    hour = sched_dep_time %/% 100,\n    minute = sched_dep_time %% 100,\n    .keep = \"used\"\n  )\n#&gt; # A tibble: 336,776 × 3\n#&gt;   sched_dep_time  hour minute\n#&gt;            &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1            515     5     15\n#&gt; 2            529     5     29\n#&gt; 3            540     5     40\n#&gt; 4            545     5     45\n#&gt; 5            600     6      0\n#&gt; 6            558     5     58\n#&gt; # ℹ 336,770 more rows\n\nWe can combine that with the mean(is.na(x)) trick from Section 12.4 to see how the proportion of cancelled flights varies over the course of the day.\n我们可以将其与 Section 12.4 中的 mean(is.na(x)) 技巧结合起来，看看取消航班的比例在一天中是如何变化的。\nThe results are shown in Figure 13.1.\n结果显示在 Figure 13.1 中。\n\nflights |&gt; \n  group_by(hour = sched_dep_time %/% 100) |&gt; \n  summarize(prop_cancelled = mean(is.na(dep_time)), n = n()) |&gt; \n  filter(hour &gt; 1) |&gt; \n  ggplot(aes(x = hour, y = prop_cancelled)) +\n  geom_line(color = \"grey50\") + \n  geom_point(aes(size = n))\n\n\n\n\n\n\nFigure 13.1: A line plot with scheduled departure hour on the x-axis, and proportion of cancelled flights on the y-axis. Cancellations seem to accumulate over the course of the day until 8pm, very late flights are much less likely to be cancelled.\n\n\n\n\n\n13.4.4 Logarithms\nLogarithms are an incredibly useful transformation for dealing with data that ranges across multiple orders of magnitude and converting exponential growth to linear growth.\n对数是一种非常有用的转换，用于处理跨越多个数量级的数据，并将指数增长转换为线性增长。\nIn R, you have a choice of three logarithms: log() (the natural log, base e), log2() (base 2), and log10() (base 10).\n在 R 中，你可以选择三种对数：log() (自然对数，以 e 为底)、log2() (以 2 为底) 和 log10() (以 10 为底)。\nWe recommend using log2() or log10().\n我们推荐使用 log2() 或 log10()。\nlog2() is easy to interpret because a difference of 1 on the log scale corresponds to doubling on the original scale and a difference of -1 corresponds to halving; whereas log10() is easy to back-transform because (e.g.) 3 is 10^3 = 1000.log2() 很容易解释，因为对数尺度上的 1 个单位差异对应于原始尺度上的加倍，而 -1 的差异则对应于减半；而 log10() 很容易进行逆转换，因为（例如）3 是 10^3 = 1000。\nThe inverse of log() is exp(); to compute the inverse of log2() or log10() you’ll need to use 2^ or 10^.log() 的逆运算是 exp()；要计算 log2() 或 log10() 的逆运算，你需要使用 2^ 或 10^。\n\n13.4.5 Rounding\nUse round(x) to round a number to the nearest integer:\n使用 round(x) 将数字四舍五入到最接近的整数：\n\nround(123.456)\n#&gt; [1] 123\n\nYou can control the precision of the rounding with the second argument, digits.\n你可以使用第二个参数 digits 来控制四舍五入的精度。\nround(x, digits) rounds to the nearest 10^-n so digits = 2 will round to the nearest 0.01.round(x, digits) 会四舍五入到最接近的 10^-n，所以 digits = 2 会四舍五入到最接近的 0.01。\nThis definition is useful because it implies round(x, -3) will round to the nearest thousand, which indeed it does:\n这个定义很有用，因为它意味着 round(x, -3) 会四舍五入到最接近的千位，事实也确实如此：\n\nround(123.456, 2)  # two digits\n#&gt; [1] 123.46\nround(123.456, 1)  # one digit\n#&gt; [1] 123.5\nround(123.456, -1) # round to nearest ten\n#&gt; [1] 120\nround(123.456, -2) # round to nearest hundred\n#&gt; [1] 100\n\nThere’s one weirdness with round() that seems surprising at first glance:round() 有一个乍一看似乎很奇怪的特性：\n\nround(c(1.5, 2.5))\n#&gt; [1] 2 2\n\nround() uses what’s known as “round half to even” or Banker’s rounding: if a number is half way between two integers, it will be rounded to the even integer.round() 使用所谓的 “四舍六入五成双” 或银行家舍入法：如果一个数字正好在两个整数中间，它将被舍入到偶数。\nThis is a good strategy because it keeps the rounding unbiased: half of all 0.5s are rounded up, and half are rounded down.\n这是一个很好的策略，因为它能保持舍入的无偏性：一半的 0.5 会向上舍入，一半会向下舍入。\nround() is paired with floor() which always rounds down and ceiling() which always rounds up:round() 与 floor()（总是向下取整）和 ceiling()（总是向上取整）配对使用：\n\nx &lt;- 123.456\n\nfloor(x)\n#&gt; [1] 123\nceiling(x)\n#&gt; [1] 124\n\nThese functions don’t have a digits argument, so you can instead scale down, round, and then scale back up:\n这些函数没有 digits 参数，所以你可以先缩小，然后取整，再放大回去：\n\n# Round down to nearest two digits\nfloor(x / 0.01) * 0.01\n#&gt; [1] 123.45\n# Round up to nearest two digits\nceiling(x / 0.01) * 0.01\n#&gt; [1] 123.46\n\nYou can use the same technique if you want to round() to a multiple of some other number:\n如果你想 round() 到某个其他数字的倍数，也可以使用相同的技巧：\n\n# Round to nearest multiple of 4\nround(x / 4) * 4\n#&gt; [1] 124\n\n# Round to nearest 0.25\nround(x / 0.25) * 0.25\n#&gt; [1] 123.5\n\n\n13.4.6 Cutting numbers into ranges\nUse cut()1 to break up (aka bin) a numeric vector into discrete buckets:\n使用 cut()1 将一个数值向量分割（也称分箱）成离散的区间：\n\nx &lt;- c(1, 2, 5, 10, 15, 20)\ncut(x, breaks = c(0, 5, 10, 15, 20))\n#&gt; [1] (0,5]   (0,5]   (0,5]   (5,10]  (10,15] (15,20]\n#&gt; Levels: (0,5] (5,10] (10,15] (15,20]\n\nThe breaks don’t need to be evenly spaced:\n分割点不需要均匀分布：\n\ncut(x, breaks = c(0, 5, 10, 100))\n#&gt; [1] (0,5]    (0,5]    (0,5]    (5,10]   (10,100] (10,100]\n#&gt; Levels: (0,5] (5,10] (10,100]\n\nYou can optionally supply your own labels.\n你可以选择性地提供自己的 labels。\nNote that there should be one less labels than breaks.\n请注意，labels 的数量应该比 breaks 少一个。\n\ncut(x, \n  breaks = c(0, 5, 10, 15, 20), \n  labels = c(\"sm\", \"md\", \"lg\", \"xl\")\n)\n#&gt; [1] sm sm sm md lg xl\n#&gt; Levels: sm md lg xl\n\nAny values outside of the range of the breaks will become NA:\n任何超出分割点范围的值都将变为 NA：\n\ny &lt;- c(NA, -10, 5, 10, 30)\ncut(y, breaks = c(0, 5, 10, 15, 20))\n#&gt; [1] &lt;NA&gt;   &lt;NA&gt;   (0,5]  (5,10] &lt;NA&gt;  \n#&gt; Levels: (0,5] (5,10] (10,15] (15,20]\n\nSee the documentation for other useful arguments like right and include.lowest, which control if the intervals are [a, b) or (a, b] and if the lowest interval should be [a, b].\n请参阅文档以了解其他有用的参数，如 right 和 include.lowest，它们控制区间是 [a, b) 还是 (a, b]，以及最低的区间是否应为 [a, b]。\n\n13.4.7 Cumulative and rolling aggregates\nBase R provides cumsum(), cumprod(), cummin(), cummax() for running, or cumulative, sums, products, mins and maxes.\nBase R 提供了 cumsum()、cumprod()、cummin()、cummax() 用于计算游动或累积的和、积、最小值和最大值。\ndplyr provides cummean() for cumulative means.\ndplyr 提供了 cummean() 用于计算累积均值。\nCumulative sums tend to come up the most in practice:\n累积和在实践中最为常见：\n\nx &lt;- 1:10\ncumsum(x)\n#&gt;  [1]  1  3  6 10 15 21 28 36 45 55\n\nIf you need more complex rolling or sliding aggregates, try the slider package.\n如果你需要更复杂的滚动或滑动聚合，可以试试 slider 包。\n\n13.4.8 Exercises\n\nExplain in words what each line of the code used to generate Figure 13.1 does.\nWhat trigonometric functions does R provide? Guess some names and look up the documentation. Do they use degrees or radians?\n\nCurrently dep_time and sched_dep_time are convenient to look at, but hard to compute with because they’re not really continuous numbers. You can see the basic problem by running the code below: there’s a gap between each hour.\n\nflights |&gt; \n  filter(month == 1, day == 1) |&gt; \n  ggplot(aes(x = sched_dep_time, y = dep_delay)) +\n  geom_point()\n\nConvert them to a more truthful representation of time (either fractional hours or minutes since midnight).\n\nRound dep_time and arr_time to the nearest five minutes.",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Numbers</span>"
    ]
  },
  {
    "objectID": "numbers.html#general-transformations",
    "href": "numbers.html#general-transformations",
    "title": "13  Numbers",
    "section": "\n13.5 General transformations",
    "text": "13.5 General transformations\nThe following sections describe some general transformations which are often used with numeric vectors, but can be applied to all other column types.\n以下各节描述了一些通常用于数值向量，但也可以应用于所有其他列类型的一般转换。\n\n13.5.1 Ranks\ndplyr provides a number of ranking functions inspired by SQL, but you should always start with dplyr::min_rank().\ndplyr 提供了一些受 SQL 启发的排名函数，但你应该总是从 dplyr::min_rank() 开始。\nIt uses the typical method for dealing with ties, e.g., 1st, 2nd, 2nd, 4th.\n它使用处理平局的典型方法，例如，第 1 名、第 2 名、第 2 名、第 4 名。\n\nx &lt;- c(1, 2, 2, 3, 4, NA)\nmin_rank(x)\n#&gt; [1]  1  2  2  4  5 NA\n\nNote that the smallest values get the lowest ranks; use desc(x) to give the largest values the smallest ranks:\n注意，最小值获得最低的排名；使用 desc(x) 可以让最大值获得最低的排名：\n\nmin_rank(desc(x))\n#&gt; [1]  5  3  3  2  1 NA\n\nIf min_rank() doesn’t do what you need, look at the variants dplyr::row_number(), dplyr::dense_rank(), dplyr::percent_rank(), and dplyr::cume_dist().\n如果 min_rank() 不能满足你的需求，可以看看它的变体 dplyr::row_number()、dplyr::dense_rank()、dplyr::percent_rank() 和 dplyr::cume_dist()。\nSee the documentation for details.\n详情请参阅文档。\n\ndf &lt;- tibble(x = x)\ndf |&gt; \n  mutate(\n    row_number = row_number(x),\n    dense_rank = dense_rank(x),\n    percent_rank = percent_rank(x),\n    cume_dist = cume_dist(x)\n  )\n#&gt; # A tibble: 6 × 5\n#&gt;       x row_number dense_rank percent_rank cume_dist\n#&gt;   &lt;dbl&gt;      &lt;int&gt;      &lt;int&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1     1          1          1         0          0.2\n#&gt; 2     2          2          2         0.25       0.6\n#&gt; 3     2          3          2         0.25       0.6\n#&gt; 4     3          4          3         0.75       0.8\n#&gt; 5     4          5          4         1          1  \n#&gt; 6    NA         NA         NA        NA         NA\n\nYou can achieve many of the same results by picking the appropriate ties.method argument to base R’s rank(); you’ll probably also want to set na.last = \"keep\" to keep NAs as NA.\n通过为 base R 的 rank() 函数选择合适的 ties.method 参数，你可以实现许多相同的结果；你可能还想设置 na.last = \"keep\" 以将 NA 保留为 NA。\nrow_number() can also be used without any arguments when inside a dplyr verb.row_number() 在 dplyr 动词内部使用时也可以不带任何参数。\nIn this case, it’ll give the number of the “current” row.\n在这种情况下，它会给出 “当前” 行的行号。\nWhen combined with %% or %/% this can be a useful tool for dividing data into similarly sized groups:\n当与 %% 或 %/% 结合使用时，这可以成为将数据划分为大小相近的组的有用工具：\n\ndf &lt;- tibble(id = 1:10)\n\ndf |&gt; \n  mutate(\n    row0 = row_number() - 1,\n    three_groups = row0 %% 3,\n    three_in_each_group = row0 %/% 3\n  )\n#&gt; # A tibble: 10 × 4\n#&gt;      id  row0 three_groups three_in_each_group\n#&gt;   &lt;int&gt; &lt;dbl&gt;        &lt;dbl&gt;               &lt;dbl&gt;\n#&gt; 1     1     0            0                   0\n#&gt; 2     2     1            1                   0\n#&gt; 3     3     2            2                   0\n#&gt; 4     4     3            0                   1\n#&gt; 5     5     4            1                   1\n#&gt; 6     6     5            2                   1\n#&gt; # ℹ 4 more rows\n\n\n13.5.2 Offsets\ndplyr::lead() and dplyr::lag() allow you to refer to the values just before or just after the “current” value.dplyr::lead() 和 dplyr::lag() 允许你引用 “当前” 值之前或之后的值。\nThey return a vector of the same length as the input, padded with NAs at the start or end:\n它们返回一个与输入长度相同的向量，在开头或结尾用 NA 填充：\n\nx &lt;- c(2, 5, 11, 11, 19, 35)\nlag(x)\n#&gt; [1] NA  2  5 11 11 19\nlead(x)\n#&gt; [1]  5 11 11 19 35 NA\n\n\n\nx - lag(x) gives you the difference between the current and previous value.x - lag(x) 给出当前值与前一个值之间的差异。\n\nx - lag(x)\n#&gt; [1] NA  3  6  0  8 16\n\n`\n\n\nx == lag(x) tells you when the current value changes.x == lag(x) 告诉你当前值何时发生变化。\n\nx == lag(x)\n#&gt; [1]    NA FALSE FALSE  TRUE FALSE FALSE\n\n\n\nYou can lead or lag by more than one position by using the second argument, n.\n你可以通过使用第二个参数 n 来前导或滞后超过一个位置。\n\n13.5.3 Consecutive identifiers\nSometimes you want to start a new group every time some event occurs.\n有时你希望在每次某个事件发生时开始一个新的组。\nFor example, when you’re looking at website data, it’s common to want to break up events into sessions, where you begin a new session after a gap of more than x minutes since the last activity.\n例如，在查看网站数据时，通常希望将事件分解为会话 (sessions)，即在距离上次活动超过 x 分钟后开始一个新的会话。\nFor example, imagine you have the times when someone visited a website:\n例如，假设你有一个人访问网站的时间记录：\n\nevents &lt;- tibble(\n  time = c(0, 1, 2, 3, 5, 10, 12, 15, 17, 19, 20, 27, 28, 30)\n)\n\nAnd you’ve computed the time between each event, and figured out if there’s a gap that’s big enough to qualify:\n并且你已经计算了每个事件之间的时间，并判断是否存在足够大的间隔：\n\nevents &lt;- events |&gt; \n  mutate(\n    diff = time - lag(time, default = first(time)),\n    has_gap = diff &gt;= 5\n  )\nevents\n#&gt; # A tibble: 14 × 3\n#&gt;    time  diff has_gap\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;  \n#&gt; 1     0     0 FALSE  \n#&gt; 2     1     1 FALSE  \n#&gt; 3     2     1 FALSE  \n#&gt; 4     3     1 FALSE  \n#&gt; 5     5     2 FALSE  \n#&gt; 6    10     5 TRUE   \n#&gt; # ℹ 8 more rows\n\nBut how do we go from that logical vector to something that we can group_by()?\n但是我们如何从那个逻辑向量转变为可以用于 group_by() 的东西呢？\ncumsum(), from Section 13.4.7, comes to the rescue as gap, i.e. has_gap is TRUE, will increment group by one (Section 12.4.2):\n来自 Section 13.4.7 的 cumsum() 此时就派上用场了，因为当间隙存在时，即 has_gap 为 TRUE，group 将会加一 (Section 12.4.2)：\n\nevents |&gt; mutate(\n  group = cumsum(has_gap)\n)\n#&gt; # A tibble: 14 × 4\n#&gt;    time  diff has_gap group\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;   &lt;int&gt;\n#&gt; 1     0     0 FALSE       0\n#&gt; 2     1     1 FALSE       0\n#&gt; 3     2     1 FALSE       0\n#&gt; 4     3     1 FALSE       0\n#&gt; 5     5     2 FALSE       0\n#&gt; 6    10     5 TRUE        1\n#&gt; # ℹ 8 more rows\n\nAnother approach for creating grouping variables is consecutive_id(), which starts a new group every time one of its arguments changes.\n另一种创建分组变量的方法是 consecutive_id()，它在每次其参数之一发生变化时开始一个新的组。\nFor example, inspired by this stackoverflow question, imagine you have a data frame with a bunch of repeated values:\n例如，受 这个 stackoverflow 问题 的启发，想象你有一个包含许多重复值的数据框：\n\ndf &lt;- tibble(\n  x = c(\"a\", \"a\", \"a\", \"b\", \"c\", \"c\", \"d\", \"e\", \"a\", \"a\", \"b\", \"b\"),\n  y = c(1, 2, 3, 2, 4, 1, 3, 9, 4, 8, 10, 199)\n)\n\nIf you want to keep the first row from each repeated x, you could use group_by(), consecutive_id(), and slice_head():\n如果你想保留每个重复 x 的第一行，你可以使用 group_by()、consecutive_id() 和 slice_head()：\n\ndf |&gt; \n  group_by(id = consecutive_id(x)) |&gt; \n  slice_head(n = 1)\n#&gt; # A tibble: 7 × 3\n#&gt; # Groups:   id [7]\n#&gt;   x         y    id\n#&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 a         1     1\n#&gt; 2 b         2     2\n#&gt; 3 c         4     3\n#&gt; 4 d         3     4\n#&gt; 5 e         9     5\n#&gt; 6 a         4     6\n#&gt; # ℹ 1 more row\n\n\n13.5.4 Exercises\n\nFind the 10 most delayed flights using a ranking function. How do you want to handle ties? Carefully read the documentation for min_rank().\nWhich plane (tailnum) has the worst on-time record?\nWhat time of day should you fly if you want to avoid delays as much as possible?\nWhat does flights |&gt; group_by(dest) |&gt; filter(row_number() &lt; 4) do? What does flights |&gt; group_by(dest) |&gt; filter(row_number(dep_delay) &lt; 4) do?\nFor each destination, compute the total minutes of delay. For each flight, compute the proportion of the total delay for its destination.\n\nDelays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Using lag(), explore how the average flight delay for an hour is related to the average delay for the previous hour.\n\nflights |&gt; \n  mutate(hour = dep_time %/% 100) |&gt; \n  group_by(year, month, day, hour) |&gt; \n  summarize(\n    dep_delay = mean(dep_delay, na.rm = TRUE),\n    n = n(),\n    .groups = \"drop\"\n  ) |&gt; \n  filter(n &gt; 5)\n\n\nLook at each destination. Can you find flights that are suspiciously fast (i.e. flights that represent a potential data entry error)? Compute the air time of a flight relative to the shortest flight to that destination. Which flights were most delayed in the air?\nFind all destinations that are flown by at least two carriers. Use those destinations to come up with a relative ranking of the carriers based on their performance for the same destination.",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Numbers</span>"
    ]
  },
  {
    "objectID": "numbers.html#numeric-summaries",
    "href": "numbers.html#numeric-summaries",
    "title": "13  Numbers",
    "section": "\n13.6 Numeric summaries",
    "text": "13.6 Numeric summaries\nJust using the counts, means, and sums that we’ve introduced already can get you a long way, but R provides many other useful summary functions.\n仅使用我们已经介绍过的计数、均值和总和就可以让你走得很远，但 R 提供了许多其他有用的摘要函数。\nHere is a selection that you might find useful.\n这里是一些你可能会觉得有用的选择。\n\n13.6.1 Center\nSo far, we’ve mostly used mean() to summarize the center of a vector of values.\n到目前为止，我们主要使用 mean() 来概括一个数值向量的中心。\nAs we’ve seen in Section 3.6, because the mean is the sum divided by the count, it is sensitive to even just a few unusually high or low values.\n正如我们在 Section 3.6 中看到的，因为均值是总和除以计数，所以它对哪怕是少数几个异常高或低的值都很敏感。\nAn alternative is to use the median(), which finds a value that lies in the “middle” of the vector, i.e. 50% of the values are above it and 50% are below it.\n另一种方法是使用 median()，它会找到一个位于向量“中间”的值，即 50% 的值在它之上，50% 的值在它之下。\nDepending on the shape of the distribution of the variable you’re interested in, mean or median might be a better measure of center.\n根据你所关心变量的分布形状，均值或中位数可能是更好的中心度量。\nFor example, for symmetric distributions we generally report the mean while for skewed distributions we usually report the median.\n例如，对于对称分布，我们通常报告均值，而对于偏态分布，我们通常报告中位数。\nFigure 13.2 compares the mean vs. the median departure delay (in minutes) for each destination.Figure 13.2 比较了每个目的地的平均出发延迟（分钟）与中位数出发延迟。\nThe median delay is always smaller than the mean delay because flights sometimes leave multiple hours late, but never leave multiple hours early.\n中位数延迟总是小于平均延迟，因为航班有时会晚点数小时，但从不会提前数小时出发。\n\nflights |&gt;\n  group_by(year, month, day) |&gt;\n  summarize(\n    mean = mean(dep_delay, na.rm = TRUE),\n    median = median(dep_delay, na.rm = TRUE),\n    n = n(),\n    .groups = \"drop\"\n  ) |&gt; \n  ggplot(aes(x = mean, y = median)) + \n  geom_abline(slope = 1, intercept = 0, color = \"white\", linewidth = 2) +\n  geom_point()\n\n\n\n\n\n\nFigure 13.2: A scatterplot showing the differences of summarizing daily departure delay with median instead of mean.\n\n\n\n\nYou might also wonder about the mode, or the most common value.\n你可能还会想知道众数，即最常见的值。\nThis is a summary that only works well for very simple cases (which is why you might have learned about it in high school), but it doesn’t work well for many real datasets.\n这是一个只在非常简单的情况下才有效的摘要（这就是为什么你可能在高中学过它），但它对许多真实数据集效果不佳。\nIf the data is discrete, there may be multiple most common values, and if the data is continuous, there might be no most common value because every value is ever so slightly different.\n如果数据是离散的，可能会有多个最常见的值；如果数据是连续的，可能没有最常见的值，因为每个值都略有不同。\nFor these reasons, the mode tends not to be used by statisticians and there’s no mode function included in base R2.\n由于这些原因，统计学家倾向于不使用众数，并且 base R 中没有包含众数函数2。\n\n13.6.2 Minimum, maximum, and quantiles\nWhat if you’re interested in locations other than the center?\n如果你对中心以外的位置感兴趣怎么办？\nmin() and max() will give you the largest and smallest values.min() 和 max() 会给你最大值和最小值。\nAnother powerful tool is quantile() which is a generalization of the median: quantile(x, 0.25) will find the value of x that is greater than 25% of the values, quantile(x, 0.5) is equivalent to the median, and quantile(x, 0.95) will find the value that’s greater than 95% of the values.\n另一个强大的工具是 quantile()，它是中位数的推广：quantile(x, 0.25) 将找到 x 中大于 25% 值的值，quantile(x, 0.5) 等同于中位数，而 quantile(x, 0.95) 将找到大于 95% 值的值。\nFor the flights data, you might want to look at the 95% quantile of delays rather than the maximum, because it will ignore the 5% of most delayed flights which can be quite extreme.\n对于 flights 数据，你可能想查看延迟的 95% 分位数而不是最大值，因为它会忽略最延迟的 5% 航班，这些航班可能非常极端。\n\nflights |&gt;\n  group_by(year, month, day) |&gt;\n  summarize(\n    max = max(dep_delay, na.rm = TRUE),\n    q95 = quantile(dep_delay, 0.95, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n#&gt; # A tibble: 365 × 5\n#&gt;    year month   day   max   q95\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  2013     1     1   853  70.1\n#&gt; 2  2013     1     2   379  85  \n#&gt; 3  2013     1     3   291  68  \n#&gt; 4  2013     1     4   288  60  \n#&gt; 5  2013     1     5   327  41  \n#&gt; 6  2013     1     6   202  51  \n#&gt; # ℹ 359 more rows\n\n\n13.6.3 Spread\nSometimes you’re not so interested in where the bulk of the data lies, but in how it is spread out.\n有时你不太关心数据的主体在哪里，而关心它是如何分布的。\nTwo commonly used summaries are the standard deviation, sd(x), and the inter-quartile range, IQR().\n两个常用的摘要是标准差 sd(x) 和四分位距 IQR()。\nWe won’t explain sd() here since you’re probably already familiar with it, but IQR() might be new — it’s quantile(x, 0.75) - quantile(x, 0.25) and gives you the range that contains the middle 50% of the data.\n我们在这里不会解释 sd()，因为你可能已经熟悉它了，但 IQR() 可能对你来说是新的——它是 quantile(x, 0.75) - quantile(x, 0.25)，并给出包含中间 50% 数据的范围。\nWe can use this to reveal a small oddity in the flights data.\n我们可以用这个来揭示 flights 数据中的一个小小的奇特之处。\nYou might expect the spread of the distance between origin and destination to be zero, since airports are always in the same place.\n你可能期望始发地和目的地之间距离的离散程度为零，因为机场总是在同一个地方。\nBut the code below reveals a data oddity for airport EGE:\n但是下面的代码揭示了关于 EGE 机场的一个数据异常：\n\nflights |&gt; \n  group_by(origin, dest) |&gt; \n  summarize(\n    distance_iqr = IQR(distance), \n    n = n(),\n    .groups = \"drop\"\n  ) |&gt; \n  filter(distance_iqr &gt; 0)\n#&gt; # A tibble: 2 × 4\n#&gt;   origin dest  distance_iqr     n\n#&gt;   &lt;chr&gt;  &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 EWR    EGE              1   110\n#&gt; 2 JFK    EGE              1   103\n\n\n13.6.4 Distributions\nIt’s worth remembering that all of the summary statistics described above are a way of reducing the distribution down to a single number.\n值得记住的是，上面描述的所有摘要统计量都是将分布简化为单个数字的一种方式。\nThis means that they’re fundamentally reductive, and if you pick the wrong summary, you can easily miss important differences between groups.\n这意味着它们在根本上是简化的，如果你选择了错误的摘要，你很容易会错过组间的重要差异。\nThat’s why it’s always a good idea to visualize the distribution before committing to your summary statistics.\n这就是为什么在确定你的摘要统计数据之前，可视化分布总是一个好主意。\nFigure 13.3 shows the overall distribution of departure delays.Figure 13.3 显示了出发延误的总体分布。\nThe distribution is so skewed that we have to zoom in to see the bulk of the data.\n该分布非常偏斜，以至于我们必须放大才能看到数据的主体部分。\nThis suggests that the mean is unlikely to be a good summary and we might prefer the median instead.\n这表明均值不太可能是一个好的摘要，我们可能更倾向于使用中位数。\n\n\n\n\n\n\n\nFigure 13.3: (Left) The histogram of the full data is extremely skewed making it hard to get any details. (Right) Zooming into delays of less than two hours makes it possible to see what’s happening with the bulk of the observations.\n\n\n\n\nIt’s also a good idea to check that distributions for subgroups resemble the whole.\n检查子组的分布是否与整体相似也是一个好主意。\nIn the following plot 365 frequency polygons of dep_delay, one for each day, are overlaid.\n在下面的图中，叠加了 365 个 dep_delay 的频率多边形，每天一个。\nThe distributions seem to follow a common pattern, suggesting it’s fine to use the same summary for each day.\n这些分布似乎遵循一个共同的模式，这表明每天使用相同的摘要是可以的。\n\nflights |&gt;\n  filter(dep_delay &lt; 120) |&gt; \n  ggplot(aes(x = dep_delay, group = interaction(day, month))) + \n  geom_freqpoly(binwidth = 5, alpha = 1/5)\n\n\n\n\n\n\n\nDon’t be afraid to explore your own custom summaries specifically tailored for the data that you’re working with.\n不要害怕探索为你正在处理的数据量身定制的自定义摘要。\nIn this case, that might mean separately summarizing the flights that left early vs. the flights that left late, or given that the values are so heavily skewed, you might try a log-transformation.\n在这种情况下，这可能意味着分别总结提前起飞的航班和晚点起飞的航班，或者鉴于数值偏斜严重，你可以尝试进行对数转换。\nFinally, don’t forget what you learned in Section 3.6: whenever creating numerical summaries, it’s a good idea to include the number of observations in each group.\n最后，别忘了你在 Section 3.6 中学到的：每当创建数值摘要时，最好包含每个组中的观测数量。\n\n13.6.5 Positions\nThere’s one final type of summary that’s useful for numeric vectors, but also works with every other type of value: extracting a value at a specific position: first(x), last(x), and nth(x, n).\n还有最后一种对数值向量很有用，但也适用于所有其他类型值的摘要：提取特定位置的值：first(x)、last(x) 和 nth(x, n)。\nFor example, we can find the first, fifth and last departure for each day:\n例如，我们可以找到每天的第一次、第五次和最后一次出发：\n\nflights |&gt; \n  group_by(year, month, day) |&gt; \n  summarize(\n    first_dep = first(dep_time, na_rm = TRUE), \n    fifth_dep = nth(dep_time, 5, na_rm = TRUE),\n    last_dep = last(dep_time, na_rm = TRUE)\n  )\n#&gt; `summarise()` has grouped output by 'year', 'month'. You can override using\n#&gt; the `.groups` argument.\n#&gt; # A tibble: 365 × 6\n#&gt; # Groups:   year, month [12]\n#&gt;    year month   day first_dep fifth_dep last_dep\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;int&gt;     &lt;int&gt;    &lt;int&gt;\n#&gt; 1  2013     1     1       517       554     2356\n#&gt; 2  2013     1     2        42       535     2354\n#&gt; 3  2013     1     3        32       520     2349\n#&gt; 4  2013     1     4        25       531     2358\n#&gt; 5  2013     1     5        14       534     2357\n#&gt; 6  2013     1     6        16       555     2355\n#&gt; # ℹ 359 more rows\n\n(NB: Because dplyr functions use _ to separate components of function and arguments names, these functions use na_rm instead of na.rm.)\n（注意：因为 dplyr 函数使用 _ 来分隔函数和参数名称的组成部分，所以这些函数使用 na_rm 而不是 na.rm。）\nIf you’re familiar with [, which we’ll come back to in Section 27.2, you might wonder if you ever need these functions.\n如果你熟悉 [（我们将在 Section 27.2 中回过头来讨论），你可能会想知道你是否真的需要这些函数。\nThere are three reasons: the default argument allows you to provide a default if the specified position doesn’t exist, the order_by argument allows you to locally override the order of the rows, and the na_rm argument allows you to drop missing values.\n有三个原因：default 参数允许你在指定位置不存在时提供一个默认值，order_by 参数允许你局部覆盖行的顺序，而 na_rm 参数允许你删除缺失值。\nExtracting values at positions is complementary to filtering on ranks.\n按位置提取值与按排名筛选是互补的。\nFiltering gives you all variables, with each observation in a separate row:\n筛选会给你所有变量，每个观测值占一行：\n\nflights |&gt; \n  group_by(year, month, day) |&gt; \n  mutate(r = min_rank(sched_dep_time)) |&gt; \n  filter(r %in% c(1, max(r)))\n#&gt; # A tibble: 1,195 × 20\n#&gt; # Groups:   year, month, day [365]\n#&gt;    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n#&gt; 1  2013     1     1      517            515         2      830            819\n#&gt; 2  2013     1     1     2353           2359        -6      425            445\n#&gt; 3  2013     1     1     2353           2359        -6      418            442\n#&gt; 4  2013     1     1     2356           2359        -3      425            437\n#&gt; 5  2013     1     2       42           2359        43      518            442\n#&gt; 6  2013     1     2      458            500        -2      703            650\n#&gt; # ℹ 1,189 more rows\n#&gt; # ℹ 12 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, …\n\n\n13.6.6 With mutate()\n\nAs the names suggest, the summary functions are typically paired with summarize().\n正如其名，摘要函数通常与 summarize() 配对使用。\nHowever, because of the recycling rules we discussed in Section 13.4.1 they can also be usefully paired with mutate(), particularly when you want do some sort of group standardization.\n然而，由于我们在 Section 13.4.1 中讨论的循环规则，它们也可以与 mutate() 有效地配对，特别是当你想要进行某种分组标准化时。\nFor example:\n例如：\n\nx / sum(x) calculates the proportion of a total.x / sum(x) 计算占总数的比例。\n(x - mean(x)) / sd(x) computes a Z-score (standardized to mean 0 and sd 1).(x - mean(x)) / sd(x) 计算一个 Z 分数（标准化为均值为 0，标准差为 1）。\n(x - min(x)) / (max(x) - min(x)) standardizes to range [0, 1].(x - min(x)) / (max(x) - min(x)) 将数据标准化到 [0, 1] 范围。\nx / first(x) computes an index based on the first observation.x / first(x) 根据第一个观测值计算一个指数。\n\n13.6.7 Exercises\n\nBrainstorm at least 5 different ways to assess the typical delay characteristics of a group of flights. When is mean() useful? When is median() useful? When might you want to use something else? Should you use arrival delay or departure delay? Why might you want to use data from planes?\nWhich destinations show the greatest variation in air speed?\nCreate a plot to further explore the adventures of EGE. Can you find any evidence that the airport moved locations? Can you find another variable that might explain the difference?",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Numbers</span>"
    ]
  },
  {
    "objectID": "numbers.html#summary",
    "href": "numbers.html#summary",
    "title": "13  Numbers",
    "section": "\n13.7 Summary",
    "text": "13.7 Summary\nYou’re already familiar with many tools for working with numbers, and after reading this chapter you now know how to use them in R.\n你已经熟悉许多处理数字的工具，在阅读本章后，你现在知道如何在 R 中使用它们了。\nYou’ve also learned a handful of useful general transformations that are commonly, but not exclusively, applied to numeric vectors like ranks and offsets.\n你还学到了一些有用的通用转换，它们通常（但非唯一）应用于像排名和偏移量这样的数值向量。\nFinally, you worked through a number of numeric summaries, and discussed a few of the statistical challenges that you should consider.\n最后，你学习了一些数值摘要，并讨论了一些你应该考虑的统计挑战。\nOver the next two chapters, we’ll dive into working with strings with the stringr package.\n在接下来的两章中，我们将深入探讨使用 stringr 包处理字符串。\nStrings are a big topic so they get two chapters, one on the fundamentals of strings and one on regular expressions.\n字符串是一个很大的主题，所以它们有两章的篇幅，一章关于字符串的基础知识，另一章关于正则表达式。",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Numbers</span>"
    ]
  },
  {
    "objectID": "numbers.html#footnotes",
    "href": "numbers.html#footnotes",
    "title": "13  Numbers",
    "section": "",
    "text": "ggplot2 provides some helpers for common cases in cut_interval(), cut_number(), and cut_width(). ggplot2 is an admittedly weird place for these functions to live, but they are useful as part of histogram computation and were written before any other parts of the tidyverse existed.↩︎\nThe mode() function does something quite different!↩︎",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Numbers</span>"
    ]
  },
  {
    "objectID": "strings.html",
    "href": "strings.html",
    "title": "14  Strings",
    "section": "",
    "text": "14.1 Introduction\nSo far, you’ve used a bunch of strings without learning much about the details. Now it’s time to dive into them, learn what makes strings tick, and master some of the powerful string manipulation tools you have at your disposal.\n到目前为止，你已经使用了很多字符串，但没有学习太多细节。现在是时候深入研究它们，了解字符串的工作原理，并掌握一些你可以使用的强大字符串操作工具了。\nWe’ll begin with the details of creating strings and character vectors. You’ll then dive into creating strings from data, then the opposite: extracting strings from data. We’ll then discuss tools that work with individual letters. The chapter finishes with functions that work with individual letters and a brief discussion of where your expectations from English might steer you wrong when working with other languages.\n我们将从创建字符串和字符向量的细节开始。然后你将深入学习如何从数据创建字符串，以及反过来：从数据中提取字符串。接着我们将讨论处理单个字母的工具。本章最后会介绍处理单个字母的函数，并简要讨论在处理其他语言时，你基于英语的预期可能会如何误导你。\nWe’ll keep working with strings in the next chapter, where you’ll learn more about the power of regular expressions.\n在下一章中，我们将继续学习字符串，届时你将了解更多关于正则表达式 (regular expressions) 的强大功能。",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "strings.html#introduction",
    "href": "strings.html#introduction",
    "title": "14  Strings",
    "section": "",
    "text": "14.1.1 Prerequisites\nIn this chapter, we’ll use functions from the stringr package, which is part of the core tidyverse. We’ll also use the babynames data since it provides some fun strings to manipulate.\n在本章中，我们将使用 stringr 包中的函数，它是核心 tidyverse 的一部分。我们还将使用 babynames 数据，因为它提供了一些有趣的字符串可供操作。\n\nlibrary(tidyverse)\nlibrary(babynames)\n\nYou can quickly tell when you’re using a stringr function because all stringr functions start with str_. This is particularly useful if you use RStudio because typing str_ will trigger autocomplete, allowing you to jog your memory of the available functions.\n你可以很快判断出你正在使用的是 stringr 函数，因为所有的 stringr 函数都以 str_ 开头。如果你使用 RStudio，这一点特别有用，因为输入 str_ 会触发自动补全，从而帮助你记起可用的函数。",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "strings.html#creating-a-string",
    "href": "strings.html#creating-a-string",
    "title": "14  Strings",
    "section": "\n14.2 Creating a string",
    "text": "14.2 Creating a string\nWe’ve created strings in passing earlier in the book but didn’t discuss the details. Firstly, you can create a string using either single quotes (') or double quotes (\"). There’s no difference in behavior between the two, so in the interests of consistency, the tidyverse style guide recommends using \", unless the string contains multiple \".\n在本书前面部分，我们不经意间创建了字符串，但没有讨论细节。首先，你可以使用单引号 (') 或双引号 (\") 来创建字符串。两者在行为上没有区别，因此为了保持一致性，tidyverse 风格指南 建议使用 \"，除非字符串本身包含多个 \"。\n\nstring1 &lt;- \"This is a string\"\nstring2 &lt;- 'If I want to include a \"quote\" inside a string, I use single quotes'\n\nIf you forget to close a quote, you’ll see +, the continuation prompt:\n如果你忘记关闭引号，你会看到 +，即续行提示符：\n&gt; \"This is a string without a closing quote\n+ \n+ \n+ HELP I'M STUCK IN A STRING\nIf this happens to you and you can’t figure out which quote to close, press Escape to cancel and try again.\n如果你遇到这种情况，并且不知道该关闭哪个引号，请按 Escape 键取消并重试。\n\n14.2.1 Escapes\nTo include a literal single or double quote in a string, you can use \\ to “escape” it:\n要在字符串中包含字面上的单引号或双引号，你可以使用 \\ 来“转义”它：\n\ndouble_quote &lt;- \"\\\"\" # or '\"'\nsingle_quote &lt;- '\\'' # or \"'\"\n\nSo if you want to include a literal backslash in your string, you’ll need to escape it: \"\\\\\":\n因此，如果你想在字符串中包含一个字面上的反斜杠，你需要对它进行转义：\"\\\\\"：\n\nbackslash &lt;- \"\\\\\"\n\nBeware that the printed representation of a string is not the same as the string itself because the printed representation shows the escapes (in other words, when you print a string, you can copy and paste the output to recreate that string). To see the raw contents of the string, use str_view()1:\n注意，字符串的打印表示形式与字符串本身并不相同，因为打印表示形式会显示转义字符（换句话说，当你打印一个字符串时，你可以复制并粘贴输出来重新创建该字符串）。要查看字符串的原始内容，请使用 str_view()1：\n\nx &lt;- c(single_quote, double_quote, backslash)\nx\n#&gt; [1] \"'\"  \"\\\"\" \"\\\\\"\nstr_view(x)\n#&gt; [1] │ '\n#&gt; [2] │ \"\n#&gt; [3] │ \\\n\n\n14.2.2 Raw strings\nCreating a string with multiple quotes or backslashes gets confusing quickly. To illustrate the problem, let’s create a string that contains the contents of the code block where we define the double_quote and single_quote variables:\n创建一个包含多个引号或反斜杠的字符串很快就会变得混乱。为了说明这个问题，让我们创建一个字符串，它包含我们定义 double_quote 和 single_quote 变量的代码块的内容：\n\ntricky &lt;- \"double_quote &lt;- \\\"\\\\\\\"\\\" # or '\\\"'\nsingle_quote &lt;- '\\\\'' # or \\\"'\\\"\"\nstr_view(tricky)\n#&gt; [1] │ double_quote &lt;- \"\\\"\" # or '\"'\n#&gt;     │ single_quote &lt;- '\\'' # or \"'\"\n\nThat’s a lot of backslashes! (This is sometimes called leaning toothpick syndrome.) To eliminate the escaping, you can instead use a raw string2:\n这里有太多的反斜杠了！（这有时被称为“斜杠牙签综合症” (leaning toothpick syndrome)）。为了消除转义，你可以改用原始字符串 (raw string)2：\n\ntricky &lt;- r\"(double_quote &lt;- \"\\\"\" # or '\"'\nsingle_quote &lt;- '\\'' # or \"'\")\"\nstr_view(tricky)\n#&gt; [1] │ double_quote &lt;- \"\\\"\" # or '\"'\n#&gt;     │ single_quote &lt;- '\\'' # or \"'\"\n\nA raw string usually starts with r\"( and finishes with )\". But if your string contains )\" you can instead use r\"[]\" or r\"{}\", and if that’s still not enough, you can insert any number of dashes to make the opening and closing pairs unique, e.g., r\"--()--\", r\"---()---\", etc. Raw strings are flexible enough to handle any text.\n原始字符串通常以 r\"( 开始，以 )\" 结束。但是，如果你的字符串包含 )\"，你可以改用 r\"[]\" 或 r\"{}\"，如果这还不够，你可以插入任意数量的破折号来使开始和结束对唯一，例如 r\"--()--\"、r\"---()---\" 等。原始字符串足够灵活，可以处理任何文本。\n\n14.2.3 Other special characters\nAs well as \\\", \\', and \\\\, there are a handful of other special characters that may come in handy. The most common are \\n, a new line, and \\t, tab. You’ll also sometimes see strings containing Unicode escapes that start with \\u or \\U. This is a way of writing non-English characters that work on all systems. You can see the complete list of other special characters in ?Quotes.\n除了 \\\"、\\' 和 \\\\，还有一些其他可能派上用场的特殊字符。最常见的是 \\n（换行符）和 \\t（制表符）。你有时也会看到包含以 \\u 或 \\U 开头的 Unicode 转义序列的字符串。这是一种在所有系统上都能正常工作的非英文字符的书写方式。你可以在 ?Quotes 中看到其他特殊字符的完整列表。\n\nx &lt;- c(\"one\\ntwo\", \"one\\ttwo\", \"\\u00b5\", \"\\U0001f604\")\nx\n#&gt; [1] \"one\\ntwo\" \"one\\ttwo\" \"µ\"        \"😄\"\nstr_view(x)\n#&gt; [1] │ one\n#&gt;     │ two\n#&gt; [2] │ one{\\t}two\n#&gt; [3] │ µ\n#&gt; [4] │ 😄\n\nNote that str_view() uses curly braces for tabs to make them easier to spot3. One of the challenges of working with text is that there’s a variety of ways that white space can end up in the text, so this background helps you recognize that something strange is going on.\n请注意 str_view() 对制表符使用花括号，以便更容易发现它们3。处理文本的挑战之一是，空白字符 (white space) 可能以多种方式出现在文本中，所以这个背景知识可以帮助你识别出是否发生了异常情况。\n\n14.2.4 Exercises\n\n\nCreate strings that contain the following values:\n\nHe said \"That's amazing!\"\n\\a\\b\\c\\d\n\\\\\\\\\\\\\n\n\n\nCreate the string in your R session and print it. What happens to the special “\\u00a0”? How does str_view() display it? Can you do a little googling to figure out what this special character is?\n{r}     x &lt;- \"This\\u00a0is\\u00a0tricky\"",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "strings.html#creating-many-strings-from-data",
    "href": "strings.html#creating-many-strings-from-data",
    "title": "14  Strings",
    "section": "\n14.3 Creating many strings from data",
    "text": "14.3 Creating many strings from data\nNow that you’ve learned the basics of creating a string or two by “hand”, we’ll go into the details of creating strings from other strings. This will help you solve the common problem where you have some text you wrote that you want to combine with strings from a data frame. For example, you might combine “Hello” with a name variable to create a greeting. We’ll show you how to do this with str_c() and str_glue() and how you can use them with mutate(). That naturally raises the question of what stringr functions you might use with summarize(), so we’ll finish this section with a discussion of str_flatten(), which is a summary function for strings.\n现在你已经学会了如何“手动”创建一两个字符串的基础知识，接下来我们将深入探讨如何从其他字符串创建字符串。这将帮助你解决一个常见问题：你有一些自己编写的文本，并希望将其与数据框中的字符串结合起来。例如，你可能想将 “Hello” 与一个 name 变量结合起来创建一个问候语。我们将向你展示如何使用 str_c() 和 str_glue() 来实现这一点，以及如何将它们与 mutate() 一起使用。这自然引出了一个问题：你可以将哪些 stringr 函数与 summarize() 一起使用？因此，本节最后将讨论 str_flatten()，这是一个用于字符串的汇总函数。\n\n14.3.1 str_c()\n\nstr_c() takes any number of vectors as arguments and returns a character vector:str_c() 接受任意数量的向量作为参数，并返回一个字符向量：\n\nstr_c(\"x\", \"y\")\n#&gt; [1] \"xy\"\nstr_c(\"x\", \"y\", \"z\")\n#&gt; [1] \"xyz\"\nstr_c(\"Hello \", c(\"John\", \"Susan\"))\n#&gt; [1] \"Hello John\"  \"Hello Susan\"\n\nstr_c() is very similar to the base paste0(), but is designed to be used with mutate() by obeying the usual tidyverse rules for recycling and propagating missing values:str_c() 与基础函数 paste0() 非常相似，但它遵循 tidyverse 通常的回收规则和缺失值传播规则，因此被设计为与 mutate() 一起使用：\n\ndf &lt;- tibble(name = c(\"Flora\", \"David\", \"Terra\", NA))\ndf |&gt; mutate(greeting = str_c(\"Hi \", name, \"!\"))\n#&gt; # A tibble: 4 × 2\n#&gt;   name  greeting \n#&gt;   &lt;chr&gt; &lt;chr&gt;    \n#&gt; 1 Flora Hi Flora!\n#&gt; 2 David Hi David!\n#&gt; 3 Terra Hi Terra!\n#&gt; 4 &lt;NA&gt;  &lt;NA&gt;\n\nIf you want missing values to display in another way, use coalesce() to replace them. Depending on what you want, you might use it either inside or outside of str_c():\n如果你想让缺失值以其他方式显示，可以使用 coalesce() 来替换它们。根据你的需求，你可以在 str_c() 内部或外部使用它：\n\ndf |&gt; \n  mutate(\n    greeting1 = str_c(\"Hi \", coalesce(name, \"you\"), \"!\"),\n    greeting2 = coalesce(str_c(\"Hi \", name, \"!\"), \"Hi!\")\n  )\n#&gt; # A tibble: 4 × 3\n#&gt;   name  greeting1 greeting2\n#&gt;   &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;    \n#&gt; 1 Flora Hi Flora! Hi Flora!\n#&gt; 2 David Hi David! Hi David!\n#&gt; 3 Terra Hi Terra! Hi Terra!\n#&gt; 4 &lt;NA&gt;  Hi you!   Hi!\n\n\n14.3.2 str_glue()\n\nIf you are mixing many fixed and variable strings with str_c(), you’ll notice that you type a lot of \"s, making it hard to see the overall goal of the code. An alternative approach is provided by the glue package via str_glue()4. You give it a single string that has a special feature: anything inside {} will be evaluated like it’s outside of the quotes:\n如果你使用 str_c() 混合许多固定的和可变的字符串，你会发现你输入了大量的 \"，这使得代码的整体目标难以看清。glue 包通过 str_glue()4 提供了另一种方法。你给它一个具有特殊功能的单一字符串：在 {} 内的任何内容都会像在引号之外一样被求值：\n\ndf |&gt; mutate(greeting = str_glue(\"Hi {name}!\"))\n#&gt; # A tibble: 4 × 2\n#&gt;   name  greeting \n#&gt;   &lt;chr&gt; &lt;glue&gt;   \n#&gt; 1 Flora Hi Flora!\n#&gt; 2 David Hi David!\n#&gt; 3 Terra Hi Terra!\n#&gt; 4 &lt;NA&gt;  Hi NA!\n\nAs you can see, str_glue() currently converts missing values to the string \"NA\", unfortunately making it inconsistent with str_c().\n正如你所见，str_glue() 目前将缺失值转换为字符串 \"NA\"，不幸的是，这使其与 str_c() 不一致。\nYou also might wonder what happens if you need to include a regular { or } in your string. You’re on the right track if you guess you’ll need to escape it somehow. The trick is that glue uses a slightly different escaping technique: instead of prefixing with special character like \\, you double up the special characters:\n你可能还会想，如果需要在字符串中包含常规的 { 或 }，会发生什么。如果你猜到需要以某种方式转义它，那你就猜对了。诀窍是 glue 使用一种稍有不同的转义技术：不是像 \\ 那样使用特殊字符作为前缀，而是将特殊字符加倍：\n\ndf |&gt; mutate(greeting = str_glue(\"{{Hi {name}!}}\"))\n#&gt; # A tibble: 4 × 2\n#&gt;   name  greeting   \n#&gt;   &lt;chr&gt; &lt;glue&gt;     \n#&gt; 1 Flora {Hi Flora!}\n#&gt; 2 David {Hi David!}\n#&gt; 3 Terra {Hi Terra!}\n#&gt; 4 &lt;NA&gt;  {Hi NA!}\n\n\n14.3.3 str_flatten()\n\nstr_c() and str_glue() work well with mutate() because their output is the same length as their inputs. What if you want a function that works well with summarize(), i.e. something that always returns a single string? That’s the job of str_flatten()5: it takes a character vector and combines each element of the vector into a single string:str_c() 和 str_glue() 与 mutate() 配合得很好，因为它们的输出长度与输入长度相同。如果你想要一个能与 summarize() 很好地配合使用的函数，即一个总是返回单个字符串的函数，该怎么办？这就是 str_flatten()5 的工作：它接受一个字符向量，并将向量的每个元素合并成一个单一的字符串：\n\nstr_flatten(c(\"x\", \"y\", \"z\"))\n#&gt; [1] \"xyz\"\nstr_flatten(c(\"x\", \"y\", \"z\"), \", \")\n#&gt; [1] \"x, y, z\"\nstr_flatten(c(\"x\", \"y\", \"z\"), \", \", last = \", and \")\n#&gt; [1] \"x, y, and z\"\n\nThis makes it work well with summarize():\n这使得它能与 summarize() 很好地配合使用：\n\ndf &lt;- tribble(\n  ~ name, ~ fruit,\n  \"Carmen\", \"banana\",\n  \"Carmen\", \"apple\",\n  \"Marvin\", \"nectarine\",\n  \"Terence\", \"cantaloupe\",\n  \"Terence\", \"papaya\",\n  \"Terence\", \"mandarin\"\n)\ndf |&gt;\n  group_by(name) |&gt; \n  summarize(fruits = str_flatten(fruit, \", \"))\n#&gt; # A tibble: 3 × 2\n#&gt;   name    fruits                      \n#&gt;   &lt;chr&gt;   &lt;chr&gt;                       \n#&gt; 1 Carmen  banana, apple               \n#&gt; 2 Marvin  nectarine                   \n#&gt; 3 Terence cantaloupe, papaya, mandarin\n\n\n14.3.4 Exercises\n\n\nCompare and contrast the results of paste0() with str_c() for the following inputs:\n\nstr_c(\"hi \", NA)\nstr_c(letters[1:2], letters[1:3])\n\n\nWhat’s the difference between paste() and paste0()? How can you recreate the equivalent of paste() with str_c()?\n\nConvert the following expressions from str_c() to str_glue() or vice versa:\n\nstr_c(\"The price of \", food, \" is \", price)\nstr_glue(\"I'm {age} years old and live in {country}\")\nstr_c(\"\\\\section{\", title, \"}\")",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "strings.html#extracting-data-from-strings",
    "href": "strings.html#extracting-data-from-strings",
    "title": "14  Strings",
    "section": "\n14.4 Extracting data from strings",
    "text": "14.4 Extracting data from strings\nIt’s very common for multiple variables to be crammed together into a single string. In this section, you’ll learn how to use four tidyr functions to extract them:\n将多个变量塞进一个字符串里是非常常见的。在本节中，你将学习如何使用四个 tidyr 函数来提取它们：\n\ndf |&gt; separate_longer_delim(col, delim)\ndf |&gt; separate_longer_position(col, width)\ndf |&gt; separate_wider_delim(col, delim, names)\ndf |&gt; separate_wider_position(col, widths)\n\nIf you look closely, you can see there’s a common pattern here: separate_, then longer or wider, then _, then by delim or position. That’s because these four functions are composed of two simpler primitives:\n如果你仔细观察，你会发现这里有一个共同的模式：separate_，然后是 longer 或 wider，然后是 _，再然后是 delim 或 position。这是因为这四个函数是由两个更简单的原语组成的：\n\nJust like with pivot_longer() and pivot_wider(), _longer functions make the input data frame longer by creating new rows and _wider functions make the input data frame wider by generating new columns.\n就像 pivot_longer() 和 pivot_wider() 一样，_longer 函数通过创建新行来使输入数据框变长，而 _wider 函数通过生成新列来使输入数据框变宽。\ndelim splits up a string with a delimiter like \", \" or \" \"; position splits at specified widths, like c(3, 5, 2).delim 使用像 \", \" 或 \" \" 这样的分隔符 (delimiter) 来分割字符串；position 则在指定的宽度处进行分割，例如 c(3, 5, 2)。\n\nWe’ll return to the last member of this family, separate_wider_regex(), in Chapter 15. It’s the most flexible of the wider functions, but you need to know something about regular expressions before you can use it.\n我们将在 Chapter 15 中回到这个家族的最后一个成员 separate_wider_regex()。它是 wider 函数中最灵活的一个，但在使用它之前，你需要对正则表达式有所了解。\nThe following two sections will give you the basic idea behind these separate functions, first separating into rows (which is a little simpler) and then separating into columns. We’ll finish off by discussing the tools that the wider functions give you to diagnose problems.\n接下来的两节将为你介绍这些 separate 函数背后的基本思想，首先是分拆到行（这稍微简单一些），然后是分拆到列。最后，我们将讨论 wider 函数提供的用于诊断问题的工具。\n\n14.4.1 Separating into rows\nSeparating a string into rows tends to be most useful when the number of components varies from row to row. The most common case is requiring separate_longer_delim() to split based on a delimiter:\n当组件数量因行而异时，将字符串分拆到行通常最有用。最常见的情况是需要 separate_longer_delim() 基于分隔符进行分割：\n\ndf1 &lt;- tibble(x = c(\"a,b,c\", \"d,e\", \"f\"))\ndf1 |&gt; \n  separate_longer_delim(x, delim = \",\")\n#&gt; # A tibble: 6 × 1\n#&gt;   x    \n#&gt;   &lt;chr&gt;\n#&gt; 1 a    \n#&gt; 2 b    \n#&gt; 3 c    \n#&gt; 4 d    \n#&gt; 5 e    \n#&gt; 6 f\n\nIt’s rarer to see separate_longer_position() in the wild, but some older datasets do use a very compact format where each character is used to record a value:\n在实际应用中很少见到 separate_longer_position()，但一些较旧的数据集确实会使用一种非常紧凑的格式，其中每个字符都用于记录一个值：\n\ndf2 &lt;- tibble(x = c(\"1211\", \"131\", \"21\"))\ndf2 |&gt; \n  separate_longer_position(x, width = 1)\n#&gt; # A tibble: 9 × 1\n#&gt;   x    \n#&gt;   &lt;chr&gt;\n#&gt; 1 1    \n#&gt; 2 2    \n#&gt; 3 1    \n#&gt; 4 1    \n#&gt; 5 1    \n#&gt; 6 3    \n#&gt; # ℹ 3 more rows\n\n\n14.4.2 Separating into columns\nSeparating a string into columns tends to be most useful when there are a fixed number of components in each string, and you want to spread them into columns. They are slightly more complicated than their longer equivalents because you need to name the columns. For example, in this following dataset, x is made up of a code, an edition number, and a year, separated by \".\". To use separate_wider_delim(), we supply the delimiter and the names in two arguments:\n当每个字符串中都有固定数量的组件，并且你想将它们分散到列中时，将字符串分拆到列通常最有用。它们比 longer 对应的函数稍微复杂一些，因为你需要为列命名。例如，在下面的数据集中，x 由一个代码、一个版本号和一个年份组成，用 \".\" 分隔。要使用 separate_wider_delim()，我们在两个参数中提供分隔符和名称：\n\ndf3 &lt;- tibble(x = c(\"a10.1.2022\", \"b10.2.2011\", \"e15.1.2015\"))\ndf3 |&gt; \n  separate_wider_delim(\n    x,\n    delim = \".\",\n    names = c(\"code\", \"edition\", \"year\")\n  )\n#&gt; # A tibble: 3 × 3\n#&gt;   code  edition year \n#&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;\n#&gt; 1 a10   1       2022 \n#&gt; 2 b10   2       2011 \n#&gt; 3 e15   1       2015\n\nIf a specific piece is not useful you can use an NA name to omit it from the results:\n如果某个特定的部分没有用，你可以使用 NA 名称将其从结果中省略：\n\ndf3 |&gt; \n  separate_wider_delim(\n    x,\n    delim = \".\",\n    names = c(\"code\", NA, \"year\")\n  )\n#&gt; # A tibble: 3 × 2\n#&gt;   code  year \n#&gt;   &lt;chr&gt; &lt;chr&gt;\n#&gt; 1 a10   2022 \n#&gt; 2 b10   2011 \n#&gt; 3 e15   2015\n\nseparate_wider_position() works a little differently because you typically want to specify the width of each column. So you give it a named integer vector, where the name gives the name of the new column, and the value is the number of characters it occupies. You can omit values from the output by not naming them:separate_wider_position() 的工作方式稍有不同，因为你通常需要指定每列的宽度。因此，你需要给它一个命名的整数向量，其中名称给出新列的名称，值是它占用的字符数。你可以通过不命名来从输出中省略值：\n\ndf4 &lt;- tibble(x = c(\"202215TX\", \"202122LA\", \"202325CA\")) \ndf4 |&gt; \n  separate_wider_position(\n    x,\n    widths = c(year = 4, age = 2, state = 2)\n  )\n#&gt; # A tibble: 3 × 3\n#&gt;   year  age   state\n#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n#&gt; 1 2022  15    TX   \n#&gt; 2 2021  22    LA   \n#&gt; 3 2023  25    CA\n\n\n14.4.3 Diagnosing widening problems\nseparate_wider_delim()6 requires a fixed and known set of columns. What happens if some of the rows don’t have the expected number of pieces? There are two possible problems, too few or too many pieces, so separate_wider_delim() provides two arguments to help: too_few and too_many. Let’s first look at the too_few case with the following sample dataset:separate_wider_delim()6 需要一个固定且已知的列集合。如果某些行没有预期的片段数量会发生什么？可能存在两种问题，片段太少或太多，因此 separate_wider_delim() 提供了两个参数来帮助解决：too_few 和 too_many。让我们首先用以下示例数据集看一下 too_few 的情况：\n\ndf &lt;- tibble(x = c(\"1-1-1\", \"1-1-2\", \"1-3\", \"1-3-2\", \"1\"))\n\ndf |&gt; \n  separate_wider_delim(\n    x,\n    delim = \"-\",\n    names = c(\"x\", \"y\", \"z\")\n  )\n#&gt; Error in `separate_wider_delim()`:\n#&gt; ! Expected 3 pieces in each element of `x`.\n#&gt; ! 2 values were too short.\n#&gt; ℹ Use `too_few = \"debug\"` to diagnose the problem.\n#&gt; ℹ Use `too_few = \"align_start\"/\"align_end\"` to silence this message.\n\nYou’ll notice that we get an error, but the error gives us some suggestions on how you might proceed. Let’s start by debugging the problem:\n你会注意到我们收到了一个错误，但这个错误为我们提供了一些关于如何继续操作的建议。让我们从调试问题开始：\n\ndebug &lt;- df |&gt; \n  separate_wider_delim(\n    x,\n    delim = \"-\",\n    names = c(\"x\", \"y\", \"z\"),\n    too_few = \"debug\"\n  )\n#&gt; Warning: Debug mode activated: adding variables `x_ok`, `x_pieces`, and\n#&gt; `x_remainder`.\ndebug\n#&gt; # A tibble: 5 × 6\n#&gt;   x     y     z     x_ok  x_pieces x_remainder\n#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt;    &lt;int&gt; &lt;chr&gt;      \n#&gt; 1 1-1-1 1     1     TRUE         3 \"\"         \n#&gt; 2 1-1-2 1     2     TRUE         3 \"\"         \n#&gt; 3 1-3   3     &lt;NA&gt;  FALSE        2 \"\"         \n#&gt; 4 1-3-2 3     2     TRUE         3 \"\"         \n#&gt; 5 1     &lt;NA&gt;  &lt;NA&gt;  FALSE        1 \"\"\n\nWhen you use the debug mode, you get three extra columns added to the output: x_ok, x_pieces, and x_remainder (if you separate a variable with a different name, you’ll get a different prefix). Here, x_ok lets you quickly find the inputs that failed:\n当你使用调试模式时，输出中会添加三个额外的列：x_ok、x_pieces 和 x_remainder（如果你分割一个不同名称的变量，你会得到一个不同的前缀）。在这里，x_ok 可以让你快速找到失败的输入：\n\ndebug |&gt; filter(!x_ok)\n#&gt; # A tibble: 2 × 6\n#&gt;   x     y     z     x_ok  x_pieces x_remainder\n#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt;    &lt;int&gt; &lt;chr&gt;      \n#&gt; 1 1-3   3     &lt;NA&gt;  FALSE        2 \"\"         \n#&gt; 2 1     &lt;NA&gt;  &lt;NA&gt;  FALSE        1 \"\"\n\nx_pieces tells us how many pieces were found, compared to the expected 3 (the length of names). x_remainder isn’t useful when there are too few pieces, but we’ll see it again shortly.x_pieces 告诉我们找到了多少个片段，与预期的 3 个（names 的长度）相比。当片段太少时，x_remainder 没有用，但我们很快会再次看到它。\nSometimes looking at this debugging information will reveal a problem with your delimiter strategy or suggest that you need to do more preprocessing before separating. In that case, fix the problem upstream and make sure to remove too_few = \"debug\" to ensure that new problems become errors.\n有时查看这些调试信息会揭示你的分隔符策略存在问题，或者建议你在分割之前需要进行更多的预处理。在这种情况下，请修复上游的问题，并确保删除 too_few = \"debug\" 以确保新问题会成为错误。\nIn other cases, you may want to fill in the missing pieces with NAs and move on. That’s the job of too_few = \"align_start\" and too_few = \"align_end\" which allow you to control where the NAs should go:\n在其他情况下，你可能希望用 NA 填充缺失的片段然后继续。这是 too_few = \"align_start\" 和 too_few = \"align_end\" 的工作，它们允许你控制 NA 应该放在哪里：\n\ndf |&gt; \n  separate_wider_delim(\n    x,\n    delim = \"-\",\n    names = c(\"x\", \"y\", \"z\"),\n    too_few = \"align_start\"\n  )\n#&gt; # A tibble: 5 × 3\n#&gt;   x     y     z    \n#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n#&gt; 1 1     1     1    \n#&gt; 2 1     1     2    \n#&gt; 3 1     3     &lt;NA&gt; \n#&gt; 4 1     3     2    \n#&gt; 5 1     &lt;NA&gt;  &lt;NA&gt;\n\nThe same principles apply if you have too many pieces:\n如果你有太多片段，同样的原则也适用：\n\ndf &lt;- tibble(x = c(\"1-1-1\", \"1-1-2\", \"1-3-5-6\", \"1-3-2\", \"1-3-5-7-9\"))\n\ndf |&gt; \n  separate_wider_delim(\n    x,\n    delim = \"-\",\n    names = c(\"x\", \"y\", \"z\")\n  )\n#&gt; Error in `separate_wider_delim()`:\n#&gt; ! Expected 3 pieces in each element of `x`.\n#&gt; ! 2 values were too long.\n#&gt; ℹ Use `too_many = \"debug\"` to diagnose the problem.\n#&gt; ℹ Use `too_many = \"drop\"/\"merge\"` to silence this message.\n\nBut now, when we debug the result, you can see the purpose of x_remainder:\n但是现在，当我们调试结果时，你可以看到 x_remainder 的用途：\n\ndebug &lt;- df |&gt; \n  separate_wider_delim(\n    x,\n    delim = \"-\",\n    names = c(\"x\", \"y\", \"z\"),\n    too_many = \"debug\"\n  )\n#&gt; Warning: Debug mode activated: adding variables `x_ok`, `x_pieces`, and\n#&gt; `x_remainder`.\ndebug |&gt; filter(!x_ok)\n#&gt; # A tibble: 2 × 6\n#&gt;   x         y     z     x_ok  x_pieces x_remainder\n#&gt;   &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt;    &lt;int&gt; &lt;chr&gt;      \n#&gt; 1 1-3-5-6   3     5     FALSE        4 -6         \n#&gt; 2 1-3-5-7-9 3     5     FALSE        5 -7-9\n\nYou have a slightly different set of options for handling too many pieces: you can either silently “drop” any additional pieces or “merge” them all into the final column:\n对于处理过多片段，你有一套稍微不同的选项：你可以默默地“丢弃”(drop) 任何额外的片段，或者将它们全部“合并”(merge) 到最后一列：\n\ndf |&gt; \n  separate_wider_delim(\n    x,\n    delim = \"-\",\n    names = c(\"x\", \"y\", \"z\"),\n    too_many = \"drop\"\n  )\n#&gt; # A tibble: 5 × 3\n#&gt;   x     y     z    \n#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n#&gt; 1 1     1     1    \n#&gt; 2 1     1     2    \n#&gt; 3 1     3     5    \n#&gt; 4 1     3     2    \n#&gt; 5 1     3     5\n\n\ndf |&gt; \n  separate_wider_delim(\n    x,\n    delim = \"-\",\n    names = c(\"x\", \"y\", \"z\"),\n    too_many = \"merge\"\n  )\n#&gt; # A tibble: 5 × 3\n#&gt;   x     y     z    \n#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n#&gt; 1 1     1     1    \n#&gt; 2 1     1     2    \n#&gt; 3 1     3     5-6  \n#&gt; 4 1     3     2    \n#&gt; 5 1     3     5-7-9",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "strings.html#letters",
    "href": "strings.html#letters",
    "title": "14  Strings",
    "section": "\n14.5 Letters",
    "text": "14.5 Letters\nIn this section, we’ll introduce you to functions that allow you to work with the individual letters within a string. You’ll learn how to find the length of a string, extract substrings, and handle long strings in plots and tables.\n在本节中，我们将向你介绍一些可以处理字符串中单个字母的函数。你将学习如何查找字符串的长度、提取子字符串以及在图表和表格中处理长字符串。\n\n14.5.1 Length\nstr_length() tells you the number of letters in the string:str_length() 会告诉你字符串中的字母数量：\n\nstr_length(c(\"a\", \"R for data science\", NA))\n#&gt; [1]  1 18 NA\n\nYou could use this with count() to find the distribution of lengths of US babynames and then with filter() to look at the longest names, which happen to have 15 letters7:\n你可以将其与 count() 一起使用，以查找美国婴儿名字长度的分布，然后与 filter() 一起使用，以查看最长的名字，这些名字恰好有 15 个字母7：\n\nbabynames |&gt;\n  count(length = str_length(name), wt = n)\n#&gt; # A tibble: 14 × 2\n#&gt;   length        n\n#&gt;    &lt;int&gt;    &lt;int&gt;\n#&gt; 1      2   338150\n#&gt; 2      3  8589596\n#&gt; 3      4 48506739\n#&gt; 4      5 87011607\n#&gt; 5      6 90749404\n#&gt; 6      7 72120767\n#&gt; # ℹ 8 more rows\n\nbabynames |&gt; \n  filter(str_length(name) == 15) |&gt; \n  count(name, wt = n, sort = TRUE)\n#&gt; # A tibble: 34 × 2\n#&gt;   name                n\n#&gt;   &lt;chr&gt;           &lt;int&gt;\n#&gt; 1 Franciscojavier   123\n#&gt; 2 Christopherjohn   118\n#&gt; 3 Johnchristopher   118\n#&gt; 4 Christopherjame   108\n#&gt; 5 Christophermich    52\n#&gt; 6 Ryanchristopher    45\n#&gt; # ℹ 28 more rows\n\n\n14.5.2 Subsetting\nYou can extract parts of a string using str_sub(string, start, end), where start and end are the positions where the substring should start and end. The start and end arguments are inclusive, so the length of the returned string will be end - start + 1:\n你可以使用 str_sub(string, start, end) 来提取字符串的一部分，其中 start 和 end 是子字符串应该开始和结束的位置。start 和 end 参数是包含性的，所以返回的字符串长度将是 end - start + 1：\n\nx &lt;- c(\"Apple\", \"Banana\", \"Pear\")\nstr_sub(x, 1, 3)\n#&gt; [1] \"App\" \"Ban\" \"Pea\"\n\nYou can use negative values to count back from the end of the string: -1 is the last character, -2 is the second to last character, etc.\n你可以使用负值从字符串末尾向后计数：-1 是最后一个字符，-2 是倒数第二个字符，依此类推。\n\nstr_sub(x, -3, -1)\n#&gt; [1] \"ple\" \"ana\" \"ear\"\n\nNote that str_sub() won’t fail if the string is too short: it will just return as much as possible:\n请注意，如果字符串太短，str_sub() 不会失败：它只会尽可能多地返回内容：\n\nstr_sub(\"a\", 1, 5)\n#&gt; [1] \"a\"\n\nWe could use str_sub() with mutate() to find the first and last letter of each name:\n我们可以将 str_sub() 与 mutate() 结合使用，找出每个名字的首字母和尾字母：\n\nbabynames |&gt; \n  mutate(\n    first = str_sub(name, 1, 1),\n    last = str_sub(name, -1, -1)\n  )\n#&gt; # A tibble: 1,924,665 × 7\n#&gt;    year sex   name          n   prop first last \n#&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n#&gt; 1  1880 F     Mary       7065 0.0724 M     y    \n#&gt; 2  1880 F     Anna       2604 0.0267 A     a    \n#&gt; 3  1880 F     Emma       2003 0.0205 E     a    \n#&gt; 4  1880 F     Elizabeth  1939 0.0199 E     h    \n#&gt; 5  1880 F     Minnie     1746 0.0179 M     e    \n#&gt; 6  1880 F     Margaret   1578 0.0162 M     t    \n#&gt; # ℹ 1,924,659 more rows\n\n\n14.5.3 Exercises\n\nWhen computing the distribution of the length of babynames, why did we use wt = n?\n\nUse str_length() and str_sub() to extract the middle letter from each baby name. What will you do if the string has an even number of characters?\n\nAre there any major trends in the length of babynames over time? What about the popularity of first and last letters?",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "strings.html#sec-other-languages",
    "href": "strings.html#sec-other-languages",
    "title": "14  Strings",
    "section": "\n14.6 Non-English text",
    "text": "14.6 Non-English text\nSo far, we’ve focused on English language text which is particularly easy to work with for two reasons. Firstly, the English alphabet is relatively simple: there are just 26 letters. Secondly (and maybe more importantly), the computing infrastructure we use today was predominantly designed by English speakers. Unfortunately, we don’t have room for a full treatment of non-English languages. Still, we wanted to draw your attention to some of the biggest challenges you might encounter: encoding, letter variations, and locale-dependent functions.\n到目前为止，我们一直专注于英文文本，这种文本特别容易处理，原因有二。首先，英文字母相对简单：只有 26 个字母。其次（也许更重要的是），我们今天使用的计算基础设施主要是由英语使用者设计的。不幸的是，我们没有足够的篇幅来全面处理非英语语言。不过，我们想提醒你注意一些你可能遇到的最大挑战：编码 (encoding)、字母变体和依赖于区域设置 (locale-dependent) 的函数。\n\n14.6.1 Encoding\nWhen working with non-English text, the first challenge is often the encoding. To understand what’s going on, we need to dive into how computers represent strings. In R, we can get at the underlying representation of a string using charToRaw():\n在处理非英文文本时，第一个挑战通常是编码 (encoding)。要理解发生了什么，我们需要深入了解计算机如何表示字符串。在 R 中，我们可以使用 charToRaw() 来获取字符串的底层表示：\n\ncharToRaw(\"Hadley\")\n#&gt; [1] 48 61 64 6c 65 79\n\nEach of these six hexadecimal numbers represents one letter: 48 is H, 61 is a, and so on. The mapping from hexadecimal number to character is called the encoding, and in this case, the encoding is called ASCII. ASCII does a great job of representing English characters because it’s the American Standard Code for Information Interchange.\n这六个十六进制数中的每一个都代表一个字母：48 是 H，61 是 a，依此类推。从十六进制数到字符的映射称为编码，在这种情况下，编码被称为 ASCII。ASCII 在表示英文字符方面做得很好，因为它是美国信息交换标准代码。\nThings aren’t so easy for languages other than English. In the early days of computing, there were many competing standards for encoding non-English characters. For example, there were two different encodings for Europe: Latin1 (aka ISO-8859-1) was used for Western European languages, and Latin2 (aka ISO-8859-2) was used for Central European languages. In Latin1, the byte b1 is “±”, but in Latin2, it’s “ą”! Fortunately, today there is one standard that is supported almost everywhere: UTF-8. UTF-8 can encode just about every character used by humans today and many extra symbols like emojis.\n对于非英语语言来说，事情就没那么简单了。在计算的早期，有许多竞争性的标准用于编码非英文字符。例如，欧洲有两种不同的编码：Latin1（又名 ISO-8859-1）用于西欧语言，而 Latin2（又名 ISO-8859-2）用于中欧语言。在 Latin1 中，字节 b1 是 “±”，但在 Latin2 中，它是 “ą”！幸运的是，如今有一个几乎在任何地方都得到支持的标准：UTF-8。UTF-8 几乎可以编码当今人类使用的所有字符以及许多额外的符号，如表情符号 (emojis)。\nreadr uses UTF-8 everywhere. This is a good default but will fail for data produced by older systems that don’t use UTF-8. If this happens, your strings will look weird when you print them. Sometimes just one or two characters might be messed up; other times, you’ll get complete gibberish. For example here are two inline CSVs with unusual encodings8:\nreadr 在任何地方都使用 UTF-8。这是一个很好的默认设置，但对于由不使用 UTF-8 的旧系统生成的数据，它会失败。如果发生这种情况，你的字符串在打印时会看起来很奇怪。有时可能只有一两个字符被弄乱；其他时候，你会得到完全的乱码。例如，这里有两个带有不寻常编码的内联 CSV8：\n\nx1 &lt;- \"text\\nEl Ni\\xf1o was particularly bad this year\"\nread_csv(x1)$text\n#&gt; [1] \"El Ni\\xf1o was particularly bad this year\"\n\nx2 &lt;- \"text\\n\\x82\\xb1\\x82\\xf1\\x82\\xc9\\x82\\xbf\\x82\\xcd\"\nread_csv(x2)$text\n#&gt; [1] \"\\x82\\xb1\\x82\\xf1\\x82ɂ\\xbf\\x82\\xcd\"\n\nTo read these correctly, you specify the encoding via the locale argument:\n要正确读取这些文件，你需要通过 locale 参数指定编码：\n\nread_csv(x1, locale = locale(encoding = \"Latin1\"))$text\n#&gt; [1] \"El Niño was particularly bad this year\"\n\nread_csv(x2, locale = locale(encoding = \"Shift-JIS\"))$text\n#&gt; [1] \"こんにちは\"\n\nHow do you find the correct encoding? If you’re lucky, it’ll be included somewhere in the data documentation. Unfortunately, that’s rarely the case, so readr provides guess_encoding() to help you figure it out. It’s not foolproof and works better when you have lots of text (unlike here), but it’s a reasonable place to start. Expect to try a few different encodings before you find the right one.\n你如何找到正确的编码？如果幸运的话，它会包含在数据文档的某个地方。不幸的是，这种情况很少见，所以 readr 提供了 guess_encoding() 来帮助你确定它。它并非万无一失，并且在你拥有大量文本时效果更好（不像这里），但它是一个合理的起点。预计你需要尝试几种不同的编码才能找到正确的那一个。\nEncodings are a rich and complex topic; we’ve only scratched the surface here. If you’d like to learn more, we recommend reading the detailed explanation at http://kunststube.net/encoding/.\n编码是一个内容丰富且复杂的主题；我们在这里只是浅尝辄止。如果你想了解更多，我们建议阅读 http://kunststube.net/encoding/ 上的详细解释。\n\n14.6.2 Letter variations\nWorking in languages with accents poses a significant challenge when determining the position of letters (e.g., with str_length() and str_sub()) as accented letters might be encoded as a single individual character (e.g., ü) or as two characters by combining an unaccented letter (e.g., u) with a diacritic mark (e.g., ¨). For example, this code shows two ways of representing ü that look identical:\n在处理带重音符号的语言时，确定字母的位置（例如，使用 str_length() 和 str_sub()）会带来重大挑战，因为带重音的字母可能被编码为单个独立字符（例如，ü），或者通过组合一个不带重音的字母（例如，u）和一个变音符号（例如，¨）而被编码为两个字符。例如，这段代码显示了两种看起来完全相同的表示 ü 的方式：\n\nu &lt;- c(\"\\u00fc\", \"u\\u0308\")\nstr_view(u)\n#&gt; [1] │ ü\n#&gt; [2] │ ü\n\nBut both strings differ in length, and their first characters are different:\n但这两个字符串的长度不同，它们的第一个字符也不同：\n\nstr_length(u)\n#&gt; [1] 1 2\nstr_sub(u, 1, 1)\n#&gt; [1] \"ü\" \"u\"\n\nFinally, note that a comparison of these strings with == interprets these strings as different, while the handy str_equal() function in stringr recognizes that both have the same appearance:\n最后，请注意，使用 == 比较这些字符串会将其解释为不同的字符串，而 stringr 中方便的 str_equal() 函数则能识别出两者具有相同的外观：\n\nu[[1]] == u[[2]]\n#&gt; [1] FALSE\n\nstr_equal(u[[1]], u[[2]])\n#&gt; [1] TRUE\n\n\n14.6.3 Locale-dependent functions\nFinally, there are a handful of stringr functions whose behavior depends on your locale. A locale is similar to a language but includes an optional region specifier to handle regional variations within a language. A locale is specified by a lower-case language abbreviation, optionally followed by a _ and an upper-case region identifier. For example, “en” is English, “en_GB” is British English, and “en_US” is American English. If you don’t already know the code for your language, Wikipedia has a good list, and you can see which are supported in stringr by looking at stringi::stri_locale_list().\n最后，还有一些 stringr 函数的行为取决于你的区域设置 (locale)。locale 类似于一种语言，但包含一个可选的区域说明符，以处理一种语言内部的区域差异。locale 由小写语言缩写指定，后面可选择性地跟一个 _ 和一个大写的区域标识符。例如，“en” 是英语，“en_GB” 是英式英语，“en_US” 是美式英语。如果你还不知道你语言的代码，维基百科 有一个很好的列表，你也可以通过查看 stringi::stri_locale_list() 来看 stringr 支持哪些。\nBase R string functions automatically use the locale set by your operating system. This means that base R string functions do what you expect for your language, but your code might work differently if you share it with someone who lives in a different country. To avoid this problem, stringr defaults to English rules by using the “en” locale and requires you to specify the locale argument to override it. Fortunately, there are only two sets of functions where the locale really matters: changing case and sorting.\n基础 R 字符串函数会自动使用你的操作系统设置的 locale。这意味着基础 R 字符串函数会按照你语言的预期工作，但如果你的代码与生活在不同国家的人共享，它的工作方式可能会有所不同。为避免此问题，stringr 默认使用 “en” locale，即英语规则，并要求你指定 locale 参数来覆盖它。幸运的是，只有两组函数的 locale 真正重要：改变大小写和排序。\nThe rules for changing cases differ among languages. For example, Turkish has two i’s: with and without a dot. Since they’re two distinct letters, they’re capitalized differently:\n不同语言的大小写转换规则不同。例如，土耳其语中有两个 i：带点的和不带点的。由于它们是两个不同的字母，它们的大写方式也不同：\n\nstr_to_upper(c(\"i\", \"ı\"))\n#&gt; [1] \"I\" \"I\"\nstr_to_upper(c(\"i\", \"ı\"), locale = \"tr\")\n#&gt; [1] \"İ\" \"I\"\n\nSorting strings depends on the order of the alphabet, and the order of the alphabet is not the same in every language9! Here’s an example: in Czech, “ch” is a compound letter that appears after h in the alphabet.\n字符串排序取决于字母表的顺序，而并非每种语言的字母表顺序都相同9！这里有一个例子：在捷克语中，“ch” 是一个复合字母，在字母表中出现在 h 之后。\n\nstr_sort(c(\"a\", \"c\", \"ch\", \"h\", \"z\"))\n#&gt; [1] \"a\"  \"c\"  \"ch\" \"h\"  \"z\"\nstr_sort(c(\"a\", \"c\", \"ch\", \"h\", \"z\"), locale = \"cs\")\n#&gt; [1] \"a\"  \"c\"  \"h\"  \"ch\" \"z\"\n\nThis also comes up when sorting strings with dplyr::arrange(), which is why it also has a locale argument.\n在使用 dplyr::arrange() 对字符串进行排序时也会遇到这种情况，这就是为什么它也有一个 locale 参数。",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "strings.html#summary",
    "href": "strings.html#summary",
    "title": "14  Strings",
    "section": "\n14.7 Summary",
    "text": "14.7 Summary\nIn this chapter, you’ve learned about some of the power of the stringr package: how to create, combine, and extract strings, and about some of the challenges you might face with non-English strings. Now it’s time to learn one of the most important and powerful tools for working with strings: regular expressions. Regular expressions are a very concise but very expressive language for describing patterns within strings and are the topic of the next chapter.\n在本章中，你学习了 stringr 包的一些强大功能：如何创建、组合和提取字符串，以及在处理非英语字符串时可能面临的一些挑战。现在是时候学习处理字符串最重要和最强大的工具之一：正则表达式。正则表达式是一种非常简洁但表达能力极强的语言，用于描述字符串内的模式，这也是下一章的主题。",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "strings.html#footnotes",
    "href": "strings.html#footnotes",
    "title": "14  Strings",
    "section": "",
    "text": "Or use the base R function writeLines().↩︎\nAvailable in R 4.0.0 and above.↩︎\nstr_view() also uses color to bring tabs, spaces, matches, etc. to your attention. The colors don’t currently show up in the book, but you’ll notice them when running code interactively.↩︎\nIf you’re not using stringr, you can also access it directly with glue::glue().↩︎\nThe base R equivalent is paste() used with the collapse argument.↩︎\nThe same principles apply to separate_wider_position() and separate_wider_regex().↩︎\nLooking at these entries, we’d guess that the babynames data drops spaces or hyphens and truncates after 15 letters.↩︎\nHere I’m using the special \\x to encode binary data directly into a string.↩︎\nSorting in languages that don’t have an alphabet, like Chinese, is more complicated still.↩︎",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "regexps.html",
    "href": "regexps.html",
    "title": "15  Regular expressions",
    "section": "",
    "text": "15.1 Introduction\nIn Chapter 14, you learned a whole bunch of useful functions for working with strings. This chapter will focus on functions that use regular expressions, a concise and powerful language for describing patterns within strings. The term “regular expression” is a bit of a mouthful, so most people abbreviate it to “regex”1 or “regexp”.\n在 Chapter 14 中，你学习了一系列处理字符串的有用函数。本章将重点介绍使用正则表达式的函数，这是一种用于描述字符串内模式的简洁而强大的语言。术语“regular expression”有点拗口，所以大多数人将其缩写为“regex”1 或“regexp”。\nThe chapter starts with the basics of regular expressions and the most useful stringr functions for data analysis. We’ll then expand your knowledge of patterns and cover seven important new topics (escaping, anchoring, character classes, shorthand classes, quantifiers, precedence, and grouping). Next, we’ll talk about some of the other types of patterns that stringr functions can work with and the various “flags” that allow you to tweak the operation of regular expressions. We’ll finish with a survey of other places in the tidyverse and base R where you might use regexes.\n本章首先介绍正则表达式的基础知识以及用于数据分析的最有用的 stringr 函数。然后，我们将扩展你的模式知识，并涵盖七个重要的新主题（转义、锚定、字符类、简写类、量词、优先级和分组）。接下来，我们将讨论 stringr 函数可以处理的其他一些模式类型以及允许你调整正则表达式操作的各种“标志”。最后，我们将概述在 tidyverse 和基础 R 中可能使用正则表达式的其他地方。",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Regular expressions</span>"
    ]
  },
  {
    "objectID": "regexps.html#introduction",
    "href": "regexps.html#introduction",
    "title": "15  Regular expressions",
    "section": "",
    "text": "15.1.1 Prerequisites\nIn this chapter, we’ll use regular expression functions from stringr and tidyr, both core members of the tidyverse, as well as data from the babynames package.\n在本章中，我们将使用 tidyverse 的核心成员 stringr 和 tidyr 中的正则表达式函数，以及来自 babynames 包的数据。\n\nlibrary(tidyverse)\nlibrary(babynames)\n\nThrough this chapter, we’ll use a mix of very simple inline examples so you can get the basic idea, the baby names data, and three character vectors from stringr:\n在本章中，我们将混合使用非常简单的内联示例，以便你了解基本概念、婴儿姓名数据以及来自 stringr 的三个字符向量：\n\nfruit contains the names of 80 fruits.fruit 包含了 80 种水果的名称。\nwords contains 980 common English words.words 包含了 980 个常见的英文单词。\nsentences contains 720 short sentences.sentences 包含了 720 个短句。",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Regular expressions</span>"
    ]
  },
  {
    "objectID": "regexps.html#sec-reg-basics",
    "href": "regexps.html#sec-reg-basics",
    "title": "15  Regular expressions",
    "section": "\n15.2 Pattern basics",
    "text": "15.2 Pattern basics\nWe’ll use str_view() to learn how regex patterns work. We used str_view() in the last chapter to better understand a string vs. its printed representation, and now we’ll use it with its second argument, a regular expression. When this is supplied, str_view() will show only the elements of the string vector that match, surrounding each match with &lt;&gt;, and, where possible, highlighting the match in blue.\n我们将使用 str_view() 来学习正则表达式模式是如何工作的。在上一章中，我们使用 str_view() 来更好地理解字符串与其打印表示之间的区别，现在我们将其与第二个参数（一个正则表达式）一起使用。当提供此参数时，str_view() 将仅显示字符串向量中匹配的元素，用 &lt;&gt; 将每个匹配项括起来，并在可能的情况下用蓝色突出显示匹配项。\nThe simplest patterns consist of letters and numbers which match those characters exactly:\n最简单的模式由字母和数字组成，它们精确匹配这些字符：\n\nstr_view(fruit, \"berry\")\n#&gt;  [6] │ bil&lt;berry&gt;\n#&gt;  [7] │ black&lt;berry&gt;\n#&gt; [10] │ blue&lt;berry&gt;\n#&gt; [11] │ boysen&lt;berry&gt;\n#&gt; [19] │ cloud&lt;berry&gt;\n#&gt; [21] │ cran&lt;berry&gt;\n#&gt; ... and 8 more\n\nLetters and numbers match exactly and are called literal characters. Most punctuation characters, like ., +, *, [, ], and ?, have special meanings2 and are called metacharacters. For example, . will match any character3, so \"a.\" will match any string that contains an “a” followed by another character :\n字母和数字精确匹配，被称为字面字符 (literal characters)。大多数标点符号，如 .、+、*、[、] 和 ?，具有特殊含义2，被称为元字符 (metacharacters)。例如，. 将匹配任何字符3，所以 \"a.\" 将匹配任何包含一个“a”后跟另一个字符的字符串：\n\nstr_view(c(\"a\", \"ab\", \"ae\", \"bd\", \"ea\", \"eab\"), \"a.\")\n#&gt; [2] │ &lt;ab&gt;\n#&gt; [3] │ &lt;ae&gt;\n#&gt; [6] │ e&lt;ab&gt;\n\nOr we could find all the fruits that contain an “a”, followed by three letters, followed by an “e”:\n或者我们可以找到所有包含一个“a”，后跟三个字母，再后跟一个“e”的水果：\n\nstr_view(fruit, \"a...e\")\n#&gt;  [1] │ &lt;apple&gt;\n#&gt;  [7] │ bl&lt;ackbe&gt;rry\n#&gt; [48] │ mand&lt;arine&gt;\n#&gt; [51] │ nect&lt;arine&gt;\n#&gt; [62] │ pine&lt;apple&gt;\n#&gt; [64] │ pomegr&lt;anate&gt;\n#&gt; ... and 2 more\n\nQuantifiers control how many times a pattern can match:量词 (Quantifiers) 控制一个模式可以匹配多少次：\n\n? makes a pattern optional (i.e. it matches 0 or 1 times)? 使模式成为可选的（即它匹配 0 次或 1 次）\n+ lets a pattern repeat (i.e. it matches at least once) + 让模式重复（即它至少匹配一次）\n* lets a pattern be optional or repeat (i.e. it matches any number of times, including 0).* 让模式成为可选的或重复的（即它匹配任意次数，包括 0 次）。\n\n\n# ab? matches an \"a\", optionally followed by a \"b\".\nstr_view(c(\"a\", \"ab\", \"abb\"), \"ab?\")\n#&gt; [1] │ &lt;a&gt;\n#&gt; [2] │ &lt;ab&gt;\n#&gt; [3] │ &lt;ab&gt;b\n\n# ab+ matches an \"a\", followed by at least one \"b\".\nstr_view(c(\"a\", \"ab\", \"abb\"), \"ab+\")\n#&gt; [2] │ &lt;ab&gt;\n#&gt; [3] │ &lt;abb&gt;\n\n# ab* matches an \"a\", followed by any number of \"b\"s.\nstr_view(c(\"a\", \"ab\", \"abb\"), \"ab*\")\n#&gt; [1] │ &lt;a&gt;\n#&gt; [2] │ &lt;ab&gt;\n#&gt; [3] │ &lt;abb&gt;\n\nCharacter classes are defined by [] and let you match a set of characters, e.g., [abcd] matches “a”, “b”, “c”, or “d”. You can also invert the match by starting with ^: [^abcd] matches anything except “a”, “b”, “c”, or “d”. We can use this idea to find the words containing an “x” surrounded by vowels, or a “y” surrounded by consonants:字符类 (Character classes) 由 [] 定义，让你匹配一组字符，例如 [abcd] 匹配 “a”、“b”、“c” 或 “d”。你也可以通过以 ^ 开头来反转匹配：[^abcd] 匹配除 “a”、“b”、“c” 或 “d” 之外的任何内容。我们可以用这个想法来查找包含被元音包围的 “x” 或被辅音包围的 “y” 的单词：\n\nstr_view(words, \"[aeiou]x[aeiou]\")\n#&gt; [284] │ &lt;exa&gt;ct\n#&gt; [285] │ &lt;exa&gt;mple\n#&gt; [288] │ &lt;exe&gt;rcise\n#&gt; [289] │ &lt;exi&gt;st\nstr_view(words, \"[^aeiou]y[^aeiou]\")\n#&gt; [836] │ &lt;sys&gt;tem\n#&gt; [901] │ &lt;typ&gt;e\n\nYou can use alternation, |, to pick between one or more alternative patterns. For example, the following patterns look for fruits containing “apple”, “melon”, or “nut”, or a repeated vowel.\n你可以使用交替 (alternation)，即 |，来在一个或多个备选模式之间进行选择。例如，以下模式查找包含“apple”、“melon”或“nut”的水果，或者包含重复元音的水果。\n\nstr_view(fruit, \"apple|melon|nut\")\n#&gt;  [1] │ &lt;apple&gt;\n#&gt; [13] │ canary &lt;melon&gt;\n#&gt; [20] │ coco&lt;nut&gt;\n#&gt; [52] │ &lt;nut&gt;\n#&gt; [62] │ pine&lt;apple&gt;\n#&gt; [72] │ rock &lt;melon&gt;\n#&gt; ... and 1 more\nstr_view(fruit, \"aa|ee|ii|oo|uu\")\n#&gt;  [9] │ bl&lt;oo&gt;d orange\n#&gt; [33] │ g&lt;oo&gt;seberry\n#&gt; [47] │ lych&lt;ee&gt;\n#&gt; [66] │ purple mangost&lt;ee&gt;n\n\nRegular expressions are very compact and use a lot of punctuation characters, so they can seem overwhelming and hard to read at first. Don’t worry; you’ll get better with practice, and simple patterns will soon become second nature. Let’s kick off that process by practicing with some useful stringr functions.\n正则表达式非常紧凑，使用了大量的标点符号，所以初看起来可能会让人觉得不知所措，难以阅读。别担心，通过练习你会越来越熟练，简单的模式很快就会成为你的第二天性。让我们通过练习一些有用的 stringr 函数来开始这个过程。",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Regular expressions</span>"
    ]
  },
  {
    "objectID": "regexps.html#sec-stringr-regex-funs",
    "href": "regexps.html#sec-stringr-regex-funs",
    "title": "15  Regular expressions",
    "section": "\n15.3 Key functions",
    "text": "15.3 Key functions\nNow that you’ve got the basics of regular expressions under your belt, let’s use them with some stringr and tidyr functions. In the following section, you’ll learn how to detect the presence or absence of a match, how to count the number of matches, how to replace a match with fixed text, and how to extract text using a pattern.\n既然你已经掌握了正则表达式的基础知识，让我们将它们与一些 stringr 和 tidyr 函数一起使用。在接下来的部分，你将学习如何检测匹配的存在与否，如何计算匹配的数量，如何用固定文本替换匹配，以及如何使用模式提取文本。\n\n15.3.1 Detect matches\nstr_detect() returns a logical vector that is TRUE if the pattern matches an element of the character vector and FALSE otherwise:str_detect() 返回一个逻辑向量，如果模式匹配字符向量中的某个元素，则为 TRUE，否则为 FALSE：\n\nstr_detect(c(\"a\", \"b\", \"c\"), \"[aeiou]\")\n#&gt; [1]  TRUE FALSE FALSE\n\nSince str_detect() returns a logical vector of the same length as the initial vector, it pairs well with filter(). For example, this code finds all the most popular names containing a lower-case “x”:\n由于 str_detect() 返回一个与初始向量长度相同的逻辑向量，它与 filter() 配合得很好。例如，这段代码查找所有包含小写字母“x”的最受欢迎的名字：\n\nbabynames |&gt; \n  filter(str_detect(name, \"x\")) |&gt; \n  count(name, wt = n, sort = TRUE)\n#&gt; # A tibble: 974 × 2\n#&gt;   name           n\n#&gt;   &lt;chr&gt;      &lt;int&gt;\n#&gt; 1 Alexander 665492\n#&gt; 2 Alexis    399551\n#&gt; 3 Alex      278705\n#&gt; 4 Alexandra 232223\n#&gt; 5 Max       148787\n#&gt; 6 Alexa     123032\n#&gt; # ℹ 968 more rows\n\nWe can also use str_detect() with summarize() by pairing it with sum() or mean(): sum(str_detect(x, pattern)) tells you the number of observations that match and mean(str_detect(x, pattern)) tells you the proportion that match. For example, the following snippet computes and visualizes the proportion of baby names4 that contain “x”, broken down by year. It looks like they’ve radically increased in popularity lately!\n我们也可以将 str_detect() 与 summarize() 一起使用，方法是将其与 sum() 或 mean() 配对：sum(str_detect(x, pattern)) 告诉你匹配的观测数量，而 mean(str_detect(x, pattern)) 告诉你匹配的比例。例如，下面的代码片段计算并可视化了包含“x”的婴儿姓名4的比例，按年份细分。看起来它们最近的受欢迎程度急剧增加！\n\nbabynames |&gt; \n  group_by(year) |&gt; \n  summarize(prop_x = mean(str_detect(name, \"x\"))) |&gt; \n  ggplot(aes(x = year, y = prop_x)) + \n  geom_line()\n\n\n\n\n\n\n\nThere are two functions that are closely related to str_detect(): str_subset() and str_which(). str_subset() returns a character vector containing only the strings that match. str_which() returns an integer vector giving the positions of the strings that match.\n有两个与 str_detect() 密切相关的函数：str_subset() 和 str_which()。str_subset() 返回一个只包含匹配字符串的字符向量。str_which() 返回一个给出匹配字符串位置的整数向量。\n\n15.3.2 Count matches\nThe next step up in complexity from str_detect() is str_count(): rather than a true or false, it tells you how many matches there are in each string.\n比 str_detect() 更复杂一步的是 str_count()：它不是返回真或假，而是告诉你每个字符串中有多少个匹配项。\n\nx &lt;- c(\"apple\", \"banana\", \"pear\")\nstr_count(x, \"p\")\n#&gt; [1] 2 0 1\n\nNote that each match starts at the end of the previous match, i.e. regex matches never overlap. For example, in \"abababa\", how many times will the pattern \"aba\" match? Regular expressions say two, not three:\n请注意，每个匹配都从前一个匹配的末尾开始，即正则表达式的匹配从不重叠。例如，在 \"abababa\" 中，模式 \"aba\" 会匹配多少次？正则表达式会说是两次，而不是三次：\n\nstr_count(\"abababa\", \"aba\")\n#&gt; [1] 2\nstr_view(\"abababa\", \"aba\")\n#&gt; [1] │ &lt;aba&gt;b&lt;aba&gt;\n\nIt’s natural to use str_count() with mutate(). The following example uses str_count() with character classes to count the number of vowels and consonants in each name.\n很自然地，str_count() 可以与 mutate() 一起使用。下面的例子使用 str_count() 和字符类来计算每个名字中元音和辅音的数量。\n\nbabynames |&gt; \n  count(name) |&gt; \n  mutate(\n    vowels = str_count(name, \"[aeiou]\"),\n    consonants = str_count(name, \"[^aeiou]\")\n  )\n#&gt; # A tibble: 97,310 × 4\n#&gt;   name          n vowels consonants\n#&gt;   &lt;chr&gt;     &lt;int&gt;  &lt;int&gt;      &lt;int&gt;\n#&gt; 1 Aaban        10      2          3\n#&gt; 2 Aabha         5      2          3\n#&gt; 3 Aabid         2      2          3\n#&gt; 4 Aabir         1      2          3\n#&gt; 5 Aabriella     5      4          5\n#&gt; 6 Aada          1      2          2\n#&gt; # ℹ 97,304 more rows\n\nIf you look closely, you’ll notice that there’s something off with our calculations: “Aaban” contains three “a”s, but our summary reports only two vowels. That’s because regular expressions are case sensitive. There are three ways we could fix this:\n如果你仔细看，你会发现我们的计算有些问题：“Aaban” 包含三个 “a”，但我们的摘要只报告了两个元音。这是因为正则表达式是区分大小写的。我们可以通过三种方式来修正这个问题：\n\nAdd the upper case vowels to the character class: str_count(name, \"[aeiouAEIOU]\").\n将大写元音添加到字符类中：str_count(name, \"[aeiouAEIOU]\")。\nTell the regular expression to ignore case: str_count(name, regex(\"[aeiou]\", ignore_case = TRUE)). We’ll talk about more in Section 15.5.1.\n告诉正则表达式忽略大小写：str_count(name, regex(&quot;[aeiou]&quot;, ignore_case = TRUE))。我们将在 Section 15.5.1 中详细讨论。\nUse str_to_lower() to convert the names to lower case: str_count(str_to_lower(name), &quot;[aeiou]&quot;).\n使用 str_to_lower() 将名称转换为小写：str_count(str_to_lower(name), &quot;[aeiou]&quot;)。\n\nThis variety of approaches is pretty typical when working with strings — there are often multiple ways to reach your goal, either by making your pattern more complicated or by doing some preprocessing on your string. If you get stuck trying one approach, it can often be useful to switch gears and tackle the problem from a different perspective.\n这种多样化的方法在处理字符串时非常典型——通常有多种方式可以达到你的目标，要么使你的模式更复杂，要么对你的字符串进行一些预处理。如果你在尝试一种方法时遇到困难，换个角度从不同的视角来解决问题通常会很有用。\nIn this case, since we’re applying two functions to the name, I think it’s easier to transform it first:\n在这种情况下，由于我们对名称应用了两个函数，我认为先转换它会更容易：\n\nbabynames |&gt; \n  count(name) |&gt; \n  mutate(\n    name = str_to_lower(name),\n    vowels = str_count(name, \"[aeiou]\"),\n    consonants = str_count(name, \"[^aeiou]\")\n  )\n#&gt; # A tibble: 97,310 × 4\n#&gt;   name          n vowels consonants\n#&gt;   &lt;chr&gt;     &lt;int&gt;  &lt;int&gt;      &lt;int&gt;\n#&gt; 1 aaban        10      3          2\n#&gt; 2 aabha         5      3          2\n#&gt; 3 aabid         2      3          2\n#&gt; 4 aabir         1      3          2\n#&gt; 5 aabriella     5      5          4\n#&gt; 6 aada          1      3          1\n#&gt; # ℹ 97,304 more rows\n\n\n15.3.3 Replace values\nAs well as detecting and counting matches, we can also modify them with str_replace() and str_replace_all(). str_replace() replaces the first match, and as the name suggests, str_replace_all() replaces all matches.\n除了检测和计数匹配项，我们还可以使用 str_replace() 和 str_replace_all() 来修改它们。str_replace() 替换第一个匹配项，顾名思义，str_replace_all() 替换所有匹配项。\n\nx &lt;- c(\"apple\", \"pear\", \"banana\")\nstr_replace_all(x, \"[aeiou]\", \"-\")\n#&gt; [1] \"-ppl-\"  \"p--r\"   \"b-n-n-\"\n\nstr_remove() and str_remove_all() are handy shortcuts for str_replace(x, pattern, \"\"):str_remove() 和 str_remove_all() 是 str_replace(x, pattern, \"\") 的便捷快捷方式：\n\nx &lt;- c(\"apple\", \"pear\", \"banana\")\nstr_remove_all(x, \"[aeiou]\")\n#&gt; [1] \"ppl\" \"pr\"  \"bnn\"\n\nThese functions are naturally paired with mutate() when doing data cleaning, and you’ll often apply them repeatedly to peel off layers of inconsistent formatting.\n在进行数据清洗时，这些函数很自然地与 mutate() 配对使用，你通常会重复应用它们来剥离不一致的格式层。\n\n15.3.4 Extract variables\nThe last function we’ll discuss uses regular expressions to extract data out of one column into one or more new columns: separate_wider_regex(). It’s a peer of the separate_wider_position() and separate_wider_delim() functions that you learned about in Section 14.4.2. These functions live in tidyr because they operate on (columns of) data frames, rather than individual vectors.\n我们要讨论的最后一个函数使用正则表达式将数据从一列提取到一个或多个新列中：separate_wider_regex()。它与你在 Section 14.4.2 中学到的 separate_wider_position() 和 separate_wider_delim() 函数是同类。这些函数位于 tidyr 包中，因为它们作用于数据框的（列），而不是单个向量。\nLet’s create a simple dataset to show how it works. Here we have some data derived from babynames where we have the name, gender, and age of a bunch of people in a rather weird format5:\n让我们创建一个简单的数据集来展示它是如何工作的。这里我们有一些从 babynames 派生的数据，其中包含了一群人的姓名、性别和年龄，格式相当奇怪5：\n\ndf &lt;- tribble(\n  ~str,\n  \"&lt;Sheryl&gt;-F_34\",\n  \"&lt;Kisha&gt;-F_45\", \n  \"&lt;Brandon&gt;-N_33\",\n  \"&lt;Sharon&gt;-F_38\", \n  \"&lt;Penny&gt;-F_58\",\n  \"&lt;Justin&gt;-M_41\", \n  \"&lt;Patricia&gt;-F_84\", \n)\n\nTo extract this data using separate_wider_regex() we just need to construct a sequence of regular expressions that match each piece. If we want the contents of that piece to appear in the output, we give it a name:\n要使用 separate_wider_regex() 提取这些数据，我们只需要构建一系列匹配每个部分的正则表达式。如果我们希望该部分的内容出现在输出中，我们给它一个名字：\n\ndf |&gt; \n  separate_wider_regex(\n    str,\n    patterns = c(\n      \"&lt;\", \n      name = \"[A-Za-z]+\", \n      \"&gt;-\", \n      gender = \".\",\n      \"_\",\n      age = \"[0-9]+\"\n    )\n  )\n#&gt; # A tibble: 7 × 3\n#&gt;   name    gender age  \n#&gt;   &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;\n#&gt; 1 Sheryl  F      34   \n#&gt; 2 Kisha   F      45   \n#&gt; 3 Brandon N      33   \n#&gt; 4 Sharon  F      38   \n#&gt; 5 Penny   F      58   \n#&gt; 6 Justin  M      41   \n#&gt; # ℹ 1 more row\n\nIf the match fails, you can use too_few = \"debug\" to figure out what went wrong, just like separate_wider_delim() and separate_wider_position().\n如果匹配失败，你可以使用 too_few = \"debug\" 来找出问题所在，就像 separate_wider_delim() 和 separate_wider_position() 一样。\n\n15.3.5 Exercises\n\nWhat baby name has the most vowels? What name has the highest proportion of vowels? (Hint: what is the denominator?)\nReplace all forward slashes in \"a/b/c/d/e\" with backslashes. What happens if you attempt to undo the transformation by replacing all backslashes with forward slashes? (We’ll discuss the problem very soon.)\nImplement a simple version of str_to_lower() using str_replace_all().\nCreate a regular expression that will match telephone numbers as commonly written in your country.",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Regular expressions</span>"
    ]
  },
  {
    "objectID": "regexps.html#pattern-details",
    "href": "regexps.html#pattern-details",
    "title": "15  Regular expressions",
    "section": "\n15.4 Pattern details",
    "text": "15.4 Pattern details\nNow that you understand the basics of the pattern language and how to use it with some stringr and tidyr functions, it’s time to dig into more of the details. First, we’ll start with escaping, which allows you to match metacharacters that would otherwise be treated specially. Next, you’ll learn about anchors which allow you to match the start or end of the string. Then, you’ll learn more about character classes and their shortcuts which allow you to match any character from a set. Next, you’ll learn the final details of quantifiers which control how many times a pattern can match. Then, we have to cover the important (but complex) topic of operator precedence and parentheses. And we’ll finish off with some details of grouping components of the pattern.\n现在你已经了解了模式语言的基础知识以及如何将其与一些 stringr 和 tidyr 函数一起使用，是时候深入了解更多细节了。首先，我们将从转义 (escaping) 开始，它允许你匹配那些原本会被特殊处理的元字符。接下来，你将学习锚点 (anchors)，它允许你匹配字符串的开头或结尾。然后，你将学习更多关于字符类 (character classes) 及其快捷方式的知识，这些快捷方式允许你匹配集合中的任何字符。接下来，你将学习量词 (quantifiers) 的最后细节，它控制模式可以匹配多少次。然后，我们必须涵盖操作符优先级 (operator precedence) 和括号这个重要（但复杂）的主题。最后，我们将以分组 (grouping) 模式组件的一些细节作为结尾。\nThe terms we use here are the technical names for each component. They’re not always the most evocative of their purpose, but it’s very helpful to know the correct terms if you later want to Google for more details.\n我们在这里使用的术语是每个组件的技术名称。它们并不总是最能说明其用途，但如果你以后想通过 Google 搜索更多细节，了解正确的术语会非常有帮助。\n\n15.4.1 Escaping\nIn order to match a literal ., you need an escape which tells the regular expression to match metacharacters6 literally. Like strings, regexps use the backslash for escaping. So, to match a ., you need the regexp \\.. Unfortunately this creates a problem. We use strings to represent regular expressions, and \\ is also used as an escape symbol in strings. So to create the regular expression \\. we need the string \"\\\\.\", as the following example shows.\n为了匹配字面意义上的 .，你需要一个转义 (escape)，它告诉正则表达式按字面意义匹配元字符6。与字符串一样，正则表达式使用反斜杠进行转义。因此，要匹配 .，你需要正则表达式 \\.。不幸的是，这会产生一个问题。我们使用字符串来表示正则表达式，而 \\ 在字符串中也用作转义符号。所以要创建正则表达式 \\.，我们需要字符串 \"\\\\.\"，如下例所示。\n\n# To create the regular expression \\., we need to use \\\\.\ndot &lt;- \"\\\\.\"\n\n# But the expression itself only contains one \\\nstr_view(dot)\n#&gt; [1] │ \\.\n\n# And this tells R to look for an explicit .\nstr_view(c(\"abc\", \"a.c\", \"bef\"), \"a\\\\.c\")\n#&gt; [2] │ &lt;a.c&gt;\n\nIn this book, we’ll usually write regular expression without quotes, like \\.. If we need to emphasize what you’ll actually type, we’ll surround it with quotes and add extra escapes, like \"\\\\.\".\n在本书中，我们通常会不带引号地编写正则表达式，例如 \\.。如果我们需要强调你实际输入的内容，我们会用引号将其括起来并添加额外的转义，例如 \"\\\\.\"。\nIf \\ is used as an escape character in regular expressions, how do you match a literal \\? Well, you need to escape it, creating the regular expression \\\\. To create that regular expression, you need to use a string, which also needs to escape \\. That means to match a literal \\ you need to write \"\\\\\\\\\" — you need four backslashes to match one!\n如果在正则表达式中 \\ 被用作转义字符，那么如何匹配字面意义上的 \\ 呢？嗯，你需要对它进行转义，从而创建正则表达式 \\\\。要创建该正则表达式，你需要使用一个字符串，而该字符串也需要对 \\ 进行转义。这意味着要匹配一个字面意义上的 \\，你需要写成 \"\\\\\\\\\" —— 你需要四个反斜杠来匹配一个！\n\nx &lt;- \"a\\\\b\"\nstr_view(x)\n#&gt; [1] │ a\\b\nstr_view(x, \"\\\\\\\\\")\n#&gt; [1] │ a&lt;\\&gt;b\n\nAlternatively, you might find it easier to use the raw strings you learned about in Section 14.2.2). That lets you avoid one layer of escaping:\n或者，你可能会发现使用在 Section 14.2.2 中学到的原始字符串 (raw strings) 更容易。这样可以避免一层转义：\n\nstr_view(x, r\"{\\\\}\")\n#&gt; [1] │ a&lt;\\&gt;b\n\nIf you’re trying to match a literal ., $, |, *, +, ?, {, }, (, ), there’s an alternative to using a backslash escape: you can use a character class: [.], [$], [|], … all match the literal values.\n如果你试图匹配字面上的 .、$、|、*、+、?、{、}、(、)，除了使用反斜杠转义外，还有另一种选择：你可以使用字符类：[.]、[$]、[|] 等都匹配字面值。\n\nstr_view(c(\"abc\", \"a.c\", \"a*c\", \"a c\"), \"a[.]c\")\n#&gt; [2] │ &lt;a.c&gt;\nstr_view(c(\"abc\", \"a.c\", \"a*c\", \"a c\"), \".[*]c\")\n#&gt; [3] │ &lt;a*c&gt;\n\n\n15.4.2 Anchors\nBy default, regular expressions will match any part of a string. If you want to match at the start or end you need to anchor the regular expression using ^ to match the start or $ to match the end:\n默认情况下，正则表达式将匹配字符串的任何部分。如果你想在开头或结尾进行匹配，你需要使用 ^ 来锚定 (anchor) 正则表达式以匹配开头，或使用 $ 来匹配结尾：\n\nstr_view(fruit, \"^a\")\n#&gt; [1] │ &lt;a&gt;pple\n#&gt; [2] │ &lt;a&gt;pricot\n#&gt; [3] │ &lt;a&gt;vocado\nstr_view(fruit, \"a$\")\n#&gt;  [4] │ banan&lt;a&gt;\n#&gt; [15] │ cherimoy&lt;a&gt;\n#&gt; [30] │ feijo&lt;a&gt;\n#&gt; [36] │ guav&lt;a&gt;\n#&gt; [56] │ papay&lt;a&gt;\n#&gt; [74] │ satsum&lt;a&gt;\n\nIt’s tempting to think that $ should match the start of a string, because that’s how we write dollar amounts, but that’s not what regular expressions want.\n我们很自然地会认为 $ 应该匹配字符串的开头，因为我们就是这样写美元金额的，但正则表达式并不是这样设计的。\nTo force a regular expression to match only the full string, anchor it with both ^ and $:\n要强制一个正则表达式只匹配整个字符串，请使用 ^ 和 $ 将其锚定：\n\nstr_view(fruit, \"apple\")\n#&gt;  [1] │ &lt;apple&gt;\n#&gt; [62] │ pine&lt;apple&gt;\nstr_view(fruit, \"^apple$\")\n#&gt; [1] │ &lt;apple&gt;\n\nYou can also match the boundary between words (i.e. the start or end of a word) with \\b. This can be particularly useful when using RStudio’s find and replace tool. For example, if to find all uses of sum(), you can search for \\bsum\\b to avoid matching summarize, summary, rowsum and so on:\n你还可以使用 \\b 来匹配单词之间的边界（即单词的开头或结尾）。这在使用 RStudio 的查找和替换工具时特别有用。例如，如果要查找 sum() 的所有用法，你可以搜索 \\bsum\\b 以避免匹配 summarize、summary、rowsum 等：\n\nx &lt;- c(\"summary(x)\", \"summarize(df)\", \"rowsum(x)\", \"sum(x)\")\nstr_view(x, \"sum\")\n#&gt; [1] │ &lt;sum&gt;mary(x)\n#&gt; [2] │ &lt;sum&gt;marize(df)\n#&gt; [3] │ row&lt;sum&gt;(x)\n#&gt; [4] │ &lt;sum&gt;(x)\nstr_view(x, \"\\\\bsum\\\\b\")\n#&gt; [4] │ &lt;sum&gt;(x)\n\nWhen used alone, anchors will produce a zero-width match:\n当单独使用时，锚点会产生一个零宽度匹配：\n\nstr_view(\"abc\", c(\"$\", \"^\", \"\\\\b\"))\n#&gt; [1] │ abc&lt;&gt;\n#&gt; [2] │ &lt;&gt;abc\n#&gt; [3] │ &lt;&gt;abc&lt;&gt;\n\nThis helps you understand what happens when you replace a standalone anchor:\n这有助于你理解当你替换一个独立的锚点时会发生什么：\n\nstr_replace_all(\"abc\", c(\"$\", \"^\", \"\\\\b\"), \"--\")\n#&gt; [1] \"abc--\"   \"--abc\"   \"--abc--\"\n\n\n15.4.3 Character classes\nA character class, or character set, allows you to match any character in a set. As we discussed above, you can construct your own sets with [], where [abc] matches “a”, “b”, or “c” and [^abc] matches any character except “a”, “b”, or “c”. Apart from ^ there are two other characters that have special meaning inside of []:字符类 (character class)，或称字符集 (set)，允许你匹配一个集合中的任何字符。如上所述，你可以使用 [] 构建自己的集合，其中 [abc] 匹配 “a”、“b” 或 “c”，而 [^abc] 匹配除 “a”、“b” 或 “c” 之外的任何字符。除了 ^，在 [] 内部还有另外两个具有特殊含义的字符：\n\n- defines a range, e.g., [a-z] matches any lower case letter and [0-9] matches any number.- 定义一个范围，例如 [a-z] 匹配任何小写字母，[0-9] 匹配任何数字。\n\\ escapes special characters, so [\\^-\\]] matches ^, -, or ].\\ 转义特殊字符，所以 [\\^-\\]] 匹配 ^、- 或 ]。\n\nHere are few examples:\n这里有一些例子：\n\nx &lt;- \"abcd ABCD 12345 -!@#%.\"\nstr_view(x, \"[abc]+\")\n#&gt; [1] │ &lt;abc&gt;d ABCD 12345 -!@#%.\nstr_view(x, \"[a-z]+\")\n#&gt; [1] │ &lt;abcd&gt; ABCD 12345 -!@#%.\nstr_view(x, \"[^a-z0-9]+\")\n#&gt; [1] │ abcd&lt; ABCD &gt;12345&lt; -!@#%.&gt;\n\n# You need an escape to match characters that are otherwise\n# special inside of []\nstr_view(\"a-b-c\", \"[a-c]\")\n#&gt; [1] │ &lt;a&gt;-&lt;b&gt;-&lt;c&gt;\nstr_view(\"a-b-c\", \"[a\\\\-c]\")\n#&gt; [1] │ &lt;a&gt;&lt;-&gt;b&lt;-&gt;&lt;c&gt;\n\nSome character classes are used so commonly that they get their own shortcut. You’ve already seen ., which matches any character apart from a newline. There are three other particularly useful pairs7:\n有些字符类非常常用，以至于它们有自己的快捷方式。你已经见过了 .，它匹配除换行符以外的任何字符。还有另外三对特别有用的快捷方式7：\n\n\\d matches any digit;\\D matches anything that isn’t a digit.\\d 匹配任何数字； \\D 匹配任何非数字的字符。\n\\s matches any whitespace (e.g., space, tab, newline);\\S matches anything that isn’t whitespace.\\s 匹配任何空白字符（例如，空格、制表符、换行符）；\\S 匹配任何非空白字符。\n\\w matches any “word” character, i.e. letters and numbers;\\W matches any “non-word” character.\\w 匹配任何“单词”字符，即字母和数字；\\W 匹配任何“非单词”字符。\n\nThe following code demonstrates the six shortcuts with a selection of letters, numbers, and punctuation characters.\n下面的代码用一些字母、数字和标点符号演示了这六个快捷方式。\n\nx &lt;- \"abcd ABCD 12345 -!@#%.\"\nstr_view(x, \"\\\\d+\")\n#&gt; [1] │ abcd ABCD &lt;12345&gt; -!@#%.\nstr_view(x, \"\\\\D+\")\n#&gt; [1] │ &lt;abcd ABCD &gt;12345&lt; -!@#%.&gt;\nstr_view(x, \"\\\\s+\")\n#&gt; [1] │ abcd&lt; &gt;ABCD&lt; &gt;12345&lt; &gt;-!@#%.\nstr_view(x, \"\\\\S+\")\n#&gt; [1] │ &lt;abcd&gt; &lt;ABCD&gt; &lt;12345&gt; &lt;-!@#%.&gt;\nstr_view(x, \"\\\\w+\")\n#&gt; [1] │ &lt;abcd&gt; &lt;ABCD&gt; &lt;12345&gt; -!@#%.\nstr_view(x, \"\\\\W+\")\n#&gt; [1] │ abcd&lt; &gt;ABCD&lt; &gt;12345&lt; -!@#%.&gt;\n\n\n15.4.4 Quantifiers\nQuantifiers control how many times a pattern matches. In Section 15.2 you learned about ? (0 or 1 matches), + (1 or more matches), and * (0 or more matches). For example, colou?r will match American or British spelling, \\d+ will match one or more digits, and \\s? will optionally match a single item of whitespace. You can also specify the number of matches precisely with {}:量词 (Quantifiers) 控制一个模式匹配的次数。在 Section 15.2 中你学习了 ?（0 或 1 次匹配）、+（1 次或多次匹配）和 *（0 次或多次匹配）。例如，colou?r 将匹配美式或英式拼写，\\d+ 将匹配一个或多个数字，\\s? 将可选地匹配一个空白项。你还可以使用 {} 精确指定匹配次数：\n\n{n} matches exactly n times.{n} 精确匹配 n 次。\n{n,} matches at least n times. {n,} 至少匹配 n 次。\n{n,m} matches between n and m times.{n,m} 匹配 n 到 m 次。\n\n15.4.5 Operator precedence and parentheses\nWhat does ab+ match? Does it match “a” followed by one or more “b”s, or does it match “ab” repeated any number of times? What does ^a|b$ match? Does it match the complete string a or the complete string b, or does it match a string starting with a or a string ending with b?ab+ 匹配什么？是匹配一个 “a” 后面跟着一个或多个 “b”，还是匹配 “ab” 重复任意次数？^a|b$ 匹配什么？是匹配完整的字符串 “a” 或完整的字符串 “b”，还是匹配以 “a” 开头的字符串或以 “b” 结尾的字符串？\nThe answer to these questions is determined by operator precedence, similar to the PEMDAS or BEDMAS rules you might have learned in school. You know that a + b * c is equivalent to a + (b * c) not (a + b) * c because * has higher precedence and + has lower precedence: you compute * before +.\n这些问题的答案由运算符优先级决定，类似于你在学校可能学到的 PEMDAS 或 BEDMAS 规则。你知道 a + b * c 等同于 a + (b * c) 而不是 (a + b) * c，因为 * 的优先级高于 +：你先计算 * 再计算 +。\nSimilarly, regular expressions have their own precedence rules: quantifiers have high precedence and alternation has low precedence which means that ab+ is equivalent to a(b+), and ^a|b$ is equivalent to (^a)|(b$). Just like with algebra, you can use parentheses to override the usual order. But unlike algebra you’re unlikely to remember the precedence rules for regexes, so feel free to use parentheses liberally.\n类似地，正则表达式也有自己的优先级规则：量词具有高优先级，而交替具有低优先级，这意味着 ab+ 等价于 a(b+)，而 ^a|b$ 等价于 (^a)|(b$)。就像代数一样，你可以使用括号来覆盖通常的顺序。但与代数不同，你不太可能记住正则表达式的优先级规则，所以请随意大量使用括号。\n\n15.4.6 Grouping and capturing\nAs well as overriding operator precedence, parentheses have another important effect: they create capturing groups that allow you to use sub-components of the match.\n除了覆盖运算符优先级，括号还有另一个重要作用：它们创建了捕获组 (capturing groups)，允许你使用匹配的子组件。\nThe first way to use a capturing group is to refer back to it within a match with back reference: \\1 refers to the match contained in the first parenthesis, \\2 in the second parenthesis, and so on. For example, the following pattern finds all fruits that have a repeated pair of letters:\n使用捕获组的第一种方法是在匹配中使用反向引用 (back reference) 来引用它：\\1 引用第一个括号中包含的匹配项，\\2 引用第二个括号中的匹配项，依此类推。例如，以下模式查找所有具有重复字母对的水果：\n\nstr_view(fruit, \"(..)\\\\1\")\n#&gt;  [4] │ b&lt;anan&gt;a\n#&gt; [20] │ &lt;coco&gt;nut\n#&gt; [22] │ &lt;cucu&gt;mber\n#&gt; [41] │ &lt;juju&gt;be\n#&gt; [56] │ &lt;papa&gt;ya\n#&gt; [73] │ s&lt;alal&gt; berry\n\nAnd this one finds all words that start and end with the same pair of letters:\n而这个则查找所有以相同字母对开头和结尾的单词：\n\nstr_view(words, \"^(..).*\\\\1$\")\n#&gt; [152] │ &lt;church&gt;\n#&gt; [217] │ &lt;decide&gt;\n#&gt; [617] │ &lt;photograph&gt;\n#&gt; [699] │ &lt;require&gt;\n#&gt; [739] │ &lt;sense&gt;\n\nYou can also use back references in str_replace(). For example, this code switches the order of the second and third words in sentences:\n你也可以在 str_replace() 中使用反向引用。例如，这段代码交换了 sentences 中第二个和第三个单词的顺序：\n\nsentences |&gt; \n  str_replace(\"(\\\\w+) (\\\\w+) (\\\\w+)\", \"\\\\1 \\\\3 \\\\2\") |&gt; \n  str_view()\n#&gt; [1] │ The canoe birch slid on the smooth planks.\n#&gt; [2] │ Glue sheet the to the dark blue background.\n#&gt; [3] │ It's to easy tell the depth of a well.\n#&gt; [4] │ These a days chicken leg is a rare dish.\n#&gt; [5] │ Rice often is served in round bowls.\n#&gt; [6] │ The of juice lemons makes fine punch.\n#&gt; ... and 714 more\n\nIf you want to extract the matches for each group you can use str_match(). But str_match() returns a matrix, so it’s not particularly easy to work with8:\n如果你想提取每个组的匹配项，可以使用 str_match()。但是 str_match() 返回一个矩阵，所以处理起来不是特别容易8：\n\nsentences |&gt; \n  str_match(\"the (\\\\w+) (\\\\w+)\") |&gt; \n  head()\n#&gt;      [,1]                [,2]     [,3]    \n#&gt; [1,] \"the smooth planks\" \"smooth\" \"planks\"\n#&gt; [2,] \"the sheet to\"      \"sheet\"  \"to\"    \n#&gt; [3,] \"the depth of\"      \"depth\"  \"of\"    \n#&gt; [4,] NA                  NA       NA      \n#&gt; [5,] NA                  NA       NA      \n#&gt; [6,] NA                  NA       NA\n\nYou could convert to a tibble and name the columns:\n你可以将其转换为一个 tibble 并命名列：\n\nsentences |&gt; \n  str_match(\"the (\\\\w+) (\\\\w+)\") |&gt; \n  as_tibble(.name_repair = \"minimal\") |&gt; \n  set_names(\"match\", \"word1\", \"word2\")\n#&gt; # A tibble: 720 × 3\n#&gt;   match             word1  word2 \n#&gt;   &lt;chr&gt;             &lt;chr&gt;  &lt;chr&gt; \n#&gt; 1 the smooth planks smooth planks\n#&gt; 2 the sheet to      sheet  to    \n#&gt; 3 the depth of      depth  of    \n#&gt; 4 &lt;NA&gt;              &lt;NA&gt;   &lt;NA&gt;  \n#&gt; 5 &lt;NA&gt;              &lt;NA&gt;   &lt;NA&gt;  \n#&gt; 6 &lt;NA&gt;              &lt;NA&gt;   &lt;NA&gt;  \n#&gt; # ℹ 714 more rows\n\nBut then you’ve basically recreated your own version of separate_wider_regex(). Indeed, behind the scenes, separate_wider_regex() converts your vector of patterns to a single regex that uses grouping to capture the named components.\n但这样一来，你基本上就重新创建了自己版本的 separate_wider_regex()。实际上，在幕后，separate_wider_regex() 会将你的模式向量转换为一个单一的正则表达式，该表达式使用分组来捕获命名的组件。\nOccasionally, you’ll want to use parentheses without creating matching groups. You can create a non-capturing group with (?:).\n有时，你会想使用括号而不创建匹配组。你可以使用 (?:) 创建一个非捕获组。\n\nx &lt;- c(\"a gray cat\", \"a grey dog\")\nstr_match(x, \"gr(e|a)y\")\n#&gt;      [,1]   [,2]\n#&gt; [1,] \"gray\" \"a\" \n#&gt; [2,] \"grey\" \"e\"\nstr_match(x, \"gr(?:e|a)y\")\n#&gt;      [,1]  \n#&gt; [1,] \"gray\"\n#&gt; [2,] \"grey\"\n\n\n15.4.7 Exercises\n\nHow would you match the literal string \"'\\? How about \"$^$\"?\nExplain why each of these patterns don’t match a \\: \"\\\", \"\\\\\", \"\\\\\\\".\n\nGiven the corpus of common words in stringr::words, create regular expressions that find all words that:\n\nStart with “y”.\nDon’t start with “y”.\nEnd with “x”.\nAre exactly three letters long. (Don’t cheat by using str_length()!)\nHave seven letters or more.\nContain a vowel-consonant pair.\nContain at least two vowel-consonant pairs in a row.\nOnly consist of repeated vowel-consonant pairs.\n\n\nCreate 11 regular expressions that match the British or American spellings for each of the following words: airplane/aeroplane, aluminum/aluminium, analog/analogue, ass/arse, center/centre, defense/defence, donut/doughnut, gray/grey, modeling/modelling, skeptic/sceptic, summarize/summarise. Try and make the shortest possible regex!\nSwitch the first and last letters in words. Which of those strings are still words?\n\nDescribe in words what these regular expressions match: (read carefully to see if each entry is a regular expression or a string that defines a regular expression.)\n\n^.*$\n\"\\\\{.+\\\\}\"\n\\d{4}-\\d{2}-\\d{2}\n\"\\\\\\\\{4}\"\n\\..\\..\\..\n(.)\\1\\1\n\"(..)\\\\1\"\n\n\nSolve the beginner regexp crosswords at https://regexcrossword.com/challenges/beginner.",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Regular expressions</span>"
    ]
  },
  {
    "objectID": "regexps.html#pattern-control",
    "href": "regexps.html#pattern-control",
    "title": "15  Regular expressions",
    "section": "\n15.5 Pattern control",
    "text": "15.5 Pattern control\nIt’s possible to exercise extra control over the details of the match by using a pattern object instead of just a string. This allows you to control the so called regex flags and match various types of fixed strings, as described below.\n通过使用模式对象而不仅仅是字符串，可以对匹配的细节进行额外的控制。这允许你控制所谓的正则表达式标志，并匹配各种类型的固定字符串，如下所述。\n\n15.5.1 Regex flags\nThere are a number of settings that can be used to control the details of the regexp. These settings are often called flags in other programming languages. In stringr, you can use these by wrapping the pattern in a call to regex(). The most useful flag is probably ignore_case = TRUE because it allows characters to match either their uppercase or lowercase forms:\n有许多设置可用于控制正则表达式的细节。这些设置在其他编程语言中通常被称为标志 (flags)。在 stringr 中，你可以通过将模式包装在对 regex() 的调用中使用它们。最有用的标志可能是 ignore_case = TRUE，因为它允许字符匹配其大写或小写形式：\n\nbananas &lt;- c(\"banana\", \"Banana\", \"BANANA\")\nstr_view(bananas, \"banana\")\n#&gt; [1] │ &lt;banana&gt;\nstr_view(bananas, regex(\"banana\", ignore_case = TRUE))\n#&gt; [1] │ &lt;banana&gt;\n#&gt; [2] │ &lt;Banana&gt;\n#&gt; [3] │ &lt;BANANA&gt;\n\nIf you’re doing a lot of work with multiline strings (i.e. strings that contain \\n), dotalland multiline may also be useful:\n如果你正在处理大量多行字符串（即包含 \\n 的字符串），dotall 和 multiline 也可能很有用：\n\n\ndotall = TRUE lets . match everything, including \\n:dotall = TRUE 让 . 匹配所有内容，包括 \\n：\n\nx &lt;- \"Line 1\\nLine 2\\nLine 3\"\nstr_view(x, \".Line\")\nstr_view(x, regex(\".Line\", dotall = TRUE))\n#&gt; [1] │ Line 1&lt;\n#&gt;     │ Line&gt; 2&lt;\n#&gt;     │ Line&gt; 3\n\n\n\nmultiline = TRUE makes ^ and $ match the start and end of each line rather than the start and end of the complete string:multiline = TRUE 使 ^ 和 $ 匹配每行的开头和结尾，而不是整个字符串的开头和结尾：\n\nx &lt;- \"Line 1\\nLine 2\\nLine 3\"\nstr_view(x, \"^Line\")\n#&gt; [1] │ &lt;Line&gt; 1\n#&gt;     │ Line 2\n#&gt;     │ Line 3\nstr_view(x, regex(\"^Line\", multiline = TRUE))\n#&gt; [1] │ &lt;Line&gt; 1\n#&gt;     │ &lt;Line&gt; 2\n#&gt;     │ &lt;Line&gt; 3\n\n\n\nFinally, if you’re writing a complicated regular expression and you’re worried you might not understand it in the future, you might try comments = TRUE. It tweaks the pattern language to ignore spaces and new lines, as well as everything after #. This allows you to use comments and whitespace to make complex regular expressions more understandable9, as in the following example:\n最后，如果你正在编写一个复杂的正则表达式，并且担心将来可能无法理解它，你可以尝试使用 comments = TRUE。它会调整模式语言，使其忽略空格和换行符，以及 # 之后的所有内容。这允许你使用注释和空白来使复杂的正则表达式更易于理解9，如下例所示：\n\nphone &lt;- regex(\n  r\"(\n    \\(?     # optional opening parens\n    (\\d{3}) # area code\n    [)-]?  # optional closing parens or dash\n    \\ ?     # optional space\n    (\\d{3}) # another three numbers\n    [\\ -]?  # optional space or dash\n    (\\d{4}) # four more numbers\n  )\", \n  comments = TRUE\n)\n\nstr_extract(c(\"514-791-8141\", \"(123) 456 7890\", \"123456\"), phone)\n#&gt; [1] \"514-791-8141\"   \"(123) 456 7890\" NA\n\nIf you’re using comments and want to match a space, newline, or #, you’ll need to escape it with \\.\n如果你正在使用注释并且想要匹配空格、换行符或 #，你需要使用 \\ 对其进行转义。\n\n15.5.2 Fixed matches\nYou can opt-out of the regular expression rules by using fixed():\n你可以通过使用 fixed() 来选择不使用正则表达式规则：\n\nstr_view(c(\"\", \"a\", \".\"), fixed(\".\"))\n#&gt; [3] │ &lt;.&gt;\n\nfixed() also gives you the ability to ignore case:fixed() 还让你能够忽略大小写：\n\nstr_view(\"x X\", \"X\")\n#&gt; [1] │ x &lt;X&gt;\nstr_view(\"x X\", fixed(\"X\", ignore_case = TRUE))\n#&gt; [1] │ &lt;x&gt; &lt;X&gt;\n\nIf you’re working with non-English text, you will probably want coll() instead of fixed(), as it implements the full rules for capitalization as used by the locale you specify. See Section 14.6 for more details on locales.\n如果你正在处理非英语文本，你可能需要使用 coll() 而不是 fixed()，因为它实现了你指定的 locale 所使用的完整大写规则。有关区域设置的更多详细信息，请参阅 Section 14.6。\n\nstr_view(\"i İ ı I\", fixed(\"İ\", ignore_case = TRUE))\n#&gt; [1] │ i &lt;İ&gt; ı I\nstr_view(\"i İ ı I\", coll(\"İ\", ignore_case = TRUE, locale = \"tr\"))\n#&gt; [1] │ &lt;i&gt; &lt;İ&gt; ı I",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Regular expressions</span>"
    ]
  },
  {
    "objectID": "regexps.html#practice",
    "href": "regexps.html#practice",
    "title": "15  Regular expressions",
    "section": "\n15.6 Practice",
    "text": "15.6 Practice\nTo put these ideas into practice we’ll solve a few semi-authentic problems next. We’ll discuss three general techniques:\n为了将这些想法付诸实践，我们接下来将解决一些半真实的问题。我们将讨论三种通用技术：\n\nchecking your work by creating simple positive and negative controls\n\n通过创建简单的阳性和阴性对照来检查你的工作\n\ncombining regular expressions with Boolean algebra\n\n\n\n2. 将正则表达式与布尔代数相结合 3. creating complex patterns using string manipulation\n3. 使用字符串操作创建复杂模式\n\n15.6.1 Check your work\nFirst, let’s find all sentences that start with “The”. Using the ^ anchor alone is not enough:\n首先，让我们找到所有以“The”开头的句子。仅使用 ^ 锚点是不够的：\n\nstr_view(sentences, \"^The\")\n#&gt;  [1] │ &lt;The&gt; birch canoe slid on the smooth planks.\n#&gt;  [4] │ &lt;The&gt;se days a chicken leg is a rare dish.\n#&gt;  [6] │ &lt;The&gt; juice of lemons makes fine punch.\n#&gt;  [7] │ &lt;The&gt; box was thrown beside the parked truck.\n#&gt;  [8] │ &lt;The&gt; hogs were fed chopped corn and garbage.\n#&gt; [11] │ &lt;The&gt; boy was there when the sun rose.\n#&gt; ... and 271 more\n\nBecause that pattern also matches sentences starting with words like They or These. We need to make sure that the “e” is the last letter in the word, which we can do by adding a word boundary:\n因为该模式也匹配以 They 或 These 等词开头的句子。我们需要确保 “e” 是单词中的最后一个字母，我们可以通过添加一个词边界来实现这一点：\n\nstr_view(sentences, \"^The\\\\b\")\n#&gt;  [1] │ &lt;The&gt; birch canoe slid on the smooth planks.\n#&gt;  [6] │ &lt;The&gt; juice of lemons makes fine punch.\n#&gt;  [7] │ &lt;The&gt; box was thrown beside the parked truck.\n#&gt;  [8] │ &lt;The&gt; hogs were fed chopped corn and garbage.\n#&gt; [11] │ &lt;The&gt; boy was there when the sun rose.\n#&gt; [13] │ &lt;The&gt; source of the huge river is the clear spring.\n#&gt; ... and 250 more\n\nWhat about finding all sentences that begin with a pronoun?\n那么，如何找到所有以代词开头的句子呢？\n\nstr_view(sentences, \"^She|He|It|They\\\\b\")\n#&gt;  [3] │ &lt;It&gt;'s easy to tell the depth of a well.\n#&gt; [15] │ &lt;He&gt;lp the woman get back to her feet.\n#&gt; [27] │ &lt;He&gt;r purse was full of useless trash.\n#&gt; [29] │ &lt;It&gt; snowed, rained, and hailed the same morning.\n#&gt; [63] │ &lt;He&gt; ran half way to the hardware store.\n#&gt; [90] │ &lt;He&gt; lay prone and hardly moved a limb.\n#&gt; ... and 57 more\n\nA quick inspection of the results shows that we’re getting some spurious matches. That’s because we’ve forgotten to use parentheses:\n快速检查结果显示，我们得到了一些虚假的匹配。这是因为我们忘记了使用括号：\n\nstr_view(sentences, \"^(She|He|It|They)\\\\b\")\n#&gt;   [3] │ &lt;It&gt;'s easy to tell the depth of a well.\n#&gt;  [29] │ &lt;It&gt; snowed, rained, and hailed the same morning.\n#&gt;  [63] │ &lt;He&gt; ran half way to the hardware store.\n#&gt;  [90] │ &lt;He&gt; lay prone and hardly moved a limb.\n#&gt; [116] │ &lt;He&gt; ordered peach pie with ice cream.\n#&gt; [127] │ &lt;It&gt; caught its hind paw in a rusty trap.\n#&gt; ... and 51 more\n\nYou might wonder how you might spot such a mistake if it didn’t occur in the first few matches. A good technique is to create a few positive and negative matches and use them to test that your pattern works as expected:\n你可能会想，如果这种错误没有出现在前几个匹配项中，你该如何发现它。一个好的技巧是创建一些阳性和阴性匹配，并用它们来测试你的模式是否按预期工作：\n\npos &lt;- c(\"He is a boy\", \"She had a good time\")\nneg &lt;- c(\"Shells come from the sea\", \"Hadley said 'It's a great day'\")\n\npattern &lt;- \"^(She|He|It|They)\\\\b\"\nstr_detect(pos, pattern)\n#&gt; [1] TRUE TRUE\nstr_detect(neg, pattern)\n#&gt; [1] FALSE FALSE\n\nIt’s typically much easier to come up with good positive examples than negative examples, because it takes a while before you’re good enough with regular expressions to predict where your weaknesses are. Nevertheless, they’re still useful: as you work on the problem you can slowly accumulate a collection of your mistakes, ensuring that you never make the same mistake twice.\n通常，想出好的正面例子比负面例子要容易得多，因为你需要一段时间才能熟练掌握正则表达式，从而预测你的弱点在哪里。尽管如此，它们仍然很有用：在解决问题的过程中，你可以慢慢积累你的错误集合，确保你不会犯同样的错误两次。\n\n15.6.2 Boolean operations\nImagine we want to find words that only contain consonants. One technique is to create a character class that contains all letters except for the vowels ([^aeiou]), then allow that to match any number of letters ([^aeiou]+), then force it to match the whole string by anchoring to the beginning and the end (^[^aeiou]+$):\n想象一下，我们想找到只包含辅音的单词。一种方法是创建一个包含除元音外所有字母的字符类 ([^aeiou])，然后让它匹配任意数量的字母 ([^aeiou]+)，最后通过锚定到开头和结尾 (^[^aeiou]+$) 来强制它匹配整个字符串：\n\nstr_view(words, \"^[^aeiou]+$\")\n#&gt; [123] │ &lt;by&gt;\n#&gt; [249] │ &lt;dry&gt;\n#&gt; [328] │ &lt;fly&gt;\n#&gt; [538] │ &lt;mrs&gt;\n#&gt; [895] │ &lt;try&gt;\n#&gt; [952] │ &lt;why&gt;\n\nBut you can make this problem a bit easier by flipping the problem around. Instead of looking for words that contain only consonants, we could look for words that don’t contain any vowels:\n但是你可以通过反向思考来让这个问题变得更简单。与其寻找只包含辅音的单词，我们可以寻找不包含任何元音的单词：\n\nstr_view(words[!str_detect(words, \"[aeiou]\")])\n#&gt; [1] │ by\n#&gt; [2] │ dry\n#&gt; [3] │ fly\n#&gt; [4] │ mrs\n#&gt; [5] │ try\n#&gt; [6] │ why\n\nThis is a useful technique whenever you’re dealing with logical combinations, particularly those involving “and” or “not”. For example, imagine if you want to find all words that contain “a” and “b”. There’s no “and” operator built in to regular expressions so we have to tackle it by looking for all words that contain an “a” followed by a “b”, or a “b” followed by an “a”:\n在处理逻辑组合时，这是一种有用的技术，特别是涉及“与”或“非”的组合。例如，假设你想查找所有同时包含“a”和“b”的单词。正则表达式中没有内置的“与”运算符，所以我们必须通过查找所有包含“a”后跟“b”的单词，或者“b”后跟“a”的单词来解决这个问题：\n\nstr_view(words, \"a.*b|b.*a\")\n#&gt;  [2] │ &lt;ab&gt;le\n#&gt;  [3] │ &lt;ab&gt;out\n#&gt;  [4] │ &lt;ab&gt;solute\n#&gt; [62] │ &lt;availab&gt;le\n#&gt; [66] │ &lt;ba&gt;by\n#&gt; [67] │ &lt;ba&gt;ck\n#&gt; ... and 24 more\n\nIt’s simpler to combine the results of two calls to str_detect():\n将两次调用 str_detect() 的结果结合起来会更简单：\n\nwords[str_detect(words, \"a\") & str_detect(words, \"b\")]\n#&gt;  [1] \"able\"      \"about\"     \"absolute\"  \"available\" \"baby\"      \"back\"     \n#&gt;  [7] \"bad\"       \"bag\"       \"balance\"   \"ball\"      \"bank\"      \"bar\"      \n#&gt; [13] \"base\"      \"basis\"     \"bear\"      \"beat\"      \"beauty\"    \"because\"  \n#&gt; [19] \"black\"     \"board\"     \"boat\"      \"break\"     \"brilliant\" \"britain\"  \n#&gt; [25] \"debate\"    \"husband\"   \"labour\"    \"maybe\"     \"probable\"  \"table\"\n\nWhat if we wanted to see if there was a word that contains all vowels? If we did it with patterns we’d need to generate 5! (120) different patterns:\n如果我们想看看是否有一个单词包含所有元音怎么办？如果用模式来做，我们需要生成 5! (120) 个不同的模式：\n\nwords[str_detect(words, \"a.*e.*i.*o.*u\")]\n# ...\nwords[str_detect(words, \"u.*o.*i.*e.*a\")]\n\nIt’s much simpler to combine five calls to str_detect():\n结合五次调用 str_detect() 要简单得多：\n\nwords[\n  str_detect(words, \"a\") &\n  str_detect(words, \"e\") &\n  str_detect(words, \"i\") &\n  str_detect(words, \"o\") &\n  str_detect(words, \"u\")\n]\n#&gt; character(0)\n\nIn general, if you get stuck trying to create a single regexp that solves your problem, take a step back and think if you could break the problem down into smaller pieces, solving each challenge before moving onto the next one.\n总的来说，如果你在尝试创建一个单一的正则表达式来解决问题时遇到困难，不妨退后一步，思考是否可以将问题分解成更小的部分，先解决每个挑战，然后再进入下一个。\n\n15.6.3 Creating a pattern with code\nWhat if we wanted to find all sentences that mention a color? The basic idea is simple: we just combine alternation with word boundaries.\n如果我们想找到所有提到颜色的 sentences 该怎么办？基本思想很简单：我们只需将交替与词边界结合起来。\n\nstr_view(sentences, \"\\\\b(red|green|blue)\\\\b\")\n#&gt;   [2] │ Glue the sheet to the dark &lt;blue&gt; background.\n#&gt;  [26] │ Two &lt;blue&gt; fish swam in the tank.\n#&gt;  [92] │ A wisp of cloud hung in the &lt;blue&gt; air.\n#&gt; [148] │ The spot on the blotter was made by &lt;green&gt; ink.\n#&gt; [160] │ The sofa cushion is &lt;red&gt; and of light weight.\n#&gt; [174] │ The sky that morning was clear and bright &lt;blue&gt;.\n#&gt; ... and 20 more\n\nBut as the number of colors grows, it would quickly get tedious to construct this pattern by hand. Wouldn’t it be nice if we could store the colors in a vector?\n但是随着颜色数量的增加，手动构建这个模式很快就会变得乏味。如果能把颜色存储在一个向量里，岂不是很好？\n\nrgb &lt;- c(\"red\", \"green\", \"blue\")\n\nWell, we can! We’d just need to create the pattern from the vector using str_c() and str_flatten():\n嗯，我们可以！我们只需要使用 str_c() 和 str_flatten() 从向量创建模式：\n\nstr_c(\"\\\\b(\", str_flatten(rgb, \"|\"), \")\\\\b\")\n#&gt; [1] \"\\\\b(red|green|blue)\\\\b\"\n\nWe could make this pattern more comprehensive if we had a good list of colors. One place we could start from is the list of built-in colors that R can use for plots:\n如果我们有一个好的颜色列表，我们可以使这个模式更全面。一个可以开始的地方是 R 中用于绘图的内置颜色列表：\n\nstr_view(colors())\n#&gt; [1] │ white\n#&gt; [2] │ aliceblue\n#&gt; [3] │ antiquewhite\n#&gt; [4] │ antiquewhite1\n#&gt; [5] │ antiquewhite2\n#&gt; [6] │ antiquewhite3\n#&gt; ... and 651 more\n\nBut lets first eliminate the numbered variants:\n但让我们首先消除带编号的变体：\n\ncols &lt;- colors()\ncols &lt;- cols[!str_detect(cols, \"\\\\d\")]\nstr_view(cols)\n#&gt; [1] │ white\n#&gt; [2] │ aliceblue\n#&gt; [3] │ antiquewhite\n#&gt; [4] │ aquamarine\n#&gt; [5] │ azure\n#&gt; [6] │ beige\n#&gt; ... and 137 more\n\nThen we can turn this into one giant pattern. We won’t show the pattern here because it’s huge, but you can see it working:\n然后我们可以把它变成一个巨大的模式。我们不会在这里显示这个模式，因为它太大了，但你可以看到它的工作效果：\n\npattern &lt;- str_c(\"\\\\b(\", str_flatten(cols, \"|\"), \")\\\\b\")\nstr_view(sentences, pattern)\n#&gt;   [2] │ Glue the sheet to the dark &lt;blue&gt; background.\n#&gt;  [12] │ A rod is used to catch &lt;pink&gt; &lt;salmon&gt;.\n#&gt;  [26] │ Two &lt;blue&gt; fish swam in the tank.\n#&gt;  [66] │ Cars and busses stalled in &lt;snow&gt; drifts.\n#&gt;  [92] │ A wisp of cloud hung in the &lt;blue&gt; air.\n#&gt; [112] │ Leaves turn &lt;brown&gt; and &lt;yellow&gt; in the fall.\n#&gt; ... and 57 more\n\nIn this example, cols only contains numbers and letters so you don’t need to worry about metacharacters. But in general, whenever you create patterns from existing strings it’s wise to run them through str_escape() to ensure they match literally.\n在这个例子中，cols 只包含数字和字母，所以你不需要担心元字符。但总的来说，每当你从现有字符串创建模式时，最好将它们通过 str_escape() 处理，以确保它们是字面匹配。\n\n15.6.4 Exercises\n\n\nFor each of the following challenges, try solving it by using both a single regular expression, and a combination of multiple str_detect() calls.\n\nFind all words that start or end with x.\nFind all words that start with a vowel and end with a consonant.\nAre there any words that contain at least one of each different vowel?\n\n\nConstruct patterns to find evidence for and against the rule “i before e except after c”?\ncolors() contains a number of modifiers like “lightgray” and “darkblue”. How could you automatically identify these modifiers? (Think about how you might detect and then remove the colors that are modified).\nCreate a regular expression that finds any base R dataset. You can get a list of these datasets via a special use of the data() function: data(package = \"datasets\")$results[, \"Item\"]. Note that a number of old datasets are individual vectors; these contain the name of the grouping “data frame” in parentheses, so you’ll need to strip those off.",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Regular expressions</span>"
    ]
  },
  {
    "objectID": "regexps.html#regular-expressions-in-other-places",
    "href": "regexps.html#regular-expressions-in-other-places",
    "title": "15  Regular expressions",
    "section": "\n15.7 Regular expressions in other places",
    "text": "15.7 Regular expressions in other places\nJust like in the stringr and tidyr functions, there are many other places in R where you can use regular expressions. The following sections describe some other useful functions in the wider tidyverse and base R.\n就像在 stringr 和 tidyr 函数中一样，R 中还有许多其他地方可以使用正则表达式。以下各节描述了更广泛的 tidyverse 和基础 R 中的一些其他有用函数。\n\n15.7.1 tidyverse\nThere are three other particularly useful places where you might want to use a regular expressions\n还有另外三个特别有用的地方，你可能想在其中使用正则表达式\n\nmatches(pattern) will select all variables whose name matches the supplied pattern. It’s a “tidyselect” function that you can use anywhere in any tidyverse function that selects variables (e.g., select(), rename_with() and across()).matches(pattern) 将选择所有名称与所提供模式匹配的变量。它是一个“tidyselect”函数，你可以在任何选择变量的 tidyverse 函数（例如 select()、rename_with() 和 across()）中使用它。\npivot_longer()'s names_pattern argument takes a vector of regular expressions, just like separate_wider_regex(). It’s useful when extracting data out of variable names with a complex structurepivot_longer() 的 names_pattern 参数接受一个正则表达式向量，就像 separate_wider_regex() 一样。当从具有复杂结构的变量名中提取数据时，它很有用。\nThe delim argument in separate_longer_delim() and separate_wider_delim() usually matches a fixed string, but you can use regex() to make it match a pattern. This is useful, for example, if you want to match a comma that is optionally followed by a space, i.e. regex(\", ?\").separate_longer_delim() 和 separate_wider_delim() 中的 delim 参数通常匹配一个固定的字符串，但你可以使用 regex() 使其匹配一个模式。这很有用，例如，如果你想匹配一个逗号，后面可以跟一个空格，即 regex(\", ?\")。\n\n15.7.2 Base R\napropos(pattern) searches all objects available from the global environment that match the given pattern. This is useful if you can’t quite remember the name of a function:apropos(pattern) 在全局环境中搜索所有与给定模式匹配的可用对象。如果你记不太清函数名，这个功能会很有用：\n\napropos(\"replace\")\n#&gt; [1] \"%+replace%\"       \"replace\"          \"replace_na\"      \n#&gt; [4] \"setReplaceMethod\" \"str_replace\"      \"str_replace_all\" \n#&gt; [7] \"str_replace_na\"   \"theme_replace\"\n\nlist.files(path, pattern) lists all files in path that match a regular expression pattern. For example, you can find all the R Markdown files in the current directory with:list.files(path, pattern) 列出 path 中所有匹配正则表达式 pattern 的文件。例如，你可以用以下命令找到当前目录中所有的 R Markdown 文件：\n\nhead(list.files(pattern = \"\\\\.Rmd$\"))\n#&gt; character(0)\n\nIt’s worth noting that the pattern language used by base R is very slightly different to that used by stringr. That’s because stringr is built on top of the stringi package, which is in turn built on top of the ICU engine, whereas base R functions use either the TRE engine or the PCRE engine, depending on whether or not you’ve set perl = TRUE. Fortunately, the basics of regular expressions are so well established that you’ll encounter few variations when working with the patterns you’ll learn in this book. You only need to be aware of the difference when you start to rely on advanced features like complex Unicode character ranges or special features that use the (?…) syntax.\n值得注意的是，基础 R 使用的模式语言与 stringr 使用的略有不同。这是因为 stringr 是建立在 stringi 包 之上的，而 stringi 包又是建立在 ICU 引擎 之上的，而基础 R 函数则使用 TRE 引擎 或 PCRE 引擎，这取决于你是否设置了 perl = TRUE。幸运的是，正则表达式的基础知识已经非常成熟，因此在使用本书中学到的模式时，你几乎不会遇到什么变化。你只需要在开始依赖高级功能（如复杂的 Unicode 字符范围或使用 (?…) 语法的特殊功能）时意识到这种差异。",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Regular expressions</span>"
    ]
  },
  {
    "objectID": "regexps.html#summary",
    "href": "regexps.html#summary",
    "title": "15  Regular expressions",
    "section": "\n15.8 Summary",
    "text": "15.8 Summary\nWith every punctuation character potentially overloaded with meaning, regular expressions are one of the most compact languages out there. They’re definitely confusing at first but as you train your eyes to read them and your brain to understand them, you unlock a powerful skill that you can use in R and in many other places.\n由于每个标点符号都可能被赋予多重含义，正则表达式是现存最紧凑的语言之一。它们起初确实令人困惑，但随着你训练眼睛去阅读它们、训练大脑去理解它们，你将解锁一项强大的技能，可以在 R 和许多其他地方使用。\nIn this chapter, you’ve started your journey to become a regular expression master by learning the most useful stringr functions and the most important components of the regular expression language. And there are plenty of resources to learn more.\n在本章中，你通过学习最实用的 stringr 函数和正则表达式语言最重要的组成部分，开启了成为正则表达式大师的旅程。并且有大量的资源可以让你学习更多。\nA good place to start is vignette(\"regular-expressions\", package = \"stringr\"): it documents the full set of syntax supported by stringr. Another useful reference is https://www.regular-expressions.info/. It’s not R specific, but you can use it to learn about the most advanced features of regexes and how they work under the hood.\n一个好的起点是 vignette(\"regular-expressions\", package = \"stringr\")：它记录了 stringr 支持的完整语法集。另一个有用的参考是 https://www.regular-expressions.info/。它并非 R 专属，但你可以用它来学习正则表达式最高级的功能以及它们在底层是如何工作的。\nIt’s also good to know that stringr is implemented on top of the stringi package by Marek Gagolewski. If you’re struggling to find a function that does what you need in stringr, don’t be afraid to look in stringi. You’ll find stringi very easy to pick up because it follows many of the the same conventions as stringr.\n了解 stringr 是由 Marek Gagolewski 在 stringi 包之上实现的也很有好处。如果你在 stringr 中找不到所需功能的函数，不要害怕去 stringi 中寻找。你会发现 stringi 非常容易上手，因为它遵循了许多与 stringr 相同的约定。\nIn the next chapter, we’ll talk about a data structure closely related to strings: factors. Factors are used to represent categorical data in R, i.e. data with a fixed and known set of possible values identified by a vector of strings.\n在下一章中，我们将讨论一种与字符串密切相关的数据结构：因子 (factors)。因子用于在 R 中表示分类数据，即具有一组固定的、已知的可能值的数据，这些值由一个字符串向量标识。",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Regular expressions</span>"
    ]
  },
  {
    "objectID": "regexps.html#footnotes",
    "href": "regexps.html#footnotes",
    "title": "15  Regular expressions",
    "section": "",
    "text": "You can pronounce it with either a hard-g (reg-x) or a soft-g (rej-x).↩︎\nYou’ll learn how to escape these special meanings in Section 15.4.1.↩︎\nWell, any character apart from \\n.↩︎\nThis gives us the proportion of names that contain an “x”; if you wanted the proportion of babies with a name containing an x, you’d need to perform a weighted mean.↩︎\nWe wish we could reassure you that you’d never see something this weird in real life, but unfortunately over the course of your career you’re likely to see much weirder!↩︎\nThe complete set of metacharacters is .^$\\|*+?{}[]()↩︎\nRemember, to create a regular expression containing \\d or \\s, you’ll need to escape the \\ for the string, so you’ll type \"\\\\d\" or \"\\\\s\".↩︎\nMostly because we never discuss matrices in this book!↩︎\ncomments = TRUE is particularly effective in combination with a raw string, as we use here.↩︎",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Regular expressions</span>"
    ]
  },
  {
    "objectID": "factors.html",
    "href": "factors.html",
    "title": "16  Factors",
    "section": "",
    "text": "16.1 Introduction\nFactors are used for categorical variables, variables that have a fixed and known set of possible values.\n因子 (factor) 用于处理分类变量 (categorical variable)，即取值范围是固定的、已知的有限集合的变量。\nThey are also useful when you want to display character vectors in a non-alphabetical order.\n当你想要以非字母顺序显示字符向量时，因子也很有用。\nWe’ll start by motivating why factors are needed for data analysis1 and how you can create them with factor().\n我们将首先阐述为何数据分析需要因子1，以及如何使用 factor() 函数创建它们。\nWe’ll then introduce you to the gss_cat dataset which contains a bunch of categorical variables to experiment with.\n接着，我们将向你介绍 gss_cat 数据集，其中包含许多分类变量可供你进行实验。\nYou’ll then use that dataset to practice modifying the order and values of factors, before we finish up with a discussion of ordered factors.\n然后，你将使用该数据集练习修改因子的顺序和值，最后我们将讨论有序因子 (ordered factor)。",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "factors.html#introduction",
    "href": "factors.html#introduction",
    "title": "16  Factors",
    "section": "",
    "text": "16.1.1 Prerequisites\nBase R provides some basic tools for creating and manipulating factors.\n基础 R 提供了一些用于创建和操作因子的基本工具。\nWe’ll supplement these with the forcats package, which is part of the core tidyverse.\n我们将使用 forcats 包作为补充，它是核心 tidyverse 的一部分。\nIt provides tools for dealing with categorical variables (and it’s an anagram of factors!) using a wide range of helpers for working with factors.\n它提供了一系列处理分类 ( categorical ) 变量的工具 ( forcats 是 factors 的变位词！)，包含了大量用于处理因子的辅助函数。\n\nlibrary(tidyverse)",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "factors.html#factor-basics",
    "href": "factors.html#factor-basics",
    "title": "16  Factors",
    "section": "\n16.2 Factor basics",
    "text": "16.2 Factor basics\nImagine that you have a variable that records month:\n想象一下，你有一个记录月份的变量：\n\nx1 &lt;- c(\"Dec\", \"Apr\", \"Jan\", \"Mar\")\n\nUsing a string to record this variable has two problems:\n使用字符串来记录此变量存在两个问题：\n\n\nThere are only twelve possible months, and there’s nothing saving you from typos:\n只有十二个可能的月份，但没有任何机制可以防止你输入错误：\n\nx2 &lt;- c(\"Dec\", \"Apr\", \"Jam\", \"Mar\")\n\n\n\nIt doesn’t sort in a useful way:\n它的排序方式没什么用：\n\nsort(x1)\n#&gt; [1] \"Apr\" \"Dec\" \"Jan\" \"Mar\"\n\n\n\nYou can fix both of these problems with a factor.\n你可以用因子来解决这两个问题。\nTo create a factor you must start by creating a list of the valid levels:\n要创建一个因子，你必须首先创建一个有效水平 (levels) 的列表：\n\nmonth_levels &lt;- c(\n  \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n  \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"\n)\n\nNow you can create a factor:\n现在你可以创建一个因子：\n\ny1 &lt;- factor(x1, levels = month_levels)\ny1\n#&gt; [1] Dec Apr Jan Mar\n#&gt; Levels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n\nsort(y1)\n#&gt; [1] Jan Mar Apr Dec\n#&gt; Levels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n\nAnd any values not in the level will be silently converted to NA:\n任何不在水平中的值都将被静默地转换为 NA：\n\ny2 &lt;- factor(x2, levels = month_levels)\ny2\n#&gt; [1] Dec  Apr  &lt;NA&gt; Mar \n#&gt; Levels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n\nThis seems risky, so you might want to use forcats::fct() instead:\n这看起来有风险，所以你可能想改用 forcats::fct()：\n\ny2 &lt;- fct(x2, levels = month_levels)\n#&gt; Error in `fct()`:\n#&gt; ! All values of `x` must appear in `levels` or `na`\n#&gt; ℹ Missing level: \"Jam\"\n\nIf you omit the levels, they’ll be taken from the data in alphabetical order:\n如果你省略了水平 (levels)，它们将按字母顺序从数据中提取：\n\nfactor(x1)\n#&gt; [1] Dec Apr Jan Mar\n#&gt; Levels: Apr Dec Jan Mar\n\nSorting alphabetically is slightly risky because not every computer will sort strings in the same way.\n按字母顺序排序有点风险，因为并非每台计算机都以相同的方式对字符串进行排序。\nSo forcats::fct() orders by first appearance:\n因此，forcats::fct() 会按照首次出现的顺序进行排序：\n\nfct(x1)\n#&gt; [1] Dec Apr Jan Mar\n#&gt; Levels: Dec Apr Jan Mar\n\nIf you ever need to access the set of valid levels directly, you can do so with levels():\n如果你需要直接访问有效的水平集合，可以使用 levels() 函数：\n\nlevels(y2)\n#&gt;  [1] \"Jan\" \"Feb\" \"Mar\" \"Apr\" \"May\" \"Jun\" \"Jul\" \"Aug\" \"Sep\" \"Oct\" \"Nov\" \"Dec\"\n\nYou can also create a factor when reading your data with readr with col_factor():\n你也可以在使用 readr 读取数据时，通过 col_factor() 来创建因子：\n\ncsv &lt;- \"\nmonth,value\nJan,12\nFeb,56\nMar,12\"\n\ndf &lt;- read_csv(csv, col_types = cols(month = col_factor(month_levels)))\ndf$month\n#&gt; [1] Jan Feb Mar\n#&gt; Levels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "factors.html#general-social-survey",
    "href": "factors.html#general-social-survey",
    "title": "16  Factors",
    "section": "\n16.3 General Social Survey",
    "text": "16.3 General Social Survey\nFor the rest of this chapter, we’re going to use forcats::gss_cat.\n在本章的其余部分，我们将使用 forcats::gss_cat 数据集。\nIt’s a sample of data from the General Social Survey, a long-running US survey conducted by the independent research organization NORC at the University of Chicago.\n它来自 General Social Survey 的数据样本，这是由芝加哥大学的独立研究机构 NORC 进行的一项长期美国调查。\nThe survey has thousands of questions, so in gss_cat Hadley selected a handful that will illustrate some common challenges you’ll encounter when working with factors.\n该调查包含数千个问题，因此在 gss_cat 中，Hadley 选择了一些能够说明你在使用因子时会遇到的一些常见挑战的问题。\n\ngss_cat\n#&gt; # A tibble: 21,483 × 9\n#&gt;    year marital         age race  rincome        partyid           \n#&gt;   &lt;int&gt; &lt;fct&gt;         &lt;int&gt; &lt;fct&gt; &lt;fct&gt;          &lt;fct&gt;             \n#&gt; 1  2000 Never married    26 White $8000 to 9999  Ind,near rep      \n#&gt; 2  2000 Divorced         48 White $8000 to 9999  Not str republican\n#&gt; 3  2000 Widowed          67 White Not applicable Independent       \n#&gt; 4  2000 Never married    39 White Not applicable Ind,near rep      \n#&gt; 5  2000 Divorced         25 White Not applicable Not str democrat  \n#&gt; 6  2000 Married          25 White $20000 - 24999 Strong democrat   \n#&gt; # ℹ 21,477 more rows\n#&gt; # ℹ 3 more variables: relig &lt;fct&gt;, denom &lt;fct&gt;, tvhours &lt;int&gt;\n\n(Remember, since this dataset is provided by a package, you can get more information about the variables with ?gss_cat.)\n（请记住，由于该数据集是由一个包提供的，你可以使用 ?gss_cat 获取有关变量的更多信息。）\nWhen factors are stored in a tibble, you can’t see their levels so easily.\n当因子存储在 tibble 中时，你无法轻易看到它们的水平。\nOne way to view them is with count():\n查看它们的一种方法是使用 count()：\n\ngss_cat |&gt;\n  count(race)\n#&gt; # A tibble: 3 × 2\n#&gt;   race      n\n#&gt;   &lt;fct&gt; &lt;int&gt;\n#&gt; 1 Other  1959\n#&gt; 2 Black  3129\n#&gt; 3 White 16395\n\nWhen working with factors, the two most common operations are changing the order of the levels, and changing the values of the levels.\n在使用因子时，两个最常见的操作是更改水平的顺序和更改水平的值。\nThose operations are described in the sections below.\n这些操作将在下面的章节中描述。\n\n16.3.1 Exercises\n\nExplore the distribution of rincome (reported income). What makes the default bar chart hard to understand? How could you improve the plot?\nWhat is the most common relig in this survey? What’s the most common partyid?\nWhich relig does denom (denomination) apply to? How can you find out with a table? How can you find out with a visualization?",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "factors.html#sec-modifying-factor-order",
    "href": "factors.html#sec-modifying-factor-order",
    "title": "16  Factors",
    "section": "\n16.4 Modifying factor order",
    "text": "16.4 Modifying factor order\nIt’s often useful to change the order of the factor levels in a visualization.\n在可视化中，更改因子水平的顺序通常很有用。\nFor example, imagine you want to explore the average number of hours spent watching TV per day across religions:\n例如，假设你想探究不同宗教每天看电视的平均小时数：\n\nrelig_summary &lt;- gss_cat |&gt;\n  group_by(relig) |&gt;\n  summarize(\n    tvhours = mean(tvhours, na.rm = TRUE),\n    n = n()\n  )\n\nggplot(relig_summary, aes(x = tvhours, y = relig)) +\n  geom_point()\n\n\n\n\n\n\n\nIt is hard to read this plot because there’s no overall pattern.\n这张图很难解读，因为没有明显的整体模式。\nWe can improve it by reordering the levels of relig using fct_reorder().\n我们可以通过使用 fct_reorder() 重新排序 relig 的水平来改进它。\nfct_reorder() takes three arguments:fct_reorder() 接受三个参数：\n\n.f, the factor whose levels you want to modify.\n.f, 你想要修改其水平的因子。\n.x, a numeric vector that you want to use to reorder the levels.\n.x, 一个你想要用来重新排序水平的数值向量。\nOptionally, .fun, a function that’s used if there are multiple values of .x for each value of .f. The default value is median.\n可选的 .fun, 一个函数，当 .f 的每个值对应多个 .x 值时使用。默认值为 median。\n\n\nggplot(relig_summary, aes(x = tvhours, y = fct_reorder(relig, tvhours))) +\n  geom_point()\n\n\n\n\n\n\n\nReordering religion makes it much easier to see that people in the “Don’t know” category watch much more TV, and Hinduism & Other Eastern religions watch much less.\n对宗教进行重新排序后，我们可以更容易地看出，“Don’t know” 类别的人看电视的时间要多得多，而印度教 (Hinduism) 和其他东方宗教 (Other Eastern religions) 的人看电视的时间则少得多。\nAs you start making more complicated transformations, we recommend moving them out of aes() and into a separate mutate() step.\n当你开始进行更复杂的转换时，我们建议将它们从 aes() 中移出，放到一个单独的 mutate() 步骤中。\nFor example, you could rewrite the plot above as:\n例如，你可以将上面的图重写为：\n\nrelig_summary |&gt;\n  mutate(\n    relig = fct_reorder(relig, tvhours)\n  ) |&gt;\n  ggplot(aes(x = tvhours, y = relig)) +\n  geom_point()\n\nWhat if we create a similar plot looking at how average age varies across reported income level?\n如果我们创建一个类似的图，来观察平均年龄在不同报告收入水平上的变化情况，会怎么样？\n\nrincome_summary &lt;- gss_cat |&gt;\n  group_by(rincome) |&gt;\n  summarize(\n    age = mean(age, na.rm = TRUE),\n    n = n()\n  )\n\nggplot(rincome_summary, aes(x = age, y = fct_reorder(rincome, age))) +\n  geom_point()\n\n\n\n\n\n\n\nHere, arbitrarily reordering the levels isn’t a good idea!\n在这里，任意地重新排序水平不是一个好主意！\nThat’s because rincome already has a principled order that we shouldn’t mess with.\n这是因为 rincome 已经有了一个我们不应该打乱的原则性顺序。\nReserve fct_reorder() for factors whose levels are arbitrarily ordered.\n请将 fct_reorder() 用于那些水平是任意排序的因子。\nHowever, it does make sense to pull “Not applicable” to the front with the other special levels.\n然而，将 “Not applicable” 和其他特殊水平一起移到最前面是合理的。\nYou can use fct_relevel().\n你可以使用 fct_relevel()。\nIt takes a factor, .f, and then any number of levels that you want to move to the front of the line.\n它接受一个因子 .f，以及任意数量你想要移动到最前面的水平。\n\nggplot(rincome_summary, aes(x = age, y = fct_relevel(rincome, \"Not applicable\"))) +\n  geom_point()\n\n\n\n\n\n\n\nWhy do you think the average age for “Not applicable” is so high?\n你认为 “Not applicable” 的平均年龄为什么这么高？\nAnother type of reordering is useful when you are coloring the lines on a plot.\n当你在图上为线条着色时，另一种重排序也很有用。\nfct_reorder2(.f, .x, .y) reorders the factor .f by the .y values associated with the largest .x values.fct_reorder2(.f, .x, .y) 会根据与最大 .x 值相关联的 .y 值来对因子 .f 进行重排序。\nThis makes the plot easier to read because the colors of the line at the far right of the plot will line up with the legend.\n这使得图更容易阅读，因为图最右侧的线条颜色将与图例对齐。\nby_age &lt;- gss_cat |&gt;\n  filter(!is.na(age)) |&gt;\n  count(age, marital) |&gt;\n  group_by(age) |&gt;\n  mutate(\n    prop = n / sum(n)\n  )\n\nggplot(by_age, aes(x = age, y = prop, color = marital)) +\n  geom_line(linewidth = 1) +\n  scale_color_brewer(palette = \"Set1\")\n\nggplot(by_age, aes(x = age, y = prop, color = fct_reorder2(marital, age, prop))) +\n  geom_line(linewidth = 1) +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(color = \"marital\")\n\n\n\n\n\n\n\n\n\n\nFinally, for bar plots, you can use fct_infreq() to order levels in decreasing frequency: this is the simplest type of reordering because it doesn’t need any extra variables.\n最后，对于条形图，你可以使用 fct_infreq() 按频率递减的顺序排列水平：这是最简单的重排序类型，因为它不需要任何额外的变量。\nCombine it with fct_rev() if you want them in increasing frequency so that in the bar plot largest values are on the right, not the left.\n如果你希望它们按频率递增的顺序排列，以便在条形图中最大的值在右边而不是左边，可以将其与 fct_rev() 结合使用。\n\ngss_cat |&gt;\n  mutate(marital = marital |&gt; fct_infreq() |&gt; fct_rev()) |&gt;\n  ggplot(aes(x = marital)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n16.4.1 Exercises\n\nThere are some suspiciously high numbers in tvhours. Is the mean a good summary?\nFor each factor in gss_cat identify whether the order of the levels is arbitrary or principled.\nWhy did moving “Not applicable” to the front of the levels move it to the bottom of the plot?",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "factors.html#modifying-factor-levels",
    "href": "factors.html#modifying-factor-levels",
    "title": "16  Factors",
    "section": "\n16.5 Modifying factor levels",
    "text": "16.5 Modifying factor levels\nMore powerful than changing the orders of the levels is changing their values.\n比更改水平顺序更强大的是更改它们的值。\nThis allows you to clarify labels for publication, and collapse levels for high-level displays.\n这可以让你为出版物澄清标签，并为高层次的展示折叠水平。\nThe most general and powerful tool is fct_recode().\n最通用和最强大的工具是 fct_recode()。\nIt allows you to recode, or change, the value of each level.\n它允许你重新编码 (recode)，或更改每个水平的值。\nFor example, take the partyid variable from the gss_cat data frame:\n例如，以 gss_cat 数据框中的 partyid 变量为例：\n\ngss_cat |&gt; count(partyid)\n#&gt; # A tibble: 10 × 2\n#&gt;   partyid                n\n#&gt;   &lt;fct&gt;              &lt;int&gt;\n#&gt; 1 No answer            154\n#&gt; 2 Don't know             1\n#&gt; 3 Other party          393\n#&gt; 4 Strong republican   2314\n#&gt; 5 Not str republican  3032\n#&gt; 6 Ind,near rep        1791\n#&gt; # ℹ 4 more rows\n\nThe levels are terse and inconsistent.\n这些水平既简洁又不一致。\nLet’s tweak them to be longer and use a parallel construction.\n让我们调整它们，使其更长并使用并列结构。\nLike most rename and recoding functions in the tidyverse, the new values go on the left and the old values go on the right:\n像 tidyverse 中大多数重命名和重新编码的函数一样，新值在左边，旧值在右边：\n\ngss_cat |&gt;\n  mutate(\n    partyid = fct_recode(partyid,\n      \"Republican, strong\"    = \"Strong republican\",\n      \"Republican, weak\"      = \"Not str republican\",\n      \"Independent, near rep\" = \"Ind,near rep\",\n      \"Independent, near dem\" = \"Ind,near dem\",\n      \"Democrat, weak\"        = \"Not str democrat\",\n      \"Democrat, strong\"      = \"Strong democrat\"\n    )\n  ) |&gt;\n  count(partyid)\n#&gt; # A tibble: 10 × 2\n#&gt;   partyid                   n\n#&gt;   &lt;fct&gt;                 &lt;int&gt;\n#&gt; 1 No answer               154\n#&gt; 2 Don't know                1\n#&gt; 3 Other party             393\n#&gt; 4 Republican, strong     2314\n#&gt; 5 Republican, weak       3032\n#&gt; 6 Independent, near rep  1791\n#&gt; # ℹ 4 more rows\n\nfct_recode() will leave the levels that aren’t explicitly mentioned as is, and will warn you if you accidentally refer to a level that doesn’t exist.fct_recode() 会保持未明确提及的水平不变，并且如果你意外引用了一个不存在的水平，它会发出警告。\nTo combine groups, you can assign multiple old levels to the same new level:\n要合并组，你可以将多个旧水平分配给同一个新水平：\n\ngss_cat |&gt;\n  mutate(\n    partyid = fct_recode(partyid,\n      \"Republican, strong\"    = \"Strong republican\",\n      \"Republican, weak\"      = \"Not str republican\",\n      \"Independent, near rep\" = \"Ind,near rep\",\n      \"Independent, near dem\" = \"Ind,near dem\",\n      \"Democrat, weak\"        = \"Not str democrat\",\n      \"Democrat, strong\"      = \"Strong democrat\",\n      \"Other\"                 = \"No answer\",\n      \"Other\"                 = \"Don't know\",\n      \"Other\"                 = \"Other party\"\n    )\n  )\n\nUse this technique with care: if you group together categories that are truly different you will end up with misleading results.\n请谨慎使用此技术：如果将真正不同的类别组合在一起，你将得到误导性的结果。\nIf you want to collapse a lot of levels, fct_collapse() is a useful variant of fct_recode().\n如果你想折叠许多水平，fct_collapse() 是 fct_recode() 的一个有用变体。\nFor each new variable, you can provide a vector of old levels:\n对于每个新变量，你可以提供一个旧水平的向量：\n\ngss_cat |&gt;\n  mutate(\n    partyid = fct_collapse(partyid,\n      \"other\" = c(\"No answer\", \"Don't know\", \"Other party\"),\n      \"rep\" = c(\"Strong republican\", \"Not str republican\"),\n      \"ind\" = c(\"Ind,near rep\", \"Independent\", \"Ind,near dem\"),\n      \"dem\" = c(\"Not str democrat\", \"Strong democrat\")\n    )\n  ) |&gt;\n  count(partyid)\n#&gt; # A tibble: 4 × 2\n#&gt;   partyid     n\n#&gt;   &lt;fct&gt;   &lt;int&gt;\n#&gt; 1 other     548\n#&gt; 2 rep      5346\n#&gt; 3 ind      8409\n#&gt; 4 dem      7180\n\nSometimes you just want to lump together the small groups to make a plot or table simpler.\n有时你只是想把小的组合并在一起，使图表或表格更简单。\nThat’s the job of the fct_lump_*() family of functions.\n这是 fct_lump_*() 系列函数的工作。\nfct_lump_lowfreq() is a simple starting point that progressively lumps the smallest groups categories into “Other”, always keeping “Other” as the smallest category.fct_lump_lowfreq() 是一个简单的起点，它会逐步将最小的组类别合并到 “Other” 中，并始终保持 “Other” 是最小的类别。\n\ngss_cat |&gt;\n  mutate(relig = fct_lump_lowfreq(relig)) |&gt;\n  count(relig)\n#&gt; # A tibble: 2 × 2\n#&gt;   relig          n\n#&gt;   &lt;fct&gt;      &lt;int&gt;\n#&gt; 1 Protestant 10846\n#&gt; 2 Other      10637\n\nIn this case it’s not very helpful: it is true that the majority of Americans in this survey are Protestant, but we’d probably like to see some more details!\n在这种情况下，它不是很有用：确实，这项调查中的大多数美国人是新教徒 (Protestant)，但我们可能想看到更多细节！\nInstead, we can use the fct_lump_n() to specify that we want exactly 10 groups:\n相反，我们可以使用 fct_lump_n() 来指定我们想要正好 10 个组：\n\ngss_cat |&gt;\n  mutate(relig = fct_lump_n(relig, n = 10)) |&gt;\n  count(relig, sort = TRUE)\n#&gt; # A tibble: 10 × 2\n#&gt;   relig          n\n#&gt;   &lt;fct&gt;      &lt;int&gt;\n#&gt; 1 Protestant 10846\n#&gt; 2 Catholic    5124\n#&gt; 3 None        3523\n#&gt; 4 Christian    689\n#&gt; 5 Other        458\n#&gt; 6 Jewish       388\n#&gt; # ℹ 4 more rows\n\nRead the documentation to learn about fct_lump_min() and fct_lump_prop() which are useful in other cases.\n阅读文档以了解 fct_lump_min() 和 fct_lump_prop()，它们在其他情况下也很有用。\n\n16.5.1 Exercises\n\nHow have the proportions of people identifying as Democrat, Republican, and Independent changed over time?\nHow could you collapse rincome into a small set of categories?\nNotice there are 9 groups (excluding other) in the fct_lump example above. Why not 10? (Hint: type ?fct_lump, and find the default for the argument other_level is “Other”.)",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "factors.html#sec-ordered-factors",
    "href": "factors.html#sec-ordered-factors",
    "title": "16  Factors",
    "section": "\n16.6 Ordered factors",
    "text": "16.6 Ordered factors\nBefore we continue, it’s important to briefly mention a special type of factor: ordered factors.\n在继续之前，有必要简要提及一种特殊的因子：有序因子 (ordered factor)。\nCreated with the ordered() function, ordered factors imply a strict ordering between levels, but don’t specify anything about the magnitude of the differences between the levels.\n有序因子是使用 ordered() 函数创建的，它意味着水平之间存在严格的排序，但没有指定水平之间差异的大小。\nYou use ordered factors when you know there the levels are ranked, but there’s no precise numerical ranking.\n当你知道水平有排名但没有精确的数值排名时，就可以使用有序因子。\nYou can identify an ordered factor when its printed because it uses &lt; symbols between the factor levels:\n你可以通过打印输出来识别有序因子，因为它在因子水平之间使用了 &lt; 符号：\n\nordered(c(\"a\", \"b\", \"c\"))\n#&gt; [1] a b c\n#&gt; Levels: a &lt; b &lt; c\n\nIn both base R and the tidyverse, ordered factors behave very similarly to regular factors.\n在基础 R 和 tidyverse 中，有序因子的行为与常规因子非常相似。\nThere are only two places where you might notice different behavior:\n只有在两个地方你可能会注意到不同的行为：\n\nIf you map an ordered factor to color or fill in ggplot2, it will default to scale_color_viridis()/scale_fill_viridis(), a color scale that implies a ranking.\n如果你在 ggplot2 中将有序因子映射到颜色或填充，它将默认为 scale_color_viridis() / scale_fill_viridis()，这是一个暗示排名的色阶。\nIf you use an ordered predictor in a linear model, it will use “polynomial contrasts”. These are mildly useful, but you are unlikely to have heard of them unless you have a PhD in Statistics, and even then you probably don’t routinely interpret them. If you want to learn more, we recommend vignette(\"contrasts\", package = \"faux\") by Lisa DeBruine.\n如果你在线性模型中使用有序预测变量，它将使用“多项式对比” (polynomial contrasts)。这些有些用处，但除非你拥有统计学博士学位，否则你不太可能听说过它们，即使那样，你可能也不会常规地解释它们。如果你想了解更多，我们推荐 Lisa DeBruine 的 vignette(\"contrasts\", package = \"faux\")。\n\nFor the purposes of this book, correctly distinguishing between regular and ordered factors is not particularly important.\n就本书而言，正确区分常规因子和有序因子并非特别重要。\nMore broadly, however, certain fields (particularly the social sciences) do use ordered factors extensively.\n然而，在更广泛的范围内，某些领域（特别是社会科学）确实广泛使用有序因子。\nIn these contexts, it’s important to correctly identify them so that other analysis packages can offer the appropriate behavior.\n在这些情况下，正确识别它们非常重要，以便其他分析包可以提供适当的行为。",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "factors.html#summary",
    "href": "factors.html#summary",
    "title": "16  Factors",
    "section": "\n16.7 Summary",
    "text": "16.7 Summary\nThis chapter introduced you to the handy forcats package for working with factors, introducing you to the most commonly used functions.\n本章向你介绍了用于处理因子的便捷 forcats 包，并介绍了最常用的函数。\nforcats contains a wide range of other helpers that we didn’t have space to discuss here, so whenever you’re facing a factor analysis challenge that you haven’t encountered before, I highly recommend skimming the reference index to see if there’s a canned function that can help solve your problem.\nforcats 包含许多我们在此没有篇幅讨论的其他辅助函数，因此，当你遇到以前从未见过的因子分析挑战时，我强烈建议你浏览参考索引，看看是否有现成的函数可以帮助你解决问题。\nIf you want to learn more about factors after reading this chapter, we recommend reading Amelia McNamara and Nicholas Horton’s paper, Wrangling categorical data in R.\n如果你在阅读本章后想了解更多关于因子的知识，我们建议阅读 Amelia McNamara 和 Nicholas Horton 的论文，Wrangling categorical data in R。\nThis paper lays out some of the history discussed in stringsAsFactors: An unauthorized biography and stringsAsFactors = &lt;sigh&gt;, and compares the tidy approaches to categorical data outlined in this book with base R methods.\n该论文阐述了 stringsAsFactors: An unauthorized biography 和 stringsAsFactors = &lt;sigh&gt; 中讨论的一些历史，并比较了本书中概述的处理分类数据的整洁方法与基础 R 的方法。\nAn early version of the paper helped motivate and scope the forcats package; thanks Amelia & Nick!\n该论文的早期版本帮助激发并确定了 forcats 包的范围；感谢 Amelia 和 Nick！\nIn the next chapter we’ll switch gears to start learning about dates and times in R.\n在下一章中，我们将转换主题，开始学习 R 中的日期和时间。\nDates and times seem deceptively simple, but as you’ll soon see, the more you learn about them, the more complex they seem to get!\n日期和时间看起来似乎很简单，但你很快就会发现，你对它们了解得越多，它们似乎就越复杂！",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "factors.html#footnotes",
    "href": "factors.html#footnotes",
    "title": "16  Factors",
    "section": "",
    "text": "They’re also really important for modelling.↩︎",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "datetimes.html",
    "href": "datetimes.html",
    "title": "17  Dates and times",
    "section": "",
    "text": "17.1 Introduction\nThis chapter will show you how to work with dates and times in R. At first glance, dates and times seem simple. You use them all the time in your regular life, and they don’t seem to cause much confusion. However, the more you learn about dates and times, the more complicated they seem to get!\n本章将向你展示如何在 R 中处理日期和时间。乍一看，日期和时间似乎很简单。你在日常生活中一直使用它们，而且它们似乎没有引起太多困惑。然而，你对日期和时间了解得越多，它们似乎就变得越复杂！\nTo warm up think about how many days there are in a year, and how many hours there are in a day. You probably remembered that most years have 365 days, but leap years have 366. Do you know the full rule for determining if a year is a leap year1? The number of hours in a day is a little less obvious: most days have 24 hours, but in places that use daylight saving time (DST), one day each year has 23 hours and another has 25.\n为了热身，想一想一年有多少天，一天有多少小时。你可能记得大多数年份有 365 天，但闰年有 366 天。你知道判断一年是否是闰年的完整规则吗1？一天中的小时数则不那么明显：大多数日子有 24 小时，但在使用夏令时 (Daylight Saving Time, DST) 的地方，每年有一天是 23 小时，另一天是 25 小时。\nDates and times are hard because they have to reconcile two physical phenomena (the rotation of the Earth and its orbit around the sun) with a whole raft of geopolitical phenomena including months, time zones, and DST. This chapter won’t teach you every last detail about dates and times, but it will give you a solid grounding of practical skills that will help you with common data analysis challenges.\n日期和时间之所以困难，是因为它们必须调和两种物理现象（地球的自转和绕太阳的公转）与一系列地缘政治现象，包括月份、时区和夏令时。本章不会教你关于日期和时间的每一个细节，但它会为你提供坚实的实践技能基础，帮助你应对常见的数据分析挑战。\nWe’ll begin by showing you how to create date-times from various inputs, and then once you’ve got a date-time, how you can extract components like year, month, and day. We’ll then dive into the tricky topic of working with time spans, which come in a variety of flavors depending on what you’re trying to do. We’ll conclude with a brief discussion of the additional challenges posed by time zones.\n我们将首先向你展示如何从各种输入创建日期时间，然后一旦你有了日期时间，你将学习如何提取年、月、日等组件。接着，我们将深入探讨处理时间跨度这个棘手的话题，它根据你的不同需求有多种形式。最后，我们将简要讨论时区带来的额外挑战。",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Dates and times</span>"
    ]
  },
  {
    "objectID": "datetimes.html#introduction",
    "href": "datetimes.html#introduction",
    "title": "17  Dates and times",
    "section": "",
    "text": "17.1.1 Prerequisites\nThis chapter will focus on the lubridate package, which makes it easier to work with dates and times in R. As of the latest tidyverse release, lubridate is part of core tidyverse. We will also need nycflights13 for practice data.\n本章将重点介绍 lubridate 包，它使得在 R 中处理日期和时间变得更加容易。从最新的 tidyverse 版本开始，lubridate 已成为核心 tidyverse 的一部分。我们还将需要 nycflights13 包来获取练习数据。\n\nlibrary(tidyverse)\nlibrary(nycflights13)",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Dates and times</span>"
    ]
  },
  {
    "objectID": "datetimes.html#sec-creating-datetimes",
    "href": "datetimes.html#sec-creating-datetimes",
    "title": "17  Dates and times",
    "section": "\n17.2 Creating date/times",
    "text": "17.2 Creating date/times\nThere are three types of date/time data that refer to an instant in time:\n有三种类型的日期/时间数据可以指代一个时间点：\n\nA date. Tibbles print this as &lt;date&gt;.date (日期)。Tibbles 将其打印为 &lt;date&gt;。\nA time within a day. Tibbles print this as &lt;time&gt;.\n一天中的 time (时间)。Tibbles 将其打印为 &lt;time&gt;。\nA date-time is a date plus a time: it uniquely identifies an instant in time (typically to the nearest second). Tibbles print this as &lt;dttm&gt;. Base R calls these POSIXct, but that doesn’t exactly trip off the tongue.date-time (日期时间) 是日期加上时间：它唯一地标识了一个时间点（通常精确到秒）。Tibbles 将其打印为 &lt;dttm&gt;。基础 R 称之为 POSIXct，但这名字并不怎么上口。\n\nIn this chapter we are going to focus on dates and date-times as R doesn’t have a native class for storing times. If you need one, you can use the hms package.\n在本章中，我们将专注于日期和日期时间，因为 R 没有用于存储时间的原生类。如果你需要，可以使用 hms 包。\nYou should always use the simplest possible data type that works for your needs. That means if you can use a date instead of a date-time, you should. Date-times are substantially more complicated because of the need to handle time zones, which we’ll come back to at the end of the chapter.\n你应该始终使用能满足你需求的、尽可能简单的数据类型。这意味着如果你可以使用日期而不是日期时间，你就应该这样做。日期时间要复杂得多，因为需要处理时区问题，我们将在本章末尾再回到这个话题。\nTo get the current date or date-time you can use today() or now():\n要获取当前日期或日期时间，你可以使用 today() 或 now()：\n\ntoday()\n#&gt; [1] \"2025-07-11\"\nnow()\n#&gt; [1] \"2025-07-11 16:41:28 CST\"\n\nOtherwise, the following sections describe the four ways you’re likely to create a date/time:\n此外，以下各节描述了你可能用来创建日期/时间的四种方式：\n\nWhile reading a file with readr.\n使用 readr 读取文件时。\nFrom a string.\n从字符串创建。\nFrom individual date-time components.\n从单个日期时间组件创建。\nFrom an existing date/time object.\n从现有的日期/时间对象创建。\n\n\n17.2.1 During import\nIf your CSV contains an ISO8601 date or date-time, you don’t need to do anything; readr will automatically recognize it:\n如果你的 CSV 文件包含 ISO8601 格式的日期或日期时间，你什么都不用做；readr 会自动识别它：\n\ncsv &lt;- \"\n  date,datetime\n  2022-01-02,2022-01-02 05:12\n\"\nread_csv(csv)\n#&gt; # A tibble: 1 × 2\n#&gt;   date       datetime           \n#&gt;   &lt;date&gt;     &lt;dttm&gt;             \n#&gt; 1 2022-01-02 2022-01-02 05:12:00\n\nIf you haven’t heard of ISO8601 before, it’s an international standard2 for writing dates where the components of a date are organized from biggest to smallest separated by -. For example, in ISO8601 May 3 2022 is 2022-05-03. ISO8601 dates can also include times, where hour, minute, and second are separated by :, and the date and time components are separated by either a T or a space. For example, you could write 4:26pm on May 3 2022 as either 2022-05-03 16:26 or 2022-05-03T16:26.\n如果你之前没有听说过 ISO8601，它是一个书写日期的国际标准2，其中日期的各个组成部分按从大到小的顺序排列，并用 - 分隔。例如，在 ISO8601 标准中，2022 年 5 月 3 日写作 2022-05-03。ISO8601 日期也可以包含时间，其中小时、分钟和秒用 : 分隔，日期和时间部分可以用 T 或空格分隔。例如，你可以将 2022 年 5 月 3 日下午 4:26 写成 2022-05-03 16:26 或 2022-05-03T16:26。\nFor other date-time formats, you’ll need to use col_types plus col_date() or col_datetime() along with a date-time format. The date-time format used by readr is a standard used across many programming languages, describing a date component with a % followed by a single character. For example, %Y-%m-%d specifies a date that’s a year, -, month (as number) -, day. Table Table 17.1 lists all the options.\n对于其他日期时间格式，你需要使用 col_types 加上 col_date() 或 col_datetime() 以及一个日期时间格式。readr 使用的日期时间格式是许多编程语言通用的标准，用 % 后跟一个单字符来描述日期组件。例如，%Y-%m-%d 指定了一个由年、-、月（数字）、-、日组成的日期。表格 Table 17.1 列出了所有选项。\n\n\nTable 17.1: All date formats understood by readr\n\n\n\n\n\n\n\n\n\nType\nCode\nMeaning\nExample\n\n\n\nYear\n%Y\n4 digit year / 4 位数年份\n2021\n\n\n\n%y\n2 digit year / 2 位数年份\n21\n\n\nMonth\n%m\nNumber / 数字\n2\n\n\n\n%b\nAbbreviated name / 缩写名称\nFeb\n\n\n\n%B\nFull name / 完整名称\nFebruary\n\n\nDay\n%d\nOne or two digits / 一位或两位数字\n2\n\n\n\n%e\nTwo digits / 两位数\n02\n\n\nTime\n%H\n24-hour hour / 24 小时制小时\n13\n\n\n\n%I\n12-hour hour / 12 小时制小时\n1\n\n\n\n%p\nAM/PM / 上午/下午\npm\n\n\n\n%M\nMinutes / 分钟\n35\n\n\n\n%S\nSeconds / 秒\n45\n\n\n\n%OS\nSeconds with decimal component / 带小数的秒\n45.35\n\n\n\n%Z\nTime zone name / 时区名称\nAmerica/Chicago\n\n\n\n%z\nOffset from UTC / 与 UTC 的偏移量\n+0800\n\n\nOther\n%.\nSkip one non-digit / 跳过一个非数字字符\n:\n\n\n\n%*\nSkip any number of non-digits / 跳过任意数量的非数字字符\n\n\n\n\n\n\n\nAnd this code shows a few options applied to a very ambiguous date:\n下面的代码展示了将几种选项应用于一个非常模糊的日期：\n\ncsv &lt;- \"\n  date\n  01/02/15\n\"\n\nread_csv(csv, col_types = cols(date = col_date(\"%m/%d/%y\")))\n#&gt; # A tibble: 1 × 1\n#&gt;   date      \n#&gt;   &lt;date&gt;    \n#&gt; 1 2015-01-02\n\nread_csv(csv, col_types = cols(date = col_date(\"%d/%m/%y\")))\n#&gt; # A tibble: 1 × 1\n#&gt;   date      \n#&gt;   &lt;date&gt;    \n#&gt; 1 2015-02-01\n\nread_csv(csv, col_types = cols(date = col_date(\"%y/%m/%d\")))\n#&gt; # A tibble: 1 × 1\n#&gt;   date      \n#&gt;   &lt;date&gt;    \n#&gt; 1 2001-02-15\n\nNote that no matter how you specify the date format, it’s always displayed the same way once you get it into R.\n请注意，无论你如何指定日期格式，一旦将其读入 R，它的显示方式总是相同的。\nIf you’re using %b or %B and working with non-English dates, you’ll also need to provide a locale(). See the list of built-in languages in date_names_langs(), or create your own with date_names(),\n如果你正在使用 %b 或 %B 处理非英语日期，你还需要提供一个 locale()。可以在 date_names_langs() 中查看内置语言列表，或者使用 date_names() 创建自己的语言环境。\n\n17.2.2 From strings\nThe date-time specification language is powerful, but requires careful analysis of the date format. An alternative approach is to use lubridate’s helpers which attempt to automatically determine the format once you specify the order of the component. To use them, identify the order in which year, month, and day appear in your dates, then arrange “y”, “m”, and “d” in the same order. That gives you the name of the lubridate function that will parse your date. For example:\n日期时间规范语言功能强大，但需要仔细分析日期格式。另一种方法是使用 lubridate 的辅助函数，一旦你指定了组件的顺序，它们就会尝试自动确定格式。要使用它们，请确定年、月、日在你的日期中出现的顺序，然后按相同的顺序排列 “y”、“m” 和 “d”。这样你就得到了将解析你的日期的 lubridate 函数的名称。例如：\n\nymd(\"2017-01-31\")\n#&gt; [1] \"2017-01-31\"\nmdy(\"January 31st, 2017\")\n#&gt; [1] \"2017-01-31\"\ndmy(\"31-Jan-2017\")\n#&gt; [1] \"2017-01-31\"\n\nymd() and friends create dates. To create a date-time, add an underscore and one or more of “h”, “m”, and “s” to the name of the parsing function:ymd() 和它的朋友们创建的是日期。要创建日期时间，请在解析函数名称后添加一个下划线以及 “h”、“m” 和 “s” 中的一个或多个：\n\nymd_hms(\"2017-01-31 20:11:59\")\n#&gt; [1] \"2017-01-31 20:11:59 UTC\"\nmdy_hm(\"01/31/2017 08:01\")\n#&gt; [1] \"2017-01-31 08:01:00 UTC\"\n\nYou can also force the creation of a date-time from a date by supplying a timezone:\n你还可以通过提供时区来强制从日期创建日期时间：\n\nymd(\"2017-01-31\", tz = \"UTC\")\n#&gt; [1] \"2017-01-31 UTC\"\n\nHere I use the UTC3 timezone which you might also know as GMT, or Greenwich Mean Time, the time at 0° longitude4 . It doesn’t use daylight saving time, making it a bit easier to compute with .\n这里我使用了 UTC3 时区，你可能也知道它叫 GMT，即格林尼治标准时间 (Greenwich Mean Time)，是经度 0° 的时间4。它不使用夏令时，这使得计算起来更容易一些。\n\n17.2.3 From individual components\nInstead of a single string, sometimes you’ll have the individual components of the date-time spread across multiple columns. This is what we have in the flights data:\n有时，你不会有一个单一的字符串，而是将日期时间的各个组成部分分布在多个列中。flights 数据就是这种情况：\n\nflights |&gt; \n  select(year, month, day, hour, minute)\n#&gt; # A tibble: 336,776 × 5\n#&gt;    year month   day  hour minute\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1  2013     1     1     5     15\n#&gt; 2  2013     1     1     5     29\n#&gt; 3  2013     1     1     5     40\n#&gt; 4  2013     1     1     5     45\n#&gt; 5  2013     1     1     6      0\n#&gt; 6  2013     1     1     5     58\n#&gt; # ℹ 336,770 more rows\n\nTo create a date/time from this sort of input, use make_date() for dates, or make_datetime() for date-times:\n要从此类输入创建日期/时间，对日期使用 make_date()，对日期时间使用 make_datetime()：\n\nflights |&gt; \n  select(year, month, day, hour, minute) |&gt; \n  mutate(departure = make_datetime(year, month, day, hour, minute))\n#&gt; # A tibble: 336,776 × 6\n#&gt;    year month   day  hour minute departure          \n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dttm&gt;             \n#&gt; 1  2013     1     1     5     15 2013-01-01 05:15:00\n#&gt; 2  2013     1     1     5     29 2013-01-01 05:29:00\n#&gt; 3  2013     1     1     5     40 2013-01-01 05:40:00\n#&gt; 4  2013     1     1     5     45 2013-01-01 05:45:00\n#&gt; 5  2013     1     1     6      0 2013-01-01 06:00:00\n#&gt; 6  2013     1     1     5     58 2013-01-01 05:58:00\n#&gt; # ℹ 336,770 more rows\n\nLet’s do the same thing for each of the four time columns in flights. The times are represented in a slightly odd format, so we use modulus arithmetic to pull out the hour and minute components. Once we’ve created the date-time variables, we focus in on the variables we’ll explore in the rest of the chapter.\n让我们对 flights 数据中的四个时间列都执行相同的操作。这些时间以一种稍微奇怪的格式表示，所以我们使用模运算来提取小时和分钟部分。一旦我们创建了日期时间变量，我们就专注于将在本章其余部分探讨的变量。\n\nmake_datetime_100 &lt;- function(year, month, day, time) {\n  make_datetime(year, month, day, time %/% 100, time %% 100)\n}\n\nflights_dt &lt;- flights |&gt; \n  filter(!is.na(dep_time), !is.na(arr_time)) |&gt; \n  mutate(\n    dep_time = make_datetime_100(year, month, day, dep_time),\n    arr_time = make_datetime_100(year, month, day, arr_time),\n    sched_dep_time = make_datetime_100(year, month, day, sched_dep_time),\n    sched_arr_time = make_datetime_100(year, month, day, sched_arr_time)\n  ) |&gt; \n  select(origin, dest, ends_with(\"delay\"), ends_with(\"time\"))\n\nflights_dt\n#&gt; # A tibble: 328,063 × 9\n#&gt;   origin dest  dep_delay arr_delay dep_time            sched_dep_time     \n#&gt;   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;             \n#&gt; 1 EWR    IAH           2        11 2013-01-01 05:17:00 2013-01-01 05:15:00\n#&gt; 2 LGA    IAH           4        20 2013-01-01 05:33:00 2013-01-01 05:29:00\n#&gt; 3 JFK    MIA           2        33 2013-01-01 05:42:00 2013-01-01 05:40:00\n#&gt; 4 JFK    BQN          -1       -18 2013-01-01 05:44:00 2013-01-01 05:45:00\n#&gt; 5 LGA    ATL          -6       -25 2013-01-01 05:54:00 2013-01-01 06:00:00\n#&gt; 6 EWR    ORD          -4        12 2013-01-01 05:54:00 2013-01-01 05:58:00\n#&gt; # ℹ 328,057 more rows\n#&gt; # ℹ 3 more variables: arr_time &lt;dttm&gt;, sched_arr_time &lt;dttm&gt;, …\n\nWith this data, we can visualize the distribution of departure times across the year:\n有了这些数据，我们可以可视化一年中出发时间的分布：\n\nflights_dt |&gt; \n  ggplot(aes(x = dep_time)) + \n  geom_freqpoly(binwidth = 86400) # 86400 seconds = 1 day\n\n\n\n\n\n\n\nOr within a single day:\n或者在一天之内：\n\nflights_dt |&gt; \n  filter(dep_time &lt; ymd(20130102)) |&gt; \n  ggplot(aes(x = dep_time)) + \n  geom_freqpoly(binwidth = 600) # 600 s = 10 minutes\n\n\n\n\n\n\n\nNote that when you use date-times in a numeric context (like in a histogram), 1 means 1 second, so a binwidth of 86400 means one day. For dates, 1 means 1 day.\n请注意，当你在数值上下文（如直方图）中使用日期时间时，1 代表 1 秒，因此 86400 的 binwidth (组距) 代表一天。对于日期，1 代表 1 天。\n\n17.2.4 From other types\nYou may want to switch between a date-time and a date. That’s the job of as_datetime() and as_date():\n你可能想要在日期时间和日期之间进行切换。这是 as_datetime() 和 as_date() 的工作：\n\nas_datetime(today())\n#&gt; [1] \"2025-07-11 UTC\"\nas_date(now())\n#&gt; [1] \"2025-07-11\"\n\nSometimes you’ll get date/times as numeric offsets from the “Unix Epoch”, 1970-01-01. If the offset is in seconds, use as_datetime(); if it’s in days, use as_date().\n有时你会得到以 “Unix 纪元” (Unix Epoch, 1970-01-01) 为基准的数值偏移量形式的日期/时间。如果偏移量以秒为单位，使用 as_datetime()；如果以天为单位，使用 as_date()。\n\nas_datetime(60 * 60 * 10)\n#&gt; [1] \"1970-01-01 10:00:00 UTC\"\nas_date(365 * 10 + 2)\n#&gt; [1] \"1980-01-01\"\n\n\n17.2.5 Exercises\n\n\nWhat happens if you parse a string that contains invalid dates?\n\nymd(c(\"2010-10-10\", \"bananas\"))\n\n\nWhat does the tzone argument to today() do? Why is it important?\n\nFor each of the following date-times, show how you’d parse it using a readr column specification and a lubridate function.\n\nd1 &lt;- \"January 1, 2010\"\nd2 &lt;- \"2015-Mar-07\"\nd3 &lt;- \"06-Jun-2017\"\nd4 &lt;- c(\"August 19 (2015)\", \"July 1 (2015)\")\nd5 &lt;- \"12/30/14\" # Dec 30, 2014\nt1 &lt;- \"1705\"\nt2 &lt;- \"11:15:10.12 PM\"",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Dates and times</span>"
    ]
  },
  {
    "objectID": "datetimes.html#date-time-components",
    "href": "datetimes.html#date-time-components",
    "title": "17  Dates and times",
    "section": "\n17.3 Date-time components",
    "text": "17.3 Date-time components\nNow that you know how to get date-time data into R’s date-time data structures, let’s explore what you can do with them. This section will focus on the accessor functions that let you get and set individual components. The next section will look at how arithmetic works with date-times.\n既然你知道如何将日期时间数据导入 R 的日期时间数据结构中，让我们来探索一下你可以用它们做什么。本节将重点介绍允许你获取和设置单个组件的访问器函数。下一节将探讨日期时间的算术运算。\n\n17.3.1 Getting components\nYou can pull out individual parts of the date with the accessor functions year(), month(), mday() (day of the month), yday() (day of the year), wday() (day of the week), hour(), minute(), and second(). These are effectively the opposites of make_datetime().\n你可以使用访问器函数 year()、month()、mday() (月中的天)、yday() (年中的天)、wday() (周中的天)、hour()、minute() 和 second() 来提取日期的各个部分。这些函数实际上是 make_datetime() 的反向操作。\n\ndatetime &lt;- ymd_hms(\"2026-07-08 12:34:56\")\n\nyear(datetime)\n#&gt; [1] 2026\nmonth(datetime)\n#&gt; [1] 7\nmday(datetime)\n#&gt; [1] 8\n\nyday(datetime)\n#&gt; [1] 189\nwday(datetime)\n#&gt; [1] 4\n\nFor month() and wday() you can set label = TRUE to return the abbreviated name of the month or day of the week. Set abbr = FALSE to return the full name.\n对于 month() 和 wday()，你可以设置 label = TRUE 来返回月份或星期的缩写名称。设置 abbr = FALSE 可以返回完整名称。\n\nmonth(datetime, label = TRUE)\n#&gt; [1] 7月\n#&gt; 12 Levels: 1月 &lt; 2月 &lt; 3月 &lt; 4月 &lt; 5月 &lt; 6月 &lt; 7月 &lt; 8月 &lt; 9月 &lt; ... &lt; 12月\nwday(datetime, label = TRUE, abbr = FALSE)\n#&gt; [1] 星期三\n#&gt; 7 Levels: 星期日 &lt; 星期一 &lt; 星期二 &lt; 星期三 &lt; 星期四 &lt; ... &lt; 星期六\n\nWe can use wday() to see that more flights depart during the week than on the weekend:\n我们可以使用 wday() 发现，工作日起飞的航班比周末多：\n\nflights_dt |&gt; \n  mutate(wday = wday(dep_time, label = TRUE)) |&gt; \n  ggplot(aes(x = wday)) +\n  geom_bar()\n\n\n\n\n\n\n\nWe can also look at the average departure delay by minute within the hour. There’s an interesting pattern: flights leaving in minutes 20-30 and 50-60 have much lower delays than the rest of the hour!\n我们还可以按小时内的分钟查看平均起飞延误。有一个有趣的模式：在 20-30 分钟和 50-60 分钟之间起飞的航班，其延误时间远低于该小时的其他时间段！\n\nflights_dt |&gt; \n  mutate(minute = minute(dep_time)) |&gt; \n  group_by(minute) |&gt; \n  summarize(\n    avg_delay = mean(dep_delay, na.rm = TRUE),\n    n = n()\n  ) |&gt; \n  ggplot(aes(x = minute, y = avg_delay)) +\n  geom_line()\n\n\n\n\n\n\n\nInterestingly, if we look at the scheduled departure time we don’t see such a strong pattern:\n有趣的是，如果我们查看 计划 出发时间，我们看不到如此强烈的模式：\n\nsched_dep &lt;- flights_dt |&gt; \n  mutate(minute = minute(sched_dep_time)) |&gt; \n  group_by(minute) |&gt; \n  summarize(\n    avg_delay = mean(arr_delay, na.rm = TRUE),\n    n = n()\n  )\n\nggplot(sched_dep, aes(x = minute, y = avg_delay)) +\n  geom_line()\n\n\n\n\n\n\n\nSo why do we see that pattern with the actual departure times? Well, like much data collected by humans, there’s a strong bias towards flights leaving at “nice” departure times, as Figure 17.1 shows. Always be alert for this sort of pattern whenever you work with data that involves human judgement!\n那么，为什么我们会在实际出发时间中看到这种模式呢？嗯，就像许多由人类收集的数据一样，航班出发时间存在一种强烈的偏好，倾向于“整点”的出发时间，如 Figure 17.1 所示。在处理涉及人类判断的数据时，要始终警惕这种模式！\n\n#| fig-alt: |\n#|   A line plot with departure minute (0-60) on the x-axis and number of\n#|   flights (0-60000) on the y-axis. Most flights are scheduled to depart\n#|   on either the hour (~60,000) or the half hour (~35,000). Otherwise,\n#|   all most all flights are scheduled to depart on multiples of five, \n#|   with a few extra at 15, 45, and 55 minutes.\n#| echo: false\nggplot(sched_dep, aes(x = minute, y = n)) +\n  geom_line()\n\n\n\n\n\n\nFigure 17.1: A frequency polygon showing the number of flights scheduled to depart each hour. You can see a strong preference for round numbers like 0 and 30 and generally for numbers that are a multiple of five.\n\n\n\n\n\n17.3.2 Rounding\nAn alternative approach to plotting individual components is to round the date to a nearby unit of time, with floor_date(), round_date(), and ceiling_date(). Each function takes a vector of dates to adjust and then the name of the unit to round down (floor), round up (ceiling), or round to. This, for example, allows us to plot the number of flights per week:\n绘制单个组件的另一种方法是使用 floor_date()、round_date() 和 ceiling_date() 将日期舍入到附近的时间单位。每个函数都接受一个要调整的日期向量，然后是要向下舍入（floor）、向上舍入（ceiling）或四舍五入的单位名称。例如，这使我们能够绘制每周的航班数量：\n\nflights_dt |&gt; \n  count(week = floor_date(dep_time, \"week\")) |&gt; \n  ggplot(aes(x = week, y = n)) +\n  geom_line() + \n  geom_point()\n\n\n\n\n\n\n\nYou can use rounding to show the distribution of flights across the course of a day by computing the difference between dep_time and the earliest instant of that day:\n你可以通过计算 dep_time 与当天最早时刻之间的差值，来使用舍入功能显示一天中航班的分布情况：\n\nflights_dt |&gt; \n  mutate(dep_hour = dep_time - floor_date(dep_time, \"day\")) |&gt; \n  ggplot(aes(x = dep_hour)) +\n  geom_freqpoly(binwidth = 60 * 30)\n#&gt; Don't know how to automatically pick scale for object of type &lt;difftime&gt;.\n#&gt; Defaulting to continuous.\n\n\n\n\n\n\n\nComputing the difference between a pair of date-times yields a difftime (more on that in Section 17.4.3). We can convert that to an hms object to get a more useful x-axis:\n计算一对日期时间之间的差值会得到一个 difftime 对象（更多相关内容请参见 Section 17.4.3）。我们可以将其转换为 hms 对象，以获得更有用的 x 轴：\n\nflights_dt |&gt; \n  mutate(dep_hour = hms::as_hms(dep_time - floor_date(dep_time, \"day\"))) |&gt; \n  ggplot(aes(x = dep_hour)) +\n  geom_freqpoly(binwidth = 60 * 30)\n\n\n\n\n\n\n\n\n17.3.3 Modifying components\nYou can also use each accessor function to modify the components of a date/time. This doesn’t come up much in data analysis, but can be useful when cleaning data that has clearly incorrect dates.\n你还可以使用每个访问器函数来修改日期/时间的组件。这在数据分析中不常用，但在清理含有明显错误日期的数据时可能很有用。\n\n(datetime &lt;- ymd_hms(\"2026-07-08 12:34:56\"))\n#&gt; [1] \"2026-07-08 12:34:56 UTC\"\n\nyear(datetime) &lt;- 2030\ndatetime\n#&gt; [1] \"2030-07-08 12:34:56 UTC\"\nmonth(datetime) &lt;- 01\ndatetime\n#&gt; [1] \"2030-01-08 12:34:56 UTC\"\nhour(datetime) &lt;- hour(datetime) + 1\ndatetime\n#&gt; [1] \"2030-01-08 13:34:56 UTC\"\n\nAlternatively, rather than modifying an existing variable, you can create a new date-time with update(). This also allows you to set multiple values in one step:\n或者，你可以不修改现有变量，而是用 update() 创建一个新的日期时间。这也允许你一步设置多个值：\n\nupdate(datetime, year = 2030, month = 2, mday = 2, hour = 2)\n#&gt; [1] \"2030-02-02 02:34:56 UTC\"\n\nIf values are too big, they will roll-over:\n如果值太大，它们将会“滚动”进位：\n\nupdate(ymd(\"2023-02-01\"), mday = 30)\n#&gt; [1] \"2023-03-02\"\nupdate(ymd(\"2023-02-01\"), hour = 400)\n#&gt; [1] \"2023-02-17 16:00:00 UTC\"\n\n\n17.3.4 Exercises\n\nHow does the distribution of flight times within a day change over the course of the year?\nCompare dep_time, sched_dep_time and dep_delay. Are they consistent? Explain your findings.\nCompare air_time with the duration between the departure and arrival. Explain your findings. (Hint: consider the location of the airport.)\nHow does the average delay time change over the course of a day? Should you use dep_time or sched_dep_time? Why?\nOn what day of the week should you leave if you want to minimise the chance of a delay?\nWhat makes the distribution of diamonds$carat and flights$sched_dep_time similar?\nConfirm our hypothesis that the early departures of flights in minutes 20-30 and 50-60 are caused by scheduled flights that leave early. Hint: create a binary variable that tells you whether or not a flight was delayed.",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Dates and times</span>"
    ]
  },
  {
    "objectID": "datetimes.html#time-spans",
    "href": "datetimes.html#time-spans",
    "title": "17  Dates and times",
    "section": "\n17.4 Time spans",
    "text": "17.4 Time spans\nNext you’ll learn about how arithmetic with dates works, including subtraction, addition, and division. Along the way, you’ll learn about three important classes that represent time spans:\n接下来，你将学习日期算术的运作方式，包括减法、加法和除法。在此过程中，你将了解三个代表时间跨度的重要类：\n\nDurations, which represent an exact number of seconds.Durations (时长)，表示精确的秒数。\nPeriods, which represent human units like weeks and months.Periods (周期)，表示人类使用的单位，如周和月。\nIntervals, which represent a starting and ending point.Intervals (时间间隔)，表示一个起点和一个终点。\n\nHow do you pick between duration, periods, and intervals? As always, pick the simplest data structure that solves your problem. If you only care about physical time, use a duration; if you need to add human times, use a period; if you need to figure out how long a span is in human units, use an interval.\n你如何在 duration、period 和 interval 之间做出选择？一如既往，选择能解决你问题的最简单的数据结构。如果你只关心物理时间，请使用 duration；如果你需要添加人类时间单位，请使用 period；如果你需要计算一个时间跨度在人类单位中有多长，请使用 interval。\n\n17.4.1 Durations\nIn R, when you subtract two dates, you get a difftime object:\n在 R 中，当你减去两个日期时，你会得到一个 difftime 对象：\n\n# How old is Hadley?\nh_age &lt;- today() - ymd(\"1979-10-14\")\nh_age\n#&gt; Time difference of 16707 days\n\nA difftime class object records a time span of seconds, minutes, hours, days, or weeks. This ambiguity can make difftimes a little painful to work with, so lubridate provides an alternative which always uses seconds: the duration.difftime 类对象记录一个以秒、分钟、小时、天或周为单位的时间跨度。这种模糊性使得使用 difftimes 有些痛苦，所以 lubridate 提供了一个总是使用秒的替代方案：duration (时长)。\n\nas.duration(h_age)\n#&gt; [1] \"1443484800s (~45.74 years)\"\n\nDurations come with a bunch of convenient constructors:\nDuration 有许多方便的构造函数：\n\ndseconds(15)\n#&gt; [1] \"15s\"\ndminutes(10)\n#&gt; [1] \"600s (~10 minutes)\"\ndhours(c(12, 24))\n#&gt; [1] \"43200s (~12 hours)\" \"86400s (~1 days)\"\nddays(0:5)\n#&gt; [1] \"0s\"                \"86400s (~1 days)\"  \"172800s (~2 days)\"\n#&gt; [4] \"259200s (~3 days)\" \"345600s (~4 days)\" \"432000s (~5 days)\"\ndweeks(3)\n#&gt; [1] \"1814400s (~3 weeks)\"\ndyears(1)\n#&gt; [1] \"31557600s (~1 years)\"\n\nDurations always record the time span in seconds. Larger units are created by converting minutes, hours, days, weeks, and years to seconds: 60 seconds in a minute, 60 minutes in an hour, 24 hours in a day, and 7 days in a week. Larger time units are more problematic. A year uses the “average” number of days in a year, i.e. 365.25. There’s no way to convert a month to a duration, because there’s just too much variation.\nDuration 总是以秒为单位记录时间跨度。更大的单位是通过将分钟、小时、天、周和年转换为秒来创建的：1 分钟 60 秒，1 小时 60 分钟，1 天 24 小时，1 周 7 天。更大的时间单位则更有问题。一年使用“平均”天数，即 365.25 天。无法将一个月转换为 duration，因为月份的变化太大。\nYou can add and multiply durations:\n你可以对 duration 进行加法和乘法运算：\n\n2 * dyears(1)\n#&gt; [1] \"63115200s (~2 years)\"\ndyears(1) + dweeks(12) + dhours(15)\n#&gt; [1] \"38869200s (~1.23 years)\"\n\nYou can add and subtract durations to and from days:\n你可以将 duration 与日期进行加减运算：\n\ntomorrow &lt;- today() + ddays(1)\nlast_year &lt;- today() - dyears(1)\n\nHowever, because durations represent an exact number of seconds, sometimes you might get an unexpected result:\n然而，因为 duration 表示的是精确的秒数，有时你可能会得到意想不到的结果：\n\none_am &lt;- ymd_hms(\"2026-03-08 01:00:00\", tz = \"America/New_York\")\n\none_am\n#&gt; [1] \"2026-03-08 01:00:00 EST\"\none_am + ddays(1)\n#&gt; [1] \"2026-03-09 02:00:00 EDT\"\n\nWhy is one day after 1am March 8, 2am March 9? If you look carefully at the date you might also notice that the time zones have changed. March 8 only has 23 hours because it’s when DST starts, so if we add a full days worth of seconds we end up with a different time.\n为什么 3 月 8 日凌晨 1 点之后的一天是 3 月 9 日凌晨 2 点？如果你仔细观察日期，你可能还会注意到时区已经改变了。3 月 8 日只有 23 个小时，因为那是夏令时 (DST) 开始的时候，所以如果我们加上一整天的秒数，我们最终会得到一个不同的时间。\n\n17.4.2 Periods\nTo solve this problem, lubridate provides periods. Periods are time spans but don’t have a fixed length in seconds, instead they work with “human” times, like days and months. That allows them to work in a more intuitive way:\n为了解决这个问题，lubridate 提供了 periods (周期)。Periods 也是时间跨度，但没有固定的秒数长度，而是使用“人类”时间单位，如天和月。这让它们能以更直观的方式工作：\n\none_am\n#&gt; [1] \"2026-03-08 01:00:00 EST\"\none_am + days(1)\n#&gt; [1] \"2026-03-09 01:00:00 EDT\"\n\nLike durations, periods can be created with a number of friendly constructor functions.\n与 duration 类似，period 也可以用许多方便的构造函数创建。\n\nhours(c(12, 24))\n#&gt; [1] \"12H 0M 0S\" \"24H 0M 0S\"\ndays(7)\n#&gt; [1] \"7d 0H 0M 0S\"\nmonths(1:6)\n#&gt; [1] \"1m 0d 0H 0M 0S\" \"2m 0d 0H 0M 0S\" \"3m 0d 0H 0M 0S\" \"4m 0d 0H 0M 0S\"\n#&gt; [5] \"5m 0d 0H 0M 0S\" \"6m 0d 0H 0M 0S\"\n\nYou can add and multiply periods:\n你可以对 period 进行加法和乘法运算：\n\n10 * (months(6) + days(1))\n#&gt; [1] \"60m 10d 0H 0M 0S\"\ndays(50) + hours(25) + minutes(2)\n#&gt; [1] \"50d 25H 2M 0S\"\n\nAnd of course, add them to dates. Compared to durations, periods are more likely to do what you expect:\n当然，也可以把它们加到日期上。与 duration 相比，period 更可能按照你的预期工作：\n\n# A leap year\nymd(\"2024-01-01\") + dyears(1)\n#&gt; [1] \"2024-12-31 06:00:00 UTC\"\nymd(\"2024-01-01\") + years(1)\n#&gt; [1] \"2025-01-01\"\n\n# Daylight saving time\none_am + ddays(1)\n#&gt; [1] \"2026-03-09 02:00:00 EDT\"\none_am + days(1)\n#&gt; [1] \"2026-03-09 01:00:00 EDT\"\n\nLet’s use periods to fix an oddity related to our flight dates. Some planes appear to have arrived at their destination before they departed from New York City.\n让我们用 period 来修正航班日期中的一个奇怪之处。有些飞机似乎在从纽约市起飞之前就到达了目的地。\n\nflights_dt |&gt; \n  filter(arr_time &lt; dep_time) \n#&gt; # A tibble: 10,633 × 9\n#&gt;   origin dest  dep_delay arr_delay dep_time            sched_dep_time     \n#&gt;   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;             \n#&gt; 1 EWR    BQN           9        -4 2013-01-01 19:29:00 2013-01-01 19:20:00\n#&gt; 2 JFK    DFW          59        NA 2013-01-01 19:39:00 2013-01-01 18:40:00\n#&gt; 3 EWR    TPA          -2         9 2013-01-01 20:58:00 2013-01-01 21:00:00\n#&gt; 4 EWR    SJU          -6       -12 2013-01-01 21:02:00 2013-01-01 21:08:00\n#&gt; 5 EWR    SFO          11       -14 2013-01-01 21:08:00 2013-01-01 20:57:00\n#&gt; 6 LGA    FLL         -10        -2 2013-01-01 21:20:00 2013-01-01 21:30:00\n#&gt; # ℹ 10,627 more rows\n#&gt; # ℹ 3 more variables: arr_time &lt;dttm&gt;, sched_arr_time &lt;dttm&gt;, …\n\nThese are overnight flights. We used the same date information for both the departure and the arrival times, but these flights arrived on the following day. We can fix this by adding days(1) to the arrival time of each overnight flight.\n这些是夜间航班。我们对出发和到达时间使用了相同的日期信息，但这些航班是在第二天到达的。我们可以通过给每个夜间航班的到达时间加上 days(1) 来修正这个问题。\n\nflights_dt &lt;- flights_dt |&gt; \n  mutate(\n    overnight = arr_time &lt; dep_time,\n    arr_time = arr_time + days(overnight),\n    sched_arr_time = sched_arr_time + days(overnight)\n  )\n\nNow all of our flights obey the laws of physics.\n现在我们所有的航班都遵守物理定律了。\n\nflights_dt |&gt; \n  filter(arr_time &lt; dep_time) \n#&gt; # A tibble: 0 × 10\n#&gt; # ℹ 10 variables: origin &lt;chr&gt;, dest &lt;chr&gt;, dep_delay &lt;dbl&gt;,\n#&gt; #   arr_delay &lt;dbl&gt;, dep_time &lt;dttm&gt;, sched_dep_time &lt;dttm&gt;, …\n\n\n17.4.3 Intervals\nWhat does dyears(1) / ddays(365) return? It’s not quite one, because dyears() is defined as the number of seconds per average year, which is 365.25 days.dyears(1) / ddays(365) 返回什么？结果不完全是 1，因为 dyears() 被定义为平均每年的秒数，即 365.25 天。\nWhat does years(1) / days(1) return? Well, if the year was 2015 it should return 365, but if it was 2016, it should return 366! There’s not quite enough information for lubridate to give a single clear answer. What it does instead is give an estimate:years(1) / days(1) 会返回什么？嗯，如果年份是 2015 年，它应该返回 365，但如果是 2016 年，它应该返回 366！lubridate 没有足够的信息给出一个明确的答案。它所做的是给出一个估计值：\n\nyears(1) / days(1)\n#&gt; [1] 365.25\n\nIf you want a more accurate measurement, you’ll have to use an interval. An interval is a pair of starting and ending date times, or you can think of it as a duration with a starting point.\n如果你想要更精确的测量，就必须使用 interval (时间间隔)。一个 interval 是一对起始和结束的日期时间，或者你可以把它看作是一个有起点的 duration。\nYou can create an interval by writing start %--% end:\n你可以通过 start %--% end 的方式创建一个 interval：\n\ny2023 &lt;- ymd(\"2023-01-01\") %--% ymd(\"2024-01-01\")\ny2024 &lt;- ymd(\"2024-01-01\") %--% ymd(\"2025-01-01\")\n\ny2023\n#&gt; [1] 2023-01-01 UTC--2024-01-01 UTC\ny2024\n#&gt; [1] 2024-01-01 UTC--2025-01-01 UTC\n\nYou could then divide it by days() to find out how many days fit in the year:\n然后你可以用它除以 days() 来计算出一年有多少天：\n\ny2023 / days(1)\n#&gt; [1] 365\ny2024 / days(1)\n#&gt; [1] 366\n\n\n17.4.4 Exercises\n\nExplain days(!overnight) and days(overnight) to someone who has just started learning R. What is the key fact you need to know?\nCreate a vector of dates giving the first day of every month in 2015. Create a vector of dates giving the first day of every month in the current year.\nWrite a function that given your birthday (as a date), returns how old you are in years.\nWhy can’t (today() %--% (today() + years(1))) / months(1) work?",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Dates and times</span>"
    ]
  },
  {
    "objectID": "datetimes.html#time-zones",
    "href": "datetimes.html#time-zones",
    "title": "17  Dates and times",
    "section": "\n17.5 Time zones",
    "text": "17.5 Time zones\nTime zones are an enormously complicated topic because of their interaction with geopolitical entities. Fortunately we don’t need to dig into all the details as they’re not all important for data analysis, but there are a few challenges we’ll need to tackle head on.\n时区是一个极其复杂的话题，因为它与地缘政治实体相互作用。幸运的是，我们不需要深入了解所有细节，因为它们对数据分析并非都重要，但我们仍需要正面解决一些挑战。\nThe first challenge is that everyday names of time zones tend to be ambiguous. For example, if you’re American you’re probably familiar with EST, or Eastern Standard Time. However, both Australia and Canada also have EST! To avoid confusion, R uses the international standard IANA time zones. These use a consistent naming scheme {area}/{location}, typically in the form {continent}/{city} or {ocean}/{city}. Examples include “America/New_York”, “Europe/Paris”, and “Pacific/Auckland”.\n第一个挑战是，日常使用的时区名称往往是模糊的。例如，如果你是美国人，你可能熟悉 EST，即东部标准时间。然而，澳大利亚和加拿大也都有 EST！为了避免混淆，R 使用国际标准的 IANA 时区。这些时区使用一致的命名方案 {区域}/{地点}，通常形式为 {大洲}/{城市} 或 {大洋}/{城市}。例如 “America/New_York”、“Europe/Paris” 和 “Pacific/Auckland”。\nYou might wonder why the time zone uses a city, when typically you think of time zones as associated with a country or region within a country. This is because the IANA database has to record decades worth of time zone rules. Over the course of decades, countries change names (or break apart) fairly frequently, but city names tend to stay the same. Another problem is that the name needs to reflect not only the current behavior, but also the complete history. For example, there are time zones for both “America/New_York” and “America/Detroit”. These cities both currently use Eastern Standard Time but in 1969-1972 Michigan (the state in which Detroit is located), did not follow DST, so it needs a different name. It’s worth reading the raw time zone database (available at https://www.iana.org/time-zones) just to read some of these stories!\n你可能会想，为什么时区使用城市名称，而通常你认为时区是与一个国家或国家内的某个地区相关联的。这是因为 IANA 数据库必须记录数十年的时区规则。几十年来，国家名称（或分裂）变化相当频繁，但城市名称往往保持不变。另一个问题是，名称不仅需要反映当前的行为，还需要反映完整的历史。例如，“America/New_York” 和 “America/Detroit” 都有时区。这两个城市目前都使用东部标准时间，但在 1969-1972 年，密歇根州（底特律所在的州）没有遵循夏令时，所以它需要一个不同的名称。值得阅读原始时区数据库（可在 https://www.iana.org/time-zones 获得）来了解这些故事！\nYou can find out what R thinks your current time zone is with Sys.timezone():\n你可以使用 Sys.timezone() 查明 R 认为你当前所在的时区：\n\nSys.timezone()\n#&gt; [1] \"Asia/Shanghai\"\n\n(If R doesn’t know, you’ll get an NA.)\n（如果 R 不知道，你会得到一个 NA。）\nAnd see the complete list of all time zone names with OlsonNames():\n并使用 OlsonNames() 查看所有时区名称的完整列表：\n\nlength(OlsonNames())\n#&gt; [1] 597\nhead(OlsonNames())\n#&gt; [1] \"Africa/Abidjan\"     \"Africa/Accra\"       \"Africa/Addis_Ababa\"\n#&gt; [4] \"Africa/Algiers\"     \"Africa/Asmara\"      \"Africa/Asmera\"\n\nIn R, the time zone is an attribute of the date-time that only controls printing. For example, these three objects represent the same instant in time:\n在 R 中，时区是日期时间的一个属性，只控制打印显示。例如，下面这三个对象代表同一个时间点：\n\nx1 &lt;- ymd_hms(\"2024-06-01 12:00:00\", tz = \"America/New_York\")\nx1\n#&gt; [1] \"2024-06-01 12:00:00 EDT\"\n\nx2 &lt;- ymd_hms(\"2024-06-01 18:00:00\", tz = \"Europe/Copenhagen\")\nx2\n#&gt; [1] \"2024-06-01 18:00:00 CEST\"\n\nx3 &lt;- ymd_hms(\"2024-06-02 04:00:00\", tz = \"Pacific/Auckland\")\nx3\n#&gt; [1] \"2024-06-02 04:00:00 NZST\"\n\nYou can verify that they’re the same time using subtraction:\n你可以通过减法来验证它们是同一时间：\n\nx1 - x2\n#&gt; Time difference of 0 secs\nx1 - x3\n#&gt; Time difference of 0 secs\n\nUnless otherwise specified, lubridate always uses UTC. UTC (Coordinated Universal Time) is the standard time zone used by the scientific community and is roughly equivalent to GMT (Greenwich Mean Time). It does not have DST, which makes a convenient representation for computation. Operations that combine date-times, like c(), will often drop the time zone. In that case, the date-times will display in the time zone of the first element:\n除非另有说明，lubridate 总是使用 UTC。UTC（协调世界时）是科学界使用的标准时区，大致相当于 GMT（格林尼治标准时间）。它没有夏令时，这使得它成为一个方便的计算表示。像 c() 这样组合日期时间的操作通常会丢弃时区信息。在这种情况下，日期时间将以第一个元素的时区显示：\n\nx4 &lt;- c(x1, x2, x3)\nx4\n#&gt; [1] \"2024-06-01 12:00:00 EDT\" \"2024-06-01 12:00:00 EDT\"\n#&gt; [3] \"2024-06-01 12:00:00 EDT\"\n\nYou can change the time zone in two ways:\n你可以通过两种方式更改时区：\n\n\nKeep the instant in time the same, and change how it’s displayed. Use this when the instant is correct, but you want a more natural display.\n保持时间点不变，只改变其显示方式。当时间点正确，但你想要更自然的显示时使用此方法。\n\nx4a &lt;- with_tz(x4, tzone = \"Australia/Lord_Howe\")\nx4a\n#&gt; [1] \"2024-06-02 02:30:00 +1030\" \"2024-06-02 02:30:00 +1030\"\n#&gt; [3] \"2024-06-02 02:30:00 +1030\"\nx4a - x4\n#&gt; Time differences in secs\n#&gt; [1] 0 0 0\n\n(This also illustrates another challenge of times zones: they’re not all integer hour offsets!)\n（这也说明了时区的另一个挑战：它们并非都是整数小时的偏移量！）\n\n\nChange the underlying instant in time. Use this when you have an instant that has been labelled with the incorrect time zone, and you need to fix it.\n改变底层的时间点。当你有一个被标记了错误时区的时间点，并且需要修正它时使用此方法。\n\nx4b &lt;- force_tz(x4, tzone = \"Australia/Lord_Howe\")\nx4b\n#&gt; [1] \"2024-06-01 12:00:00 +1030\" \"2024-06-01 12:00:00 +1030\"\n#&gt; [3] \"2024-06-01 12:00:00 +1030\"\nx4b - x4\n#&gt; Time differences in hours\n#&gt; [1] -14.5 -14.5 -14.5",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Dates and times</span>"
    ]
  },
  {
    "objectID": "datetimes.html#summary",
    "href": "datetimes.html#summary",
    "title": "17  Dates and times",
    "section": "\n17.6 Summary",
    "text": "17.6 Summary\nThis chapter has introduced you to the tools that lubridate provides to help you work with date-time data. Working with dates and times can seem harder than necessary, but hopefully this chapter has helped you see why — date-times are more complex than they seem at first glance, and handling every possible situation adds complexity. Even if your data never crosses a day light savings boundary or involves a leap year, the functions need to be able to handle it.\n本章向你介绍了 lubridate 提供的用于处理日期时间数据的工具。处理日期和时间似乎比必要的要困难，但希望本章能帮助你理解其中的原因——日期时间比初看起来要复杂得多，处理每一种可能的情况都会增加复杂性。即使你的数据从未跨越夏令时边界或涉及闰年，这些函数也需要能够处理这些情况。\nThe next chapter gives a round up of missing values. You’ve seen them in a few places and have no doubt encounter in your own analysis, and it’s now time to provide a grab bag of useful techniques for dealing with them.\n下一章将对缺失值进行总结。你已经在一些地方见过它们，并且毫无疑问在自己的分析中也遇到过，现在是时候提供一系列处理它们的有用技巧了。",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Dates and times</span>"
    ]
  },
  {
    "objectID": "datetimes.html#footnotes",
    "href": "datetimes.html#footnotes",
    "title": "17  Dates and times",
    "section": "",
    "text": "A year is a leap year if it’s divisible by 4, unless it’s also divisible by 100, except if it’s also divisible by 400. In other words, in every set of 400 years, there’s 97 leap years.↩︎\nhttps://xkcd.com/1179/↩︎\nYou might wonder what UTC stands for. It’s a compromise between the English “Coordinated Universal Time” and French “Temps Universel Coordonné”.↩︎\nNo prizes for guessing which country came up with the longitude system.↩︎",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Dates and times</span>"
    ]
  },
  {
    "objectID": "missing-values.html",
    "href": "missing-values.html",
    "title": "18  Missing values",
    "section": "",
    "text": "18.1 Introduction\nYou’ve already learned the basics of missing values earlier in the book.\n你已经在本书的前面部分学习了缺失值的基础知识。\nYou first saw them in Chapter 1 where they resulted in a warning when making a plot as well as in Section 3.5.2 where they interfered with computing summary statistics, and you learned about their infectious nature and how to check for their presence in Section 12.2.2.\n你第一次见到它们是在 Chapter 1 中，它们在制作图表时导致了一个警告；在 Section 3.5.2 中，它们干扰了摘要统计的计算；在 Section 12.2.2 中，你学习了它们的传染性以及如何检查它们的存在。\nNow we’ll come back to them in more depth, so you can learn more of the details.\n现在我们将更深入地探讨它们，以便你了解更多细节。\nWe’ll start by discussing some general tools for working with missing values recorded as NAs.\n我们将从讨论一些处理被记录为 NA 的缺失值的通用工具开始。\nWe’ll then explore the idea of implicitly missing values, values that are simply absent from your data, and show some tools you can use to make them explicit.\n然后，我们将探讨隐式缺失值的概念，即那些根本不存在于你的数据中的值，并展示一些可以用来将它们显式化的工具。\nWe’ll finish off with a related discussion of empty groups, caused by factor levels that don’t appear in the data.\n最后，我们将以一个相关的讨论结束，即空组，这是由未出现在数据中的因子水平引起的。",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Missing values</span>"
    ]
  },
  {
    "objectID": "missing-values.html#introduction",
    "href": "missing-values.html#introduction",
    "title": "18  Missing values",
    "section": "",
    "text": "18.1.1 Prerequisites\nThe functions for working with missing data mostly come from dplyr and tidyr, which are core members of the tidyverse.\n处理缺失数据的函数主要来自 dplyr 和 tidyr，它们是 tidyverse 的核心成员。\n\nlibrary(tidyverse)",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Missing values</span>"
    ]
  },
  {
    "objectID": "missing-values.html#explicit-missing-values",
    "href": "missing-values.html#explicit-missing-values",
    "title": "18  Missing values",
    "section": "\n18.2 Explicit missing values",
    "text": "18.2 Explicit missing values\nTo begin, let’s explore a few handy tools for creating or eliminating missing explicit values, i.e. cells where you see an NA.\n首先，让我们来探索一些方便的工具，用于创建或消除显式缺失值，即那些你看到 NA 的单元格。\n\n18.2.1 Last observation carried forward\nA common use for missing values is as a data entry convenience.\n缺失值的一个常见用途是作为数据录入的便利手段。\nWhen data is entered by hand, missing values sometimes indicate that the value in the previous row has been repeated (or carried forward):\n当手动输入数据时，缺失值有时表示前一行的值被重复（或结转）了：\n\ntreatment &lt;- tribble(\n  ~person,           ~treatment, ~response,\n  \"Derrick Whitmore\", 1,         7,\n  NA,                 2,         10,\n  NA,                 3,         NA,\n  \"Katherine Burke\",  1,         4\n)\n\nYou can fill in these missing values with tidyr::fill().\n你可以使用 tidyr::fill() 来填充这些缺失值。\nIt works like select(), taking a set of columns:\n它的工作方式类似于 select()，接受一组列：\n\ntreatment |&gt;\n  fill(everything())\n#&gt; # A tibble: 4 × 3\n#&gt;   person           treatment response\n#&gt;   &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 Derrick Whitmore         1        7\n#&gt; 2 Derrick Whitmore         2       10\n#&gt; 3 Derrick Whitmore         3       10\n#&gt; 4 Katherine Burke          1        4\n\nThis treatment is sometimes called “last observation carried forward”, or locf for short.\n这种处理方法有时被称为“末次观测值结转法”，简称 locf (last observation carried forward)。\nYou can use the .direction argument to fill in missing values that have been generated in more exotic ways.\n你可以使用 .direction 参数来填充以更特殊方式生成的缺失值。\n\n18.2.2 Fixed values\nSome times missing values represent some fixed and known value, most commonly 0.\n有时缺失值代表某个固定的已知值，最常见的是 0。\nYou can use dplyr::coalesce() to replace them:\n你可以使用 dplyr::coalesce() 来替换它们：\n\nx &lt;- c(1, 4, 5, 7, NA)\ncoalesce(x, 0)\n#&gt; [1] 1 4 5 7 0\n\nSometimes you’ll hit the opposite problem where some concrete value actually represents a missing value.\n有时你会遇到相反的问题，即某个具体的值实际上代表一个缺失值。\nThis typically arises in data generated by older software that doesn’t have a proper way to represent missing values, so it must instead use some special value like 99 or -999.\n这通常发生在由旧软件生成的数据中，这些软件没有合适的方式来表示缺失值，因此必须使用一些特殊值，如 99 或 -999。\nIf possible, handle this when reading in the data, for example, by using the na argument to readr::read_csv(), e.g., read_csv(path, na = \"99\").\n如果可能的话，在读入数据时处理这个问题，例如，通过使用 readr::read_csv() 的 na 参数，如 read_csv(path, na = \"99\")。\nIf you discover the problem later, or your data source doesn’t provide a way to handle it on read, you can use dplyr::na_if():\n如果你后来才发现这个问题，或者你的数据源没有提供在读取时处理它的方法，你可以使用 dplyr::na_if()：\n\nx &lt;- c(1, 4, 5, 7, -99)\nna_if(x, -99)\n#&gt; [1]  1  4  5  7 NA\n\n\n18.2.3 NaN\nBefore we continue, there’s one special type of missing value that you’ll encounter from time to time: a NaN (pronounced “nan”), or not a number.\n在继续之前，有一种你偶尔会遇到的特殊类型的缺失值：NaN（发音为“nan”），即 not a number (非数值)。\nIt’s not that important to know about because it generally behaves just like NA:\n了解它并不是那么重要，因为它通常表现得就像 NA 一样：\n\nx &lt;- c(NA, NaN)\nx * 10\n#&gt; [1]  NA NaN\nx == 1\n#&gt; [1] NA NA\nis.na(x)\n#&gt; [1] TRUE TRUE\n\nIn the rare case you need to distinguish an NA from a NaN, you can use is.nan(x).\n在极少数情况下，如果你需要区分 NA 和 NaN，可以使用 is.nan(x)。\nYou’ll generally encounter a NaN when you perform a mathematical operation that has an indeterminate result:\n你通常会在执行结果不确定的数学运算时遇到 NaN：\n\n0 / 0 \n#&gt; [1] NaN\n0 * Inf\n#&gt; [1] NaN\nInf - Inf\n#&gt; [1] NaN\nsqrt(-1)\n#&gt; Warning in sqrt(-1): NaNs produced\n#&gt; [1] NaN",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Missing values</span>"
    ]
  },
  {
    "objectID": "missing-values.html#sec-missing-implicit",
    "href": "missing-values.html#sec-missing-implicit",
    "title": "18  Missing values",
    "section": "\n18.3 Implicit missing values",
    "text": "18.3 Implicit missing values\nSo far we’ve talked about missing values that are explicitly missing, i.e. you can see an NA in your data.\n到目前为止，我们讨论的都是显式缺失的值，也就是说，你可以在数据中看到一个 NA。\nBut missing values can also be implicitly missing, if an entire row of data is simply absent from the data.\n但缺失值也可能是隐式的，如果一整行数据根本就不在数据中。\nLet’s illustrate the difference with a simple dataset that records the price of some stock each quarter:\n让我们用一个记录某只股票每个季度价格的简单数据集来说明这种差异：\n\nstocks &lt;- tibble(\n  year  = c(2020, 2020, 2020, 2020, 2021, 2021, 2021),\n  qtr   = c(   1,    2,    3,    4,    2,    3,    4),\n  price = c(1.88, 0.59, 0.35,   NA, 0.92, 0.17, 2.66)\n)\n\nThis dataset has two missing observations:\n这个数据集有两个缺失的观测值：\n\nThe price in the fourth quarter of 2020 is explicitly missing, because its value is NA.\n2020 年第四季度的 price 是显式缺失的，因为它的值是 NA。\nThe price for the first quarter of 2021 is implicitly missing, because it simply does not appear in the dataset.\n2021 年第一季度的 price 是隐式缺失的，因为它根本没有出现在数据集中。\n\nOne way to think about the difference is with this Zen-like koan:\n理解这种差异的一种方式是这个富有禅意的公案：\n\nAn explicit missing value is the presence of an absence.\nAn implicit missing value is the absence of a presence.\n显式缺失是“无”之所在。\n隐式缺失是“在”之所无。\n\nSometimes you want to make implicit missings explicit in order to have something physical to work with.\n有时你想要将隐式缺失显式化，以便有一个实体可以操作。\nIn other cases, explicit missings are forced upon you by the structure of the data and you want to get rid of them.\n在其他情况下，数据的结构会迫使你面对显式缺失，而你想要摆脱它们。\nThe following sections discuss some tools for moving between implicit and explicit missingness.\n以下各节讨论了一些在隐式和显式缺失之间转换的工具。\n\n18.3.1 Pivoting\nYou’ve already seen one tool that can make implicit missings explicit and vice versa: pivoting.\n你已经见过一个可以在隐式缺失和显式缺失之间相互转换的工具：透视 (pivoting)。\nMaking data wider can make implicit missing values explicit because every combination of the rows and new columns must have some value.\n将数据变宽可以使隐式缺失值显式化，因为行和新列的每种组合都必须有某个值。\nFor example, if we pivot stocks to put the quarter in the columns, both missing values become explicit:\n例如，如果我们将 stocks 数据进行透视，把 quarter 放到列中，那么两个缺失值都会变得显式：\n\nstocks |&gt;\n  pivot_wider(\n    names_from = qtr, \n    values_from = price\n  )\n#&gt; # A tibble: 2 × 5\n#&gt;    year   `1`   `2`   `3`   `4`\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  2020  1.88  0.59  0.35 NA   \n#&gt; 2  2021 NA     0.92  0.17  2.66\n\nBy default, making data longer preserves explicit missing values, but if they are structurally missing values that only exist because the data is not tidy, you can drop them (make them implicit) by setting values_drop_na = TRUE.\n默认情况下，将数据变长会保留显式缺失值，但如果它们是由于数据不整洁而存在的结构性缺失值，你可以通过设置 values_drop_na = TRUE 来丢弃它们（使其变为隐式）。\nSee the examples in Section 5.2 for more details.\n更多细节请参见 Section 5.2 中的示例。\n\n18.3.2 Complete\ntidyr::complete() allows you to generate explicit missing values by providing a set of variables that define the combination of rows that should exist.tidyr::complete() 允许你通过提供一组定义应存在的行组合的变量来生成显式缺失值。\nFor example, we know that all combinations of year and qtr should exist in the stocks data:\n例如，我们知道 stocks 数据中应该存在 year 和 qtr 的所有组合：\n\nstocks |&gt;\n  complete(year, qtr)\n#&gt; # A tibble: 8 × 3\n#&gt;    year   qtr price\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  2020     1  1.88\n#&gt; 2  2020     2  0.59\n#&gt; 3  2020     3  0.35\n#&gt; 4  2020     4 NA   \n#&gt; 5  2021     1 NA   \n#&gt; 6  2021     2  0.92\n#&gt; # ℹ 2 more rows\n\nTypically, you’ll call complete() with names of existing variables, filling in the missing combinations.\n通常，你会使用现有变量的名称来调用 complete()，以填补缺失的组合。\nHowever, sometimes the individual variables are themselves incomplete, so you can instead provide your own data.\n然而，有时单个变量本身就是不完整的，所以你可以提供自己的数据。\nFor example, you might know that the stocks dataset is supposed to run from 2019 to 2021, so you could explicitly supply those values for year:\n例如，你可能知道 stocks 数据集应该从 2019 年运行到 2021 年，所以你可以明确地为 year 提供这些值：\n\nstocks |&gt;\n  complete(year = 2019:2021, qtr)\n#&gt; # A tibble: 12 × 3\n#&gt;    year   qtr price\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  2019     1 NA   \n#&gt; 2  2019     2 NA   \n#&gt; 3  2019     3 NA   \n#&gt; 4  2019     4 NA   \n#&gt; 5  2020     1  1.88\n#&gt; 6  2020     2  0.59\n#&gt; # ℹ 6 more rows\n\nIf the range of a variable is correct, but not all values are present, you could use full_seq(x, 1) to generate all values from min(x) to max(x) spaced out by 1.\n如果一个变量的范围是正确的，但并非所有值都存在，你可以使用 full_seq(x, 1) 来生成从 min(x) 到 max(x) 之间所有以 1 为间隔的值。\nIn some cases, the complete set of observations can’t be generated by a simple combination of variables.\n在某些情况下，完整的观测集无法通过变量的简单组合生成。\nIn that case, you can do manually what complete() does for you: create a data frame that contains all the rows that should exist (using whatever combination of techniques you need), then combine it with your original dataset with dplyr::full_join().\n在这种情况下，你可以手动完成 complete() 为你做的事情：创建一个包含所有应存在的行的数据框（使用你需要的任何技术组合），然后使用 dplyr::full_join() 将其与原始数据集结合起来。\n\n18.3.3 Joins\nThis brings us to another important way of revealing implicitly missing observations: joins.\n这就引出了另一种揭示隐式缺失观测值的重要方法：连接 (joins)。\nYou’ll learn more about joins in Chapter 19, but we wanted to quickly mention them to you here since you can often only know that values are missing from one dataset when you compare it to another.\n你将在 Chapter 19 中学习更多关于连接的知识，但我们想在这里快速提及它们，因为你通常只有在将一个数据集与另一个数据集进行比较时，才能知道其中的值是缺失的。\ndplyr::anti_join(x, y) is a particularly useful tool here because it selects only the rows in x that don’t have a match in y.dplyr::anti_join(x, y) 在这里是一个特别有用的工具，因为它只选择 x 中在 y 中没有匹配项的行。\nFor example, we can use two anti_join()s to reveal that we’re missing information for four airports and 722 planes mentioned in flights:\n例如，我们可以使用两个 anti_join() 来揭示我们缺少 flights 中提到的四个机场和 722 架飞机的信息：\n\nlibrary(nycflights13)\n\nflights |&gt; \n  distinct(faa = dest) |&gt; \n  anti_join(airports)\n#&gt; Joining with `by = join_by(faa)`\n#&gt; # A tibble: 4 × 1\n#&gt;   faa  \n#&gt;   &lt;chr&gt;\n#&gt; 1 BQN  \n#&gt; 2 SJU  \n#&gt; 3 STT  \n#&gt; 4 PSE\n\nflights |&gt; \n  distinct(tailnum) |&gt; \n  anti_join(planes)\n#&gt; Joining with `by = join_by(tailnum)`\n#&gt; # A tibble: 722 × 1\n#&gt;   tailnum\n#&gt;   &lt;chr&gt;  \n#&gt; 1 N3ALAA \n#&gt; 2 N3DUAA \n#&gt; 3 N542MQ \n#&gt; 4 N730MQ \n#&gt; 5 N9EAMQ \n#&gt; 6 N532UA \n#&gt; # ℹ 716 more rows\n\n\n18.3.4 Exercises\n\nCan you find any relationship between the carrier and the rows that appear to be missing from planes?",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Missing values</span>"
    ]
  },
  {
    "objectID": "missing-values.html#factors-and-empty-groups",
    "href": "missing-values.html#factors-and-empty-groups",
    "title": "18  Missing values",
    "section": "\n18.4 Factors and empty groups",
    "text": "18.4 Factors and empty groups\nA final type of missingness is the empty group, a group that doesn’t contain any observations, which can arise when working with factors.\n最后一种缺失类型是空组，即不包含任何观测值的组，这在使用因子 (factors) 时可能会出现。\nFor example, imagine we have a dataset that contains some health information about people:\n例如，假设我们有一个包含一些人健康信息的数据集：\n\nhealth &lt;- tibble(\n  name   = c(\"Ikaia\", \"Oletta\", \"Leriah\", \"Dashay\", \"Tresaun\"),\n  smoker = factor(c(\"no\", \"no\", \"no\", \"no\", \"no\"), levels = c(\"yes\", \"no\")),\n  age    = c(34, 88, 75, 47, 56),\n)\n\nAnd we want to count the number of smokers with dplyr::count():\n我们想用 dplyr::count() 来计算吸烟者的数量：\n\nhealth |&gt; count(smoker)\n#&gt; # A tibble: 1 × 2\n#&gt;   smoker     n\n#&gt;   &lt;fct&gt;  &lt;int&gt;\n#&gt; 1 no         5\n\nThis dataset only contains non-smokers, but we know that smokers exist; the group of non-smokers is empty.\n这个数据集只包含非吸烟者，但我们知道吸烟者是存在的；吸烟者这个组是空的。\nWe can request count() to keep all the groups, even those not seen in the data by using .drop = FALSE:\n我们可以通过使用 .drop = FALSE 来要求 count() 保留所有的组，即使是那些在数据中未出现的组：\n\nhealth |&gt; count(smoker, .drop = FALSE)\n#&gt; # A tibble: 2 × 2\n#&gt;   smoker     n\n#&gt;   &lt;fct&gt;  &lt;int&gt;\n#&gt; 1 yes        0\n#&gt; 2 no         5\n\nThe same principle applies to ggplot2’s discrete axes, which will also drop levels that don’t have any values.\n同样的原则也适用于 ggplot2 的离散坐标轴，它也会丢弃没有任何值的水平 (levels)。\nYou can force them to display by supplying drop = FALSE to the appropriate discrete axis:\n你可以通过向相应的离散坐标轴提供 drop = FALSE 来强制显示它们：\nggplot(health, aes(x = smoker)) +\n  geom_bar() +\n  scale_x_discrete()\n\nggplot(health, aes(x = smoker)) +\n  geom_bar() +\n  scale_x_discrete(drop = FALSE)\n\n\n\n\n\n\n\n\n\n\nThe same problem comes up more generally with dplyr::group_by().\n更普遍地，dplyr::group_by() 也会出现同样的问题。\nAnd again you can use .drop = FALSE to preserve all factor levels:\n同样，你可以使用 .drop = FALSE 来保留所有的因子水平：\n\nhealth |&gt; \n  group_by(smoker, .drop = FALSE) |&gt; \n  summarize(\n    n = n(),\n    mean_age = mean(age),\n    min_age = min(age),\n    max_age = max(age),\n    sd_age = sd(age)\n  )\n#&gt; # A tibble: 2 × 6\n#&gt;   smoker     n mean_age min_age max_age sd_age\n#&gt;   &lt;fct&gt;  &lt;int&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 yes        0      NaN     Inf    -Inf   NA  \n#&gt; 2 no         5       60      34      88   21.6\n\nWe get some interesting results here because when summarizing an empty group, the summary functions are applied to zero-length vectors.\n我们在这里得到了一些有趣的结果，因为当对一个空组进行汇总时，汇总函数被应用于长度为零的向量。\nThere’s an important distinction between empty vectors, which have length 0, and missing values, each of which has length 1.\n空向量（长度为 0）和缺失值（每个长度为 1）之间有一个重要的区别。\n\n# A vector containing two missing values\nx1 &lt;- c(NA, NA)\nlength(x1)\n#&gt; [1] 2\n\n# A vector containing nothing\nx2 &lt;- numeric()\nlength(x2)\n#&gt; [1] 0\n\nAll summary functions work with zero-length vectors, but they may return results that are surprising at first glance.\n所有的汇总函数都可以处理零长度向量，但它们返回的结果乍一看可能会令人惊讶。\nHere we see mean(age) returning NaN because mean(age) = sum(age)/length(age) which here is 0/0.\n在这里我们看到 mean(age) 返回 NaN，因为 mean(age) = sum(age)/length(age)，在这里是 0/0。\nmax() and min() return -Inf and Inf for empty vectors so if you combine the results with a non-empty vector of new data and recompute you’ll get the minimum or maximum of the new data1.\n对于空向量，max() 和 min() 会返回 -Inf 和 Inf，所以如果你将结果与一个新的非空向量数据结合起来重新计算，你将得到新数据的最小值或最大值1。\nSometimes a simpler approach is to perform the summary and then make the implicit missings explicit with complete().\n有时，一个更简单的方法是先执行汇总，然后使用 complete() 将隐式缺失显式化。\n\nhealth |&gt; \n  group_by(smoker) |&gt; \n  summarize(\n    n = n(),\n    mean_age = mean(age),\n    min_age = min(age),\n    max_age = max(age),\n    sd_age = sd(age)\n  ) |&gt; \n  complete(smoker)\n#&gt; # A tibble: 2 × 6\n#&gt;   smoker     n mean_age min_age max_age sd_age\n#&gt;   &lt;fct&gt;  &lt;int&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 yes       NA       NA      NA      NA   NA  \n#&gt; 2 no         5       60      34      88   21.6\n\nThe main drawback of this approach is that you get an NA for the count, even though you know that it should be zero.\n这种方法的主要缺点是，尽管你知道计数应该为零，但你却得到了一个 NA。",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Missing values</span>"
    ]
  },
  {
    "objectID": "missing-values.html#summary",
    "href": "missing-values.html#summary",
    "title": "18  Missing values",
    "section": "\n18.5 Summary",
    "text": "18.5 Summary\nMissing values are weird!\n缺失值很奇怪！\nSometimes they’re recorded as an explicit NA but other times you only notice them by their absence.\n有时它们被记录为显式的 NA，但其他时候你只能通过它们的缺席来注意到它们。\nThis chapter has given you some tools for working with explicit missing values, tools for uncovering implicit missing values, and discussed some of the ways that implicit can become explicit and vice versa.\n本章为你提供了一些处理显式缺失值的工具，一些揭示隐式缺失值的工具，并讨论了隐式如何变为显式以及反之亦然的一些方法。\nIn the next chapter, we tackle the final chapter in this part of the book: joins.\n在下一章中，我们将探讨本书这一部分的最后一章：连接 (joins)。\nThis is a bit of a change from the chapters so far because we’re going to discuss tools that work with data frames as a whole, not something that you put inside a data frame.\n这与到目前为止的章节有些不同，因为我们将要讨论的是作用于整个数据框的工具，而不是你放在数据框内部的东西。",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Missing values</span>"
    ]
  },
  {
    "objectID": "missing-values.html#footnotes",
    "href": "missing-values.html#footnotes",
    "title": "18  Missing values",
    "section": "",
    "text": "In other words, min(c(x, y)) is always equal to min(min(x), min(y)).↩︎",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Missing values</span>"
    ]
  },
  {
    "objectID": "joins.html",
    "href": "joins.html",
    "title": "19  Joins",
    "section": "",
    "text": "19.1 Introduction\nIt’s rare that a data analysis involves only a single data frame.\n数据分析很少只涉及单个数据框。\nTypically you have many data frames, and you must join them together to answer the questions that you’re interested in.\n通常你会有很多数据框，你必须将它们 连接 (join) 在一起才能回答你感兴趣的问题。\nThis chapter will introduce you to two important types of joins:\n本章将向你介绍两种重要的连接类型：\nWe’ll begin by discussing keys, the variables used to connect a pair of data frames in a join.\n我们将从讨论键 (keys) 开始，键是用于在连接中连接一对数据框的变量。\nWe cement the theory with an examination of the keys in the datasets from the nycflights13 package, then use that knowledge to start joining data frames together.\n我们将通过检查 nycflights13 包中数据集的键来巩固理论，然后利用这些知识开始将数据框连接在一起。\nNext we’ll discuss how joins work, focusing on their action on the rows.\n接下来我们将讨论连接的工作原理，重点关注它们对行的操作。\nWe’ll finish up with a discussion of non-equi joins, a family of joins that provide a more flexible way of matching keys than the default equality relationship.\n最后，我们将讨论非等值连接 (non-equi joins)，这类连接提供了一种比默认的相等关系更灵活的键匹配方式。",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Joins</span>"
    ]
  },
  {
    "objectID": "joins.html#introduction",
    "href": "joins.html#introduction",
    "title": "19  Joins",
    "section": "",
    "text": "Mutating joins, which add new variables to one data frame from matching observations in another.\n修改连接 (Mutating joins)，它将一个数据框中的匹配观测值的新变量添加到另一个数据框中。\nFiltering joins, which filter observations from one data frame based on whether or not they match an observation in another.\n过滤连接 (Filtering joins)，它根据一个数据框中的观测值是否与另一个数据框中的观测值匹配来过滤它们。\n\n\n\n\n\n\n19.1.1 Prerequisites\nIn this chapter, we’ll explore the five related datasets from nycflights13 using the join functions from dplyr.\n在本章中，我们将使用 dplyr 中的连接函数来探索 nycflights13 中的五个相关数据集。\n\nlibrary(tidyverse)\nlibrary(nycflights13)",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Joins</span>"
    ]
  },
  {
    "objectID": "joins.html#keys",
    "href": "joins.html#keys",
    "title": "19  Joins",
    "section": "\n19.2 Keys",
    "text": "19.2 Keys\nTo understand joins, you need to first understand how two tables can be connected through a pair of keys, within each table.\n要理解连接，你首先需要理解两个表如何通过每个表内的一对键进行连接。\nIn this section, you’ll learn about the two types of key and see examples of both in the datasets of the nycflights13 package.\n在本节中，你将学习两种类型的键，并在 nycflights13 包的数据集中看到这两种键的示例。\nYou’ll also learn how to check that your keys are valid, and what to do if your table lacks a key.\n你还将学习如何检查你的键是否有效，以及当你的表缺少键时该怎么做。\n\n19.2.1 Primary and foreign keys\nEvery join involves a pair of keys: a primary key and a foreign key.\n每个连接都涉及一对键：主键 (primary key) 和外键 (foreign key)。\nA primary key is a variable or set of variables that uniquely identifies each observation.主键 (primary key) 是一个或一组唯一标识每个观测值的变量。\nWhen more than one variable is needed, the key is called a compound key. For example, in nycflights13:\n当需要多个变量时，该键称为 复合键 (compound key)。例如，在 nycflights13 中：\n\n\nairlines records two pieces of data about each airline: its carrier code and its full name. You can identify an airline with its two letter carrier code, making carrier the primary key.airlines 记录了每家航空公司的两部分数据：其承运人代码 (carrier code) 和其全名。你可以用其两位字母的承运人代码来识别一家航空公司，因此 carrier 是主键。\n\nairlines\n#&gt; # A tibble: 16 × 2\n#&gt;   carrier name                    \n#&gt;   &lt;chr&gt;   &lt;chr&gt;                   \n#&gt; 1 9E      Endeavor Air Inc.       \n#&gt; 2 AA      American Airlines Inc.  \n#&gt; 3 AS      Alaska Airlines Inc.    \n#&gt; 4 B6      JetBlue Airways         \n#&gt; 5 DL      Delta Air Lines Inc.    \n#&gt; 6 EV      ExpressJet Airlines Inc.\n#&gt; # ℹ 10 more rows\n\n\n\nairports records data about each airport. You can identify each airport by its three letter airport code, making faa the primary key.airports 记录了每个机场的数据。你可以用其三位字母的机场代码来识别每个机场，因此 faa 是主键。\n\nairports\n#&gt; # A tibble: 1,458 × 8\n#&gt;   faa   name                            lat   lon   alt    tz dst  \n#&gt;   &lt;chr&gt; &lt;chr&gt;                         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n#&gt; 1 04G   Lansdowne Airport              41.1 -80.6  1044    -5 A    \n#&gt; 2 06A   Moton Field Municipal Airport  32.5 -85.7   264    -6 A    \n#&gt; 3 06C   Schaumburg Regional            42.0 -88.1   801    -6 A    \n#&gt; 4 06N   Randall Airport                41.4 -74.4   523    -5 A    \n#&gt; 5 09J   Jekyll Island Airport          31.1 -81.4    11    -5 A    \n#&gt; 6 0A9   Elizabethton Municipal Airpo…  36.4 -82.2  1593    -5 A    \n#&gt; # ℹ 1,452 more rows\n#&gt; # ℹ 1 more variable: tzone &lt;chr&gt;\n\n\n\nplanes records data about each plane. You can identify a plane by its tail number, making tailnum the primary key.planes 记录了每架飞机的数据。你可以用其尾号 (tail number) 来识别一架飞机，因此 tailnum 是主键。\n\nplanes\n#&gt; # A tibble: 3,322 × 9\n#&gt;   tailnum  year type              manufacturer    model     engines\n#&gt;   &lt;chr&gt;   &lt;int&gt; &lt;chr&gt;             &lt;chr&gt;           &lt;chr&gt;       &lt;int&gt;\n#&gt; 1 N10156   2004 Fixed wing multi… EMBRAER         EMB-145XR       2\n#&gt; 2 N102UW   1998 Fixed wing multi… AIRBUS INDUSTR… A320-214        2\n#&gt; 3 N103US   1999 Fixed wing multi… AIRBUS INDUSTR… A320-214        2\n#&gt; 4 N104UW   1999 Fixed wing multi… AIRBUS INDUSTR… A320-214        2\n#&gt; 5 N10575   2002 Fixed wing multi… EMBRAER         EMB-145LR       2\n#&gt; 6 N105UW   1999 Fixed wing multi… AIRBUS INDUSTR… A320-214        2\n#&gt; # ℹ 3,316 more rows\n#&gt; # ℹ 3 more variables: seats &lt;int&gt;, speed &lt;int&gt;, engine &lt;chr&gt;\n\n\n\nweather records data about the weather at the origin airports. You can identify each observation by the combination of location and time, making origin and time_hour the compound primary key.weather 记录了始发机场的天气数据。你可以通过位置和时间的组合来识别每个观测值，因此 origin 和 time_hour 是复合主键。\n\nweather\n#&gt; # A tibble: 26,115 × 15\n#&gt;   origin  year month   day  hour  temp  dewp humid wind_dir\n#&gt;   &lt;chr&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 EWR     2013     1     1     1  39.0  26.1  59.4      270\n#&gt; 2 EWR     2013     1     1     2  39.0  27.0  61.6      250\n#&gt; 3 EWR     2013     1     1     3  39.0  28.0  64.4      240\n#&gt; 4 EWR     2013     1     1     4  39.9  28.0  62.2      250\n#&gt; 5 EWR     2013     1     1     5  39.0  28.0  64.4      260\n#&gt; 6 EWR     2013     1     1     6  37.9  28.0  67.2      240\n#&gt; # ℹ 26,109 more rows\n#&gt; # ℹ 6 more variables: wind_speed &lt;dbl&gt;, wind_gust &lt;dbl&gt;, …\n\n\n\nA foreign key is a variable (or set of variables) that corresponds to a primary key in another table.外键 (foreign key) 是一个（或一组）与另一个表中的主键相对应的变量。\nFor example:\n例如：\n\nflights$tailnum is a foreign key that corresponds to the primary key planes$tailnum.flights$tailnum 是一个外键，它对应于主键 planes$tailnum。\nflights$carrier is a foreign key that corresponds to the primary key airlines$carrier.flights$carrier 是一个外键，它对应于主键 airlines$carrier。\nflights$origin is a foreign key that corresponds to the primary key airports$faa.flights$origin 是一个外键，它对应于主键 airports$faa。\nflights$dest is a foreign key that corresponds to the primary key airports$faa.flights$dest 是一个外键，它对应于主键 airports$faa。\nflights$origin-flights$time_hour is a compound foreign key that corresponds to the compound primary key weather$origin-weather$time_hour.flights$origin-flights$time_hour 是一个复合外键，它对应于复合主键 weather$origin-weather$time_hour。\n\nThese relationships are summarized visually in Figure 19.1.\n这些关系在 Figure 19.1 中进行了可视化总结。\n\n\n\n\n\n\n\nFigure 19.1: Connections between all five data frames in the nycflights13 package. Variables making up a primary key are colored grey, and are connected to their corresponding foreign keys with arrows.\n\n\n\n\nYou’ll notice a nice feature in the design of these keys: the primary and foreign keys almost always have the same names, which, as you’ll see shortly, will make your joining life much easier.\n你会注意到这些键的设计中有一个很好的特性：主键和外键几乎总是有相同的名称，正如你很快就会看到的，这将使你的连接工作变得容易得多。\nIt’s also worth noting the opposite relationship: almost every variable name used in multiple tables has the same meaning in each place.\n同样值得注意的是相反的关系：几乎每个在多个表中使用的变量名在每个地方都有相同的含义。\nThere’s only one exception: year means year of departure in flights and year manufactured in planes.\n只有一个例外：year 在 flights 中表示起飞年份，在 planes 中表示制造年份。\nThis will become important when we start actually joining tables together.\n当我们开始实际连接表时，这一点将变得很重要。\n\n19.2.2 Checking primary keys\nNow that that we’ve identified the primary keys in each table, it’s good practice to verify that they do indeed uniquely identify each observation.\n既然我们已经确定了每个表中的主键，那么验证它们确实唯一地标识了每个观测值是一个好习惯。\nOne way to do that is to count() the primary keys and look for entries where n is greater than one.\n一种方法是使用 count() 计算主键，并查找 n 大于 1 的条目。\nThis reveals that planes and weather both look good:\n这表明 planes 和 weather 都看起来不错：\n\nplanes |&gt; \n  count(tailnum) |&gt; \n  filter(n &gt; 1)\n#&gt; # A tibble: 0 × 2\n#&gt; # ℹ 2 variables: tailnum &lt;chr&gt;, n &lt;int&gt;\n\nweather |&gt; \n  count(time_hour, origin) |&gt; \n  filter(n &gt; 1)\n#&gt; # A tibble: 0 × 3\n#&gt; # ℹ 3 variables: time_hour &lt;dttm&gt;, origin &lt;chr&gt;, n &lt;int&gt;\n\nYou should also check for missing values in your primary keys — if a value is missing then it can’t identify an observation!\n你还应该检查主键中的缺失值——如果一个值是缺失的，那么它就无法识别一个观测值！\n\nplanes |&gt; \n  filter(is.na(tailnum))\n#&gt; # A tibble: 0 × 9\n#&gt; # ℹ 9 variables: tailnum &lt;chr&gt;, year &lt;int&gt;, type &lt;chr&gt;, manufacturer &lt;chr&gt;,\n#&gt; #   model &lt;chr&gt;, engines &lt;int&gt;, seats &lt;int&gt;, speed &lt;int&gt;, engine &lt;chr&gt;\n\nweather |&gt; \n  filter(is.na(time_hour) | is.na(origin))\n#&gt; # A tibble: 0 × 15\n#&gt; # ℹ 15 variables: origin &lt;chr&gt;, year &lt;int&gt;, month &lt;int&gt;, day &lt;int&gt;,\n#&gt; #   hour &lt;int&gt;, temp &lt;dbl&gt;, dewp &lt;dbl&gt;, humid &lt;dbl&gt;, wind_dir &lt;dbl&gt;, …\n\n\n19.2.3 Surrogate keys\nSo far we haven’t talked about the primary key for flights.\n到目前为止，我们还没有讨论 flights 的主键。\nIt’s not super important here, because there are no data frames that use it as a foreign key, but it’s still useful to consider because it’s easier to work with observations if we have some way to describe them to others.\n在这里它不是特别重要，因为没有数据框使用它作为外键，但它仍然值得考虑，因为如果我们有某种方式向他人描述观测值，处理它们会更容易。\nAfter a little thinking and experimentation, we determined that there are three variables that together uniquely identify each flight:\n经过一番思考和实验，我们确定有三个变量可以共同唯一地识别每个航班：\n\nflights |&gt; \n  count(time_hour, carrier, flight) |&gt; \n  filter(n &gt; 1)\n#&gt; # A tibble: 0 × 4\n#&gt; # ℹ 4 variables: time_hour &lt;dttm&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, n &lt;int&gt;\n\nDoes the absence of duplicates automatically make time_hour-carrier-flight a primary key?\n没有重复值是否会自动使 time_hour-carrier-flight 成为主键？\nIt’s certainly a good start, but it doesn’t guarantee it.\n这当然是一个好的开始，但并不能保证。\nFor example, are altitude and latitude a good primary key for airports?\n例如，高度和纬度是 airports 的好主键吗？\n\nairports |&gt;\n  count(alt, lat) |&gt; \n  filter(n &gt; 1)\n#&gt; # A tibble: 1 × 3\n#&gt;     alt   lat     n\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n#&gt; 1    13  40.6     2\n\nIdentifying an airport by its altitude and latitude is clearly a bad idea, and in general it’s not possible to know from the data alone whether or not a combination of variables makes a good a primary key.\n通过高度和纬度来识别一个机场显然是一个坏主意，而且通常来说，仅从数据本身无法判断一个变量组合是否能构成一个好的主键。\nBut for flights, the combination of time_hour, carrier, and flight seems reasonable because it would be really confusing for an airline and its customers if there were multiple flights with the same flight number in the air at the same time.\n但对于航班来说，time_hour、carrier 和 flight 的组合似乎是合理的，因为如果同一时间有多架相同航班号的飞机在空中，对航空公司及其乘客来说会非常混乱。\nThat said, we might be better off introducing a simple numeric surrogate key using the row number:\n话虽如此，我们最好还是使用行号引入一个简单的数字代理键 (surrogate key)：\n\nflights2 &lt;- flights |&gt; \n  mutate(id = row_number(), .before = 1)\nflights2\n#&gt; # A tibble: 336,776 × 20\n#&gt;      id  year month   day dep_time sched_dep_time dep_delay arr_time\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n#&gt; 1     1  2013     1     1      517            515         2      830\n#&gt; 2     2  2013     1     1      533            529         4      850\n#&gt; 3     3  2013     1     1      542            540         2      923\n#&gt; 4     4  2013     1     1      544            545        -1     1004\n#&gt; 5     5  2013     1     1      554            600        -6      812\n#&gt; 6     6  2013     1     1      554            558        -4      740\n#&gt; # ℹ 336,770 more rows\n#&gt; # ℹ 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, …\n\nSurrogate keys can be particularly useful when communicating to other humans: it’s much easier to tell someone to take a look at flight 2001 than to say look at UA430 which departed 9am 2013-01-03.\n代理键在与人交流时特别有用：告诉别人查看 2001 号航班比说查看 2013 年 1 月 3 日上午 9 点起飞的 UA430 航班要容易得多。\n\n19.2.4 Exercises\n\nWe forgot to draw the relationship between weather and airports in Figure 19.1. What is the relationship and how should it appear in the diagram?\nweather only contains information for the three origin airports in NYC. If it contained weather records for all airports in the USA, what additional connection would it make to flights?\nThe year, month, day, hour, and origin variables almost form a compound key for weather, but there’s one hour that has duplicate observations. Can you figure out what’s special about that hour?\nWe know that some days of the year are special and fewer people than usual fly on them (e.g., Christmas eve and Christmas day). How might you represent that data as a data frame? What would be the primary key? How would it connect to the existing data frames?\nDraw a diagram illustrating the connections between the Batting, People, and Salaries data frames in the Lahman package. Draw another diagram that shows the relationship between People, Managers, AwardsManagers. How would you characterize the relationship between the Batting, Pitching, and Fielding data frames?",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Joins</span>"
    ]
  },
  {
    "objectID": "joins.html#sec-mutating-joins",
    "href": "joins.html#sec-mutating-joins",
    "title": "19  Joins",
    "section": "\n19.3 Basic joins",
    "text": "19.3 Basic joins\nNow that you understand how data frames are connected via keys, we can start using joins to better understand the flights dataset.\n既然你已经了解了数据框是如何通过键连接的，我们就可以开始使用连接来更好地理解 flights 数据集了。\ndplyr provides six join functions: left_join(), inner_join(), right_join(), full_join(), semi_join(), and anti_join(). They all have the same interface: they take a pair of data frames (x and y) and return a data frame.\ndplyr 提供了六个连接函数：left_join()、inner_join()、right_join()、full_join()、semi_join() 和 anti_join()。它们都具有相同的接口：接收一对数据框 (x 和 y) 并返回一个数据框。\nThe order of the rows and columns in the output is primarily determined by x.\n输出中行和列的顺序主要由 x 决定。\nIn this section, you’ll learn how to use one mutating join, left_join(), and two filtering joins, semi_join() and anti_join().\n在本节中，你将学习如何使用一个修改连接 (left_join()) 和两个过滤连接 (semi_join() 和 anti_join())。\nIn the next section, you’ll learn exactly how these functions work, and about the remaining inner_join(), right_join() and full_join().\n在下一节中，你将确切地学习这些函数的工作原理，以及剩下的 inner_join()、right_join() 和 full_join()。\n\n19.3.1 Mutating joins\nA mutating join allows you to combine variables from two data frames: it first matches observations by their keys, then copies across variables from one data frame to the other.修改连接 (mutating join) 允许你合并来自两个数据框的变量：它首先通过它们的键匹配观测值，然后将变量从一个数据框复制到另一个。\nLike mutate(), the join functions add variables to the right, so if your dataset has many variables, you won’t see the new ones.\n与 mutate() 类似，连接函数会将变量添加到右侧，所以如果你的数据集有很多变量，你将看不到新添加的变量。\nFor these examples, we’ll make it easier to see what’s going on by creating a narrower dataset with just six variables1:\n对于这些例子，我们将创建一个只包含六个变量的更窄的数据集，以便更容易地看清楚发生了什么1：\n\nflights2 &lt;- flights |&gt; \n  select(year, time_hour, origin, dest, tailnum, carrier)\nflights2\n#&gt; # A tibble: 336,776 × 6\n#&gt;    year time_hour           origin dest  tailnum carrier\n#&gt;   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  \n#&gt; 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA     \n#&gt; 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA     \n#&gt; 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA     \n#&gt; 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6     \n#&gt; 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL     \n#&gt; 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA     \n#&gt; # ℹ 336,770 more rows\n\nThere are four types of mutating join, but there’s one that you’ll use almost all of the time: left_join().\n有四种类型的修改连接，但有一种你几乎会一直使用：left_join()。\nIt’s special because the output will always have the same rows as x, the data frame you’re joining to2.\n它之所以特殊，是因为输出将始终与你所连接的数据框 x 具有相同的行2。\nThe primary use of left_join() is to add in additional metadata.left_join() 的主要用途是添加额外的元数据 (metadata)。\nFor example, we can use left_join() to add the full airline name to the flights2 data:\n例如，我们可以使用 left_join() 将完整的航空公司名称添加到 flights2 数据中：\n\nflights2 |&gt;\n  left_join(airlines)\n#&gt; Joining with `by = join_by(carrier)`\n#&gt; # A tibble: 336,776 × 7\n#&gt;    year time_hour           origin dest  tailnum carrier name                \n#&gt;   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;               \n#&gt; 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      United Air Lines In…\n#&gt; 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      United Air Lines In…\n#&gt; 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      American Airlines I…\n#&gt; 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      JetBlue Airways     \n#&gt; 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      Delta Air Lines Inc.\n#&gt; 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      United Air Lines In…\n#&gt; # ℹ 336,770 more rows\n\nOr we could find out the temperature and wind speed when each plane departed:\n或者我们可以找出每架飞机起飞时的温度和风速：\n\nflights2 |&gt; \n  left_join(weather |&gt; select(origin, time_hour, temp, wind_speed))\n#&gt; Joining with `by = join_by(time_hour, origin)`\n#&gt; # A tibble: 336,776 × 8\n#&gt;    year time_hour           origin dest  tailnum carrier  temp wind_speed\n#&gt;   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA       39.0       12.7\n#&gt; 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA       39.9       15.0\n#&gt; 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA       39.0       15.0\n#&gt; 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6       39.0       15.0\n#&gt; 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL       39.9       16.1\n#&gt; 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA       39.0       12.7\n#&gt; # ℹ 336,770 more rows\n\nOr what size of plane was flying:\n或者当时飞的是什么尺寸的飞机：\n\nflights2 |&gt; \n  left_join(planes |&gt; select(tailnum, type, engines, seats))\n#&gt; Joining with `by = join_by(tailnum)`\n#&gt; # A tibble: 336,776 × 9\n#&gt;    year time_hour           origin dest  tailnum carrier type                \n#&gt;   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;               \n#&gt; 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      Fixed wing multi en…\n#&gt; 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      Fixed wing multi en…\n#&gt; 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      Fixed wing multi en…\n#&gt; 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      Fixed wing multi en…\n#&gt; 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      Fixed wing multi en…\n#&gt; 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      Fixed wing multi en…\n#&gt; # ℹ 336,770 more rows\n#&gt; # ℹ 2 more variables: engines &lt;int&gt;, seats &lt;int&gt;\n\nWhen left_join() fails to find a match for a row in x, it fills in the new variables with missing values.\n当 left_join() 无法为 x 中的某一行找到匹配项时，它会用缺失值填充新变量。\nFor example, there’s no information about the plane with tail number N3ALAA so the type, engines, and seats will be missing:\n例如，没有关于尾号为 N3ALAA 的飞机的信息，所以 type、engines 和 seats 将会是缺失值：\n\nflights2 |&gt; \n  filter(tailnum == \"N3ALAA\") |&gt; \n  left_join(planes |&gt; select(tailnum, type, engines, seats))\n#&gt; Joining with `by = join_by(tailnum)`\n#&gt; # A tibble: 63 × 9\n#&gt;    year time_hour           origin dest  tailnum carrier type  engines seats\n#&gt;   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n#&gt; 2  2013 2013-01-02 18:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n#&gt; 3  2013 2013-01-03 06:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n#&gt; 4  2013 2013-01-07 19:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n#&gt; 5  2013 2013-01-08 17:00:00 JFK    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n#&gt; 6  2013 2013-01-16 06:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n#&gt; # ℹ 57 more rows\n\nWe’ll come back to this problem a few times in the rest of the chapter.\n我们将在本章的其余部分几次回到这个问题。\n\n19.3.2 Specifying join keys\nBy default, left_join() will use all variables that appear in both data frames as the join key, the so called natural join.\n默认情况下，left_join() 会将两个数据框中都出现的所有变量用作连接键，这被称为 自然 (natural) 连接。\nThis is a useful heuristic, but it doesn’t always work.\n这是一个有用的启发式方法，但并不总是有效。\nFor example, what happens if we try to join flights2 with the complete planes dataset?\n例如，如果我们尝试将 flights2 与完整的 planes 数据集连接，会发生什么？\n\nflights2 |&gt; \n  left_join(planes)\n#&gt; Joining with `by = join_by(year, tailnum)`\n#&gt; # A tibble: 336,776 × 13\n#&gt;    year time_hour           origin dest  tailnum carrier type  manufacturer\n#&gt;   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;       \n#&gt; 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      &lt;NA&gt;  &lt;NA&gt;        \n#&gt; 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      &lt;NA&gt;  &lt;NA&gt;        \n#&gt; 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      &lt;NA&gt;  &lt;NA&gt;        \n#&gt; 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      &lt;NA&gt;  &lt;NA&gt;        \n#&gt; 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      &lt;NA&gt;  &lt;NA&gt;        \n#&gt; 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      &lt;NA&gt;  &lt;NA&gt;        \n#&gt; # ℹ 336,770 more rows\n#&gt; # ℹ 5 more variables: model &lt;chr&gt;, engines &lt;int&gt;, seats &lt;int&gt;, …\n\nWe get a lot of missing matches because our join is trying to use tailnum and year as a compound key.\n我们得到了很多缺失的匹配，因为我们的连接试图使用 tailnum 和 year 作为复合键。\nBoth flights and planes have a year column but they mean different things: flights$year is the year the flight occurred and planes$year is the year the plane was built.flights 和 planes 都有一个 year 列，但它们的含义不同：flights$year 是航班发生的年份，而 planes$year 是飞机制造的年份。\nWe only want to join on tailnum so we need to provide an explicit specification with join_by():\n我们只想在 tailnum 上进行连接，所以我们需要使用 join_by() 提供一个明确的规范：\n\nflights2 |&gt; \n  left_join(planes, join_by(tailnum))\n#&gt; # A tibble: 336,776 × 14\n#&gt;   year.x time_hour           origin dest  tailnum carrier year.y\n#&gt;    &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;int&gt;\n#&gt; 1   2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA        1999\n#&gt; 2   2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA        1998\n#&gt; 3   2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA        1990\n#&gt; 4   2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6        2012\n#&gt; 5   2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL        1991\n#&gt; 6   2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA        2012\n#&gt; # ℹ 336,770 more rows\n#&gt; # ℹ 7 more variables: type &lt;chr&gt;, manufacturer &lt;chr&gt;, model &lt;chr&gt;, …\n\nNote that the year variables are disambiguated in the output with a suffix (year.x and year.y), which tells you whether the variable came from the x or y argument.\n请注意，year 变量在输出中通过后缀（year.x 和 year.y）来消除歧义，这告诉您变量是来自 x 参数还是 y 参数。\nYou can override the default suffixes with the suffix argument.\n您可以使用 suffix 参数覆盖默认后缀。\njoin_by(tailnum) is short for join_by(tailnum == tailnum).join_by(tailnum) 是 join_by(tailnum == tailnum) 的简写。\nIt’s important to know about this fuller form for two reasons.\n了解这种更完整的形式很重要，原因有二。\nFirstly, it describes the relationship between the two tables: the keys must be equal.\n首先，它描述了两个表之间的关系：键必须相等。\nThat’s why this type of join is often called an equi join.\n这就是为什么这种类型的连接通常被称为 等值连接 (equi join)。\nYou’ll learn about non-equi joins in Section 19.5.\n你将在 Section 19.5 中学习非等值连接。\nSecondly, it’s how you specify different join keys in each table.\n其次，这是你在每个表中指定不同连接键的方式。\nFor example, there are two ways to join the flight2 and airports table: either by dest or origin:\n例如，有两种方法可以连接 flight2 和 airports 表：通过 dest 或 origin：\n\nflights2 |&gt; \n  left_join(airports, join_by(dest == faa))\n#&gt; # A tibble: 336,776 × 13\n#&gt;    year time_hour           origin dest  tailnum carrier name                \n#&gt;   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;               \n#&gt; 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      George Bush Interco…\n#&gt; 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      George Bush Interco…\n#&gt; 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      Miami Intl          \n#&gt; 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      &lt;NA&gt;                \n#&gt; 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      Hartsfield Jackson …\n#&gt; 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      Chicago Ohare Intl  \n#&gt; # ℹ 336,770 more rows\n#&gt; # ℹ 6 more variables: lat &lt;dbl&gt;, lon &lt;dbl&gt;, alt &lt;dbl&gt;, tz &lt;dbl&gt;, …\n\nflights2 |&gt; \n  left_join(airports, join_by(origin == faa))\n#&gt; # A tibble: 336,776 × 13\n#&gt;    year time_hour           origin dest  tailnum carrier name               \n#&gt;   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;              \n#&gt; 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      Newark Liberty Intl\n#&gt; 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      La Guardia         \n#&gt; 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      John F Kennedy Intl\n#&gt; 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      John F Kennedy Intl\n#&gt; 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      La Guardia         \n#&gt; 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      Newark Liberty Intl\n#&gt; # ℹ 336,770 more rows\n#&gt; # ℹ 6 more variables: lat &lt;dbl&gt;, lon &lt;dbl&gt;, alt &lt;dbl&gt;, tz &lt;dbl&gt;, …\n\nIn older code you might see a different way of specifying the join keys, using a character vector:\n在旧的代码中，你可能会看到一种不同的指定连接键的方式，即使用字符向量：\n\nby = \"x\" corresponds to join_by(x).by = \"x\" 对应于 join_by(x)。\nby = c(\"a\" = \"x\") corresponds to join_by(a == x). by = c(&quot;a&quot; = &quot;x&quot;) 对应于 join_by(a == x)。\n\nNow that it exists, we prefer join_by() since it provides a clearer and more flexible specification.\n既然它已经存在，我们更喜欢 join_by()，因为它提供了更清晰、更灵活的规范。\ninner_join(), right_join(), full_join() have the same interface as left_join().inner_join()、right_join()、full_join() 与 left_join() 具有相同的接口。\nThe difference is which rows they keep: left join keeps all the rows in x, the right join keeps all rows in y, the full join keeps all rows in either x or y, and the inner join only keeps rows that occur in both x and y.\n区别在于它们保留哪些行：左连接保留 x 中的所有行，右连接保留 y 中的所有行，全连接保留 x 或 y 中的所有行，而内连接只保留同时出现在 x 和 y 中的行。\nWe’ll come back to these in more detail later.\n我们稍后会更详细地讨论这些。\n\n19.3.3 Filtering joins\nAs you might guess the primary action of a filtering join is to filter the rows.\n你可能已经猜到，过滤连接 (filtering join) 的主要作用是过滤行。\nThere are two types: semi-joins and anti-joins.\n有两种类型：半连接 (semi-joins) 和反连接 (anti-joins)。\nSemi-joins keep all rows in x that have a match in y.半连接 (Semi-joins) 保留 x 中所有在 y 中有匹配的行。\nFor example, we could use a semi-join to filter the airports dataset to show just the origin airports:\n例如，我们可以使用半连接来过滤 airports 数据集，只显示始发机场：\n\nairports |&gt; \n  semi_join(flights2, join_by(faa == origin))\n#&gt; # A tibble: 3 × 8\n#&gt;   faa   name                  lat   lon   alt    tz dst   tzone           \n#&gt;   &lt;chr&gt; &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;           \n#&gt; 1 EWR   Newark Liberty Intl  40.7 -74.2    18    -5 A     America/New_York\n#&gt; 2 JFK   John F Kennedy Intl  40.6 -73.8    13    -5 A     America/New_York\n#&gt; 3 LGA   La Guardia           40.8 -73.9    22    -5 A     America/New_York\n\nOr just the destinations:\n或者只显示目的地：\n\nairports |&gt; \n  semi_join(flights2, join_by(faa == dest))\n#&gt; # A tibble: 101 × 8\n#&gt;   faa   name                     lat    lon   alt    tz dst   tzone          \n#&gt;   &lt;chr&gt; &lt;chr&gt;                  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;          \n#&gt; 1 ABQ   Albuquerque Internati…  35.0 -107.   5355    -7 A     America/Denver \n#&gt; 2 ACK   Nantucket Mem           41.3  -70.1    48    -5 A     America/New_Yo…\n#&gt; 3 ALB   Albany Intl             42.7  -73.8   285    -5 A     America/New_Yo…\n#&gt; 4 ANC   Ted Stevens Anchorage…  61.2 -150.    152    -9 A     America/Anchor…\n#&gt; 5 ATL   Hartsfield Jackson At…  33.6  -84.4  1026    -5 A     America/New_Yo…\n#&gt; 6 AUS   Austin Bergstrom Intl   30.2  -97.7   542    -6 A     America/Chicago\n#&gt; # ℹ 95 more rows\n\nAnti-joins are the opposite: they return all rows in x that don’t have a match in y.反连接 (Anti-joins) 则相反：它们返回 x 中所有在 y 中没有匹配的行。\nThey’re useful for finding missing values that are implicit in the data, the topic of Section 18.3.\n它们对于查找数据中 隐式 (implicit) 的缺失值很有用，这是 Section 18.3 的主题。\nImplicitly missing values don’t show up as NAs but instead only exist as an absence.\n隐式缺失值不会显示为 NA，而是仅作为一种缺席而存在。\nFor example, we can find rows that are missing from airports by looking for flights that don’t have a matching destination airport:\n例如，我们可以通过查找没有匹配目的地机场的航班来找到 airports 中缺失的行：\n\nflights2 |&gt; \n  anti_join(airports, join_by(dest == faa)) |&gt; \n  distinct(dest)\n#&gt; # A tibble: 4 × 1\n#&gt;   dest \n#&gt;   &lt;chr&gt;\n#&gt; 1 BQN  \n#&gt; 2 SJU  \n#&gt; 3 STT  \n#&gt; 4 PSE\n\nOr we can find which tailnums are missing from planes:\n或者我们可以找出 planes 中缺少哪些 tailnum：\n\nflights2 |&gt;\n  anti_join(planes, join_by(tailnum)) |&gt; \n  distinct(tailnum)\n#&gt; # A tibble: 722 × 1\n#&gt;   tailnum\n#&gt;   &lt;chr&gt;  \n#&gt; 1 N3ALAA \n#&gt; 2 N3DUAA \n#&gt; 3 N542MQ \n#&gt; 4 N730MQ \n#&gt; 5 N9EAMQ \n#&gt; 6 N532UA \n#&gt; # ℹ 716 more rows\n\n\n19.3.4 Exercises\n\nFind the 48 hours (over the course of the whole year) that have the worst delays. Cross-reference it with the weather data. Can you see any patterns?\n\nImagine you’ve found the top 10 most popular destinations using this code:\n\ntop_dest &lt;- flights2 |&gt;\n  count(dest, sort = TRUE) |&gt;\n  head(10)\n\nHow can you find all flights to those destinations?\n\nDoes every departing flight have corresponding weather data for that hour?\nWhat do the tail numbers that don’t have a matching record in planes have in common? (Hint: one variable explains ~90% of the problems.)\nAdd a column to planes that lists every carrier that has flown that plane. You might expect that there’s an implicit relationship between plane and airline, because each plane is flown by a single airline. Confirm or reject this hypothesis using the tools you’ve learned in previous chapters.\nAdd the latitude and the longitude of the origin and destination airport to flights. Is it easier to rename the columns before or after the join?\n\nCompute the average delay by destination, then join on the airports data frame so you can show the spatial distribution of delays. Here’s an easy way to draw a map of the United States:\n\nairports |&gt;\n  semi_join(flights, join_by(faa == dest)) |&gt;\n  ggplot(aes(x = lon, y = lat)) +\n    borders(\"state\") +\n    geom_point() +\n    coord_quickmap()\n\nYou might want to use the size or color of the points to display the average delay for each airport.\n\nWhat happened on June 13 2013? Draw a map of the delays, and then use Google to cross-reference with the weather.",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Joins</span>"
    ]
  },
  {
    "objectID": "joins.html#how-do-joins-work",
    "href": "joins.html#how-do-joins-work",
    "title": "19  Joins",
    "section": "\n19.4 How do joins work?",
    "text": "19.4 How do joins work?\nNow that you’ve used joins a few times it’s time to learn more about how they work, focusing on how each row in x matches rows in y.\n既然你已经多次使用连接，现在是时候更深入地了解它们的工作原理了，重点是 x 中的每一行如何与 y 中的行匹配。\nWe’ll begin by introducing a visual representation of joins, using the simple tibbles defined below and shown in Figure 19.2.\n我们将从介绍连接的可视化表示开始，使用下面定义的简单 tibble，并在 Figure 19.2 中展示。\nIn these examples we’ll use a single key called key and a single value column (val_x and val_y), but the ideas all generalize to multiple keys and multiple values.\n在这些例子中，我们将使用一个名为 key 的单个键和一个单个值列（val_x 和 val_y），但这些思想都可以推广到多个键和多个值。\n\nx &lt;- tribble(\n  ~key, ~val_x,\n     1, \"x1\",\n     2, \"x2\",\n     3, \"x3\"\n)\ny &lt;- tribble(\n  ~key, ~val_y,\n     1, \"y1\",\n     2, \"y2\",\n     4, \"y3\"\n)\n\n\n\n\n\n\n\n\nFigure 19.2: Graphical representation of two simple tables. The colored key columns map background color to key value. The grey columns represent the “value” columns that are carried along for the ride.\n\n\n\n\nFigure 19.3 introduces the foundation for our visual representation.Figure 19.3 介绍了我们可视化表示的基础。\nIt shows all potential matches between x and y as the intersection between lines drawn from each row of x and each row of y.\n它将 x 和 y 之间的所有潜在匹配显示为从 x 的每一行和 y 的每一行画出的线的交点。\nThe rows and columns in the output are primarily determined by x, so the x table is horizontal and lines up with the output.\n输出中的行和列主要由 x 决定，所以 x 表是水平的，并与输出对齐。\n\n\n\n\n\n\n\nFigure 19.3: To understand how joins work, it’s useful to think of every possible match. Here we show that with a grid of connecting lines.\n\n\n\n\nTo describe a specific type of join, we indicate matches with dots.\n为了描述特定类型的连接，我们用点来表示匹配。\nThe matches determine the rows in the output, a new data frame that contains the key, the x values, and the y values.\n匹配项决定了输出中的行，这是一个包含键、x 值和 y 值的新数据框。\nFor example, Figure 19.4 shows an inner join, where rows are retained if and only if the keys are equal.\n例如，Figure 19.4 展示了一个内连接，其中只有当键相等时，行才会被保留。\n\n\n\n\n\n\n\nFigure 19.4: An inner join matches each row in x to the row in y that has the same value of key. Each match becomes a row in the output.\n\n\n\n\nWe can apply the same principles to explain the outer joins, which keep observations that appear in at least one of the data frames.\n我们可以应用相同的原则来解释 外连接 (outer joins)，它保留出现在至少一个数据框中的观测值。\nThese joins work by adding an additional “virtual” observation to each data frame.\n这些连接通过向每个数据框添加一个额外的“虚拟”观测值来工作。\nThis observation has a key that matches if no other key matches, and values filled with NA.\n该观测值有一个在没有其他键匹配时能够匹配的键，以及填充了 NA 的值。\nThere are three types of outer joins:\n有三种类型的外连接：\n\nA left join keeps all observations in x, Figure 19.5. Every row of x is preserved in the output because it can fall back to matching a row of NAs in y.\n\n左连接 (left join) 保留 x 中的所有观测值，见 Figure 19.5。 x 的每一行都在输出中被保留，因为它可以回退到匹配 y 中一行 NA 值。\n\n\n\n\n\n\n\nFigure 19.5: A visual representation of the left join where every row in x appears in the output.\n\n\n\n\n\nA right join keeps all observations in y, Figure 19.6. Every row of y is preserved in the output because it can fall back to matching a row of NAs in x. The output still matches x as much as possible; any extra rows from y are added to the end.\n\n右连接 (right join) 保留 y 中的所有观测值，见 Figure 19.6。 y 的每一行都在输出中被保留，因为它可以回退到匹配 x 中一行 NA 值。输出仍然尽可能多地与 x 匹配；来自 y 的任何多余的行都被添加到末尾。\n\n\n\n\n\n\n\nFigure 19.6: A visual representation of the right join where every row of y appears in the output.\n\n\n\n\n\nA full join keeps all observations that appear in x or y, Figure 19.7. Every row of x and y is included in the output because both x and y have a fall back row of NAs. Again, the output starts with all rows from x, followed by the remaining unmatched y rows.\n\n全连接 (full join) 保留所有出现在 x 或 y 中的观测值，见 Figure 19.7。 x 和 y 的每一行都包含在输出中，因为 x 和 y 都有一个由 NA 值构成的备用行。同样，输出以 x 的所有行开始，然后是剩余的未匹配的 y 行。\n\n\n\n\n\n\n\nFigure 19.7: A visual representation of the full join where every row in x and y appears in the output.\n\n\n\n\n\n\nAnother way to show how the types of outer join differ is with a Venn diagram, as in Figure 19.8.\n另一种展示外连接类型差异的方法是使用韦恩图 (Venn diagram)，如 Figure 19.8 所示。\nHowever, this is not a great representation because while it might jog your memory about which rows are preserved, it fails to illustrate what’s happening with the columns.\n然而，这不是一个很好的表示方法，因为它虽然可能帮助你记起哪些行被保留了，但却无法说明列发生了什么。\n\n\n\n\n\n\n\nFigure 19.8: Venn diagrams showing the difference between inner, left, right, and full joins.\n\n\n\n\nThe joins shown here are the so-called equi joins, where rows match if the keys are equal.\n这里展示的连接是所谓的 等值 连接 (equi joins)，其中如果键相等，则行匹配。\nEqui joins are the most common type of join, so we’ll typically omit the equi prefix, and just say “inner join” rather than “equi inner join”.\n等值连接是最常见的连接类型，所以我们通常会省略“等值”这个前缀，只说“内连接”而不是“等值内连接”。\nWe’ll come back to non-equi joins in Section 19.5.\n我们将在 Section 19.5 回顾非等值连接。\n\n19.4.1 Row matching\nSo far we’ve explored what happens if a row in x matches zero or one row in y.\n到目前为止，我们已经探讨了如果 x 中的一行与 y 中的零行或一行匹配时会发生什么。\nWhat happens if it matches more than one row?\n如果它匹配多于一行会发生什么？\nTo understand what’s going on let’s first narrow our focus to the inner_join() and then draw a picture, Figure 19.9.\n为了理解发生了什么，让我们首先将注意力集中在 inner_join() 上，然后画一幅图，即 Figure 19.9。\n\n\n\n\n\n\n\nFigure 19.9: The three ways a row in x can match. x1 matches one row in y, x2 matches two rows in y, x3 matches zero rows in y. Note that while there are three rows in x and three rows in the output, there isn’t a direct correspondence between the rows.\n\n\n\n\nThere are three possible outcomes for a row in x:x 中的一行有三种可能的结果：\n\nIf it doesn’t match anything, it’s dropped.\n如果它不匹配任何东西，它就会被丢弃。\nIf it matches 1 row in y, it’s preserved.\n如果它与 y 中的 1 行匹配，它将被保留。\nIf it matches more than 1 row in y, it’s duplicated once for each match.\n如果它与 y 中的多于 1 行匹配，它将为每个匹配复制一次。\n\nIn principle, this means that there’s no guaranteed correspondence between the rows in the output and the rows in x, but in practice, this rarely causes problems.\n原则上，这意味着输出中的行与 x 中的行之间没有保证的对应关系，但在实践中，这很少会引起问题。\nThere is, however, one particularly dangerous case which can cause a combinatorial explosion of rows.\n然而，有一种特别危险的情况，可能会导致行的组合爆炸。\nImagine joining the following two tables:\n想象一下连接以下两个表：\n\ndf1 &lt;- tibble(key = c(1, 2, 2), val_x = c(\"x1\", \"x2\", \"x3\"))\ndf2 &lt;- tibble(key = c(1, 2, 2), val_y = c(\"y1\", \"y2\", \"y3\"))\n\nWhile the first row in df1 only matches one row in df2, the second and third rows both match two rows.\n虽然 df1 中的第一行只匹配 df2 中的一行，但第二行和第三行都匹配两行。\nThis is sometimes called a many-to-many join, and will cause dplyr to emit a warning:\n这有时被称为多对多连接，并且会导致 dplyr 发出警告：\n\ndf1 |&gt; \n  inner_join(df2, join_by(key))\n#&gt; Warning in inner_join(df1, df2, join_by(key)): Detected an unexpected many-to-many relationship between `x` and `y`.\n#&gt; ℹ Row 2 of `x` matches multiple rows in `y`.\n#&gt; ℹ Row 2 of `y` matches multiple rows in `x`.\n#&gt; ℹ If a many-to-many relationship is expected, set `relationship =\n#&gt;   \"many-to-many\"` to silence this warning.\n#&gt; # A tibble: 5 × 3\n#&gt;     key val_x val_y\n#&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n#&gt; 1     1 x1    y1   \n#&gt; 2     2 x2    y2   \n#&gt; 3     2 x2    y3   \n#&gt; 4     2 x3    y2   \n#&gt; 5     2 x3    y3\n\nIf you are doing this deliberately, you can set relationship = \"many-to-many\", as the warning suggests.\n如果你是故意这样做的，可以像警告建议的那样设置 relationship = \"many-to-many\"。\n\n19.4.2 Filtering joins\nThe number of matches also determines the behavior of the filtering joins.\n匹配的数量也决定了过滤连接的行为。\nThe semi-join keeps rows in x that have one or more matches in y, as in Figure 19.10.\n半连接 (semi-join) 保留 x 中在 y 中有一个或多个匹配的行，如 Figure 19.10 所示。\nThe anti-join keeps rows in x that match zero rows in y, as in Figure 19.11.\n反连接 (anti-join) 保留 x 中与 y 中零行匹配的行，如 Figure 19.11 所示。\nIn both cases, only the existence of a match is important; it doesn’t matter how many times it matches.\n在这两种情况下，只有匹配的存在才重要；它匹配多少次并不重要。\nThis means that filtering joins never duplicate rows like mutating joins do.\n这意味着过滤连接从不像修改连接那样复制行。\n\n\n\n\n\n\n\nFigure 19.10: In a semi-join it only matters that there is a match; otherwise values in y don’t affect the output.\n\n\n\n\n\n\n\n\n\n\n\nFigure 19.11: An anti-join is the inverse of a semi-join, dropping rows from x that have a match in y.",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Joins</span>"
    ]
  },
  {
    "objectID": "joins.html#sec-non-equi-joins",
    "href": "joins.html#sec-non-equi-joins",
    "title": "19  Joins",
    "section": "\n19.5 Non-equi joins",
    "text": "19.5 Non-equi joins\nSo far you’ve only seen equi joins, joins where the rows match if the x key equals the y key.\n到目前为止，你只看到了等值连接，即当 x 键等于 y 键时行匹配的连接。\nNow we’re going to relax that restriction and discuss other ways of determining if a pair of rows match.\n现在我们将放宽这个限制，讨论其他确定一对行是否匹配的方法。\nBut before we can do that, we need to revisit a simplification we made above.\n但在我们这样做之前，我们需要重新审视我们上面做的一个简化。\nIn equi joins the x keys and y are always equal, so we only need to show one in the output.\n在等值连接中，x 键和 y 键总是相等的，所以我们只需要在输出中显示一个。\nWe can request that dplyr keep both keys with keep = TRUE, leading to the code below and the re-drawn inner_join() in Figure 19.12.\n我们可以请求 dplyr 使用 keep = TRUE 来保留两个键，从而得到下面的代码和 Figure 19.12 中重新绘制的 inner_join()。\n\nx |&gt; inner_join(y, join_by(key == key), keep = TRUE)\n#&gt; # A tibble: 2 × 4\n#&gt;   key.x val_x key.y val_y\n#&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;\n#&gt; 1     1 x1        1 y1   \n#&gt; 2     2 x2        2 y2\n\n\n\n\n\n\n\n\nFigure 19.12: An inner join showing both x and y keys in the output.\n\n\n\n\nWhen we move away from equi joins we’ll always show the keys, because the key values will often be different.\n当我们不再使用等值连接时，我们将始终显示键，因为键值通常会不同。\nFor example, instead of matching only when the x$key and y$key are equal, we could match whenever the x$key is greater than or equal to the y$key, leading to Figure 19.13.\n例如，我们可以不只在 x$key 和 y$key 相等时进行匹配，而是在 x$key 大于或等于 y$key 时进行匹配，这导致了 Figure 19.13。\ndplyr’s join functions understand this distinction equi and non-equi joins so will always show both keys when you perform a non-equi join.\ndplyr 的连接函数理解等值连接和非等值连接之间的区别，因此当你执行非等值连接时，它总是会显示两个键。\n\n\n\n\n\n\n\nFigure 19.13: A non-equi join where the x key must be greater than or equal to the y key. Many rows generate multiple matches.\n\n\n\n\nNon-equi join isn’t a particularly useful term because it only tells you what the join is not, not what it is. dplyr helps by identifying four particularly useful types of non-equi join:\n非等值连接 (Non-equi join) 并不是一个特别有用的术语，因为它只告诉你连接不是什么，而不是它是什么。dplyr 通过识别四种特别有用的非等值连接类型来提供帮助：\n\nCross joins match every pair of rows.交叉连接 (Cross joins) 匹配每一对行。\nInequality joins use &lt;, &lt;=, &gt;, and &gt;= instead of ==.不等连接 (Inequality joins) 使用 &lt;、&lt;=、&gt; 和 &gt;= 代替 ==。\nRolling joins are similar to inequality joins but only find the closest match.滚动连接 (Rolling joins) 类似于不等连接，但只查找最接近的匹配。\nOverlap joins are a special type of inequality join designed to work with ranges.重叠连接 (Overlap joins) 是一种特殊类型的不等连接，旨在处理范围。\n\nEach of these is described in more detail in the following sections.\n以下各节将更详细地描述这些内容。\n\n19.5.1 Cross joins\nA cross join matches everything, as in Figure 19.14, generating the Cartesian product of rows.\n交叉连接匹配所有内容，如 Figure 19.14 所示，生成行的笛卡尔积 (Cartesian product)。\nThis means the output will have nrow(x) * nrow(y) rows.\n这意味着输出将有 nrow(x) * nrow(y) 行。\n\n\n\n\n\n\n\nFigure 19.14: A cross join matches each row in x with every row in y.\n\n\n\n\nCross joins are useful when generating permutations.\n交叉连接在生成排列时很有用。\nFor example, the code below generates every possible pair of names.\n例如，下面的代码生成了所有可能的姓名对。\nSince we’re joining df to itself, this is sometimes called a self-join.\n由于我们将 df 与其自身连接，这有时被称为 自连接 (self-join)。\nCross joins use a different join function because there’s no distinction between inner/left/right/full when you’re matching every row.\n交叉连接使用不同的连接函数，因为当你匹配每一行时，内/左/右/全连接之间没有区别。\n\ndf &lt;- tibble(name = c(\"John\", \"Simon\", \"Tracy\", \"Max\"))\ndf |&gt; cross_join(df)\n#&gt; # A tibble: 16 × 2\n#&gt;   name.x name.y\n#&gt;   &lt;chr&gt;  &lt;chr&gt; \n#&gt; 1 John   John  \n#&gt; 2 John   Simon \n#&gt; 3 John   Tracy \n#&gt; 4 John   Max   \n#&gt; 5 Simon  John  \n#&gt; 6 Simon  Simon \n#&gt; # ℹ 10 more rows\n\n\n19.5.2 Inequality joins\nInequality joins use &lt;, &lt;=, &gt;=, or &gt; to restrict the set of possible matches, as in Figure 19.13 and Figure 19.15.\n不等连接使用 &lt;、&lt;=、&gt;= 或 &gt; 来限制可能的匹配集合，如 Figure 19.13 和 Figure 19.15 所示。\n\n\n\n\n\n\n\nFigure 19.15: An inequality join where x is joined to y on rows where the key of x is less than the key of y. This makes a triangular shape in the top-left corner.\n\n\n\n\nInequality joins are extremely general, so general that it’s hard to come up with meaningful specific use cases.\n不等连接非常通用，以至于很难想出有意义的特定用例。\nOne small useful technique is to use them to restrict the cross join so that instead of generating all permutations, we generate all combinations:\n一个有用的小技巧是使用它们来限制交叉连接，这样我们就不是生成所有排列，而是生成所有组合：\n\ndf &lt;- tibble(id = 1:4, name = c(\"John\", \"Simon\", \"Tracy\", \"Max\"))\n\ndf |&gt; inner_join(df, join_by(id &lt; id))\n#&gt; # A tibble: 6 × 4\n#&gt;    id.x name.x  id.y name.y\n#&gt;   &lt;int&gt; &lt;chr&gt;  &lt;int&gt; &lt;chr&gt; \n#&gt; 1     1 John       2 Simon \n#&gt; 2     1 John       3 Tracy \n#&gt; 3     1 John       4 Max   \n#&gt; 4     2 Simon      3 Tracy \n#&gt; 5     2 Simon      4 Max   \n#&gt; 6     3 Tracy      4 Max\n\n\n19.5.3 Rolling joins\nRolling joins are a special type of inequality join where instead of getting every row that satisfies the inequality, you get just the closest row, as in Figure 19.16.\n滚动连接是一种特殊类型的不等连接，在这种连接中，你不是得到满足不等式的每一行，而只是得到最接近的那一行，如 Figure 19.16 所示。\nYou can turn any inequality join into a rolling join by adding closest().\n你可以通过添加 closest() 将任何不等连接转换为滚动连接。\nFor example join_by(closest(x &lt;= y)) matches the smallest y that’s greater than or equal to x, and join_by(closest(x &gt; y)) matches the biggest y that’s less than x.\n例如，join_by(closest(x &lt;= y)) 匹配大于或等于 x 的最小 y，而 join_by(closest(x &gt; y)) 匹配小于 x 的最大 y。\n\n\n\n\n\n\n\nFigure 19.16: A rolling join is similar to a greater-than-or-equal inequality join but only matches the first value.\n\n\n\n\nRolling joins are particularly useful when you have two tables of dates that don’t perfectly line up and you want to find (e.g.) the closest date in table 1 that comes before (or after) some date in table 2.\n当你拥有两个日期不完全对齐的表格，并且想要查找（例如）表 1 中在表 2 中某个日期之前（或之后）的最接近日期时，滚动连接特别有用。\nFor example, imagine that you’re in charge of the party planning commission for your office.\n例如，假设你负责办公室的派对策划委员会。\nYour company is rather cheap so instead of having individual parties, you only have a party once each quarter.\n你的公司相当小气，所以你们每个季度只举办一次派对，而不是举办个人派对。\nThe rules for determining when a party will be held are a little complex: parties are always on a Monday, you skip the first week of January since a lot of people are on holiday, and the first Monday of Q3 2022 is July 4, so that has to be pushed back a week.\n决定派对何时举行的规则有点复杂：派对总是在星期一，一月份的第一周会跳过，因为很多人都在度假，而 2022 年第三季度的第一个星期一是 7 月 4 日，所以必须推迟一周。\nThat leads to the following party days:\n这导致了以下派对日期：\n\nparties &lt;- tibble(\n  q = 1:4,\n  party = ymd(c(\"2022-01-10\", \"2022-04-04\", \"2022-07-11\", \"2022-10-03\"))\n)\n\nNow imagine that you have a table of employee birthdays:\n现在想象一下，你有一张员工生日表：\n\nset.seed(123)\nemployees &lt;- tibble(\n  name = sample(babynames::babynames$name, 100),\n  birthday = ymd(\"2022-01-01\") + (sample(365, 100, replace = TRUE) - 1)\n)\nemployees\n#&gt; # A tibble: 100 × 2\n#&gt;   name     birthday  \n#&gt;   &lt;chr&gt;    &lt;date&gt;    \n#&gt; 1 Kemba    2022-01-22\n#&gt; 2 Orean    2022-06-26\n#&gt; 3 Kirstyn  2022-02-11\n#&gt; 4 Amparo   2022-11-11\n#&gt; 5 Belen    2022-03-25\n#&gt; 6 Rayshaun 2022-01-11\n#&gt; # ℹ 94 more rows\n\nAnd for each employee we want to find the last party date that comes before (or on) their birthday.\n对于每位员工，我们都想找到在他们生日之前（或当天）的最后一个派对日期。\nWe can express that with a rolling join:\n我们可以用滚动连接来表达：\n\nemployees |&gt; \n  left_join(parties, join_by(closest(birthday &gt;= party)))\n#&gt; # A tibble: 100 × 4\n#&gt;   name     birthday       q party     \n#&gt;   &lt;chr&gt;    &lt;date&gt;     &lt;int&gt; &lt;date&gt;    \n#&gt; 1 Kemba    2022-01-22     1 2022-01-10\n#&gt; 2 Orean    2022-06-26     2 2022-04-04\n#&gt; 3 Kirstyn  2022-02-11     1 2022-01-10\n#&gt; 4 Amparo   2022-11-11     4 2022-10-03\n#&gt; 5 Belen    2022-03-25     1 2022-01-10\n#&gt; 6 Rayshaun 2022-01-11     1 2022-01-10\n#&gt; # ℹ 94 more rows\n\nThere is, however, one problem with this approach: the folks with birthdays before January 10 don’t get a party:\n然而，这种方法存在一个问题：1 月 10 日之前生日的人没有派对：\n\nemployees |&gt; \n  anti_join(parties, join_by(closest(birthday &gt;= party)))\n#&gt; # A tibble: 2 × 2\n#&gt;   name   birthday  \n#&gt;   &lt;chr&gt;  &lt;date&gt;    \n#&gt; 1 Maks   2022-01-07\n#&gt; 2 Nalani 2022-01-04\n\nTo resolve that issue we’ll need to tackle the problem a different way, with overlap joins.\n为了解决这个问题，我们需要用一种不同的方式来处理，即使用重叠连接。\n\n19.5.4 Overlap joins\nOverlap joins provide three helpers that use inequality joins to make it easier to work with intervals:\n重叠连接提供了三个使用不等连接的辅助函数，使处理区间变得更容易：\n\nbetween(x, y_lower, y_upper) is short for x &gt;= y_lower, x &lt;= y_upper.between(x, y_lower, y_upper) 是 x &gt;= y_lower, x &lt;= y_upper 的简写。\nwithin(x_lower, x_upper, y_lower, y_upper) is short for x_lower &gt;= y_lower, x_upper &lt;= y_upper.within(x_lower, x_upper, y_lower, y_upper) 是 x_lower &gt;= y_lower, x_upper &lt;= y_upper 的简写。\noverlaps(x_lower, x_upper, y_lower, y_upper) is short for x_lower &lt;= y_upper, x_upper &gt;= y_lower.overlaps(x_lower, x_upper, y_lower, y_upper) 是 x_lower &lt;= y_upper, x_upper &gt;= y_lower 的简写。\n\nLet’s continue the birthday example to see how you might use them.\n让我们继续看生日的例子，看看你可能会如何使用它们。\nThere’s one problem with the strategy we used above: there’s no party preceding the birthdays Jan 1-9.\n我们上面使用的策略有一个问题：1 月 1 日至 9 日生日之前没有派对。\nSo it might be better to be explicit about the date ranges that each party spans, and make a special case for those early birthdays:\n所以最好明确每个派对涵盖的日期范围，并为那些早早过生日的人做一个特例：\n\nparties &lt;- tibble(\n  q = 1:4,\n  party = ymd(c(\"2022-01-10\", \"2022-04-04\", \"2022-07-11\", \"2022-10-03\")),\n  start = ymd(c(\"2022-01-01\", \"2022-04-04\", \"2022-07-11\", \"2022-10-03\")),\n  end = ymd(c(\"2022-04-03\", \"2022-07-11\", \"2022-10-02\", \"2022-12-31\"))\n)\nparties\n#&gt; # A tibble: 4 × 4\n#&gt;       q party      start      end       \n#&gt;   &lt;int&gt; &lt;date&gt;     &lt;date&gt;     &lt;date&gt;    \n#&gt; 1     1 2022-01-10 2022-01-01 2022-04-03\n#&gt; 2     2 2022-04-04 2022-04-04 2022-07-11\n#&gt; 3     3 2022-07-11 2022-07-11 2022-10-02\n#&gt; 4     4 2022-10-03 2022-10-03 2022-12-31\n\nHadley is hopelessly bad at data entry so he also wanted to check that the party periods don’t overlap.\nHadley 在数据录入方面非常糟糕，所以他还想检查一下派对时段是否重叠。\nOne way to do this is by using a self-join to check if any start-end interval overlap with another:\n一种方法是使用自连接来检查是否有任何开始-结束区间与另一个区间重叠：\n\nparties |&gt; \n  inner_join(parties, join_by(overlaps(start, end, start, end), q &lt; q)) |&gt; \n  select(start.x, end.x, start.y, end.y)\n#&gt; # A tibble: 1 × 4\n#&gt;   start.x    end.x      start.y    end.y     \n#&gt;   &lt;date&gt;     &lt;date&gt;     &lt;date&gt;     &lt;date&gt;    \n#&gt; 1 2022-04-04 2022-07-11 2022-07-11 2022-10-02\n\nOoops, there is an overlap, so let’s fix that problem and continue:\n哎呀，有重叠，我们来解决这个问题然后继续：\n\nparties &lt;- tibble(\n  q = 1:4,\n  party = ymd(c(\"2022-01-10\", \"2022-04-04\", \"2022-07-11\", \"2022-10-03\")),\n  start = ymd(c(\"2022-01-01\", \"2022-04-04\", \"2022-07-11\", \"2022-10-03\")),\n  end = ymd(c(\"2022-04-03\", \"2022-07-10\", \"2022-10-02\", \"2022-12-31\"))\n)\n\nNow we can match each employee to their party.\n现在我们可以将每位员工与他们的派对匹配起来。\nThis is a good place to use unmatched = \"error\" because we want to quickly find out if any employees didn’t get assigned a party.\n这里很适合使用 unmatched = \"error\"，因为我们想快速找出是否有任何员工没有被分配到派对。\n\nemployees |&gt; \n  inner_join(parties, join_by(between(birthday, start, end)), unmatched = \"error\")\n#&gt; # A tibble: 100 × 6\n#&gt;   name     birthday       q party      start      end       \n#&gt;   &lt;chr&gt;    &lt;date&gt;     &lt;int&gt; &lt;date&gt;     &lt;date&gt;     &lt;date&gt;    \n#&gt; 1 Kemba    2022-01-22     1 2022-01-10 2022-01-01 2022-04-03\n#&gt; 2 Orean    2022-06-26     2 2022-04-04 2022-04-04 2022-07-10\n#&gt; 3 Kirstyn  2022-02-11     1 2022-01-10 2022-01-01 2022-04-03\n#&gt; 4 Amparo   2022-11-11     4 2022-10-03 2022-10-03 2022-12-31\n#&gt; 5 Belen    2022-03-25     1 2022-01-10 2022-01-01 2022-04-03\n#&gt; 6 Rayshaun 2022-01-11     1 2022-01-10 2022-01-01 2022-04-03\n#&gt; # ℹ 94 more rows\n\n\n19.5.5 Exercises\n\n\nCan you explain what’s happening with the keys in this equi join? Why are they different?\n\nx |&gt; full_join(y, join_by(key == key))\n#&gt; # A tibble: 4 × 3\n#&gt;     key val_x val_y\n#&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n#&gt; 1     1 x1    y1   \n#&gt; 2     2 x2    y2   \n#&gt; 3     3 x3    &lt;NA&gt; \n#&gt; 4     4 &lt;NA&gt;  y3\n\nx |&gt; full_join(y, join_by(key == key), keep = TRUE)\n#&gt; # A tibble: 4 × 4\n#&gt;   key.x val_x key.y val_y\n#&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;\n#&gt; 1     1 x1        1 y1   \n#&gt; 2     2 x2        2 y2   \n#&gt; 3     3 x3       NA &lt;NA&gt; \n#&gt; 4    NA &lt;NA&gt;      4 y3\n\n\nWhen finding if any party period overlapped with another party period we used q &lt; q in the join_by()? Why? What happens if you remove this inequality?",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Joins</span>"
    ]
  },
  {
    "objectID": "joins.html#summary",
    "href": "joins.html#summary",
    "title": "19  Joins",
    "section": "\n19.6 Summary",
    "text": "19.6 Summary\nIn this chapter, you’ve learned how to use mutating and filtering joins to combine data from a pair of data frames.\n在本章中，你学习了如何使用修改连接和过滤连接来合并来自一对数据框的数据。\nAlong the way you learned how to identify keys, and the difference between primary and foreign keys.\n在此过程中，你学会了如何识别键，以及主键和外键之间的区别。\nYou also understand how joins work and how to figure out how many rows the output will have.\n你还了解了连接的工作原理以及如何确定输出将有多少行。\nFinally, you’ve gained a glimpse into the power of non-equi joins and seen a few interesting use cases.\n最后，你对非等值连接的强大功能有了初步的了解，并看到了一些有趣的用例。\nThis chapter concludes the “Transform” part of the book where the focus was on the tools you could use with individual columns and tibbles.\n本章结束了本书的“转换”部分，该部分的重点是你可以用于单个列和 tibble 的工具。\nYou learned about dplyr and base functions for working with logical vectors, numbers, and complete tables, stringr functions for working with strings, lubridate functions for working with date-times, and forcats functions for working with factors.\n你学习了用于处理逻辑向量、数字和完整表格的 dplyr 和基础函数，用于处理字符串的 stringr 函数，用于处理日期时间的 lubridate 函数，以及用于处理因子的 forcats 函数。\nIn the next part of the book, you’ll learn more about getting various types of data into R in a tidy form.\n在本书的下一部分，你将学习更多关于如何将各种类型的数据以整洁的形式导入 R 的知识。",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Joins</span>"
    ]
  },
  {
    "objectID": "joins.html#footnotes",
    "href": "joins.html#footnotes",
    "title": "19  Joins",
    "section": "",
    "text": "Remember that in RStudio you can also use View() to avoid this problem.↩︎\nThat’s not 100% true, but you’ll get a warning whenever it isn’t.↩︎",
    "crumbs": [
      "Transform",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Joins</span>"
    ]
  },
  {
    "objectID": "import.html",
    "href": "import.html",
    "title": "Import",
    "section": "",
    "text": "In this part of the book, you’ll learn how to import a wider range of data into R, as well as how to get it into a form useful for analysis. Sometimes this is just a matter of calling a function from the appropriate data import package. But in more complex cases it might require both tidying and transformation in order to get to the tidy rectangle that you’d prefer to work with.\n在本书的这一部分中，你将学习如何将更广泛的数据导入 R，以及如何将其整理成可用于分析的形式。有时，这只是调用相应数据导入包中的一个函数的问题。但在更复杂的情况下，可能需要进行整理和转换，才能得到你更喜欢使用的整洁矩形数据。\n\n\n\n\n\n\n\nFigure 1: Data import is the beginning of the data science process; without data you can’t do data science!\n\n\n\n\nIn this part of the book you’ll learn how to access data stored in the following ways:\n在本书的这一部分，你将学习如何访问以下列方式存储的数据：\n\nIn 20  Spreadsheets, you’ll learn how to import data from Excel spreadsheets and Google Sheets.\n在 20  Spreadsheets 中，你将学习如何从 Excel 电子表格和 Google Sheets 导入数据。\nIn 21  Databases, you’ll learn about getting data out of a database and into R (and you’ll also learn a little about how to get data out of R and into a database).\n在 21  Databases 中，你将学习如何将数据从数据库中取出并导入 R (你还会学到一点如何将数据从 R 中取出并导入数据库)。\nIn 22  Arrow, you’ll learn about Arrow, a powerful tool for working with out-of-memory data, particularly when it’s stored in the parquet format.\n在 22  Arrow 中，你将学习 Arrow，这是一个处理内存不足 (out-of-memory) 数据的强大工具，尤其是当数据以 parquet 格式存储时。\nIn 23  Hierarchical data, you’ll learn how to work with hierarchical data, including the deeply nested lists produced by data stored in the JSON format.\n在 23  Hierarchical data 中，你将学习如何处理分层数据，包括由 JSON 格式存储的数据产生的深度嵌套列表。\nIn 24  Web scraping, you’ll learn web “scraping”, the art and science of extracting data from web pages.\n在 24  Web scraping 中，你将学习网络“抓取” (scraping)，即从网页中提取数据的艺术和科学。\n\nThere are two important tidyverse packages that we don’t discuss here: haven and xml2. If you’re working with data from SPSS, Stata, and SAS files, check out the haven package, https://haven.tidyverse.org. If you’re working with XML data, check out the xml2 package, https://xml2.r-lib.org. Otherwise, you’ll need to do some research to figure which package you’ll need to use; google is your friend here 😃.\n有两个重要的 tidyverse 包我们在这里不讨论：haven 和 xml2。如果你正在处理来自 SPSS、Stata 和 SAS 文件的数据，请查看 haven 包，https://haven.tidyverse.org。如果你正在处理 XML 数据，请查看 xml2 包，https://xml2.r-lib.org。否则，你需要做一些研究来找出你需要使用的包；谷歌是你的好朋友 😃。",
    "crumbs": [
      "Import"
    ]
  },
  {
    "objectID": "spreadsheets.html",
    "href": "spreadsheets.html",
    "title": "20  Spreadsheets",
    "section": "",
    "text": "20.1 Introduction\nIn Chapter 7 you learned about importing data from plain text files like .csv and .tsv. Now it’s time to learn how to get data out of a spreadsheet, either an Excel spreadsheet or a Google Sheet. This will build on much of what you’ve learned in Chapter 7, but we will also discuss additional considerations and complexities when working with data from spreadsheets.\n在 Chapter 7 中，你学习了如何从 .csv 和 .tsv 等纯文本文件导入数据。现在是时候学习如何从电子表格中获取数据了，无论是 Excel 电子表格还是 Google Sheet。本节将建立在你在 Chapter 7 中学到的大部分知识之上，但我们也会讨论处理电子表格数据时的额外考量和复杂性。\nIf you or your collaborators are using spreadsheets for organizing data, we strongly recommend reading the paper “Data Organization in Spreadsheets” by Karl Broman and Kara Woo: https://doi.org/10.1080/00031305.2017.1375989. The best practices presented in this paper will save you much headache when you import data from a spreadsheet into R to analyze and visualize.\n如果你或你的合作者正在使用电子表格来组织数据，我们强烈建议阅读 Karl Broman 和 Kara Woo 的论文 “Data Organization in Spreadsheets”：https://doi.org/10.1080/00031305.2017.1375989。当你将数据从电子表格导入 R 进行分析和可视化时，本文提出的最佳实践将为你省去很多麻烦。",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Spreadsheets</span>"
    ]
  },
  {
    "objectID": "spreadsheets.html#excel",
    "href": "spreadsheets.html#excel",
    "title": "20  Spreadsheets",
    "section": "\n20.2 Excel",
    "text": "20.2 Excel\nMicrosoft Excel is a widely used spreadsheet software program where data are organized in worksheets inside of spreadsheet files.\nMicrosoft Excel 是一款广泛使用的电子表格软件程序，数据被组织在电子表格文件内的工作表 (worksheets) 中。\n\n20.2.1 Prerequisites\nIn this section, you’ll learn how to load data from Excel spreadsheets in R with the readxl package. This package is non-core tidyverse, so you need to load it explicitly, but it is installed automatically when you install the tidyverse package. Later, we’ll also use the writexl package, which allows us to create Excel spreadsheets.\n在本节中，你将学习如何使用 readxl 包在 R 中加载 Excel 电子表格中的数据。这个包不是 tidyverse 的核心包，所以你需要显式加载它，但它会在你安装 tidyverse 包时自动安装。稍后，我们还将使用 writexl 包，它允许我们创建 Excel 电子表格。\n\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(writexl)\n\n\n20.2.2 Getting started\nMost of readxl’s functions allow you to load Excel spreadsheets into R:\nreadxl 包的大部分函数都可以让你将 Excel 电子表格加载到 R 中：\n\nread_xls() reads Excel files with xls format.read_xls() 读取 xls 格式的 Excel 文件。\nread_xlsx() read Excel files with xlsx format.read_xlsx() 读取 xlsx 格式的 Excel 文件。\nread_excel() can read files with both xls and xlsx format. It guesses the file type based on the input.read_excel() 可以读取 xls 和 xlsx 两种格式的文件。它会根据输入猜测文件类型。\n\nThese functions all have similar syntax just like other functions we have previously introduced for reading other types of files, e.g., read_csv(), read_table(), etc. For the rest of the chapter we will focus on using read_excel().\n这些函数都具有相似的语法，就像我们之前介绍的用于读取其他类型文件的函数一样，例如 read_csv()、read_table() 等。在本章的其余部分，我们将重点使用 read_excel()。\n\n20.2.3 Reading Excel spreadsheets\nFigure 20.1 shows what the spreadsheet we’re going to read into R looks like in Excel. This spreadsheet can be downloaded as an Excel file from https://docs.google.com/spreadsheets/d/1V1nPp1tzOuutXFLb3G9Eyxi3qxeEhnOXUzL5_BcCQ0w/.Figure 20.1 展示了我们即将读入 R 的电子表格在 Excel 中的样子。该电子表格可以从 https://docs.google.com/spreadsheets/d/1V1nPp1tzOuutXFLb3G9Eyxi3qxeEhnOXUzL5_BcCQ0w/ 下载为 Excel 文件。\n\n\n\n\n\n\n\nFigure 20.1: Spreadsheet called students.xlsx in Excel.\n\n\n\n\nThe first argument to read_excel() is the path to the file to read.read_excel() 的第一个参数是要读取的文件的路径。\n\nstudents &lt;- read_excel(\"data/students.xlsx\")\n\nread_excel() will read the file in as a tibble.read_excel() 会将文件读入为一个 tibble。\n\nstudents\n#&gt; # A tibble: 6 × 5\n#&gt;   `Student ID` `Full Name`      favourite.food     mealPlan            AGE  \n#&gt;          &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n#&gt; 1            1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n#&gt; 2            2 Barclay Lynn     French fries       Lunch only          5    \n#&gt; 3            3 Jayendra Lyne    N/A                Breakfast and lunch 7    \n#&gt; 4            4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n#&gt; 5            5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n#&gt; 6            6 Güvenç Attila    Ice cream          Lunch only          6\n\nWe have six students in the data and five variables on each student. However there are a few things we might want to address in this dataset:\n数据中有六名学生，每名学生有五个变量。然而，在这个数据集中，我们可能需要解决一些问题：\n\n\nThe column names are all over the place. You can provide column names that follow a consistent format; we recommend snake_case using the col_names argument.\n列名五花八门。你可以提供遵循一致格式的列名；我们推荐使用 col_names 参数来指定 snake_case 格式的列名。\n\nread_excel(\n  \"data/students.xlsx\",\n  col_names = c(\"student_id\", \"full_name\", \"favourite_food\", \"meal_plan\", \"age\")\n)\n#&gt; # A tibble: 7 × 5\n#&gt;   student_id full_name        favourite_food     meal_plan           age  \n#&gt;   &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n#&gt; 1 Student ID Full Name        favourite.food     mealPlan            AGE  \n#&gt; 2 1          Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n#&gt; 3 2          Barclay Lynn     French fries       Lunch only          5    \n#&gt; 4 3          Jayendra Lyne    N/A                Breakfast and lunch 7    \n#&gt; 5 4          Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n#&gt; 6 5          Chidiegwu Dunkel Pizza              Breakfast and lunch five \n#&gt; 7 6          Güvenç Attila    Ice cream          Lunch only          6\n\nUnfortunately, this didn’t quite do the trick. We now have the variable names we want, but what was previously the header row now shows up as the first observation in the data. You can explicitly skip that row using the skip argument.\n不幸的是，这并没有完全解决问题。我们现在有了想要的变量名，但之前的标题行现在作为数据中的第一个观测值出现了。你可以使用 skip 参数明确跳过那一行。\n\nread_excel(\n  \"data/students.xlsx\",\n  col_names = c(\"student_id\", \"full_name\", \"favourite_food\", \"meal_plan\", \"age\"),\n  skip = 1\n)\n#&gt; # A tibble: 6 × 5\n#&gt;   student_id full_name        favourite_food     meal_plan           age  \n#&gt;        &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n#&gt; 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n#&gt; 2          2 Barclay Lynn     French fries       Lunch only          5    \n#&gt; 3          3 Jayendra Lyne    N/A                Breakfast and lunch 7    \n#&gt; 4          4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n#&gt; 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n#&gt; 6          6 Güvenç Attila    Ice cream          Lunch only          6\n\n\n\nIn the favourite_food column, one of the observations is N/A, which stands for “not available” but it’s currently not recognized as an NA (note the contrast between this N/A and the age of the fourth student in the list). You can specify which character strings should be recognized as NAs with the na argument. By default, only \"\" (empty string, or, in the case of reading from a spreadsheet, an empty cell or a cell with the formula =NA()) is recognized as an NA.\n在 favourite_food 列中，有一个观测值是 N/A，它代表“不可用 (not available)”，但目前没有被识别为 NA（注意这个 N/A 和列表中第四个学生的年龄之间的对比）。你可以使用 na 参数指定哪些字符串应该被识别为 NA。默认情况下，只有 \"\"（空字符串，或者在从电子表格读取时，是一个空单元格或带有公式 =NA() 的单元格）被识别为 NA。\n\nread_excel(\n  \"data/students.xlsx\",\n  col_names = c(\"student_id\", \"full_name\", \"favourite_food\", \"meal_plan\", \"age\"),\n  skip = 1,\n  na = c(\"\", \"N/A\")\n)\n#&gt; # A tibble: 6 × 5\n#&gt;   student_id full_name        favourite_food     meal_plan           age  \n#&gt;        &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n#&gt; 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n#&gt; 2          2 Barclay Lynn     French fries       Lunch only          5    \n#&gt; 3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch 7    \n#&gt; 4          4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n#&gt; 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n#&gt; 6          6 Güvenç Attila    Ice cream          Lunch only          6\n\n\n\nOne other remaining issue is that age is read in as a character variable, but it really should be numeric. Just like with read_csv() and friends for reading data from flat files, you can supply a col_types argument to read_excel() and specify the column types for the variables you read in. The syntax is a bit different, though. Your options are \"skip\", \"guess\", \"logical\", \"numeric\", \"date\", \"text\" or \"list\".\n另一个遗留问题是 age 被读作字符变量，但它实际上应该是数值型。就像使用 read_csv() 及其系列函数从平面文件读取数据一样，你可以为 read_excel() 提供一个 col_types 参数，并为你读入的变量指定列类型。不过，语法有点不同。你的选项有 \"skip\"、\"guess\"、\"logical\"、\"numeric\"、\"date\"、\"text\" 或 \"list\"。\n\nread_excel(\n  \"data/students.xlsx\",\n  col_names = c(\"student_id\", \"full_name\", \"favourite_food\", \"meal_plan\", \"age\"),\n  skip = 1,\n  na = c(\"\", \"N/A\"),\n  col_types = c(\"numeric\", \"text\", \"text\", \"text\", \"numeric\")\n)\n#&gt; Warning: Expecting numeric in E6 / R6C5: got 'five'\n#&gt; # A tibble: 6 × 5\n#&gt;   student_id full_name        favourite_food     meal_plan             age\n#&gt;        &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;dbl&gt;\n#&gt; 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4\n#&gt; 2          2 Barclay Lynn     French fries       Lunch only              5\n#&gt; 3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch     7\n#&gt; 4          4 Leon Rossini     Anchovies          Lunch only             NA\n#&gt; 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch    NA\n#&gt; 6          6 Güvenç Attila    Ice cream          Lunch only              6\n\nHowever, this didn’t quite produce the desired result either. By specifying that age should be numeric, we have turned the one cell with the non-numeric entry (which had the value five) into an NA. In this case, we should read age in as \"text\" and then make the change once the data is loaded in R.\n然而，这也没有产生预期的结果。通过指定 age 应该是数值型，我们将那个带有非数值条目（其值为 five）的单元格转换为了 NA。在这种情况下，我们应该将 age 读作 \"text\"，然后在数据加载到 R 后再进行更改。\n\nstudents &lt;- read_excel(\n  \"data/students.xlsx\",\n  col_names = c(\"student_id\", \"full_name\", \"favourite_food\", \"meal_plan\", \"age\"),\n  skip = 1,\n  na = c(\"\", \"N/A\"),\n  col_types = c(\"numeric\", \"text\", \"text\", \"text\", \"text\")\n)\n\nstudents &lt;- students |&gt;\n  mutate(\n    age = if_else(age == \"five\", \"5\", age),\n    age = parse_number(age)\n  )\n\nstudents\n#&gt; # A tibble: 6 × 5\n#&gt;   student_id full_name        favourite_food     meal_plan             age\n#&gt;        &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;dbl&gt;\n#&gt; 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4\n#&gt; 2          2 Barclay Lynn     French fries       Lunch only              5\n#&gt; 3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch     7\n#&gt; 4          4 Leon Rossini     Anchovies          Lunch only             NA\n#&gt; 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch     5\n#&gt; 6          6 Güvenç Attila    Ice cream          Lunch only              6\n\n\n\nIt took us multiple steps and trial-and-error to load the data in exactly the format we want, and this is not unexpected. Data science is an iterative process, and the process of iteration can be even more tedious when reading data in from spreadsheets compared to other plain text, rectangular data files because humans tend to input data into spreadsheets and use them not just for data storage but also for sharing and communication.\n我们通过多个步骤和反复试错才将数据加载成我们想要的确切格式，这并不意外。数据科学是一个迭代过程，与从其他纯文本、矩形数据文件读取数据相比，从电子表格中读取数据的迭代过程可能更加繁琐，因为人们倾向于将数据输入电子表格，并不仅仅将其用于数据存储，还用于共享和交流。\nThere is no way to know exactly what the data will look like until you load it and take a look at it. Well, there is one way, actually. You can open the file in Excel and take a peek. If you’re going to do so, we recommend making a copy of the Excel file to open and browse interactively while leaving the original data file untouched and reading into R from the untouched file. This will ensure you don’t accidentally overwrite anything in the spreadsheet while inspecting it. You should also not be afraid of doing what we did here: load the data, take a peek, make adjustments to your code, load it again, and repeat until you’re happy with the result.\n除非你加载并查看数据，否则无法确切知道数据会是什么样子。嗯，实际上，有一种方法。你可以在 Excel 中打开文件看一看。如果你打算这样做，我们建议你复制一份 Excel 文件来打开和交互式浏览，同时保持原始数据文件不变，并从原始文件读入 R。这将确保你在检查电子表格时不会意外覆盖任何内容。你也不应该害怕像我们这里所做的那样：加载数据，看一看，调整你的代码，再次加载，然后重复这个过程，直到你对结果满意为止。\n\n20.2.4 Reading worksheets\nAn important feature that distinguishes spreadsheets from flat files is the notion of multiple sheets, called worksheets. Figure 20.2 shows an Excel spreadsheet with multiple worksheets. The data come from the palmerpenguins package, and you can download this spreadsheet as an Excel file from https://docs.google.com/spreadsheets/d/1aFu8lnD_g0yjF5O-K6SFgSEWiHPpgvFCF0NY9D6LXnY/. Each worksheet contains information on penguins from a different island where data were collected.\n将电子表格与平面文件区分开来的一个重要特性是多张工作表（worksheets）的概念。Figure 20.2 展示了一个包含多个工作表的 Excel 电子表格。数据来自 palmerpenguins 包，你可以从 https://docs.google.com/spreadsheets/d/1aFu8lnD_g0yjF5O-K6SFgSEWiHPpgvFCF0NY9D6LXnY/ 下载此电子表格的 Excel 文件。每个工作表都包含了来自不同岛屿的企鹅信息，这些数据是在这些岛屿上收集的。\n\n\n\n\n\n\n\nFigure 20.2: Spreadsheet called penguins.xlsx in Excel containing three worksheets.\n\n\n\n\nYou can read a single worksheet from a spreadsheet with the sheet argument in read_excel(). The default, which we’ve been relying on up until now, is the first sheet.\n你可以使用 read_excel() 中的 sheet 参数从电子表格中读取单个工作表。到目前为止我们一直依赖的默认设置是第一个工作表。\n\nread_excel(\"data/penguins.xlsx\", sheet = \"Torgersen Island\")\n#&gt; # A tibble: 52 × 8\n#&gt;   species island    bill_length_mm     bill_depth_mm      flipper_length_mm\n#&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;              &lt;chr&gt;              &lt;chr&gt;            \n#&gt; 1 Adelie  Torgersen 39.1               18.7               181              \n#&gt; 2 Adelie  Torgersen 39.5               17.399999999999999 186              \n#&gt; 3 Adelie  Torgersen 40.299999999999997 18                 195              \n#&gt; 4 Adelie  Torgersen NA                 NA                 NA               \n#&gt; 5 Adelie  Torgersen 36.700000000000003 19.3               193              \n#&gt; 6 Adelie  Torgersen 39.299999999999997 20.6               190              \n#&gt; # ℹ 46 more rows\n#&gt; # ℹ 3 more variables: body_mass_g &lt;chr&gt;, sex &lt;chr&gt;, year &lt;dbl&gt;\n\nSome variables that appear to contain numerical data are read in as characters due to the character string \"NA\" not being recognized as a true NA.\n由于字符串 \"NA\" 未被识别为真正的 NA，一些看起来包含数值数据的变量被读入为字符型。\n\npenguins_torgersen &lt;- read_excel(\"data/penguins.xlsx\", sheet = \"Torgersen Island\", na = \"NA\")\n\npenguins_torgersen\n#&gt; # A tibble: 52 × 8\n#&gt;   species island    bill_length_mm bill_depth_mm flipper_length_mm\n#&gt;   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;\n#&gt; 1 Adelie  Torgersen           39.1          18.7               181\n#&gt; 2 Adelie  Torgersen           39.5          17.4               186\n#&gt; 3 Adelie  Torgersen           40.3          18                 195\n#&gt; 4 Adelie  Torgersen           NA            NA                  NA\n#&gt; 5 Adelie  Torgersen           36.7          19.3               193\n#&gt; 6 Adelie  Torgersen           39.3          20.6               190\n#&gt; # ℹ 46 more rows\n#&gt; # ℹ 3 more variables: body_mass_g &lt;dbl&gt;, sex &lt;chr&gt;, year &lt;dbl&gt;\n\nAlternatively, you can use excel_sheets() to get information on all worksheets in an Excel spreadsheet, and then read the one(s) you’re interested in.\n或者，你可以使用 excel_sheets() 获取 Excel 电子表格中所有工作表的信息，然后读取你感兴趣的一个或多个工作表。\n\nexcel_sheets(\"data/penguins.xlsx\")\n#&gt; [1] \"Torgersen Island\" \"Biscoe Island\"    \"Dream Island\"\n\nOnce you know the names of the worksheets, you can read them in individually with read_excel().\n一旦你知道了工作表的名称，就可以使用 read_excel() 单独将它们读入。\n\npenguins_biscoe &lt;- read_excel(\"data/penguins.xlsx\", sheet = \"Biscoe Island\", na = \"NA\")\npenguins_dream  &lt;- read_excel(\"data/penguins.xlsx\", sheet = \"Dream Island\", na = \"NA\")\n\nIn this case the full penguins dataset is spread across three worksheets in the spreadsheet. Each worksheet has the same number of columns but different numbers of rows.\n在这种情况下，完整的企鹅数据集分布在电子表格的三个工作表中。每个工作表的列数相同，但行数不同。\n\ndim(penguins_torgersen)\n#&gt; [1] 52  8\ndim(penguins_biscoe)\n#&gt; [1] 168   8\ndim(penguins_dream)\n#&gt; [1] 124   8\n\nWe can put them together with bind_rows().\n我们可以使用 bind_rows() 将它们合并在一起。\n\npenguins &lt;- bind_rows(penguins_torgersen, penguins_biscoe, penguins_dream)\npenguins\n#&gt; # A tibble: 344 × 8\n#&gt;   species island    bill_length_mm bill_depth_mm flipper_length_mm\n#&gt;   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;\n#&gt; 1 Adelie  Torgersen           39.1          18.7               181\n#&gt; 2 Adelie  Torgersen           39.5          17.4               186\n#&gt; 3 Adelie  Torgersen           40.3          18                 195\n#&gt; 4 Adelie  Torgersen           NA            NA                  NA\n#&gt; 5 Adelie  Torgersen           36.7          19.3               193\n#&gt; 6 Adelie  Torgersen           39.3          20.6               190\n#&gt; # ℹ 338 more rows\n#&gt; # ℹ 3 more variables: body_mass_g &lt;dbl&gt;, sex &lt;chr&gt;, year &lt;dbl&gt;\n\nIn Chapter 26 we’ll talk about ways of doing this sort of task without repetitive code.\n在 Chapter 26 中，我们将讨论如何在没有重复代码的情况下完成这类任务。\n\n20.2.5 Reading part of a sheet\nSince many use Excel spreadsheets for presentation as well as for data storage, it’s quite common to find cell entries in a spreadsheet that are not part of the data you want to read into R. Figure 20.3 shows such a spreadsheet: in the middle of the sheet is what looks like a data frame but there is extraneous text in cells above and below the data.\n由于许多人使用 Excel 电子表格进行演示和数据存储，因此在电子表格中发现不属于您想读入 R 的数据的单元格条目是很常见的。Figure 20.3 展示了这样一个电子表格：在工作表的中间看起来像一个数据框，但在数据的上方和下方单元格中有无关的文本。\n\n\n\n\n\n\n\nFigure 20.3: Spreadsheet called deaths.xlsx in Excel.\n\n\n\n\nThis spreadsheet is one of the example spreadsheets provided in the readxl package. You can use the readxl_example() function to locate the spreadsheet on your system in the directory where the package is installed. This function returns the path to the spreadsheet, which you can use in read_excel() as usual.\n这个电子表格是 readxl 包中提供的示例电子表格之一。你可以使用 readxl_example() 函数在你的系统上定位到包安装目录下的这个电子表格。该函数返回电子表格的路径，你可以像往常一样在 read_excel() 中使用它。\n\ndeaths_path &lt;- readxl_example(\"deaths.xlsx\")\ndeaths &lt;- read_excel(deaths_path)\n#&gt; New names:\n#&gt; • `` -&gt; `...2`\n#&gt; • `` -&gt; `...3`\n#&gt; • `` -&gt; `...4`\n#&gt; • `` -&gt; `...5`\n#&gt; • `` -&gt; `...6`\ndeaths\n#&gt; # A tibble: 18 × 6\n#&gt;   `Lots of people`    ...2       ...3  ...4     ...5          ...6           \n#&gt;   &lt;chr&gt;               &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;         &lt;chr&gt;          \n#&gt; 1 simply cannot resi… &lt;NA&gt;       &lt;NA&gt;  &lt;NA&gt;     &lt;NA&gt;          some notes     \n#&gt; 2 at                  the        top   &lt;NA&gt;     of            their spreadsh…\n#&gt; 3 or                  merging    &lt;NA&gt;  &lt;NA&gt;     &lt;NA&gt;          cells          \n#&gt; 4 Name                Profession Age   Has kids Date of birth Date of death  \n#&gt; 5 David Bowie         musician   69    TRUE     17175         42379          \n#&gt; 6 Carrie Fisher       actor      60    TRUE     20749         42731          \n#&gt; # ℹ 12 more rows\n\nThe top three rows and the bottom four rows are not part of the data frame. It’s possible to eliminate these extraneous rows using the skip and n_max arguments, but we recommend using cell ranges. In Excel, the top left cell is A1. As you move across columns to the right, the cell label moves down the alphabet, i.e. B1, C1, etc. And as you move down a column, the number in the cell label increases, i.e. A2, A3, etc.\n最上面的三行和最下面的四行不属于数据框。可以使用 skip 和 n_max 参数来消除这些多余的行，但我们建议使用单元格范围。在 Excel 中，左上角的单元格是 A1。当你向右移动列时，单元格标签会按字母表顺序移动，即 B1、C1 等。当你向下移动一列时，单元格标签中的数字会增加，即 A2、A3 等。\nHere the data we want to read in starts in cell A5 and ends in cell F15. In spreadsheet notation, this is A5:F15, which we supply to the range argument:\n在这里，我们想要读入的数据从单元格 A5 开始，到单元格 F15 结束。在电子表格表示法中，这是 A5:F15，我们将其提供给 range 参数：\n\nread_excel(deaths_path, range = \"A5:F15\")\n#&gt; # A tibble: 10 × 6\n#&gt;   Name          Profession   Age `Has kids` `Date of birth`    \n#&gt;   &lt;chr&gt;         &lt;chr&gt;      &lt;dbl&gt; &lt;lgl&gt;      &lt;dttm&gt;             \n#&gt; 1 David Bowie   musician      69 TRUE       1947-01-08 00:00:00\n#&gt; 2 Carrie Fisher actor         60 TRUE       1956-10-21 00:00:00\n#&gt; 3 Chuck Berry   musician      90 TRUE       1926-10-18 00:00:00\n#&gt; 4 Bill Paxton   actor         61 TRUE       1955-05-17 00:00:00\n#&gt; 5 Prince        musician      57 TRUE       1958-06-07 00:00:00\n#&gt; 6 Alan Rickman  actor         69 FALSE      1946-02-21 00:00:00\n#&gt; # ℹ 4 more rows\n#&gt; # ℹ 1 more variable: `Date of death` &lt;dttm&gt;\n\n\n20.2.6 Data types\nIn CSV files, all values are strings. This is not particularly true to the data, but it is simple: everything is a string.\n在 CSV 文件中，所有值都是字符串。这对于数据来说并非特别真实，但它很简单：一切都是字符串。\nThe underlying data in Excel spreadsheets is more complex. A cell can be one of four things:\nExcel 电子表格中的底层数据更为复杂。一个单元格可以是以下四种类型之一：\n\nA boolean, like TRUE, FALSE, or NA.\n布尔值，如 TRUE、FALSE 或 NA。\nA number, like “10” or “10.5”.\n数字，如 “10” 或 “10.5”。\nA datetime, which can also include time like “11/1/21” or “11/1/21 3:00 PM”.\n日期时间，也可以包含时间，如 “11/1/21” 或 “11/1/21 3:00 PM”。\nA text string, like “ten”.\n文本字符串，如 “ten”。\n\nWhen working with spreadsheet data, it’s important to keep in mind that the underlying data can be very different than what you see in the cell. For example, Excel has no notion of an integer. All numbers are stored as floating points, but you can choose to display the data with a customizable number of decimal points. Similarly, dates are actually stored as numbers, specifically the number of seconds since January 1, 1970. You can customize how you display the date by applying formatting in Excel. Confusingly, it’s also possible to have something that looks like a number but is actually a string (e.g., type '10 into a cell in Excel).\n在使用电子表格数据时，重要的是要记住，底层数据可能与你在单元格中看到的大相径庭。例如，Excel 没有整数的概念。所有数字都以浮点数形式存储，但你可以选择以可自定义的小数位数来显示数据。同样，日期实际上是作为数字存储的，具体来说是从 1970 年 1 月 1 日以来的秒数。你可以通过在 Excel 中应用格式来自定义日期的显示方式。令人困惑的是，也可能存在看起来像数字但实际上是字符串的情况（例如，在 Excel 单元格中输入 '10）。\nThese differences between how the underlying data are stored vs. how they’re displayed can cause surprises when the data are loaded into R. By default readxl will guess the data type in a given column. A recommended workflow is to let readxl guess the column types, confirm that you’re happy with the guessed column types, and if not, go back and re-import specifying col_types as shown in Section 20.2.3.\n底层数据的存储方式与显示方式之间的这些差异，在将数据加载到 R 时可能会带来意外。默认情况下，readxl 会猜测给定列的数据类型。推荐的工作流程是让 readxl 猜测列类型，确认你对猜测的列类型满意，如果不满意，则返回并重新导入，并如 Section 20.2.3 所示指定 col_types。\nAnother challenge is when you have a column in your Excel spreadsheet that has a mix of these types, e.g., some cells are numeric, others text, others dates. When importing the data into R readxl has to make some decisions. In these cases you can set the type for this column to \"list\", which will load the column as a list of length 1 vectors, where the type of each element of the vector is guessed.\n另一个挑战是当你的 Excel 电子表格中的一列混合了这些类型时，例如，一些单元格是数字，一些是文本，还有一些是日期。在将数据导入 R 时，readxl 必须做出一些决定。在这些情况下，你可以将该列的类型设置为 \"list\"，这将把该列加载为一个长度为 1 的向量列表，其中向量的每个元素的类型都是被猜测的。\n\n\n\n\n\n\nSometimes data is stored in more exotic ways, like the color of the cell background, or whether or not the text is bold. In such cases, you might find the tidyxl package useful. See https://nacnudus.github.io/spreadsheet-munging-strategies/ for more on strategies for working with non-tabular data from Excel.\n有时数据以更奇特的方式存储，比如单元格的背景颜色，或者文本是否为粗体。在这种情况下，你可能会发现 tidyxl 包很有用。有关处理来自 Excel 的非表格数据的策略，请参阅 https://nacnudus.github.io/spreadsheet-munging-strategies/。\n\n\n\n\n20.2.7 Writing to Excel\nLet’s create a small data frame that we can then write out. Note that item is a factor and quantity is an integer.\n让我们创建一个小的数据框，然后可以将其写出。请注意，item 是一个因子 (factor)，quantity 是一个整数 (integer)。\n\nbake_sale &lt;- tibble(\n  item     = factor(c(\"brownie\", \"cupcake\", \"cookie\")),\n  quantity = c(10, 5, 8)\n)\n\nbake_sale\n#&gt; # A tibble: 3 × 2\n#&gt;   item    quantity\n#&gt;   &lt;fct&gt;      &lt;dbl&gt;\n#&gt; 1 brownie       10\n#&gt; 2 cupcake        5\n#&gt; 3 cookie         8\n\nYou can write data back to disk as an Excel file using the write_xlsx() function from the writexl package:\n你可以使用来自 writexl 包 的 write_xlsx() 函数将数据写回磁盘，保存为 Excel 文件：\n\nwrite_xlsx(bake_sale, path = \"data/bake-sale.xlsx\")\n\nFigure 20.4 shows what the data looks like in Excel. Note that column names are included and bolded. These can be turned off by setting col_names and format_headers arguments to FALSE.Figure 20.4 展示了数据在 Excel 中的样子。请注意，列名被包含并加粗显示。可以通过将 col_names 和 format_headers 参数设置为 FALSE 来关闭这些功能。\n\n\n\n\n\n\n\nFigure 20.4: Spreadsheet called bake-sale.xlsx in Excel.\n\n\n\n\nJust like reading from a CSV, information on data type is lost when we read the data back in. This makes Excel files unreliable for caching interim results as well. For alternatives, see Section 7.5.\n就像从 CSV 读取一样，当我们再次读入数据时，关于数据类型的信息会丢失。这也使得 Excel 文件不适合用于缓存临时结果。有关替代方案，请参见 Section 7.5。\n\nread_excel(\"data/bake-sale.xlsx\")\n#&gt; # A tibble: 3 × 2\n#&gt;   item    quantity\n#&gt;   &lt;chr&gt;      &lt;dbl&gt;\n#&gt; 1 brownie       10\n#&gt; 2 cupcake        5\n#&gt; 3 cookie         8\n\n\n20.2.8 Formatted output\nThe writexl package is a light-weight solution for writing a simple Excel spreadsheet, but if you’re interested in additional features like writing to sheets within a spreadsheet and styling, you will want to use the openxlsx package. We won’t go into the details of using this package here, but we recommend reading https://ycphs.github.io/openxlsx/articles/Formatting.html for an extensive discussion on further formatting functionality for data written from R to Excel with openxlsx.\nwritexl 包是一个用于编写简单 Excel 电子表格的轻量级解决方案，但如果你对更多功能感兴趣，例如写入电子表格中的工作表和设置样式，你会想要使用 openxlsx 包。我们在这里不会详细介绍如何使用这个包，但我们推荐阅读 https://ycphs.github.io/openxlsx/articles/Formatting.html，其中广泛讨论了使用 openxlsx 将数据从 R 写入 Excel 的进一步格式化功能。\nNote that this package is not part of the tidyverse so the functions and workflows may feel unfamiliar. For example, function names are camelCase, multiple functions can’t be composed in pipelines, and arguments are in a different order than they tend to be in the tidyverse. However, this is ok. As your R learning and usage expands outside of this book you will encounter lots of different styles used in various R packages that you might use to accomplish specific goals in R. A good way of familiarizing yourself with the coding style used in a new package is to run the examples provided in function documentation to get a feel for the syntax and the output formats as well as reading any vignettes that might come with the package.\n请注意，这个包不是 tidyverse 的一部分，因此函数和工作流程可能会让人感到陌生。例如，函数名是驼峰式命名法 (camelCase)，多个函数不能在管道中组合使用，并且参数的顺序也与 tidyverse 中的通常顺序不同。然而，这没关系。随着你的 R 学习和使用范围扩展到本书之外，你会遇到各种 R 包中使用的许多不同风格，你可能会用它们来完成 R 中的特定目标。熟悉一个新包中使用的编码风格的一个好方法是运行函数文档中提供的示例，以感受其语法和输出格式，并阅读包可能附带的任何 vignettes。\n\n20.2.9 Exercises\n\n\nIn an Excel file, create the following dataset and save it as survey.xlsx. Alternatively, you can download it as an Excel file from here.\n\n\n\n\n\n\n\n\nThen, read it into R, with survey_id as a character variable and n_pets as a numerical variable.\n\n#&gt; # A tibble: 6 × 2\n#&gt;   survey_id n_pets\n#&gt;   &lt;chr&gt;      &lt;dbl&gt;\n#&gt; 1 1              0\n#&gt; 2 2              1\n#&gt; 3 3             NA\n#&gt; 4 4              2\n#&gt; 5 5              2\n#&gt; 6 6             NA\n\n\n\nIn another Excel file, create the following dataset and save it as roster.xlsx. Alternatively, you can download it as an Excel file from here.\n\n\n\n\n\n\n\n\nThen, read it into R. The resulting data frame should be called roster and should look like the following.\n\n#&gt; # A tibble: 12 × 3\n#&gt;    group subgroup    id\n#&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n#&gt;  1     1 A            1\n#&gt;  2     1 A            2\n#&gt;  3     1 A            3\n#&gt;  4     1 B            4\n#&gt;  5     1 B            5\n#&gt;  6     1 B            6\n#&gt;  7     1 B            7\n#&gt;  8     2 A            8\n#&gt;  9     2 A            9\n#&gt; 10     2 B           10\n#&gt; 11     2 B           11\n#&gt; 12     2 B           12\n\n\n\nIn a new Excel file, create the following dataset and save it as sales.xlsx. Alternatively, you can download it as an Excel file from here.\n\n\n\n\n\n\n\n\na. Read sales.xlsx in and save as sales. The data frame should look like the following, with id and n as column names and with 9 rows.\n\n#&gt; # A tibble: 9 × 2\n#&gt;   id      n    \n#&gt;   &lt;chr&gt;   &lt;chr&gt;\n#&gt; 1 Brand 1 n    \n#&gt; 2 1234    8    \n#&gt; 3 8721    2    \n#&gt; 4 1822    3    \n#&gt; 5 Brand 2 n    \n#&gt; 6 3333    1    \n#&gt; 7 2156    3    \n#&gt; 8 3987    6    \n#&gt; 9 3216    5\n\nb. Modify sales further to get it into the following tidy format with three columns (brand, id, and n) and 7 rows of data. Note that id and n are numeric, brand is a character variable.\n\n#&gt; # A tibble: 7 × 3\n#&gt;   brand      id     n\n#&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Brand 1  1234     8\n#&gt; 2 Brand 1  8721     2\n#&gt; 3 Brand 1  1822     3\n#&gt; 4 Brand 2  3333     1\n#&gt; 5 Brand 2  2156     3\n#&gt; 6 Brand 2  3987     6\n#&gt; 7 Brand 2  3216     5\n\n\nRecreate the bake_sale data frame, write it out to an Excel file using the write.xlsx() function from the openxlsx package.\nIn Chapter 7 you learned about the janitor::clean_names() function to turn column names into snake case. Read the students.xlsx file that we introduced earlier in this section and use this function to “clean” the column names.\nWhat happens if you try to read in a file with .xlsx extension with read_xls()?",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Spreadsheets</span>"
    ]
  },
  {
    "objectID": "spreadsheets.html#google-sheets",
    "href": "spreadsheets.html#google-sheets",
    "title": "20  Spreadsheets",
    "section": "\n20.3 Google Sheets",
    "text": "20.3 Google Sheets\nGoogle Sheets is another widely used spreadsheet program. It’s free and web-based. Just like with Excel, in Google Sheets data are organized in worksheets (also called sheets) inside of spreadsheet files.\nGoogle Sheets 是另一款广泛使用的电子表格程序。它免费且基于网络。就像 Excel 一样，在 Google Sheets 中，数据被组织在电子表格文件内部的工作表 (worksheets，也称为 sheets) 中。\n\n20.3.1 Prerequisites\nThis section will also focus on spreadsheets, but this time you’ll be loading data from a Google Sheet with the googlesheets4 package. This package is non-core tidyverse as well, you need to load it explicitly.\n本节也将重点讨论电子表格，但这次你将使用 googlesheets4 包从 Google Sheet 加载数据。这个包同样不是 tidyverse 的核心包，你需要显式地加载它。\n\nlibrary(googlesheets4)\nlibrary(tidyverse)\n\nA quick note about the name of the package: googlesheets4 uses v4 of the Sheets API v4 to provide an R interface to Google Sheets, hence the name.\n关于包名的一点说明：googlesheets4 使用了 Sheets API v4 的第 4 版来提供 R 与 Google Sheets 的接口，因此得名。\n\n20.3.2 Getting started\nThe main function of the googlesheets4 package is read_sheet(), which reads a Google Sheet from a URL or a file id. This function also goes by the name range_read().\ngooglesheets4 包的主要函数是 read_sheet()，它可以从一个 URL 或文件 ID 读取 Google Sheet。这个函数也叫 range_read()。\nYou can also create a brand new sheet with gs4_create() or write to an existing sheet with sheet_write() and friends.\n你也可以用 gs4_create() 创建一个全新的工作表，或者用 sheet_write() 及其相关函数向现有的工作表写入数据。\nIn this section we’ll work with the same datasets as the ones in the Excel section to highlight similarities and differences between workflows for reading data from Excel and Google Sheets. readxl and googlesheets4 packages are both designed to mimic the functionality of the readr package, which provides the read_csv() function you’ve seen in Chapter 7. Therefore, many of the tasks can be accomplished with simply swapping out read_excel() for read_sheet(). However you’ll also see that Excel and Google Sheets don’t behave in exactly the same way, therefore other tasks may require further updates to the function calls.\n在本节中，我们将使用与 Excel 部分相同的数据集，以突显从 Excel 和 Google Sheets 读取数据的工作流程之间的异同。readxl 和 googlesheets4 包都被设计为模仿 readr 包的功能，后者提供了你在 Chapter 7 中见过的 read_csv() 函数。因此，许多任务只需将 read_excel() 替换为 read_sheet() 即可完成。然而，你也会发现 Excel 和 Google Sheets 的行为不完全相同，因此其他任务可能需要对函数调用进行进一步的更新。\n\n20.3.3 Reading Google Sheets\nFigure 20.5 shows what the spreadsheet we’re going to read into R looks like in Google Sheets. This is the same dataset as in Figure 20.1, except it’s stored in a Google Sheet instead of Excel.Figure 20.5 展示了我们将要读入 R 的电子表格在 Google Sheets 中的样子。这与 Figure 20.1 中的数据集相同，只是它存储在 Google Sheet 中而不是 Excel 中。\n\n\n\n\n\n\n\nFigure 20.5: Google Sheet called students in a browser window.\n\n\n\n\nThe first argument to read_sheet() is the URL of the file to read, and it returns a tibble:https://docs.google.com/spreadsheets/d/1V1nPp1tzOuutXFLb3G9Eyxi3qxeEhnOXUzL5_BcCQ0w. These URLs are not pleasant to work with, so you’ll often want to identify a sheet by its ID.read_sheet() 的第一个参数是要读取的文件的 URL，它返回一个 tibble：https://docs.google.com/spreadsheets/d/1V1nPp1tzOuutXFLb3G9Eyxi3qxeEhnOXUzL5_BcCQ0w。 这些 URL 使用起来并不方便，所以你通常会希望通过其 ID 来识别工作表。\n\ngs4_deauth()\n\n\nstudents_sheet_id &lt;- \"1V1nPp1tzOuutXFLb3G9Eyxi3qxeEhnOXUzL5_BcCQ0w\"\nstudents &lt;- read_sheet(students_sheet_id)\n#&gt; ✔ Reading from students.\n#&gt; ✔ Range Sheet1.\nstudents\n#&gt; # A tibble: 6 × 5\n#&gt;   `Student ID` `Full Name`      favourite.food     mealPlan            AGE   \n#&gt;          &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;list&gt;\n#&gt; 1            1 Sunil Huffmann   Strawberry yoghurt Lunch only          &lt;dbl&gt; \n#&gt; 2            2 Barclay Lynn     French fries       Lunch only          &lt;dbl&gt; \n#&gt; 3            3 Jayendra Lyne    N/A                Breakfast and lunch &lt;dbl&gt; \n#&gt; 4            4 Leon Rossini     Anchovies          Lunch only          &lt;NULL&gt;\n#&gt; 5            5 Chidiegwu Dunkel Pizza              Breakfast and lunch &lt;chr&gt; \n#&gt; 6            6 Güvenç Attila    Ice cream          Lunch only          &lt;dbl&gt;\n\nJust like we did with read_excel(), we can supply column names, NA strings, and column types to read_sheet().\n就像我们对 read_excel() 所做的那样，我们可以为 read_sheet() 提供列名、NA 字符串和列类型。\n\nstudents &lt;- read_sheet(\n  students_sheet_id,\n  col_names = c(\"student_id\", \"full_name\", \"favourite_food\", \"meal_plan\", \"age\"),\n  skip = 1,\n  na = c(\"\", \"N/A\"),\n  col_types = \"dcccc\"\n)\n#&gt; ✔ Reading from students.\n#&gt; ✔ Range 2:10000000.\n\nstudents\n#&gt; # A tibble: 6 × 5\n#&gt;   student_id full_name        favourite_food     meal_plan           age  \n#&gt;        &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n#&gt; 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n#&gt; 2          2 Barclay Lynn     French fries       Lunch only          5    \n#&gt; 3          3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch 7    \n#&gt; 4          4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n#&gt; 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n#&gt; 6          6 Güvenç Attila    Ice cream          Lunch only          6\n\nNote that we defined column types a bit differently here, using short codes. For example, “dcccc” stands for “double, character, character, character, character”.\n请注意，我们在这里用短代码定义列类型的方式有些不同。例如，“dcccc” 代表 “double, character, character, character, character”。\nIt’s also possible to read individual sheets from Google Sheets as well. Let’s read the “Torgersen Island” sheet from the penguins Google Sheet:\n也可以从 Google Sheets 读取单个工作表。让我们从 penguins Google Sheet 中读取名为 “Torgersen Island” 的工作表：\n\npenguins_sheet_id &lt;- \"1aFu8lnD_g0yjF5O-K6SFgSEWiHPpgvFCF0NY9D6LXnY\"\nread_sheet(penguins_sheet_id, sheet = \"Torgersen Island\")\n#&gt; ✔ Reading from penguins.\n#&gt; ✔ Range ''Torgersen Island''.\n#&gt; # A tibble: 52 × 8\n#&gt;   species island    bill_length_mm bill_depth_mm flipper_length_mm\n#&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;list&gt;         &lt;list&gt;        &lt;list&gt;           \n#&gt; 1 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;        \n#&gt; 2 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;        \n#&gt; 3 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;        \n#&gt; 4 Adelie  Torgersen &lt;chr [1]&gt;      &lt;chr [1]&gt;     &lt;chr [1]&gt;        \n#&gt; 5 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;        \n#&gt; 6 Adelie  Torgersen &lt;dbl [1]&gt;      &lt;dbl [1]&gt;     &lt;dbl [1]&gt;        \n#&gt; # ℹ 46 more rows\n#&gt; # ℹ 3 more variables: body_mass_g &lt;list&gt;, sex &lt;chr&gt;, year &lt;dbl&gt;\n\nYou can obtain a list of all sheets within a Google Sheet with sheet_names():\n你可以使用 sheet_names() 获取 Google Sheet 中所有工作表的列表：\n\nsheet_names(penguins_sheet_id)\n#&gt; [1] \"Torgersen Island\" \"Biscoe Island\"    \"Dream Island\"\n\nFinally, just like with read_excel(), we can read in a portion of a Google Sheet by defining a range in read_sheet(). Note that we’re also using the gs4_example() function below to locate an example Google Sheet that comes with the googlesheets4 package.\n最后，就像使用 read_excel() 一样，我们可以通过在 read_sheet() 中定义一个 range 来读取 Google Sheet 的一部分。请注意，下面我们还使用了 gs4_example() 函数来定位一个 googlesheets4 包自带的示例 Google Sheet。\n\ndeaths_url &lt;- gs4_example(\"deaths\")\ndeaths &lt;- read_sheet(deaths_url, range = \"A5:F15\")\n#&gt; ✔ Reading from deaths.\n#&gt; ✔ Range A5:F15.\ndeaths\n#&gt; # A tibble: 10 × 6\n#&gt;   Name          Profession   Age `Has kids` `Date of birth`    \n#&gt;   &lt;chr&gt;         &lt;chr&gt;      &lt;dbl&gt; &lt;lgl&gt;      &lt;dttm&gt;             \n#&gt; 1 David Bowie   musician      69 TRUE       1947-01-08 00:00:00\n#&gt; 2 Carrie Fisher actor         60 TRUE       1956-10-21 00:00:00\n#&gt; 3 Chuck Berry   musician      90 TRUE       1926-10-18 00:00:00\n#&gt; 4 Bill Paxton   actor         61 TRUE       1955-05-17 00:00:00\n#&gt; 5 Prince        musician      57 TRUE       1958-06-07 00:00:00\n#&gt; 6 Alan Rickman  actor         69 FALSE      1946-02-21 00:00:00\n#&gt; # ℹ 4 more rows\n#&gt; # ℹ 1 more variable: `Date of death` &lt;dttm&gt;\n\n\n20.3.4 Writing to Google Sheets\nYou can write from R to Google Sheets with write_sheet(). The first argument is the data frame to write, and the second argument is the name (or other identifier) of the Google Sheet to write to:\n你可以使用 write_sheet() 从 R 写入数据到 Google Sheets。第一个参数是要写入的数据框，第二个参数是要写入的 Google Sheet 的名称（或其他标识符）：\n\nwrite_sheet(bake_sale, ss = \"bake-sale\")\n\nIf you’d like to write your data to a specific (work)sheet inside a Google Sheet, you can specify that with the sheet argument as well.\n如果你想将数据写入 Google Sheet 中的特定工作表 (worksheet)，你也可以通过 sheet 参数来指定。\n\nwrite_sheet(bake_sale, ss = \"bake-sale\", sheet = \"Sales\")\n\n\n20.3.5 Authentication\nWhile you can read from a public Google Sheet without authenticating with your Google account and with gs4_deauth(), reading a private sheet or writing to a sheet requires authentication so that googlesheets4 can view and manage your Google Sheets.\n虽然你可以使用 gs4_deauth() 在不通过 Google 账户认证的情况下读取公开的 Google Sheet，但读取私有工作表或向工作表写入数据则需要认证，以便 googlesheets4 可以查看和管理你的 Google Sheets。\nWhen you attempt to read in a sheet that requires authentication, googlesheets4 will direct you to a web browser with a prompt to sign in to your Google account and grant permission to operate on your behalf with Google Sheets. However, if you want to specify a specific Google account, authentication scope, etc. you can do so with gs4_auth(), e.g., gs4_auth(email = \"mine@example.com\"), which will force the use of a token associated with a specific email. For further authentication details, we recommend reading the documentation googlesheets4 auth vignette: https://googlesheets4.tidyverse.org/articles/auth.html.\n当你尝试读取需要认证的工作表时，googlesheets4 会将你引导至一个网页浏览器，提示你登录 Google 账户并授权其代表你操作 Google Sheets。但是，如果你想指定一个特定的 Google 账户、认证范围等，你可以使用 gs4_auth() 来实现，例如 gs4_auth(email = \"mine@example.com\")，这将强制使用与特定电子邮件关联的令牌。有关更多认证详情，我们建议阅读 googlesheets4 认证 vignette 文档：https://googlesheets4.tidyverse.org/articles/auth.html。\n\n20.3.6 Exercises\n\nRead the students dataset from earlier in the chapter from Excel and also from Google Sheets, with no additional arguments supplied to the read_excel() and read_sheet() functions. Are the resulting data frames in R exactly the same? If not, how are they different?\nRead the Google Sheet titled survey from https://pos.it/r4ds-survey, with survey_id as a character variable and n_pets as a numerical variable.\n\nRead the Google Sheet titled roster from https://pos.it/r4ds-roster. The resulting data frame should be called roster and should look like the following.\n\n#&gt; # A tibble: 12 × 3\n#&gt;    group subgroup    id\n#&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n#&gt;  1     1 A            1\n#&gt;  2     1 A            2\n#&gt;  3     1 A            3\n#&gt;  4     1 B            4\n#&gt;  5     1 B            5\n#&gt;  6     1 B            6\n#&gt;  7     1 B            7\n#&gt;  8     2 A            8\n#&gt;  9     2 A            9\n#&gt; 10     2 B           10\n#&gt; 11     2 B           11\n#&gt; 12     2 B           12",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Spreadsheets</span>"
    ]
  },
  {
    "objectID": "spreadsheets.html#summary",
    "href": "spreadsheets.html#summary",
    "title": "20  Spreadsheets",
    "section": "\n20.4 Summary",
    "text": "20.4 Summary\nMicrosoft Excel and Google Sheets are two of the most popular spreadsheet systems. Being able to interact with data stored in Excel and Google Sheets files directly from R is a superpower! In this chapter you learned how to read data into R from spreadsheets from Excel with read_excel() from the readxl package and from Google Sheets with read_sheet() from the googlesheets4 package. These functions work very similarly to each other and have similar arguments for specifying column names, NA strings, rows to skip on top of the file you’re reading in, etc. Additionally, both functions make it possible to read a single sheet from a spreadsheet as well.\nMicrosoft Excel 和 Google Sheets 是两种最流行的电子表格系统。能够直接从 R 中与存储在 Excel 和 Google Sheets 文件中的数据进行交互是一项超能力！在本章中，你学习了如何使用 readxl 包中的 read_excel() 函数从 Excel 电子表格中读取数据到 R，以及如何使用 googlesheets4 包中的 read_sheet() 函数从 Google Sheets 中读取数据。这两个函数的工作方式非常相似，并且有类似的参数用于指定列名、NA 字符串、在读取文件顶部时跳过的行数等。此外，这两个函数都支持从电子表格中读取单个工作表。\nOn the other hand, writing to an Excel file requires a different package and function (writexl::write_xlsx()) while you can write to a Google Sheet with the googlesheets4 package, with write_sheet().\n另一方面，写入 Excel 文件需要使用不同的包和函数 (writexl::write_xlsx())，而你可以使用 googlesheets4 包中的 write_sheet() 函数来写入 Google Sheet。\nIn the next chapter, you’ll learn about a different data source and how to read data from that source into R: databases.\n在下一章中，你将学习另一种不同的数据源，以及如何将该来源的数据读入 R：数据库。",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Spreadsheets</span>"
    ]
  },
  {
    "objectID": "databases.html",
    "href": "databases.html",
    "title": "21  Databases",
    "section": "",
    "text": "21.1 Introduction\nA huge amount of data lives in databases, so it’s essential that you know how to access it.\n大量的数据都存储在数据库中，因此了解如何访问这些数据至关重要。\nSometimes you can ask someone to download a snapshot into a .csv for you, but this gets painful quickly: every time you need to make a change you’ll have to communicate with another human.\n有时，你可以请别人为你下载一个快照到 .csv 文件中，但这很快就会变得痛苦：每次你需要做更改时，都必须与另一个人沟通。\nYou want to be able to reach into the database directly to get the data you need, when you need it.\n你希望能够直接访问数据库，在需要的时候获取所需的数据。\nIn this chapter, you’ll first learn the basics of the DBI package: how to use it to connect to a database and then retrieve data with a SQL1 query.\n在本章中，你将首先学习 DBI 包的基础知识：如何使用它连接到数据库，然后通过 SQL1 查询来检索数据。\nSQL, short for structured query language, is the lingua franca of databases, and is an important language for all data scientists to learn.SQL 是结构化查询语言（structured query language）的缩写，是数据库的通用语言，也是所有数据科学家需要学习的重要语言。\nThat said, we’re not going to start with SQL, but instead we’ll teach you dbplyr, which can translate your dplyr code to the SQL.\n尽管如此，我们不会从 SQL 开始，而是会教你 dbplyr，它可以将你的 dplyr 代码翻译成 SQL。\nWe’ll use that as a way to teach you some of the most important features of SQL.\n我们将以此为途径，教你一些 SQL 最重要的特性。\nYou won’t become a SQL master by the end of the chapter, but you will be able to identify the most important components and understand what they do.\n在本章结束时，你不会成为 SQL 大师，但你将能够识别出最重要的组成部分并理解它们的作用。",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "databases.html#introduction",
    "href": "databases.html#introduction",
    "title": "21  Databases",
    "section": "",
    "text": "21.1.1 Prerequisites\nIn this chapter, we’ll introduce DBI and dbplyr.\n在本章中，我们将介绍 DBI 和 dbplyr。\nDBI is a low-level interface that connects to databases and executes SQL; dbplyr is a high-level interface that translates your dplyr code to SQL queries then executes them with DBI.\nDBI 是一个连接数据库并执行 SQL 的底层接口；dbplyr 是一个高层接口，它将你的 dplyr 代码翻译成 SQL 查询，然后通过 DBI 执行它们。\n\nlibrary(DBI)\nlibrary(dbplyr)\nlibrary(tidyverse)",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "databases.html#database-basics",
    "href": "databases.html#database-basics",
    "title": "21  Databases",
    "section": "\n21.2 Database basics",
    "text": "21.2 Database basics\nAt the simplest level, you can think about a database as a collection of data frames, called tables in database terminology.\n在最简单的层面上，你可以把数据库看作是数据框的集合，在数据库术语中称为表 (tables)。\nLike a data frame, a database table is a collection of named columns, where every value in the column is the same type.\n与数据框一样，数据库表是命名列的集合，其中每列中的所有值都具有相同的类型。\nThere are three high level differences between data frames and database tables:\n数据框和数据库表之间有三个高级别的区别：\n\nDatabase tables are stored on disk and can be arbitrarily large.\nData frames are stored in memory, and are fundamentally limited (although that limit is still plenty large for many problems).\n数据库表存储在磁盘上，可以任意大。\n数据框存储在内存中，并且有根本的大小限制（尽管这个限制对于许多问题来说仍然足够大）。\nDatabase tables almost always have indexes.\nMuch like the index of a book, a database index makes it possible to quickly find rows of interest without having to look at every single row.\nData frames and tibbles don’t have indexes, but data.tables do, which is one of the reasons that they’re so fast.\n数据库表几乎总是有索引。\n就像书的索引一样，数据库索引可以快速找到感兴趣的行，而无需查看每一行。\n数据框和 tibbles 没有索引，但 data.tables 有，这也是它们速度如此之快的原因之一。\nMost classical databases are optimized for rapidly collecting data, not analyzing existing data.\nThese databases are called row-oriented because the data is stored row-by-row, rather than column-by-column like R.\nMore recently, there’s been much development of column-oriented databases that make analyzing the existing data much faster.\n大多数传统数据库都为快速收集数据而优化，而不是为分析现有数据而优化。\n这些数据库被称为行式存储 (row-oriented)，因为数据是按行存储的，而不是像 R 那样按列存储。\n近年来，列式存储 (column-oriented) 数据库得到了长足发展，它们使分析现有数据变得快得多。\n\nDatabases are run by database management systems (DBMS’s for short), which come in three basic forms:\n数据库由数据库管理系统（简称 DBMS）运行，主要有三种基本形式：\n\nClient-server DBMS’s run on a powerful central server, which you connect to from your computer (the client). They are great for sharing data with multiple people in an organization. Popular client-server DBMS’s include PostgreSQL, MariaDB, SQL Server, and Oracle.客户端-服务器 (Client-server) 模式的 DBMS 运行在一台强大的中央服务器上，你从你的计算机（客户端）连接到它。它们非常适合在组织内与多人共享数据。流行的客户端-服务器 DBMS 包括 PostgreSQL、MariaDB、SQL Server 和 Oracle。\nCloud DBMS’s, like Snowflake, Amazon’s RedShift, and Google’s BigQuery, are similar to client server DBMS’s, but they run in the cloud. This means that they can easily handle extremely large datasets and can automatically provide more compute resources as needed.云 (Cloud) DBMS，如 Snowflake、Amazon 的 RedShift 和 Google 的 BigQuery，与客户端-服务器 DBMS 类似，但它们运行在云端。这意味着它们可以轻松处理极大的数据集，并可以根据需要自动提供更多的计算资源。\nIn-process DBMS’s, like SQLite or duckdb, run entirely on your computer. They’re great for working with large datasets where you’re the primary user.进程内 (In-process) DBMS，如 SQLite 或 duckdb，完全在你的计算机上运行。当你作为主要用户处理大型数据集时，它们是很好的选择。",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "databases.html#connecting-to-a-database",
    "href": "databases.html#connecting-to-a-database",
    "title": "21  Databases",
    "section": "\n21.3 Connecting to a database",
    "text": "21.3 Connecting to a database\nTo connect to the database from R, you’ll use a pair of packages:\n要从 R 连接到数据库，你需要使用一对包：\n\nYou’ll always use DBI (database interface) because it provides a set of generic functions that connect to the database, upload data, run SQL queries, etc.\n你将总是使用 DBI（database interface，数据库接口），因为它提供了一组通用函数，用于连接数据库、上传数据、运行 SQL 查询等。\nYou’ll also use a package tailored for the DBMS you’re connecting to.\nThis package translates the generic DBI commands into the specifics needed for a given DBMS.\nThere’s usually one package for each DBMS, e.g.\nRPostgres for PostgreSQL and RMariaDB for MySQL.\n你还需要使用一个为你所连接的 DBMS 定制的包。\n这个包会将通用的 DBI 命令翻译成特定 DBMS 所需的具体指令。\n通常每个 DBMS 都有一个对应的包，例如：\n用于 PostgreSQL 的 RPostgres 和用于 MySQL 的 RMariaDB。\n\nIf you can’t find a specific package for your DBMS, you can usually use the odbc package instead.\n如果你找不到适用于你的 DBMS 的特定包，你通常可以使用 odbc 包作为替代。\nThis uses the ODBC protocol supported by many DBMS.\n它使用了许多 DBMS 支持的 ODBC 协议。\nodbc requires a little more setup because you’ll also need to install an ODBC driver and tell the odbc package where to find it.\nodbc 需要多一些设置，因为你还需要安装一个 ODBC 驱动程序，并告诉 odbc 包在哪里可以找到它。\nConcretely, you create a database connection using DBI::dbConnect().\n具体来说，你使用 DBI::dbConnect() 创建一个数据库连接。\nThe first argument selects the DBMS2, then the second and subsequent arguments describe how to connect to it (i.e. where it lives and the credentials that you need to access it).\n第一个参数选择 DBMS2，然后第二个及后续参数描述如何连接到它（即，它在哪里以及访问它所需的凭据）。\nThe following code shows a couple of typical examples:\n以下代码展示了几个典型的例子：\n\ncon &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(), \n  username = \"foo\"\n)\ncon &lt;- DBI::dbConnect(\n  RPostgres::Postgres(), \n  hostname = \"databases.mycompany.com\", \n  port = 1234\n)\n\nThe precise details of the connection vary a lot from DBMS to DBMS so unfortunately we can’t cover all the details here.\n连接的具体细节因 DBMS 而异，所以很遗憾我们无法在这里涵盖所有细节。\nThis means you’ll need to do a little research on your own.\n这意味着你需要自己做一些研究。\nTypically you can ask the other data scientists in your team or talk to your DBA (database administrator).\n通常你可以询问团队中的其他数据科学家或与你的 DBA（数据库管理员）交流。\nThe initial setup will often take a little fiddling (and maybe some googling) to get it right, but you’ll generally only need to do it once.\n初始设置通常需要一些摸索（可能还需要谷歌搜索）才能搞定，但你通常只需要做一次。\n\n21.3.1 In this book\nSetting up a client-server or cloud DBMS would be a pain for this book, so we’ll instead use an in-process DBMS that lives entirely in an R package: duckdb.\n为本书设置一个客户端-服务器或云 DBMS 会很麻烦，所以我们将使用一个完全存在于 R 包中的进程内 DBMS：duckdb。\nThanks to the magic of DBI, the only difference between using duckdb and any other DBMS is how you’ll connect to the database.\n得益于 DBI 的魔力，使用 duckdb 和任何其他 DBMS 之间的唯一区别就是你如何连接到数据库。\nThis makes it great to teach with because you can easily run this code as well as easily take what you learn and apply it elsewhere.\n这使得它非常适合教学，因为你可以轻松地运行这段代码，也可以轻松地将学到的知识应用到其他地方。\nConnecting to duckdb is particularly simple because the defaults create a temporary database that is deleted when you quit R.\n连接到 duckdb 特别简单，因为默认设置会创建一个临时数据库，在你退出 R 时会被删除。\nThat’s great for learning because it guarantees that you’ll start from a clean slate every time you restart R:\n这对于学习来说非常棒，因为它保证了你每次重启 R 时都能从一个干净的状态开始：\n\ncon &lt;- DBI::dbConnect(duckdb::duckdb())\n\nduckdb is a high-performance database that’s designed very much for the needs of a data scientist.\nduckdb 是一个高性能数据库，其设计非常贴合数据科学家的需求。\nWe use it here because it’s very easy to get started with, but it’s also capable of handling gigabytes of data with great speed.\n我们在这里使用它，因为它非常容易上手，而且它还能以极快的速度处理数 GB 的数据。\nIf you want to use duckdb for a real data analysis project, you’ll also need to supply the dbdir argument to make a persistent database and tell duckdb where to save it.\n如果你想在一个真实的数据分析项目中使用 duckdb，你还需要提供 dbdir 参数来创建一个持久化数据库，并告诉 duckdb 将其保存在哪里。\nAssuming you’re using a project (Chapter 6), it’s reasonable to store it in the duckdb directory of the current project:\n假设你正在使用一个项目 (Chapter 6)，将其存储在当前项目的 duckdb 目录中是合理的做法：\n\ncon &lt;- DBI::dbConnect(duckdb::duckdb(), dbdir = \"duckdb\")\n\n\n21.3.2 Load some data\nSince this is a new database, we need to start by adding some data.\n由于这是一个新数据库，我们需要先添加一些数据。\nHere we’ll add mpg and diamonds datasets from ggplot2 using DBI::dbWriteTable().\n这里我们将使用 DBI::dbWriteTable() 添加来自 ggplot2 的 mpg 和 diamonds 数据集。\nThe simplest usage of dbWriteTable() needs three arguments: a database connection, the name of the table to create in the database, and a data frame of data.dbWriteTable() 的最简单用法需要三个参数：一个数据库连接、要在数据库中创建的表的名称，以及一个数据框。\n\ndbWriteTable(con, \"mpg\", ggplot2::mpg)\ndbWriteTable(con, \"diamonds\", ggplot2::diamonds)\n\nIf you’re using duckdb in a real project, we highly recommend learning about duckdb_read_csv() and duckdb_register_arrow().\n如果你在实际项目中使用 duckdb，我们强烈建议你学习 duckdb_read_csv() 和 duckdb_register_arrow()。\nThese give you powerful and performant ways to quickly load data directly into duckdb, without having to first load it into R.\n它们为你提供了强大而高效的方法，可以快速将数据直接加载到 duckdb 中，而无需先将其加载到 R 中。\nWe’ll also show off a useful technique for loading multiple files into a database in Section 26.4.1.\n我们还将在 Section 26.4.1 中展示一个将多个文件加载到数据库中的实用技巧。\n\n21.3.3 DBI basics\nYou can check that the data is loaded correctly by using a couple of other DBI functions: dbListTables() lists all tables in the database3 and dbReadTable() retrieves the contents of a table.\n你可以通过使用其他几个 DBI 函数来检查数据是否已正确加载：dbListTables() 列出数据库中的所有表3，dbReadTable() 检索表的内容。\n\ndbListTables(con)\n#&gt; [1] \"diamonds\" \"mpg\"\n\ncon |&gt; \n  dbReadTable(\"diamonds\") |&gt; \n  as_tibble()\n#&gt; # A tibble: 53,940 × 10\n#&gt;   carat cut       color clarity depth table price     x     y     z\n#&gt;   &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n#&gt; 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n#&gt; 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n#&gt; 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n#&gt; 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n#&gt; 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n#&gt; # ℹ 53,934 more rows\n\ndbReadTable() returns a data.frame so we use as_tibble() to convert it into a tibble so that it prints nicely.dbReadTable() 返回一个 data.frame，所以我们使用 as_tibble() 将其转换为 tibble，以便更好地打印输出。\nIf you already know SQL, you can use dbGetQuery() to get the results of running a query on the database:\n如果你已经了解 SQL，你可以使用 dbGetQuery() 来获取在数据库上运行查询的结果：\n\nsql &lt;- \"\n  SELECT carat, cut, clarity, color, price \n  FROM diamonds \n  WHERE price &gt; 15000\n\"\nas_tibble(dbGetQuery(con, sql))\n#&gt; # A tibble: 1,655 × 5\n#&gt;   carat cut       clarity color price\n#&gt;   &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;   &lt;fct&gt; &lt;int&gt;\n#&gt; 1  1.54 Premium   VS2     E     15002\n#&gt; 2  1.19 Ideal     VVS1    F     15005\n#&gt; 3  2.1  Premium   SI1     I     15007\n#&gt; 4  1.69 Ideal     SI1     D     15011\n#&gt; 5  1.5  Very Good VVS2    G     15013\n#&gt; 6  1.73 Very Good VS1     G     15014\n#&gt; # ℹ 1,649 more rows\n\nIf you’ve never seen SQL before, don’t worry!\n如果你以前没见过 SQL，别担心！\nYou’ll learn more about it shortly.\n你很快就会学到更多关于它的知识。\nBut if you read it carefully, you might guess that it selects five columns of the diamonds dataset and all the rows where price is greater than 15,000.\n但是如果你仔细阅读它，你可能会猜到它从 diamonds 数据集中选择了五列，以及所有 price 大于 15,000 的行。",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "databases.html#dbplyr-basics",
    "href": "databases.html#dbplyr-basics",
    "title": "21  Databases",
    "section": "\n21.4 dbplyr basics",
    "text": "21.4 dbplyr basics\nNow that we’ve connected to a database and loaded up some data, we can start to learn about dbplyr.\n现在我们已经连接到数据库并加载了一些数据，我们可以开始学习 dbplyr 了。\ndbplyr is a dplyr backend, which means that you keep writing dplyr code but the backend executes it differently.\ndbplyr 是一个 dplyr 后端 (backend)，这意味着你继续编写 dplyr 代码，但后端会以不同的方式执行它。\nIn this, dbplyr translates to SQL; other backends include dtplyr which translates to data.table, and multidplyr which executes your code on multiple cores.\n在这里，dbplyr 将代码翻译成 SQL；其他后端包括将代码翻译成 data.table 的 dtplyr，以及在多个核心上执行代码的 multidplyr。\nTo use dbplyr, you must first use tbl() to create an object that represents a database table:\n要使用 dbplyr，你必须首先使用 tbl() 创建一个代表数据库表的对象：\n\ndiamonds_db &lt;- tbl(con, \"diamonds\")\ndiamonds_db\n#&gt; # Source:   table&lt;diamonds&gt; [?? x 10]\n#&gt; # Database: DuckDB v1.3.1 [14913@Windows 10 x64:R 4.5.1/:memory:]\n#&gt;   carat cut       color clarity depth table price     x     y     z\n#&gt;   &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n#&gt; 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n#&gt; 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n#&gt; 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n#&gt; 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n#&gt; 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n#&gt; # ℹ more rows\n\n\n\n\n\n\n\nThere are two other common ways to interact with a database.\n与数据库交互还有另外两种常见方式。\nFirst, many corporate databases are very large so you need some hierarchy to keep all the tables organized.\n首先，许多企业数据库非常庞大，因此你需要某种层级结构来保持所有表的有序性。\nIn that case you might need to supply a schema, or a catalog and a schema, in order to pick the table you’re interested in:\n在这种情况下，你可能需要提供一个模式 (schema)，或者一个目录 (catalog) 和一个模式，以便选择你感兴趣的表：\n\ndiamonds_db &lt;- tbl(con, in_schema(\"sales\", \"diamonds\"))\ndiamonds_db &lt;- tbl(con, in_catalog(\"north_america\", \"sales\", \"diamonds\"))\n\nOther times you might want to use your own SQL query as a starting point:\n其他时候，你可能想用自己的 SQL 查询作为起点：\n\ndiamonds_db &lt;- tbl(con, sql(\"SELECT * FROM diamonds\"))\n\n\n\n\nThis object is lazy; when you use dplyr verbs on it, dplyr doesn’t do any work: it just records the sequence of operations that you want to perform and only performs them when needed.\n这个对象是惰性 (lazy) 的；当你对它使用 dplyr 函数时，dplyr 并不执行任何工作：它只是记录下你想要执行的操作序列，并且只在需要时才执行它们。\nFor example, take the following pipeline:\n例如，看下面这个管道：\n\nbig_diamonds_db &lt;- diamonds_db |&gt; \n  filter(price &gt; 15000) |&gt; \n  select(carat:clarity, price)\n\nbig_diamonds_db\n#&gt; # Source:   SQL [?? x 5]\n#&gt; # Database: DuckDB v1.3.1 [14913@Windows 10 x64:R 4.5.1/:memory:]\n#&gt;   carat cut       color clarity price\n#&gt;   &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n#&gt; 1  1.54 Premium   E     VS2     15002\n#&gt; 2  1.19 Ideal     F     VVS1    15005\n#&gt; 3  2.1  Premium   I     SI1     15007\n#&gt; 4  1.69 Ideal     D     SI1     15011\n#&gt; 5  1.5  Very Good G     VVS2    15013\n#&gt; 6  1.73 Very Good G     VS1     15014\n#&gt; # ℹ more rows\n\nYou can tell this object represents a database query because it prints the DBMS name at the top, and while it tells you the number of columns, it typically doesn’t know the number of rows.\n你可以看出这个对象代表一个数据库查询，因为它在顶部打印了 DBMS 的名称，而且虽然它告诉了你列数，但通常不知道行数。\nThis is because finding the total number of rows usually requires executing the complete query, something we’re trying to avoid.\n这是因为查找总行数通常需要执行完整的查询，而这正是我们试图避免的。\nYou can see the SQL code generated by the dplyr function show_query().\n你可以看到由 dplyr 函数 show_query() 生成的 SQL 代码。\nIf you know dplyr, this is a great way to learn SQL!\n如果你了解 dplyr，这是学习 SQL 的一个好方法！\nWrite some dplyr code, get dbplyr to translate it to SQL, and then try to figure out how the two languages match up.\n编写一些 dplyr 代码，让 dbplyr 将其翻译成 SQL，然后试着弄清楚这两种语言是如何对应的。\n\nbig_diamonds_db |&gt;\n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT carat, cut, color, clarity, price\n#&gt; FROM diamonds\n#&gt; WHERE (price &gt; 15000.0)\n\nTo get all the data back into R, you call collect().\n要将所有数据取回 R 中，你可以调用 collect()。\nBehind the scenes, this generates the SQL, calls dbGetQuery() to get the data, then turns the result into a tibble:\n在幕后，这会生成 SQL，调用 dbGetQuery() 获取数据，然后将结果转换为一个 tibble：\n\nbig_diamonds &lt;- big_diamonds_db |&gt; \n  collect()\nbig_diamonds\n#&gt; # A tibble: 1,655 × 5\n#&gt;   carat cut       color clarity price\n#&gt;   &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n#&gt; 1  1.54 Premium   E     VS2     15002\n#&gt; 2  1.19 Ideal     F     VVS1    15005\n#&gt; 3  2.1  Premium   I     SI1     15007\n#&gt; 4  1.69 Ideal     D     SI1     15011\n#&gt; 5  1.5  Very Good G     VVS2    15013\n#&gt; 6  1.73 Very Good G     VS1     15014\n#&gt; # ℹ 1,649 more rows\n\nTypically, you’ll use dbplyr to select the data you want from the database, performing basic filtering and aggregation using the translations described below.\n通常，你会使用 dbplyr 从数据库中选择你想要的数据，使用下面描述的转换方法执行基本的筛选和聚合。\nThen, once you’re ready to analyse the data with functions that are unique to R, you’ll collect() the data to get an in-memory tibble, and continue your work with pure R code.\n然后，当你准备好使用 R 特有的函数分析数据时，你会 collect() 数据以获得一个内存中的 tibble，并继续用纯 R 代码进行你的工作。",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "databases.html#sql",
    "href": "databases.html#sql",
    "title": "21  Databases",
    "section": "\n21.5 SQL",
    "text": "21.5 SQL\nThe rest of the chapter will teach you a little SQL through the lens of dbplyr.\n本章的其余部分将通过 dbplyr 的视角教你一些 SQL。\nIt’s a rather non-traditional introduction to SQL but we hope it will get you quickly up to speed with the basics.\n这是一个相当非传统的 SQL 入门，但我们希望它能让你快速掌握基础知识。\nLuckily, if you understand dplyr you’re in a great place to quickly pick up SQL because so many of the concepts are the same.\n幸运的是，如果你理解 dplyr，那么你很快就能学会 SQL，因为很多概念是相同的。\nWe’ll explore the relationship between dplyr and SQL using a couple of old friends from the nycflights13 package: flights and planes.\n我们将使用 nycflights13 包中的两个老朋友：flights 和 planes 来探索 dplyr 和 SQL 之间的关系。\nThese datasets are easy to get into our learning database because dbplyr comes with a function that copies the tables from nycflights13 to our database:\n这些数据集很容易导入到我们的学习数据库中，因为 dbplyr 提供了一个函数，可以将 nycflights13 中的表复制到我们的数据库中：\n\ndbplyr::copy_nycflights13(con)\n#&gt; Creating table: airlines\n#&gt; Creating table: airports\n#&gt; Creating table: flights\n#&gt; Creating table: planes\n#&gt; Creating table: weather\nflights &lt;- tbl(con, \"flights\")\nplanes &lt;- tbl(con, \"planes\")\n\n\n21.5.1 SQL basics\nThe top-level components of SQL are called statements.\nSQL 的顶层组件被称为语句 (statements)。\nCommon statements include CREATE for defining new tables, INSERT for adding data, and SELECT for retrieving data.\n常见的语句包括用于定义新表的 CREATE、用于添加数据的 INSERT 以及用于检索数据的 SELECT。\nWe will focus on SELECT statements, also called queries, because they are almost exclusively what you’ll use as a data scientist.\n我们将专注于 SELECT 语句，也称为查询 (queries)，因为它们几乎是你作为数据科学家唯一会用到的。\nA query is made up of clauses.\n一个查询由子句 (clauses) 组成。\nThere are five important clauses: SELECT, FROM, WHERE, ORDER BY, and GROUP BY. Every query must have the SELECT4 and FROM5 clauses and the simplest query is SELECT * FROM table, which selects all columns from the specified table . This is what dbplyr generates for an unadulterated table :\n有五个重要的子句：SELECT、FROM、WHERE、ORDER BY 和 GROUP BY。每个查询都必须有 SELECT4 和 FROM5 子句，最简单的查询是 SELECT * FROM table，它从指定的表中选择所有列。这是 dbplyr 为一个未经处理的表生成的代码：\n\nflights |&gt; show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT *\n#&gt; FROM flights\nplanes |&gt; show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT *\n#&gt; FROM planes\n\nWHERE and ORDER BY control which rows are included and how they are ordered:WHERE 和 ORDER BY 控制包含哪些行以及它们的排序方式：\n\nflights |&gt; \n  filter(dest == \"IAH\") |&gt; \n  arrange(dep_delay) |&gt;\n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT flights.*\n#&gt; FROM flights\n#&gt; WHERE (dest = 'IAH')\n#&gt; ORDER BY dep_delay\n\nGROUP BY converts the query to a summary, causing aggregation to happen:GROUP BY 将查询转换为摘要，从而进行聚合操作：\n\nflights |&gt; \n  group_by(dest) |&gt; \n  summarize(dep_delay = mean(dep_delay, na.rm = TRUE)) |&gt; \n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT dest, AVG(dep_delay) AS dep_delay\n#&gt; FROM flights\n#&gt; GROUP BY dest\n\nThere are two important differences between dplyr verbs and SELECT clauses:\ndplyr 函数和 SELECT 子句之间有两个重要区别：\n\nIn SQL, case doesn’t matter: you can write select, SELECT, or even SeLeCt. In this book we’ll stick with the common convention of writing SQL keywords in uppercase to distinguish them from table or variables names.\n在 SQL 中，大小写不重要：你可以写 select、SELECT，甚至 SeLeCt。在本书中，我们将遵循通常的惯例，用大写字母书写 SQL 关键字，以区别于表或变量名。\nIn SQL, order matters: you must always write the clauses in the order SELECT, FROM, WHERE, GROUP BY, ORDER BY. Confusingly, this order doesn’t match how the clauses are actually evaluated which is first FROM, then WHERE, GROUP BY, SELECT, and ORDER BY.\n在 SQL 中，顺序很重要：你必须始终按 SELECT、FROM、WHERE、GROUP BY、ORDER BY 的顺序编写子句。令人困惑的是，这个顺序与子句的实际求值顺序不匹配，实际顺序是先 FROM，然后是 WHERE、GROUP BY、SELECT 和 ORDER BY。\n\nThe following sections explore each clause in more detail.\n以下各节将更详细地探讨每个子句。\n\n\n\n\n\n\nNote that while SQL is a standard, it is extremely complex and no database follows it exactly.\n请注意，虽然 SQL 是一个标准，但它极其复杂，没有哪个数据库能完全遵循它。\nWhile the main components that we’ll focus on in this book are very similar between DBMS’s, there are many minor variations.\n虽然本书中我们关注的主要组成部分在不同 DBMS 之间非常相似，但存在许多细微的差异。\nFortunately, dbplyr is designed to handle this problem and generates different translations for different databases.\n幸运的是，dbplyr 旨在处理这个问题，并为不同的数据库生成不同的翻译。\nIt’s not perfect, but it’s continually improving, and if you hit a problem you can file an issue on GitHub to help us do better.\n它并不完美，但它在不断改进，如果你遇到问题，可以在 GitHub 上提交一个 issue，帮助我们做得更好。\n\n\n\n\n21.5.2 SELECT\nThe SELECT clause is the workhorse of queries and performs the same job as select(), mutate(), rename(), relocate(), and, as you’ll learn in the next section, summarize().SELECT 子句是查询的主力，它执行与 select()、mutate()、rename()、relocate() 相同的工作，并且，正如你将在下一节中学到的，还包括 summarize()。\nselect(), rename(), and relocate() have very direct translations to SELECT as they just affect where a column appears (if at all) along with its name:select()、rename() 和 relocate() 与 SELECT 有非常直接的转换关系，因为它们只影响列出现的位置（如果出现的话）及其名称：\n\nplanes |&gt; \n  select(tailnum, type, manufacturer, model, year) |&gt; \n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT tailnum, \"type\", manufacturer, model, \"year\"\n#&gt; FROM planes\n\nplanes |&gt; \n  select(tailnum, type, manufacturer, model, year) |&gt; \n  rename(year_built = year) |&gt; \n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT tailnum, \"type\", manufacturer, model, \"year\" AS year_built\n#&gt; FROM planes\n\nplanes |&gt; \n  select(tailnum, type, manufacturer, model, year) |&gt; \n  relocate(manufacturer, model, .before = type) |&gt; \n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT tailnum, manufacturer, model, \"type\", \"year\"\n#&gt; FROM planes\n\nThis example also shows you how SQL does renaming.\n这个例子也向你展示了 SQL 如何进行重命名。\nIn SQL terminology renaming is called aliasing and is done with AS.\n在 SQL 术语中，重命名被称为别名 (aliasing)，并使用 AS 来完成。\nNote that unlike mutate(), the old name is on the left and the new name is on the right.\n请注意，与 mutate() 不同，旧名称在左边，新名称在右边。\n\n\n\n\n\n\nIn the examples above note that \"year\" and \"type\" are wrapped in double quotes.\n在上面的例子中，请注意 \"year\" 和 \"type\" 被双引号包裹着。\nThat’s because these are reserved words in duckdb, so dbplyr quotes them to avoid any potential confusion between column/table names and SQL operators.\n这是因为它们在 duckdb 中是保留字 (reserved words)，所以 dbplyr 给它们加上引号，以避免列/表名与 SQL 操作符之间可能产生的混淆。\nWhen working with other databases you’re likely to see every variable name quoted because only a handful of client packages, like duckdb, know what all the reserved words are, so they quote everything to be safe.\n在使用其他数据库时，你很可能会看到每个变量名都被引起来，因为只有少数客户端包（如 duckdb）知道所有的保留字是什么，所以它们为了安全起见会引用所有内容。\nSELECT \"tailnum\", \"type\", \"manufacturer\", \"model\", \"year\"\nFROM \"planes\"\nSome other database systems use backticks instead of quotes:\n其他一些数据库系统使用反引号而不是引号：\nSELECT `tailnum`, `type`, `manufacturer`, `model`, `year`\nFROM `planes`\n\n\n\nThe translations for mutate() are similarly straightforward: each variable becomes a new expression in SELECT:mutate() 的转换同样直接：每个变量都成为 SELECT 中的一个新表达式：\n\nflights |&gt; \n  mutate(\n    speed = distance / (air_time / 60)\n  ) |&gt; \n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT flights.*, distance / (air_time / 60.0) AS speed\n#&gt; FROM flights\n\nWe’ll come back to the translation of individual components (like /) in Section 21.6.\n我们将在 Section 21.6 中回过头来讨论单个组件（如 /）的翻译。\n\n21.5.3 FROM\nThe FROM clause defines the data source. It’s going to be rather uninteresting for a little while, because we’re just using single tables. You’ll see more complex examples once we hit the join functions.FROM 子句定义了数据源。在一段时间内，它会显得相当无趣，因为我们只使用单个表。一旦我们接触到连接函数，你将会看到更复杂的例子。\n\n21.5.4 GROUP BY\ngroup_by() is translated to the GROUP BY6 clause and summarize() is translated to the SELECT clause:group_by() 被翻译为 GROUP BY6 子句，而 summarize() 被翻译为 SELECT 子句：\n\ndiamonds_db |&gt; \n  group_by(cut) |&gt; \n  summarize(\n    n = n(),\n    avg_price = mean(price, na.rm = TRUE)\n  ) |&gt; \n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT cut, COUNT(*) AS n, AVG(price) AS avg_price\n#&gt; FROM diamonds\n#&gt; GROUP BY cut\n\nWe’ll come back to what’s happening with the translation of n() and mean() in Section 21.6.\n我们将在 Section 21.6 回过头来讨论 n() 和 mean() 的翻译发生了什么。\n\n21.5.5 WHERE\nfilter() is translated to the WHERE clause:filter() 被翻译为 WHERE 子句：\n\nflights |&gt; \n  filter(dest == \"IAH\" | dest == \"HOU\") |&gt; \n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT flights.*\n#&gt; FROM flights\n#&gt; WHERE (dest = 'IAH' OR dest = 'HOU')\n\nflights |&gt; \n  filter(arr_delay &gt; 0 & arr_delay &lt; 20) |&gt; \n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT flights.*\n#&gt; FROM flights\n#&gt; WHERE (arr_delay &gt; 0.0 AND arr_delay &lt; 20.0)\n\nThere are a few important details to note here:\n这里有几个重要的细节需要注意：\n\n| becomes OR and & becomes AND.| 变成 OR，& 变成 AND。\nSQL uses = for comparison, not ==. SQL doesn’t have assignment, so there’s no potential for confusion there.\nSQL 使用 = 进行比较，而不是 ==。SQL 没有赋值操作，所以不存在混淆的可能。\nSQL uses only '' for strings, not \"\". In SQL, \"\" is used to identify variables, like R’s ``.\nSQL 只用 '' 表示字符串，而不用 \"\"。在 SQL 中，\"\" 用于标识变量，就像 R 中的 `` 一样。\n\nAnother useful SQL operator is IN, which is very close to R’s %in%:\n另一个有用的 SQL 运算符是 IN，它与 R 的 %in% 非常接近：\n\nflights |&gt; \n  filter(dest %in% c(\"IAH\", \"HOU\")) |&gt; \n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT flights.*\n#&gt; FROM flights\n#&gt; WHERE (dest IN ('IAH', 'HOU'))\n\nSQL uses NULL instead of NA. NULLs behave similarly to NAs. The main difference is that while they’re “infectious” in comparisons and arithmetic, they are silently dropped when summarizing. dbplyr will remind you about this behavior the first time you hit it:\nSQL 使用 NULL 而不是 NA。NULL 的行为与 NA 类似。主要区别在于，虽然它们在比较和算术运算中具有“传染性”，但在汇总时会被静默地丢弃。dbplyr 在你第一次遇到这种情况时会提醒你这个行为：\n\nflights |&gt; \n  group_by(dest) |&gt; \n  summarize(delay = mean(arr_delay))\n#&gt; Warning: Missing values are always removed in SQL aggregation functions.\n#&gt; Use `na.rm = TRUE` to silence this warning\n#&gt; This warning is displayed once every 8 hours.\n#&gt; # Source:   SQL [?? x 2]\n#&gt; # Database: DuckDB v1.3.1 [14913@Windows 10 x64:R 4.5.1/:memory:]\n#&gt;   dest  delay\n#&gt;   &lt;chr&gt; &lt;dbl&gt;\n#&gt; 1 MSY    6.49\n#&gt; 2 XNA    7.47\n#&gt; 3 ORF   10.9 \n#&gt; 4 ROC   11.6 \n#&gt; 5 ABQ    4.38\n#&gt; 6 LGA   NA   \n#&gt; # ℹ more rows\n\nIf you want to learn more about how NULLs work, you might enjoy “The Three-Valued Logic of SQL” by Markus Winand.\n如果你想更多地了解 NULL 的工作原理，你可能会喜欢 Markus Winand 的《SQL 的三值逻辑》。\nIn general, you can work with NULLs using the functions you’d use for NAs in R:\n通常，你可以使用在 R 中处理 NA 的函数来处理 NULL：\n\nflights |&gt; \n  filter(!is.na(dep_delay)) |&gt; \n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT flights.*\n#&gt; FROM flights\n#&gt; WHERE (NOT((dep_delay IS NULL)))\n\nThis SQL query illustrates one of the drawbacks of dbplyr: while the SQL is correct, it isn’t as simple as you might write by hand. In this case, you could drop the parentheses and use a special operator that’s easier to read:\n这个 SQL 查询揭示了 dbplyr 的一个缺点：虽然 SQL 是正确的，但它不像你手写的那样简洁。在这种情况下，你可以去掉括号，使用一个更易读的特殊运算符：\nWHERE \"dep_delay\" IS NOT NULL\nNote that if you filter() a variable that you created using a summarize, dbplyr will generate a HAVING clause, rather than a WHERE clause. This is a one of the idiosyncrasies of SQL: WHERE is evaluated before SELECT and GROUP BY, so SQL needs another clause that’s evaluated afterwards.\n请注意，如果你对使用 summarize 创建的变量进行 filter()，dbplyr 将生成 HAVING 子句，而不是 WHERE 子句。这是 SQL 的一个特性：WHERE 在 SELECT 和 GROUP BY 之前被评估，所以 SQL 需要另一个在之后评估的子句。\n\ndiamonds_db |&gt; \n  group_by(cut) |&gt; \n  summarize(n = n()) |&gt; \n  filter(n &gt; 100) |&gt; \n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT cut, COUNT(*) AS n\n#&gt; FROM diamonds\n#&gt; GROUP BY cut\n#&gt; HAVING (COUNT(*) &gt; 100.0)\n\n\n21.5.6 ORDER BY\nOrdering rows involves a straightforward translation from arrange() to the ORDER BY clause:\n对行进行排序，涉及从 arrange() 到 ORDER BY 子句的直接翻译：\n\nflights |&gt; \n  arrange(year, month, day, desc(dep_delay)) |&gt; \n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT flights.*\n#&gt; FROM flights\n#&gt; ORDER BY \"year\", \"month\", \"day\", dep_delay DESC\n\nNotice how desc() is translated to DESC: this is one of the many dplyr functions whose name was directly inspired by SQL.\n注意 desc() 是如何被翻译成 DESC 的：这是众多直接受 SQL 启发的 dplyr 函数之一。\n\n21.5.7 Subqueries\nSometimes it’s not possible to translate a dplyr pipeline into a single SELECT statement and you need to use a subquery. A subquery is just a query used as a data source in the FROM clause, instead of the usual table.\n有时，无法将一个 dplyr 管道翻译成单个 SELECT 语句，这时你需要使用子查询。子查询 (subquery) 就是一个在 FROM 子句中用作数据源的查询，而不是通常的表。\ndbplyr typically uses subqueries to work around limitations of SQL. For example, expressions in the SELECT clause can’t refer to columns that were just created. That means that the following (silly) dplyr pipeline needs to happen in two steps: the first (inner) query computes year1 and then the second (outer) query can compute year2.\ndbplyr 通常使用子查询来规避 SQL 的限制。例如，SELECT 子句中的表达式不能引用刚刚创建的列。这意味着下面这个（有点傻的）dplyr 管道需要分两步进行：第一步（内部）查询计算出 year1，然后第二步（外部）查询才能计算出 year2。\n\nflights |&gt; \n  mutate(\n    year1 = year + 1,\n    year2 = year1 + 1\n  ) |&gt; \n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT q01.*, year1 + 1.0 AS year2\n#&gt; FROM (\n#&gt;   SELECT flights.*, \"year\" + 1.0 AS year1\n#&gt;   FROM flights\n#&gt; ) q01\n\nYou’ll also see this if you attempted to filter() a variable that you just created. Remember, even though WHERE is written after SELECT, it’s evaluated before it, so we need a subquery in this (silly) example:\n如果你试图对一个刚刚创建的变量进行 filter()，你也会看到这种情况。记住，尽管 WHERE 写在 SELECT 之后，但它是在 SELECT 之前被评估的，所以在这个（有点傻的）例子中我们需要一个子查询：\n\nflights |&gt; \n  mutate(year1 = year + 1) |&gt; \n  filter(year1 == 2014) |&gt; \n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT q01.*\n#&gt; FROM (\n#&gt;   SELECT flights.*, \"year\" + 1.0 AS year1\n#&gt;   FROM flights\n#&gt; ) q01\n#&gt; WHERE (year1 = 2014.0)\n\nSometimes dbplyr will create a subquery where it’s not needed because it doesn’t yet know how to optimize that translation. As dbplyr improves over time, these cases will get rarer but will probably never go away.\n有时 dbplyr 会在不需要的情况下创建一个子查询，因为它还不知道如何优化该翻译。随着 dbplyr 的不断改进，这些情况会越来越少，但可能永远不会完全消失。\n\n21.5.8 Joins\nIf you’re familiar with dplyr’s joins, SQL joins are very similar. Here’s a simple example:\n如果你熟悉 dplyr 的连接 (joins)，SQL 的连接非常相似。下面是一个简单的例子：\n\nflights |&gt; \n  left_join(planes |&gt; rename(year_built = year), join_by(tailnum)) |&gt; \n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT\n#&gt;   flights.*,\n#&gt;   planes.\"year\" AS year_built,\n#&gt;   \"type\",\n#&gt;   manufacturer,\n#&gt;   model,\n#&gt;   engines,\n#&gt;   seats,\n#&gt;   speed,\n#&gt;   engine\n#&gt; FROM flights\n#&gt; LEFT JOIN planes\n#&gt;   ON (flights.tailnum = planes.tailnum)\n\nThe main thing to notice here is the syntax: SQL joins use sub-clauses of the FROM clause to bring in additional tables, using ON to define how the tables are related.\n这里主要要注意的是语法：SQL 连接使用 FROM 子句的子句来引入额外的表，并使用 ON 来定义表之间的关系。\ndplyr’s names for these functions are so closely connected to SQL that you can easily guess the equivalent SQL for inner_join(), right_join(), and full_join():\ndplyr 中这些函数的名称与 SQL 紧密相关，因此你可以轻松猜出 inner_join()、right_join() 和 full_join() 的等效 SQL：\nSELECT flights.*, \"type\", manufacturer, model, engines, seats, speed\nFROM flights\nINNER JOIN planes ON (flights.tailnum = planes.tailnum)\n\nSELECT flights.*, \"type\", manufacturer, model, engines, seats, speed\nFROM flights\nRIGHT JOIN planes ON (flights.tailnum = planes.tailnum)\n\nSELECT flights.*, \"type\", manufacturer, model, engines, seats, speed\nFROM flights\nFULL JOIN planes ON (flights.tailnum = planes.tailnum)\nYou’re likely to need many joins when working with data from a database. That’s because database tables are often stored in a highly normalized form, where each “fact” is stored in a single place and to keep a complete dataset for analysis you need to navigate a complex network of tables connected by primary and foreign keys. If you hit this scenario, the dm package, by Tobias Schieferdecker, Kirill Müller, and Darko Bergant, is a life saver. It can automatically determine the connections between tables using the constraints that DBAs often supply, visualize the connections so you can see what’s going on, and generate the joins you need to connect one table to another.\n当处理数据库中的数据时，你很可能需要进行多次连接。这是因为数据库表通常以高度规范化的形式存储，每个“事实”都存储在单一位置，为了进行分析而得到一个完整的数据集，你需要在一个由主键和外键连接的复杂表网络中穿梭。如果你遇到这种情况，由 Tobias Schieferdecker、Kirill Müller 和 Darko Bergant 开发的 dm 包 将是你的救星。它可以利用数据库管理员（DBA）通常提供的约束自动确定表之间的连接，将连接可视化以便你了解情况，并生成连接一个表到另一个表所需的连接操作。\n\n21.5.9 Other verbs\ndbplyr also translates other verbs like distinct(), slice_*(), and intersect(), and a growing selection of tidyr functions like pivot_longer() and pivot_wider(). The easiest way to see the full set of what’s currently available is to visit the dbplyr website: https://dbplyr.tidyverse.org/reference/.\ndbplyr 还可以翻译其他动词，如 distinct()、slice_*() 和 intersect()，以及越来越多的 tidyr 函数，如 pivot_longer() 和 pivot_wider()。要查看当前所有可用功能的完整列表，最简单的方法是访问 dbplyr 网站：https://dbplyr.tidyverse.org/reference/。\n\n21.5.10 Exercises\n\nWhat is distinct() translated to? How about head()?\n\nExplain what each of the following SQL queries do and try recreate them using dbplyr.\nSELECT \\* \nFROM flights\nWHERE dep_delay \\&lt; arr_delay\n\nSELECT \\*, distance / (air_time / 60) AS speed\nFROM flights",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "databases.html#sec-sql-expressions",
    "href": "databases.html#sec-sql-expressions",
    "title": "21  Databases",
    "section": "\n21.6 Function translations",
    "text": "21.6 Function translations\nSo far we’ve focused on the big picture of how dplyr verbs are translated to the clauses of a query. Now we’re going to zoom in a little and talk about the translation of the R functions that work with individual columns, e.g., what happens when you use mean(x) in a summarize()?\n到目前为止，我们主要关注了 dplyr 动词如何被翻译成查询子句的宏观层面。现在，我们将稍微深入一些，讨论处理单个列的 R 函数的翻译，例如，当你在 summarize() 中使用 mean(x) 时会发生什么？\nTo help see what’s going on, we’ll use a couple of little helper functions that run a summarize() or mutate() and show the generated SQL. That will make it a little easier to explore a few variations and see how summaries and transformations can differ.\n为了帮助理解发生了什么，我们将使用几个小辅助函数，它们会运行一个 summarize() 或 mutate() 并显示生成的 SQL。这将使我们更容易探索一些变化，并观察汇总和转换有何不同。\n\nsummarize_query &lt;- function(df, ...) {\n  df |&gt; \n    summarize(...) |&gt; \n    show_query()\n}\nmutate_query &lt;- function(df, ...) {\n  df |&gt; \n    mutate(..., .keep = \"none\") |&gt; \n    show_query()\n}\n\nLet’s dive in with some summaries! Looking at the code below you’ll notice that some summary functions, like mean(), have a relatively simple translation while others, like median(), are much more complex. The complexity is typically higher for operations that are common in statistics but less common in databases.\n让我们从一些汇总操作开始吧！看下面的代码，你会注意到一些汇总函数，比如 mean()，其翻译相对简单，而另一些，比如 median()，则要复杂得多。对于在统计学中常见但在数据库中不太常见的操作，其复杂性通常更高。\n\nflights |&gt; \n  group_by(year, month, day) |&gt;  \n  summarize_query(\n    mean = mean(arr_delay, na.rm = TRUE),\n    median = median(arr_delay, na.rm = TRUE)\n  )\n#&gt; `summarise()` has grouped output by \"year\" and \"month\". You can override\n#&gt; using the `.groups` argument.\n#&gt; &lt;SQL&gt;\n#&gt; SELECT\n#&gt;   \"year\",\n#&gt;   \"month\",\n#&gt;   \"day\",\n#&gt;   AVG(arr_delay) AS mean,\n#&gt;   MEDIAN(arr_delay) AS median\n#&gt; FROM flights\n#&gt; GROUP BY \"year\", \"month\", \"day\"\n\nThe translation of summary functions becomes more complicated when you use them inside a mutate() because they have to turn into so-called window functions. In SQL, you turn an ordinary aggregation function into a window function by adding OVER after it:\n当你在 mutate() 中使用汇总函数时，它们的翻译会变得更加复杂，因为它们必须转换成所谓的窗口 (window) 函数。在 SQL 中，你通过在普通聚合函数后添加 OVER 来将其转换为窗口函数：\n\nflights |&gt; \n  group_by(year, month, day) |&gt;  \n  mutate_query(\n    mean = mean(arr_delay, na.rm = TRUE),\n  )\n#&gt; &lt;SQL&gt;\n#&gt; SELECT\n#&gt;   \"year\",\n#&gt;   \"month\",\n#&gt;   \"day\",\n#&gt;   AVG(arr_delay) OVER (PARTITION BY \"year\", \"month\", \"day\") AS mean\n#&gt; FROM flights\n\nIn SQL, the GROUP BY clause is used exclusively for summaries so here you can see that the grouping has moved from the GROUP BY clause to OVER.\n在 SQL 中，GROUP BY 子句专用于汇总，所以在这里你可以看到分组已经从 GROUP BY 子句移到了 OVER 中。\nWindow functions include all functions that look forward or backwards, like lead() and lag() which look at the “previous” or “next” value respectively:\n窗口函数包括所有向前或向后看的函数，例如 lead() 和 lag()，它们分别查看“前一个”或“后一个”值：\n\nflights |&gt; \n  group_by(dest) |&gt;  \n  arrange(time_hour) |&gt; \n  mutate_query(\n    lead = lead(arr_delay),\n    lag = lag(arr_delay)\n  )\n#&gt; &lt;SQL&gt;\n#&gt; SELECT\n#&gt;   dest,\n#&gt;   LEAD(arr_delay, 1, NULL) OVER (PARTITION BY dest ORDER BY time_hour) AS lead,\n#&gt;   LAG(arr_delay, 1, NULL) OVER (PARTITION BY dest ORDER BY time_hour) AS lag\n#&gt; FROM flights\n#&gt; ORDER BY time_hour\n\nHere it’s important to arrange() the data, because SQL tables have no intrinsic order. In fact, if you don’t use arrange() you might get the rows back in a different order every time! Notice for window functions, the ordering information is repeated: the ORDER BY clause of the main query doesn’t automatically apply to window functions.\n在这里，对数据进行 arrange() 很重要，因为 SQL 表没有固有的顺序。事实上，如果你不使用 arrange()，每次返回的行顺序可能都不同！注意，对于窗口函数，排序信息是重复的：主查询的 ORDER BY 子句不会自动应用于窗口函数。\nAnother important SQL function is CASE WHEN. It’s used as the translation of if_else() and case_when(), the dplyr function that it directly inspired. Here are a couple of simple examples:\n另一个重要的 SQL 函数是 CASE WHEN。它被用作 if_else() 和 case_when() 的翻译，而后者正是直接受其启发的 dplyr 函数。这里有几个简单的例子：\n\nflights |&gt; \n  mutate_query(\n    description = if_else(arr_delay &gt; 0, \"delayed\", \"on-time\")\n  )\n#&gt; &lt;SQL&gt;\n#&gt; SELECT CASE WHEN (arr_delay &gt; 0.0) THEN 'delayed' WHEN NOT (arr_delay &gt; 0.0) THEN 'on-time' END AS description\n#&gt; FROM flights\nflights |&gt; \n  mutate_query(\n    description = \n      case_when(\n        arr_delay &lt; -5 ~ \"early\", \n        arr_delay &lt; 5 ~ \"on-time\",\n        arr_delay &gt;= 5 ~ \"late\"\n      )\n  )\n#&gt; &lt;SQL&gt;\n#&gt; SELECT CASE\n#&gt; WHEN (arr_delay &lt; -5.0) THEN 'early'\n#&gt; WHEN (arr_delay &lt; 5.0) THEN 'on-time'\n#&gt; WHEN (arr_delay &gt;= 5.0) THEN 'late'\n#&gt; END AS description\n#&gt; FROM flights\n\nCASE WHEN is also used for some other functions that don’t have a direct translation from R to SQL. A good example of this is cut():CASE WHEN 也用于一些其他没有从 R 到 SQL 直接翻译的函数。一个很好的例子是 cut()：\n\nflights |&gt; \n  mutate_query(\n    description =  cut(\n      arr_delay, \n      breaks = c(-Inf, -5, 5, Inf), \n      labels = c(\"early\", \"on-time\", \"late\")\n    )\n  )\n#&gt; &lt;SQL&gt;\n#&gt; SELECT CASE\n#&gt; WHEN (arr_delay &lt;= -5.0) THEN 'early'\n#&gt; WHEN (arr_delay &lt;= 5.0) THEN 'on-time'\n#&gt; WHEN (arr_delay &gt; 5.0) THEN 'late'\n#&gt; END AS description\n#&gt; FROM flights\n\ndbplyr also translates common string and date-time manipulation functions, which you can learn about in vignette(\"translation-function\", package = \"dbplyr\"). dbplyr’s translations are certainly not perfect, and there are many R functions that aren’t translated yet, but dbplyr does a surprisingly good job covering the functions that you’ll use most of the time.\ndbplyr 还可以翻译常见的字符串和日期时间操作函数，你可以在 vignette(\"translation-function\", package = \"dbplyr\") 中了解这些内容。dbplyr 的翻译当然不是完美的，还有很多 R 函数尚未被翻译，但 dbplyr 在覆盖你大部分时间会用到的函数方面做得相当不错。",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "databases.html#summary",
    "href": "databases.html#summary",
    "title": "21  Databases",
    "section": "\n21.7 Summary",
    "text": "21.7 Summary\nIn this chapter you learned how to access data from databases. We focused on dbplyr, a dplyr “backend” that allows you to write the dplyr code you’re familiar with, and have it be automatically translated to SQL. We used that translation to teach you a little SQL; it’s important to learn some SQL because it’s the most commonly used language for working with data and knowing some will make it easier for you to communicate with other data folks who don’t use R.\n在本章中，你学习了如何从数据库访问数据。我们重点介绍了 dbplyr，这是一个 dplyr 的“后端”，它允许你编写你所熟悉的 dplyr 代码，并将其自动翻译成 SQL。我们利用这种翻译教了你一点 SQL；学习一些 SQL 很重要，因为它是最常用的数据处理语言，了解一些 SQL 将使你更容易与不使用 R 的其他数据从业者交流。\nIf you’ve finished this chapter and would like to learn more about SQL, we have two recommendations:\n如果你已经完成了本章并想学习更多关于 SQL 的知识，我们有两个建议：\n\nSQL for Data Scientists by Renée M. P. Teate is an introduction to SQL designed specifically for the needs of data scientists, and includes examples of the sort of highly interconnected data you’re likely to encounter in real organizations.\nRenée M. P. Teate 的 SQL for Data Scientists 是一本专为数据科学家的需求而设计的 SQL 入门书籍，其中包含了你在真实组织中可能遇到的那种高度互联数据的示例。\nPractical SQL by Anthony DeBarros is written from the perspective of a data journalist (a data scientist specialized in telling compelling stories) and goes into more detail about getting your data into a database and running your own DBMS.\nAnthony DeBarros 的 Practical SQL 是从数据记者（一位专门讲述引人入胜故事的数据科学家）的视角撰写的，它更详细地介绍了如何将数据导入数据库以及如何运行你自己的数据库管理系统 (DBMS)。\n\nIn the next chapter, we’ll learn about another dplyr backend for working with large data: arrow. Arrow is designed for working with large files on disk, and is a natural complement to databases.\n在下一章中，我们将学习另一个用于处理大数据的 dplyr 后端：arrow。Arrow 专为处理磁盘上的大文件而设计，是数据库的天然补充。",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "databases.html#footnotes",
    "href": "databases.html#footnotes",
    "title": "21  Databases",
    "section": "",
    "text": "SQL is either pronounced “s”-“q”-“l” or “sequel”.↩︎\nTypically, this is the only function you’ll use from the client package, so we recommend using :: to pull out that one function, rather than loading the complete package with library().↩︎\nAt least, all the tables that you have permission to see.↩︎\nConfusingly, depending on the context, SELECT is either a statement or a clause.↩︎\nOk, technically, only the SELECT is required, since you can write queries like SELECT 1+1 to perform basic calculations.↩︎\nThis is no coincidence: the dplyr function name was inspired by the SQL clause.↩︎",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "arrow.html",
    "href": "arrow.html",
    "title": "22  Arrow",
    "section": "",
    "text": "22.1 Introduction\nCSV files are designed to be easily read by humans. They’re a good interchange format because they’re very simple and they can be read by every tool under the sun. But CSV files aren’t very efficient: you have to do quite a lot of work to read the data into R. In this chapter, you’ll learn about a powerful alternative: the parquet format, an open standards-based format widely used by big data systems.\nCSV 文件被设计为易于人类阅读。它们是一种很好的交换格式，因为它们非常简单，并且几乎所有工具都能读取。但是 CSV 文件效率不高：你需要做相当多的工作才能将数据读入 R。在本章中，你将学习一个强大的替代方案：parquet 格式，这是一种基于开放标准的格式，被大数据系统广泛使用。\nWe’ll pair parquet files with Apache Arrow, a multi-language toolbox designed for efficient analysis and transport of large datasets. We’ll use Apache Arrow via the arrow package, which provides a dplyr backend allowing you to analyze larger-than-memory datasets using familiar dplyr syntax. As an additional benefit, arrow is extremely fast: you’ll see some examples later in the chapter.\n我们将把 parquet 文件与 Apache Arrow 结合使用，这是一个为高效分析和传输大型数据集而设计的多语言工具箱。我们将通过 arrow 包 使用 Apache Arrow，它提供了一个 dplyr 后端，允许你使用熟悉的 dplyr 语法分析大于内存的数据集。另外一个好处是，arrow 非常快：你将在本章后面看到一些例子。\nBoth arrow and dbplyr provide dplyr backends, so you might wonder when to use each. In many cases, the choice is made for you, as the data is already in a database or in parquet files, and you’ll want to work with it as is. But if you’re starting with your own data (perhaps CSV files), you can either load it into a database or convert it to parquet. In general, it’s hard to know what will work best, so in the early stages of your analysis we’d encourage you to try both and pick the one that works the best for you.\narrow 和 dbplyr 都提供了 dplyr 后端，所以你可能会想知道何时使用哪一个。在许多情况下，选择是现成的，因为数据已经存在于数据库或 parquet 文件中，而你希望直接使用它。但如果你是从自己的数据（比如 CSV 文件）开始，你可以选择将其加载到数据库中或转换为 parquet 格式。总的来说，很难知道哪种方法效果最好，因此在你分析的早期阶段，我们鼓励你两种方法都试试，然后选择最适合你的那一种。\n(A big thanks to Danielle Navarro who contributed the initial version of this chapter.)\n（非常感谢 Danielle Navarro 贡献了本章的初版。）",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Arrow</span>"
    ]
  },
  {
    "objectID": "arrow.html#introduction",
    "href": "arrow.html#introduction",
    "title": "22  Arrow",
    "section": "",
    "text": "22.1.1 Prerequisites\nIn this chapter, we’ll continue to use the tidyverse, particularly dplyr, but we’ll pair it with the arrow package which is designed specifically for working with large data.\n在本章中，我们将继续使用 tidyverse，特别是 dplyr，但我们会将其与专门为处理大数据而设计的 arrow 包结合使用。\n\nlibrary(tidyverse)\nlibrary(arrow)\n\nLater in the chapter, we’ll also see some connections between arrow and duckdb, so we’ll also need dbplyr and duckdb.\n在本章的后面，我们还会看到 arrow 和 duckdb 之间的一些联系，所以我们还需要 dbplyr 和 duckdb。\n\nlibrary(dbplyr, warn.conflicts = FALSE)\nlibrary(duckdb)\n#&gt; Loading required package: DBI",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Arrow</span>"
    ]
  },
  {
    "objectID": "arrow.html#getting-the-data",
    "href": "arrow.html#getting-the-data",
    "title": "22  Arrow",
    "section": "\n22.2 Getting the data",
    "text": "22.2 Getting the data\nWe begin by getting a dataset worthy of these tools: a dataset of item checkouts from Seattle public libraries, available online at data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6. This dataset contains 41,389,465 rows that tell you how many times each book was checked out each month from April 2005 to October 2022.\n我们首先获取一个值得使用这些工具的数据集：西雅图公共图书馆的物品借阅数据集，可在线获取：data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6。该数据集包含 41,389,465 行，记录了从 2005 年 4 月到 2022 年 10 月期间，每本书每月被借阅的次数。\nThe following code will get you a cached copy of the data. The data is a 9GB CSV file, so it will take some time to download. I highly recommend using curl::multi_download() to get very large files as it’s built for exactly this purpose: it gives you a progress bar and it can resume the download if its interrupted.\n以下代码将为你获取数据的缓存副本。数据是一个 9GB 的 CSV 文件，因此下载需要一些时间。我强烈推荐使用 curl::multi_download() 来获取非常大的文件，因为它正是为此目的而构建的：它会提供一个进度条，并且如果下载中断可以恢复。\n\ndir.create(\"data\", showWarnings = FALSE)\n\ncurl::multi_download(\n  \"https://r4ds.s3.us-west-2.amazonaws.com/seattle-library-checkouts.csv\",\n  \"data/seattle-library-checkouts.csv\",\n  resume = TRUE\n)",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Arrow</span>"
    ]
  },
  {
    "objectID": "arrow.html#opening-a-dataset",
    "href": "arrow.html#opening-a-dataset",
    "title": "22  Arrow",
    "section": "\n22.3 Opening a dataset",
    "text": "22.3 Opening a dataset\nLet’s start by taking a look at the data. At 9 GB, this file is large enough that we probably don’t want to load the whole thing into memory. A good rule of thumb is that you usually want at least twice as much memory as the size of the data, and many laptops top out at 16 GB. This means we want to avoid read_csv() and instead use the arrow::open_dataset():\n我们先来看看数据。这个 9 GB 的文件足够大，我们可能不想把它全部加载到内存中。一个好的经验法则是，你通常需要至少是数据大小两倍的内存，而许多笔记本电脑的内存上限是 16 GB。这意味着我们要避免使用 read_csv()，而应使用 arrow::open_dataset()：\n\nseattle_csv &lt;- open_dataset(\n  sources = \"data/seattle-library-checkouts.csv\", \n  col_types = schema(ISBN = string()),\n  format = \"csv\"\n)\n\nWhat happens when this code is run? open_dataset() will scan a few thousand rows to figure out the structure of the dataset. The ISBN column contains blank values for the first 80,000 rows, so we have to specify the column type to help arrow work out the data structure. Once the data has been scanned by open_dataset(), it records what it’s found and stops; it will only read further rows as you specifically request them. This metadata is what we see if we print seattle_csv:\n当这段代码运行时会发生什么？open_dataset() 会扫描几千行来确定数据集的结构。ISBN 列在前 80,000 行中包含空值，所以我们必须指定列类型来帮助 arrow 确定数据结构。一旦 open_dataset() 扫描完数据，它会记录下所发现的信息并停止；它只会在你明确请求时才会读取更多的行。这个元数据就是我们打印 seattle_csv 时看到的内容：\n\nseattle_csv\n#&gt; FileSystemDataset with 1 csv file\n#&gt; 12 columns\n#&gt; UsageClass: string\n#&gt; CheckoutType: string\n#&gt; MaterialType: string\n#&gt; CheckoutYear: int64\n#&gt; CheckoutMonth: int64\n#&gt; Checkouts: int64\n#&gt; Title: string\n#&gt; ISBN: string\n#&gt; Creator: string\n#&gt; Subjects: string\n#&gt; Publisher: string\n#&gt; PublicationYear: string\n\nThe first line in the output tells you that seattle_csv is stored locally on-disk as a single CSV file; it will only be loaded into memory as needed. The remainder of the output tells you the column type that arrow has imputed for each column.\n输出的第一行告诉你 seattle_csv 是作为一个单独的 CSV 文件存储在本地磁盘上的；它只会在需要时才被加载到内存中。输出的其余部分告诉了你 arrow 为每一列推断出的列类型。\nWe can see what’s actually in with glimpse(). This reveals that there are ~41 million rows and 12 columns, and shows us a few values.\n我们可以用 glimpse() 来查看实际内容。这揭示了数据大约有 4100 万行和 12 列，并向我们展示了一些值。\n\nseattle_csv |&gt; glimpse()\n#&gt; FileSystemDataset with 1 csv file\n#&gt; 41,389,465 rows x 12 columns\n#&gt; $ UsageClass      &lt;string&gt; \"Physical\", \"Physical\", \"Digital\", \"Physical\", \"Ph…\n#&gt; $ CheckoutType    &lt;string&gt; \"Horizon\", \"Horizon\", \"OverDrive\", \"Horizon\", \"Hor…\n#&gt; $ MaterialType    &lt;string&gt; \"BOOK\", \"BOOK\", \"EBOOK\", \"BOOK\", \"SOUNDDISC\", \"BOO…\n#&gt; $ CheckoutYear     &lt;int64&gt; 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 20…\n#&gt; $ CheckoutMonth    &lt;int64&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,…\n#&gt; $ Checkouts        &lt;int64&gt; 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 2, 3, 2, 1, 3, 2,…\n#&gt; $ Title           &lt;string&gt; \"Super rich : a guide to having it all / Russell S…\n#&gt; $ ISBN            &lt;string&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#&gt; $ Creator         &lt;string&gt; \"Simmons, Russell\", \"Barclay, James, 1965-\", \"Tim …\n#&gt; $ Subjects        &lt;string&gt; \"Self realization, Conduct of life, Attitude Psych…\n#&gt; $ Publisher       &lt;string&gt; \"Gotham Books,\", \"Pyr,\", \"Random House, Inc.\", \"Di…\n#&gt; $ PublicationYear &lt;string&gt; \"c2011.\", \"2010.\", \"2015\", \"2005.\", \"c2004.\", \"c20…\n\nWe can start to use this dataset with dplyr verbs, using collect() to force arrow to perform the computation and return some data. For example, this code tells us the total number of checkouts per year:\n我们可以开始对这个数据集使用 dplyr 动词，并使用 collect() 来强制 arrow 执行计算并返回一些数据。例如，这段代码告诉我们每年的总借阅量：\n\nseattle_csv |&gt; \n  group_by(CheckoutYear) |&gt; \n  summarise(Checkouts = sum(Checkouts)) |&gt; \n  arrange(CheckoutYear) |&gt; \n  collect()\n#&gt; # A tibble: 18 × 2\n#&gt;   CheckoutYear Checkouts\n#&gt;          &lt;int&gt;     &lt;int&gt;\n#&gt; 1         2005   3798685\n#&gt; 2         2006   6599318\n#&gt; 3         2007   7126627\n#&gt; 4         2008   8438486\n#&gt; 5         2009   9135167\n#&gt; 6         2010   8608966\n#&gt; # ℹ 12 more rows\n\nThanks to arrow, this code will work regardless of how large the underlying dataset is. But it’s currently rather slow: on Hadley’s computer, it took ~10s to run. That’s not terrible given how much data we have, but we can make it much faster by switching to a better format.\n得益于 arrow，无论底层数据集有多大，这段代码都能正常工作。但它目前相当慢：在 Hadley 的电脑上，运行大约需要 10 秒。考虑到我们拥有的数据量，这不算太糟，但我们可以通过切换到更好的格式来让它变得快得多。",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Arrow</span>"
    ]
  },
  {
    "objectID": "arrow.html#sec-parquet",
    "href": "arrow.html#sec-parquet",
    "title": "22  Arrow",
    "section": "\n22.4 The parquet format",
    "text": "22.4 The parquet format\nTo make this data easier to work with, let’s switch to the parquet file format and split it up into multiple files. The following sections will first introduce you to parquet and partitioning, and then apply what we learned to the Seattle library data.\n为了让这个数据更容易处理，让我们切换到 parquet 文件格式，并将其分割成多个文件。接下来的部分将首先向你介绍 parquet 和分区 (partitioning)，然后将我们学到的知识应用到西雅图图书馆数据上。\n\n22.4.1 Advantages of parquet\nLike CSV, parquet is used for rectangular data, but instead of being a text format that you can read with any file editor, it’s a custom binary format designed specifically for the needs of big data. This means that:\n与 CSV 一样，parquet 用于处理矩形数据，但它不是你可以用任何文件编辑器读取的文本格式，而是一种专为大数据需求设计的自定义二进制格式。这意味着：\n\nParquet files are usually smaller than the equivalent CSV file. Parquet relies on efficient encodings to keep file size down, and supports file compression. This helps make parquet files fast because there’s less data to move from disk to memory.\nParquet 文件通常比等效的 CSV 文件小。Parquet 依赖于高效编码来减小文件大小，并支持文件压缩。这有助于使 parquet 文件运行速度更快，因为需要从磁盘移动到内存的数据更少。\nParquet files have a rich type system. As we talked about in Section 7.3, a CSV file does not provide any information about column types. For example, a CSV reader has to guess whether \"08-10-2022\" should be parsed as a string or a date. In contrast, parquet files store data in a way that records the type along with the data.\nParquet 文件拥有丰富的类型系统。正如我们在 Section 7.3 中讨论的，CSV 文件不提供任何关于列类型的信息。例如，CSV 读取器必须猜测 \"08-10-2022\" 应该被解析为字符串还是日期。相比之下，parquet 文件以一种将类型与数据一同记录的方式存储数据。\nParquet files are “column-oriented”. This means that they’re organized column-by-column, much like R’s data frame. This typically leads to better performance for data analysis tasks compared to CSV files, which are organized row-by-row.\nParquet 文件是“列式存储”(column-oriented) 的。这意味着它们是按列组织的，很像 R 的数据框。与按行组织的 CSV 文件相比，这通常会为数据分析任务带来更好的性能。\nParquet files are “chunked”, which makes it possible to work on different parts of the file at the same time, and, if you’re lucky, to skip some chunks altogether.\nParquet 文件是“分块的”(chunked)，这使得可以同时处理文件的不同部分，并且，如果幸运的话，可以完全跳过某些块。\n\nThere’s one primary disadvantage to parquet files: they are no longer “human readable”, i.e. if you look at a parquet file using readr::read_file(), you’ll just see a bunch of gibberish.\nparquet 文件有一个主要缺点：它们不再是“人类可读的”，也就是说，如果你用 readr::read_file() 查看一个 parquet 文件，你只会看到一堆乱码。\n\n22.4.2 Partitioning\nAs datasets get larger and larger, storing all the data in a single file gets increasingly painful and it’s often useful to split large datasets across many files. When this structuring is done intelligently, this strategy can lead to significant improvements in performance because many analyses will only require a subset of the files.\n随着数据集越来越大，将所有数据存储在单个文件中变得越来越痛苦，将大型数据集分割到多个文件中通常很有用。当这种结构化操作做得巧妙时，该策略可以显著提高性能，因为许多分析只需要文件的一个子集。\nThere are no hard and fast rules about how to partition your dataset: the results will depend on your data, access patterns, and the systems that read the data. You’re likely to need to do some experimentation before you find the ideal partitioning for your situation. As a rough guide, arrow suggests that you avoid files smaller than 20MB and larger than 2GB and avoid partitions that produce more than 10,000 files. You should also try to partition by variables that you filter by; as you’ll see shortly, that allows arrow to skip a lot of work by reading only the relevant files.\n关于如何对数据集进行分区，没有硬性规定：结果将取决于你的数据、访问模式以及读取数据的系统。你可能需要进行一些实验才能找到适合你情况的理想分区方案。作为一个粗略的指南，arrow 建议你避免小于 20MB 和大于 2GB 的文件，并避免产生超过 10,000 个文件的分区。你还应该尝试按你过滤时使用的变量进行分区；正如你很快会看到的，这使得 arrow 可以通过只读取相关文件来跳过大量工作。\n\n22.4.3 Rewriting the Seattle library data\nLet’s apply these ideas to the Seattle library data to see how they play out in practice. We’re going to partition by CheckoutYear, since it’s likely some analyses will only want to look at recent data and partitioning by year yields 18 chunks of a reasonable size.\n让我们将这些想法应用到西雅图图书馆数据上，看看它们在实践中是如何发挥作用的。我们将按 CheckoutYear 进行分区，因为很可能一些分析只想查看最近的数据，而按年份分区可以产生 18 个大小合适的数据块。\nTo rewrite the data we define the partition using dplyr::group_by() and then save the partitions to a directory with arrow::write_dataset(). write_dataset() has two important arguments: a directory where we’ll create the files and the format we’ll use.\n为了重写数据，我们使用 dplyr::group_by() 定义分区，然后用 arrow::write_dataset() 将分区保存到一个目录中。write_dataset() 有两个重要的参数：一个是我们将在其中创建文件的目录，另一个是我们使用的格式。\n\npq_path &lt;- \"data/seattle-library-checkouts\"\n\n\nseattle_csv |&gt;\n  group_by(CheckoutYear) |&gt;\n  write_dataset(path = pq_path, format = \"parquet\")\n\nThis takes about a minute to run; as we’ll see shortly this is an initial investment that pays off by making future operations much much faster.\n这大约需要一分钟来运行；我们很快就会看到，这是一项初始投资，它会通过使未来的操作快得多而得到回报。\nLet’s take a look at what we just produced:\n让我们看看我们刚刚生成了什么：\n\ntibble(\n  files = list.files(pq_path, recursive = TRUE),\n  size_MB = file.size(file.path(pq_path, files)) / 1024^2\n)\n#&gt; # A tibble: 18 × 2\n#&gt;   files                            size_MB\n#&gt;   &lt;chr&gt;                              &lt;dbl&gt;\n#&gt; 1 CheckoutYear=2005/part-0.parquet    109.\n#&gt; 2 CheckoutYear=2006/part-0.parquet    164.\n#&gt; 3 CheckoutYear=2007/part-0.parquet    177.\n#&gt; 4 CheckoutYear=2008/part-0.parquet    194.\n#&gt; 5 CheckoutYear=2009/part-0.parquet    214.\n#&gt; 6 CheckoutYear=2010/part-0.parquet    222.\n#&gt; # ℹ 12 more rows\n\nOur single 9GB CSV file has been rewritten into 18 parquet files. The file names use a “self-describing” convention used by the Apache Hive project. Hive-style partitions name folders with a “key=value” convention, so as you might guess, the CheckoutYear=2005 directory contains all the data where CheckoutYear is 2005. Each file is between 100 and 300 MB and the total size is now around 4 GB, a little over half the size of the original CSV file. This is as we expect since parquet is a much more efficient format.\n我们单个 9GB 的 CSV 文件已经被重写为 18 个 parquet 文件。文件名使用了 Apache Hive 项目使用的“自描述”约定。Hive 风格的分区使用“key=value”的约定来命名文件夹，所以你可能猜到，CheckoutYear=2005 目录包含了所有 CheckoutYear 为 2005 的数据。每个文件大小在 100 到 300 MB 之间，总大小现在约为 4 GB，略多于原始 CSV 文件大小的一半。这正如我们所料，因为 parquet 是一种效率高得多的格式。",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Arrow</span>"
    ]
  },
  {
    "objectID": "arrow.html#using-dplyr-with-arrow",
    "href": "arrow.html#using-dplyr-with-arrow",
    "title": "22  Arrow",
    "section": "\n22.5 Using dplyr with arrow",
    "text": "22.5 Using dplyr with arrow\nNow we’ve created these parquet files, we’ll need to read them in again. We use open_dataset() again, but this time we give it a directory:\n现在我们已经创建了这些 parquet 文件，我们需要再次将它们读入。我们再次使用 open_dataset()，但这次我们给它一个目录：\n\nseattle_pq &lt;- open_dataset(pq_path)\n\nNow we can write our dplyr pipeline. For example, we could count the total number of books checked out in each month for the last five years:\n现在我们可以编写我们的 dplyr 管道了。例如，我们可以计算过去五年每个月借出的图书总数：\n\nquery &lt;- seattle_pq |&gt; \n  filter(CheckoutYear &gt;= 2018, MaterialType == \"BOOK\") |&gt;\n  group_by(CheckoutYear, CheckoutMonth) |&gt;\n  summarize(TotalCheckouts = sum(Checkouts)) |&gt;\n  arrange(CheckoutYear, CheckoutMonth)\n\nWriting dplyr code for arrow data is conceptually similar to dbplyr, Chapter 21: you write dplyr code, which is automatically transformed into a query that the Apache Arrow C++ library understands, which is then executed when you call collect(). If we print out the query object we can see a little information about what we expect Arrow to return when the execution takes place:\n为 arrow 数据编写 dplyr 代码在概念上类似于 dbplyr，Chapter 21：你编写 dplyr 代码，它会自动转换为 Apache Arrow C++ 库能理解的查询，然后在你调用 collect() 时执行。如果我们打印出 query 对象，我们可以看到一些关于我们期望 Arrow 在执行时返回什么的信息：\n\nquery\n#&gt; FileSystemDataset (query)\n#&gt; CheckoutYear: int32\n#&gt; CheckoutMonth: int64\n#&gt; TotalCheckouts: int64\n#&gt; \n#&gt; * Grouped by CheckoutYear\n#&gt; * Sorted by CheckoutYear [asc], CheckoutMonth [asc]\n#&gt; See $.data for the source Arrow object\n\nAnd we can get the results by calling collect():\n我们可以通过调用 collect() 来获取结果：\n\nquery |&gt; collect()\n#&gt; # A tibble: 58 × 3\n#&gt; # Groups:   CheckoutYear [5]\n#&gt;   CheckoutYear CheckoutMonth TotalCheckouts\n#&gt;          &lt;int&gt;         &lt;int&gt;          &lt;int&gt;\n#&gt; 1         2018             1         355101\n#&gt; 2         2018             2         309813\n#&gt; 3         2018             3         344487\n#&gt; 4         2018             4         330988\n#&gt; 5         2018             5         318049\n#&gt; 6         2018             6         341825\n#&gt; # ℹ 52 more rows\n\nLike dbplyr, arrow only understands some R expressions, so you may not be able to write exactly the same code you usually would. However, the list of operations and functions supported is fairly extensive and continues to grow; find a complete list of currently supported functions in ?acero.\n与 dbplyr 类似，arrow 只理解部分 R 表达式，所以你可能无法完全写出你通常会写的代码。然而，支持的操作和函数列表相当广泛，并且在不断增长；可以在 ?acero 中找到当前支持的函数的完整列表。\n\n22.5.1 Performance\nLet’s take a quick look at the performance impact of switching from CSV to parquet. First, let’s time how long it takes to calculate the number of books checked out in each month of 2021, when the data is stored as a single large csv:\n让我们快速看一下从 CSV 切换到 parquet 对性能的影响。首先，我们来计时计算 2021 年每个月借阅的书籍数量需要多长时间，此时数据存储为一个单独的大型 CSV 文件：\n\nseattle_csv |&gt; \n  filter(CheckoutYear == 2021, MaterialType == \"BOOK\") |&gt;\n  group_by(CheckoutMonth) |&gt;\n  summarize(TotalCheckouts = sum(Checkouts)) |&gt;\n  arrange(desc(CheckoutMonth)) |&gt;\n  collect() |&gt; \n  system.time()\n#&gt;    user  system elapsed \n#&gt;   11.54    1.53   11.07\n\nNow let’s use our new version of the dataset in which the Seattle library checkout data has been partitioned into 18 smaller parquet files:\n现在让我们使用新版本的数据集，其中西雅图图书馆的借阅数据已被分区为 18 个较小的 parquet 文件：\n\nseattle_pq |&gt; \n  filter(CheckoutYear == 2021, MaterialType == \"BOOK\") |&gt;\n  group_by(CheckoutMonth) |&gt;\n  summarize(TotalCheckouts = sum(Checkouts)) |&gt;\n  arrange(desc(CheckoutMonth)) |&gt;\n  collect() |&gt; \n  system.time()\n#&gt;    user  system elapsed \n#&gt;    0.25    0.02    0.12\n\nThe ~100x speedup in performance is attributable to two factors: the multi-file partitioning, and the format of individual files:\n性能提升约 100 倍可归因于两个因素：多文件分区和单个文件的格式：\n\nPartitioning improves performance because this query uses CheckoutYear == 2021 to filter the data, and arrow is smart enough to recognize that it only needs to read 1 of the 18 parquet files.\n分区提高了性能，因为此查询使用 CheckoutYear == 2021 来过滤数据，而 arrow 足够智能，能够识别出它只需要读取 18 个 parquet 文件中的 1 个。\nThe parquet format improves performance by storing data in a binary format that can be read more directly into memory. The column-wise format and rich metadata means that arrow only needs to read the four columns actually used in the query (CheckoutYear, MaterialType, CheckoutMonth, and Checkouts).\nParquet 格式通过以二进制格式存储数据来提高性能，这种格式可以更直接地读入内存。其列式格式和丰富的元数据意味着 arrow 只需要读取查询中实际使用的四列（CheckoutYear、MaterialType、CheckoutMonth 和 Checkouts）。\n\nThis massive difference in performance is why it pays off to convert large CSVs to parquet!\n这种巨大的性能差异就是为什么将大型 CSV 文件转换为 parquet 是值得的！\n\n22.5.2 Using duckdb with arrow\nThere’s one last advantage of parquet and arrow — it’s very easy to turn an arrow dataset into a DuckDB database (Chapter 21) by calling arrow::to_duckdb():\nparquet 和 arrow 还有一个最后的优势——通过调用 arrow::to_duckdb()，可以非常容易地将一个 arrow 数据集转换成一个 DuckDB 数据库 (Chapter 21)：\n\nseattle_pq |&gt; \n  to_duckdb() |&gt;\n  filter(CheckoutYear &gt;= 2018, MaterialType == \"BOOK\") |&gt;\n  group_by(CheckoutYear) |&gt;\n  summarize(TotalCheckouts = sum(Checkouts)) |&gt;\n  arrange(desc(CheckoutYear)) |&gt;\n  collect()\n#&gt; Warning: Missing values are always removed in SQL aggregation functions.\n#&gt; Use `na.rm = TRUE` to silence this warning\n#&gt; This warning is displayed once every 8 hours.\n#&gt; # A tibble: 5 × 2\n#&gt;   CheckoutYear TotalCheckouts\n#&gt;          &lt;int&gt;          &lt;dbl&gt;\n#&gt; 1         2022        2431502\n#&gt; 2         2021        2266438\n#&gt; 3         2020        1241999\n#&gt; 4         2019        3931688\n#&gt; 5         2018        3987569\n\nThe neat thing about to_duckdb() is that the transfer doesn’t involve any memory copying, and speaks to the goals of the arrow ecosystem: enabling seamless transitions from one computing environment to another.to_duckdb() 的妙处在于数据传输不涉及任何内存复制，这体现了 arrow 生态系统的目标：实现从一个计算环境到另一个计算环境的无缝转换。\n\n22.5.3 Exercises\n\nFigure out the most popular book each year.\nWhich author has the most books in the Seattle library system?\nHow has checkouts of books vs ebooks changed over the last 10 years?",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Arrow</span>"
    ]
  },
  {
    "objectID": "arrow.html#summary",
    "href": "arrow.html#summary",
    "title": "22  Arrow",
    "section": "\n22.6 Summary",
    "text": "22.6 Summary\nIn this chapter, you’ve been given a taste of the arrow package, which provides a dplyr backend for working with large on-disk datasets. It can work with CSV files, and it’s much much faster if you convert your data to parquet. Parquet is a binary data format that’s designed specifically for data analysis on modern computers. Far fewer tools can work with parquet files compared to CSV, but its partitioned, compressed, and columnar structure makes it much more efficient to analyze.\n在本章中，你初步了解了 arrow 包，它为处理大型磁盘数据集提供了一个 dplyr 后端。它可以处理 CSV 文件，但如果你将数据转换为 parquet 格式，速度会快得多。Parquet 是一种专为在现代计算机上进行数据分析而设计的二进制数据格式。与 CSV 相比，能处理 parquet 文件的工具要少得多，但其分区、压缩和列式结构使其分析效率更高。\nNext up you’ll learn about your first non-rectangular data source, which you’ll handle using tools provided by the tidyr package. We’ll focus on data that comes from JSON files, but the general principles apply to tree-like data regardless of its source.\n接下来，你将学习你的第一个非矩形数据源，你将使用 tidyr 包提供的工具来处理它。我们将重点关注来自 JSON 文件的数据，但通用原则适用于任何来源的树状数据。",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Arrow</span>"
    ]
  },
  {
    "objectID": "rectangling.html",
    "href": "rectangling.html",
    "title": "23  Hierarchical data",
    "section": "",
    "text": "23.1 Introduction\nIn this chapter, you’ll learn the art of data rectangling: taking data that is fundamentally hierarchical, or tree-like, and converting it into a rectangular data frame made up of rows and columns. This is important because hierarchical data is surprisingly common, especially when working with data that comes from the web.\n在本章中，你将学习数据矩形化 (rectangling) 的艺术：将本质上是分层的或树状的数据，转换为由行和列组成的矩形数据框。这一点很重要，因为分层数据出人意料地常见，尤其是在处理来自网络的数据时。\nTo learn about rectangling, you’ll need to first learn about lists, the data structure that makes hierarchical data possible. Then you’ll learn about two crucial tidyr functions: tidyr::unnest_longer() and tidyr::unnest_wider(). We’ll then show you a few case studies, applying these simple functions again and again to solve real problems. We’ll finish off by talking about JSON, the most frequent source of hierarchical datasets and a common format for data exchange on the web.\n要学习矩形化，你需要先了解列表，这种数据结构使分层数据成为可能。然后你将学习两个关键的 tidyr 函数：tidyr::unnest_longer() 和 tidyr::unnest_wider()。接着，我们将通过一些案例研究，反复应用这些简单的函数来解决实际问题。最后，我们将讨论 JSON，它是分层数据集最常见的来源，也是网络上数据交换的常用格式。",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Hierarchical data</span>"
    ]
  },
  {
    "objectID": "rectangling.html#introduction",
    "href": "rectangling.html#introduction",
    "title": "23  Hierarchical data",
    "section": "",
    "text": "23.1.1 Prerequisites\nIn this chapter, we’ll use many functions from tidyr, a core member of the tidyverse. We’ll also use repurrrsive to provide some interesting datasets for rectangling practice, and we’ll finish by using jsonlite to read JSON files into R lists.\n在本章中，我们将使用许多来自 tidyr 的函数，它是 tidyverse 的核心成员。我们还将使用 repurrrsive 包来提供一些有趣的数据集，用于矩形化练习，最后我们将使用 jsonlite 包将 JSON 文件读入 R 列表。\n\nlibrary(tidyverse)\nlibrary(repurrrsive)\nlibrary(jsonlite)",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Hierarchical data</span>"
    ]
  },
  {
    "objectID": "rectangling.html#lists",
    "href": "rectangling.html#lists",
    "title": "23  Hierarchical data",
    "section": "\n23.2 Lists",
    "text": "23.2 Lists\nSo far you’ve worked with data frames that contain simple vectors like integers, numbers, characters, date-times, and factors. These vectors are simple because they’re homogeneous: every element is of the same data type. If you want to store elements of different types in the same vector, you’ll need a list, which you create with list():\n到目前为止，你所使用的数据框包含的是简单的向量，如整数、数字、字符、日期时间和因子。这些向量之所以简单，是因为它们是同质的：每个元素都属于相同的数据类型。如果你想在同一个向量中存储不同类型的元素，你就需要一个列表 (list)，你可以用 list() 来创建它：\n\nx1 &lt;- list(1:4, \"a\", TRUE)\nx1\n#&gt; [[1]]\n#&gt; [1] 1 2 3 4\n#&gt; \n#&gt; [[2]]\n#&gt; [1] \"a\"\n#&gt; \n#&gt; [[3]]\n#&gt; [1] TRUE\n\nIt’s often convenient to name the components, or children, of a list, which you can do in the same way as naming the columns of a tibble:\n为列表的组件（或称子元素 (children)）命名通常很方便，你可以像命名 tibble 的列一样来做：\n\nx2 &lt;- list(a = 1:2, b = 1:3, c = 1:4)\nx2\n#&gt; $a\n#&gt; [1] 1 2\n#&gt; \n#&gt; $b\n#&gt; [1] 1 2 3\n#&gt; \n#&gt; $c\n#&gt; [1] 1 2 3 4\n\nEven for these very simple lists, printing takes up quite a lot of space. A useful alternative is str(), which generates a compact display of the structure, de-emphasizing the contents:\n即使是这些非常简单的列表，打印出来也会占用相当大的空间。一个有用的替代方法是 str()，它会生成一个紧凑的结构 (structure) 显示，并淡化其内容：\n\nstr(x1)\n#&gt; List of 3\n#&gt;  $ : int [1:4] 1 2 3 4\n#&gt;  $ : chr \"a\"\n#&gt;  $ : logi TRUE\nstr(x2)\n#&gt; List of 3\n#&gt;  $ a: int [1:2] 1 2\n#&gt;  $ b: int [1:3] 1 2 3\n#&gt;  $ c: int [1:4] 1 2 3 4\n\nAs you can see, str() displays each child of the list on its own line. It displays the name, if present, then an abbreviation of the type, then the first few values.\n如你所见，str() 将列表的每个子元素显示在单独的一行上。它会显示名称（如果存在），然后是类型的缩写，最后是前几个值。\n\n23.2.1 Hierarchy\nLists can contain any type of object, including other lists. This makes them suitable for representing hierarchical (tree-like) structures:\n列表可以包含任何类型的对象，包括其他列表。这使得它们非常适合表示分层（树状）结构：\n\nx3 &lt;- list(list(1, 2), list(3, 4))\nstr(x3)\n#&gt; List of 2\n#&gt;  $ :List of 2\n#&gt;   ..$ : num 1\n#&gt;   ..$ : num 2\n#&gt;  $ :List of 2\n#&gt;   ..$ : num 3\n#&gt;   ..$ : num 4\n\nThis is notably different to c(), which generates a flat vector:\n这与 c() 函数显著不同，c() 函数会生成一个扁平的向量：\n\nc(c(1, 2), c(3, 4))\n#&gt; [1] 1 2 3 4\n\nx4 &lt;- c(list(1, 2), list(3, 4))\nstr(x4)\n#&gt; List of 4\n#&gt;  $ : num 1\n#&gt;  $ : num 2\n#&gt;  $ : num 3\n#&gt;  $ : num 4\n\nAs lists get more complex, str() gets more useful, as it lets you see the hierarchy at a glance:\n随着列表变得越来越复杂，str() 的用处也越来越大，因为它能让你一目了然地看到层级结构：\n\nx5 &lt;- list(1, list(2, list(3, list(4, list(5)))))\nstr(x5)\n#&gt; List of 2\n#&gt;  $ : num 1\n#&gt;  $ :List of 2\n#&gt;   ..$ : num 2\n#&gt;   ..$ :List of 2\n#&gt;   .. ..$ : num 3\n#&gt;   .. ..$ :List of 2\n#&gt;   .. .. ..$ : num 4\n#&gt;   .. .. ..$ :List of 1\n#&gt;   .. .. .. ..$ : num 5\n\nAs lists get even larger and more complex, str() eventually starts to fail, and you’ll need to switch to View()1. Figure 23.1 shows the result of calling View(x5). The viewer starts by showing just the top level of the list, but you can interactively expand any of the components to see more, as in Figure 23.2. RStudio will also show you the code you need to access that element, as in Figure 23.3. We’ll come back to how this code works in Section 27.3.\n当列表变得更大、更复杂时，str() 最终会变得力不从心，这时你就需要切换到 View() 1。Figure 23.1 展示了调用 View(x5) 的结果。查看器开始时只显示列表的顶层，但你可以交互式地展开任何组件以查看更多内容，如 Figure 23.2 所示。RStudio 还会向你显示访问该元素所需代码，如 Figure 23.3 所示。我们将在 Section 27.3 回顾这段代码是如何工作的。\n\n\n\n\n\n\n\nFigure 23.1: The RStudio view lets you interactively explore a complex list. The viewer opens showing only the top level of the list.\n\n\n\n\n\n\n\n\n\n\n\nFigure 23.2: Clicking on the rightward facing triangle expands that component of the list so that you can also see its children.\n\n\n\n\n\n\n\n\n\n\n\nFigure 23.3: You can repeat this operation as many times as needed to get to the data you’re interested in. Note the bottom-left corner: if you click an element of the list, RStudio will give you the subsetting code needed to access it, in this case x5[[2]][[2]][[2]].\n\n\n\n\n\n23.2.2 List-columns\nLists can also live inside a tibble, where we call them list-columns. List-columns are useful because they allow you to place objects in a tibble that wouldn’t usually belong in there. In particular, list-columns are used a lot in the tidymodels ecosystem, because they allow you to store things like model outputs or resamples in a data frame.\n列表也可以存在于 tibble 中，我们称之为列表列 (list-columns)。列表列很有用，因为它们允许你将通常不属于 tibble 的对象放入其中。特别地，列表列在 tidymodels 生态系统中被大量使用，因为它们允许你将模型输出或重采样等内容存储在数据框中。\nHere’s a simple example of a list-column:\n下面是列表列的一个简单示例：\n\ndf &lt;- tibble(\n  x = 1:2, \n  y = c(\"a\", \"b\"),\n  z = list(list(1, 2), list(3, 4, 5))\n)\ndf\n#&gt; # A tibble: 2 × 3\n#&gt;       x y     z         \n#&gt;   &lt;int&gt; &lt;chr&gt; &lt;list&gt;    \n#&gt; 1     1 a     &lt;list [2]&gt;\n#&gt; 2     2 b     &lt;list [3]&gt;\n\nThere’s nothing special about lists in a tibble; they behave like any other column:\ntibble 中的列表没有什么特别之处；它们的行为与任何其他列一样：\n\ndf |&gt; \n  filter(x == 1)\n#&gt; # A tibble: 1 × 3\n#&gt;       x y     z         \n#&gt;   &lt;int&gt; &lt;chr&gt; &lt;list&gt;    \n#&gt; 1     1 a     &lt;list [2]&gt;\n\nComputing with list-columns is harder, but that’s because computing with lists is harder in general; we’ll come back to that in Chapter 26. In this chapter, we’ll focus on unnesting list-columns out into regular variables so you can use your existing tools on them.\n使用列表列进行计算更加困难，但这是因为通常情况下使用列表进行计算就更难；我们将在 Chapter 26 回到这个问题。在本章中，我们将专注于将列表列“展开” (unnesting) 为常规变量，以便你可以使用现有的工具来处理它们。\nThe default print method just displays a rough summary of the contents. The list column could be arbitrarily complex, so there’s no good way to print it. If you want to see it, you’ll need to pull out just the one list-column and apply one of the techniques that you’ve learned above, like df |&gt; pull(z) |&gt; str() or df |&gt; pull(z) |&gt; View().\n默认的打印方法只显示了内容的粗略摘要。列表列可能任意复杂，所以没有很好的方法来打印它。如果你想查看它，你需要单独抽取出那个列表列，并应用你上面学到的技术之一，比如 df |&gt; pull(z) |&gt; str() 或 df |&gt; pull(z) |&gt; View()。\n\n\n\n\n\n\nBase R\n\n\n\nIt’s possible to put a list in a column of a data.frame, but it’s a lot fiddlier because data.frame() treats a list as a list of columns:\n可以在 data.frame 的一列中放入一个列表，但这要麻烦得多，因为 data.frame() 将列表视为列的列表：\n\ndata.frame(x = list(1:3, 3:5))\n#&gt;   x.1.3 x.3.5\n#&gt; 1     1     3\n#&gt; 2     2     4\n#&gt; 3     3     5\n\nYou can force data.frame() to treat a list as a list of rows by wrapping it in list I(), but the result doesn’t print particularly well:\n你可以通过将列表包装在 I() 中来强制 data.frame() 将其视为行的列表，但结果打印得不是特别好：\n\ndata.frame(\n  x = I(list(1:2, 3:5)), \n  y = c(\"1, 2\", \"3, 4, 5\")\n)\n#&gt;         x       y\n#&gt; 1    1, 2    1, 2\n#&gt; 2 3, 4, 5 3, 4, 5\n\nIt’s easier to use list-columns with tibbles because tibble() treats lists like vectors and the print method has been designed with lists in mind.\n使用 tibble 来处理列表列更容易，因为 tibble() 将列表视为向量，并且其打印方法是为列表而专门设计的。",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Hierarchical data</span>"
    ]
  },
  {
    "objectID": "rectangling.html#unnesting",
    "href": "rectangling.html#unnesting",
    "title": "23  Hierarchical data",
    "section": "\n23.3 Unnesting",
    "text": "23.3 Unnesting\nNow that you’ve learned the basics of lists and list-columns, let’s explore how you can turn them back into regular rows and columns. Here we’ll use very simple sample data so you can get the basic idea; in the next section we’ll switch to real data.\n既然你已经了解了列表和列表列的基础知识，让我们来探讨如何将它们转换回常规的行和列。这里我们将使用非常简单的示例数据，以便你掌握基本概念；在下一节中，我们将转向真实数据。\nList-columns tend to come in two basic forms: named and unnamed. When the children are named, they tend to have the same names in every row. For example, in df1, every element of list-column y has two elements named a and b. Named list-columns naturally unnest into columns: each named element becomes a new named column.\n列表列通常有两种基本形式：命名的和未命名的。当子元素是命名的 (named) 时，它们在每一行中往往具有相同的名称。例如，在 df1 中，列表列 y 的每个元素都有两个名为 a 和 b 的元素。命名的列表列很自然地可以展开为列：每个命名元素都会成为一个新的命名列。\n\ndf1 &lt;- tribble(\n  ~x, ~y,\n  1, list(a = 11, b = 12),\n  2, list(a = 21, b = 22),\n  3, list(a = 31, b = 32),\n)\n\nWhen the children are unnamed, the number of elements tends to vary from row-to-row. For example, in df2, the elements of list-column y are unnamed and vary in length from one to three. Unnamed list-columns naturally unnest into rows: you’ll get one row for each child.\n当子元素是未命名的 (unnamed) 时，元素的数量往往因行而异。例如，在 df2 中，列表列 y 的元素是未命名的，并且长度从一到三不等。未命名的列表列很自然地可以展开为行：每个子元素都会生成新的一行。\n\ndf2 &lt;- tribble(\n  ~x, ~y,\n  1, list(11, 12, 13),\n  2, list(21),\n  3, list(31, 32),\n)\n\ntidyr provides two functions for these two cases: unnest_wider() and unnest_longer(). The following sections explain how they work.\ntidyr 为这两种情况提供了两个函数：unnest_wider() 和 unnest_longer()。以下各节将解释它们的工作原理。\n\n23.3.1 unnest_wider()\n\nWhen each row has the same number of elements with the same names, like df1, it’s natural to put each component into its own column with unnest_wider():\n当每一行都有相同数量且名称相同的元素时，就像 df1 一样，很自然地可以使用 unnest_wider() 将每个组件放入各自的列中：\n\ndf1 |&gt; \n  unnest_wider(y)\n#&gt; # A tibble: 3 × 3\n#&gt;       x     a     b\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1    11    12\n#&gt; 2     2    21    22\n#&gt; 3     3    31    32\n\nBy default, the names of the new columns come exclusively from the names of the list elements, but you can use the names_sep argument to request that they combine the column name and the element name. This is useful for disambiguating repeated names.\n默认情况下，新列的名称完全来自列表元素的名称，但你可以使用 names_sep 参数来要求它们合并列名和元素名。这对于消除重复名称的歧义很有用。\n\ndf1 |&gt; \n  unnest_wider(y, names_sep = \"_\")\n#&gt; # A tibble: 3 × 3\n#&gt;       x   y_a   y_b\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1    11    12\n#&gt; 2     2    21    22\n#&gt; 3     3    31    32\n\n\n23.3.2 unnest_longer()\n\nWhen each row contains an unnamed list, it’s most natural to put each element into its own row with unnest_longer():\n当每一行都包含一个未命名的列表时，最自然的做法是使用 unnest_longer() 将每个元素放入其自己的行中：\n\ndf2 |&gt; \n  unnest_longer(y)\n#&gt; # A tibble: 6 × 2\n#&gt;       x     y\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1    11\n#&gt; 2     1    12\n#&gt; 3     1    13\n#&gt; 4     2    21\n#&gt; 5     3    31\n#&gt; 6     3    32\n\nNote how x is duplicated for each element inside of y: we get one row of output for each element inside the list-column. But what happens if one of the elements is empty, as in the following example?\n注意 x 是如何为 y 内的每个元素复制的：对于列表列中的每个元素，我们都会得到一行输出。但是，如果其中一个元素为空，如下面的例子所示，会发生什么呢？\n\ndf6 &lt;- tribble(\n  ~x, ~y,\n  \"a\", list(1, 2),\n  \"b\", list(3),\n  \"c\", list()\n)\ndf6 |&gt; unnest_longer(y)\n#&gt; # A tibble: 3 × 2\n#&gt;   x         y\n#&gt;   &lt;chr&gt; &lt;dbl&gt;\n#&gt; 1 a         1\n#&gt; 2 a         2\n#&gt; 3 b         3\n\nWe get zero rows in the output, so the row effectively disappears. If you want to preserve that row, adding NA in y, set keep_empty = TRUE.\n我们在输出中得到零行，因此该行实际上消失了。如果你想保留那一行，并在 y 中添加 NA，请设置 keep_empty = TRUE。\n\n23.3.3 Inconsistent types\nWhat happens if you unnest a list-column that contains different types of vector? For example, take the following dataset where the list-column y contains two numbers, a character, and a logical, which can’t normally be mixed in a single column.\n如果你展开一个包含不同类型向量的列表列，会发生什么？例如，看下面这个数据集，其中列表列 y 包含两个数字、一个字符和一个逻辑值，这些通常不能在单个列中混合。\n\ndf4 &lt;- tribble(\n  ~x, ~y,\n  \"a\", list(1),\n  \"b\", list(\"a\", TRUE, 5)\n)\n\nunnest_longer() always keeps the set of columns unchanged, while changing the number of rows. So what happens? How does unnest_longer() produce five rows while keeping everything in y?unnest_longer() 总是保持列集合不变，同时改变行的数量。那么会发生什么呢？unnest_longer() 是如何在保持 y 中所有内容的同时生成五行的呢？\n\ndf4 |&gt; \n  unnest_longer(y)\n#&gt; # A tibble: 4 × 2\n#&gt;   x     y        \n#&gt;   &lt;chr&gt; &lt;list&gt;   \n#&gt; 1 a     &lt;dbl [1]&gt;\n#&gt; 2 b     &lt;chr [1]&gt;\n#&gt; 3 b     &lt;lgl [1]&gt;\n#&gt; 4 b     &lt;dbl [1]&gt;\n\nAs you can see, the output contains a list-column, but every element of the list-column contains a single element. Because unnest_longer() can’t find a common type of vector, it keeps the original types in a list-column. You might wonder if this breaks the commandment that every element of a column must be the same type. It doesn’t: every element is a list, even though the contents are of different types.\n正如你所见，输出包含一个列表列，但该列表列的每个元素都只包含一个单一元素。因为 unnest_longer() 找不到一个通用的向量类型，所以它将原始类型保留在一个列表列中。你可能会好奇这是否违反了“列的每个元素必须是相同类型”的规定。答案是否定的：每个元素都是一个列表，尽管其内容是不同类型的。\nDealing with inconsistent types is challenging and the details depend on the precise nature of the problem and your goals, but you’ll most likely need tools from Chapter 26.\n处理不一致的类型是具有挑战性的，具体细节取决于问题的确切性质和你的目标，但你很可能需要来自 Chapter 26 的工具。\n\n23.3.4 Other functions\ntidyr has a few other useful rectangling functions that we’re not going to cover in this book:\ntidyr 还有一些其他有用的矩形化函数，我们在这本书中不会涉及：\n\nunnest_auto() automatically picks between unnest_longer() and unnest_wider() based on the structure of the list-column. It’s great for rapid exploration, but ultimately it’s a bad idea because it doesn’t force you to understand how your data is structured, and makes your code harder to understand.unnest_auto() 会根据列表列的结构自动在 unnest_longer() 和 unnest_wider() 之间进行选择。它非常适合快速探索，但最终来说，这是一个坏主意，因为它不会迫使你去理解你的数据结构，并使你的代码更难理解。\nunnest() expands both rows and columns. It’s useful when you have a list-column that contains a 2d structure like a data frame, which you don’t see in this book, but you might encounter if you use the tidymodels ecosystem.unnest() 会同时扩展行和列。当你的列表列包含像数据框这样的二维结构时，它很有用，这在本书中你不会看到，但如果你使用 tidymodels 生态系统，你可能会遇到。\n\nThese functions are good to know about as you might encounter them when reading other people’s code or tackling rarer rectangling challenges yourself.\n了解这些函数是很有好处的，因为在阅读他人的代码或自己处理更罕见的矩形化挑战时，你可能会遇到它们。\n\n23.3.5 Exercises\n\nWhat happens when you use unnest_wider() with unnamed list-columns like df2? What argument is now necessary? What happens to missing values?\nWhat happens when you use unnest_longer() with named list-columns like df1? What additional information do you get in the output? How can you suppress that extra detail?\n\nFrom time-to-time you encounter data frames with multiple list-columns with aligned values. For example, in the following data frame, the values of y and z are aligned (i.e. y and z will always have the same length within a row, and the first value of y corresponds to the first value of z). What happens if you apply two unnest_longer() calls to this data frame? How can you preserve the relationship between x and y? (Hint: carefully read the docs).\n\ndf4 &lt;- tribble(\n  ~x, ~y, ~z,\n  \"a\", list(\"y-a-1\", \"y-a-2\"), list(\"z-a-1\", \"z-a-2\"),\n  \"b\", list(\"y-b-1\", \"y-b-2\", \"y-b-3\"), list(\"z-b-1\", \"z-b-2\", \"z-b-3\")\n)",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Hierarchical data</span>"
    ]
  },
  {
    "objectID": "rectangling.html#case-studies",
    "href": "rectangling.html#case-studies",
    "title": "23  Hierarchical data",
    "section": "\n23.4 Case studies",
    "text": "23.4 Case studies\nThe main difference between the simple examples we used above and real data is that real data typically contains multiple levels of nesting that require multiple calls to unnest_longer() and/or unnest_wider(). To show that in action, this section works through three real rectangling challenges using datasets from the repurrrsive package.\n我们上面使用的简单示例与真实数据之间的主要区别在于，真实数据通常包含多个嵌套层级，需要多次调用 unnest_longer() 和/或 unnest_wider()。为了实际展示这一点，本节将使用来自 repurrrsive 包的数据集，通过三个真实的矩形化挑战来进行讲解。\n\n23.4.1 Very wide data\nWe’ll start with gh_repos. This is a list that contains data about a collection of GitHub repositories retrieved using the GitHub API. It’s a very deeply nested list so it’s difficult to show the structure in this book; we recommend exploring a little on your own with View(gh_repos) before we continue.\n我们从 gh_repos 开始。这是一个列表，其中包含通过 GitHub API 检索到的关于一组 GitHub 仓库的数据。这是一个非常深层嵌套的列表，因此很难在本书中展示其结构；我们建议在继续之前，你可以自己用 View(gh_repos) 稍作探索。\ngh_repos is a list, but our tools work with list-columns, so we’ll begin by putting it into a tibble. We call this column json for reasons we’ll get to later.gh_repos 是一个列表，但我们的工具是针对列表列工作的，所以我们首先要把它放进一个 tibble 中。我们将这一列命名为 json，原因稍后会解释。\n\nrepos &lt;- tibble(json = gh_repos)\nrepos\n#&gt; # A tibble: 6 × 1\n#&gt;   json       \n#&gt;   &lt;list&gt;     \n#&gt; 1 &lt;list [30]&gt;\n#&gt; 2 &lt;list [30]&gt;\n#&gt; 3 &lt;list [30]&gt;\n#&gt; 4 &lt;list [26]&gt;\n#&gt; 5 &lt;list [30]&gt;\n#&gt; 6 &lt;list [30]&gt;\n\nThis tibble contains 6 rows, one row for each child of gh_repos. Each row contains an unnamed list with either 26 or 30 rows. Since these are unnamed, we’ll start with unnest_longer() to put each child in its own row:\n这个 tibble 包含 6 行，gh_repos 的每个子元素占一行。每一行都包含一个未命名的列表，该列表有 26 或 30 行。由于这些列表是未命名的，我们先使用 unnest_longer() 将每个子元素放到单独的行中：\n\nrepos |&gt; \n  unnest_longer(json)\n#&gt; # A tibble: 176 × 1\n#&gt;   json             \n#&gt;   &lt;list&gt;           \n#&gt; 1 &lt;named list [68]&gt;\n#&gt; 2 &lt;named list [68]&gt;\n#&gt; 3 &lt;named list [68]&gt;\n#&gt; 4 &lt;named list [68]&gt;\n#&gt; 5 &lt;named list [68]&gt;\n#&gt; 6 &lt;named list [68]&gt;\n#&gt; # ℹ 170 more rows\n\nAt first glance, it might seem like we haven’t improved the situation: while we have more rows (176 instead of 6) each element of json is still a list. However, there’s an important difference: now each element is a named list so we can use unnest_wider() to put each element into its own column:\n乍一看，情况似乎并没有改善：虽然我们有了更多的行（176 行而不是 6 行），但 json 的每个元素仍然是一个列表。然而，有一个重要的区别：现在每个元素都是一个已命名的列表，所以我们可以使用 unnest_wider() 将每个元素放入其自己的列中：\n\nrepos |&gt; \n  unnest_longer(json) |&gt; \n  unnest_wider(json) \n#&gt; # A tibble: 176 × 68\n#&gt;         id name        full_name         owner        private html_url       \n#&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;             &lt;list&gt;       &lt;lgl&gt;   &lt;chr&gt;          \n#&gt; 1 61160198 after       gaborcsardi/after &lt;named list&gt; FALSE   https://github…\n#&gt; 2 40500181 argufy      gaborcsardi/argu… &lt;named list&gt; FALSE   https://github…\n#&gt; 3 36442442 ask         gaborcsardi/ask   &lt;named list&gt; FALSE   https://github…\n#&gt; 4 34924886 baseimports gaborcsardi/base… &lt;named list&gt; FALSE   https://github…\n#&gt; 5 61620661 citest      gaborcsardi/cite… &lt;named list&gt; FALSE   https://github…\n#&gt; 6 33907457 clisymbols  gaborcsardi/clis… &lt;named list&gt; FALSE   https://github…\n#&gt; # ℹ 170 more rows\n#&gt; # ℹ 62 more variables: description &lt;chr&gt;, fork &lt;lgl&gt;, url &lt;chr&gt;, …\n\nThis has worked but the result is a little overwhelming: there are so many columns that tibble doesn’t even print all of them! We can see them all with names(); and here we look at the first 10:\n这招奏效了，但结果有点让人不知所措：列太多了，以至于 tibble 甚至都无法全部打印出来！我们可以用 names() 查看所有列；这里我们看一下前 10 个：\n\nrepos |&gt; \n  unnest_longer(json) |&gt; \n  unnest_wider(json) |&gt; \n  names() |&gt; \n  head(10)\n#&gt;  [1] \"id\"          \"name\"        \"full_name\"   \"owner\"       \"private\"    \n#&gt;  [6] \"html_url\"    \"description\" \"fork\"        \"url\"         \"forks_url\"\n\nLet’s pull out a few that look interesting:\n让我们挑出几个看起来有趣的列：\n\nrepos |&gt; \n  unnest_longer(json) |&gt; \n  unnest_wider(json) |&gt; \n  select(id, full_name, owner, description)\n#&gt; # A tibble: 176 × 4\n#&gt;         id full_name               owner             description             \n#&gt;      &lt;int&gt; &lt;chr&gt;                   &lt;list&gt;            &lt;chr&gt;                   \n#&gt; 1 61160198 gaborcsardi/after       &lt;named list [17]&gt; Run Code in the Backgro…\n#&gt; 2 40500181 gaborcsardi/argufy      &lt;named list [17]&gt; Declarative function ar…\n#&gt; 3 36442442 gaborcsardi/ask         &lt;named list [17]&gt; Friendly CLI interactio…\n#&gt; 4 34924886 gaborcsardi/baseimports &lt;named list [17]&gt; Do we get warnings for …\n#&gt; 5 61620661 gaborcsardi/citest      &lt;named list [17]&gt; Test R package and repo…\n#&gt; 6 33907457 gaborcsardi/clisymbols  &lt;named list [17]&gt; Unicode symbols for CLI…\n#&gt; # ℹ 170 more rows\n\nYou can use this to work back to understand how gh_repos was structured: each child was a GitHub user containing a list of up to 30 GitHub repositories that they created.\n你可以利用这一点反向推导出 gh_repos 的结构：每个子元素都是一个 GitHub 用户，包含一个由他们创建的多达 30 个 GitHub 仓库的列表。\nowner is another list-column, and since it contains a named list, we can use unnest_wider() to get at the values:owner 是另一个列表列，由于它包含一个已命名的列表，我们可以使用 unnest_wider() 来获取其值：\n\nrepos |&gt; \n  unnest_longer(json) |&gt; \n  unnest_wider(json) |&gt; \n  select(id, full_name, owner, description) |&gt; \n  unnest_wider(owner)\n#&gt; Error in `unnest_wider()`:\n#&gt; ! Can't duplicate names between the affected columns and the original\n#&gt;   data.\n#&gt; ✖ These names are duplicated:\n#&gt;   ℹ `id`, from `owner`.\n#&gt; ℹ Use `names_sep` to disambiguate using the column name.\n#&gt; ℹ Or use `names_repair` to specify a repair strategy.\n\nUh oh, this list column also contains an id column and we can’t have two id columns in the same data frame. As suggested, lets use names_sep to resolve the problem:\n糟糕，这个列表列也包含一个 id 列，而我们不能在同一个数据框中有两个 id 列。根据提示，让我们使用 names_sep 来解决这个问题：\n\nrepos |&gt; \n  unnest_longer(json) |&gt; \n  unnest_wider(json) |&gt; \n  select(id, full_name, owner, description) |&gt; \n  unnest_wider(owner, names_sep = \"_\")\n#&gt; # A tibble: 176 × 20\n#&gt;         id full_name               owner_login owner_id owner_avatar_url     \n#&gt;      &lt;int&gt; &lt;chr&gt;                   &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;                \n#&gt; 1 61160198 gaborcsardi/after       gaborcsardi   660288 https://avatars.gith…\n#&gt; 2 40500181 gaborcsardi/argufy      gaborcsardi   660288 https://avatars.gith…\n#&gt; 3 36442442 gaborcsardi/ask         gaborcsardi   660288 https://avatars.gith…\n#&gt; 4 34924886 gaborcsardi/baseimports gaborcsardi   660288 https://avatars.gith…\n#&gt; 5 61620661 gaborcsardi/citest      gaborcsardi   660288 https://avatars.gith…\n#&gt; 6 33907457 gaborcsardi/clisymbols  gaborcsardi   660288 https://avatars.gith…\n#&gt; # ℹ 170 more rows\n#&gt; # ℹ 15 more variables: owner_gravatar_id &lt;chr&gt;, owner_url &lt;chr&gt;, …\n\nThis gives another wide dataset, but you can get the sense that owner appears to contain a lot of additional data about the person who “owns” the repository.\n这会产生另一个宽数据集，但你可以感觉到 owner 列似乎包含了大量关于仓库“所有者”(owner) 的附加数据。\n\n23.4.2 Relational data\nNested data is sometimes used to represent data that we’d usually spread across multiple data frames. For example, take got_chars which contains data about characters that appear in the Game of Thrones books and TV series. Like gh_repos it’s a list, so we start by turning it into a list-column of a tibble:\n嵌套数据有时用于表示我们通常会分散在多个数据框中的数据。例如，got_chars 包含了在《权力的游戏》书籍和电视剧中出现的角色的数据。和 gh_repos 一样，它是一个列表，所以我们首先将它转换成一个 tibble 的列表列：\n\nchars &lt;- tibble(json = got_chars)\nchars\n#&gt; # A tibble: 30 × 1\n#&gt;   json             \n#&gt;   &lt;list&gt;           \n#&gt; 1 &lt;named list [18]&gt;\n#&gt; 2 &lt;named list [18]&gt;\n#&gt; 3 &lt;named list [18]&gt;\n#&gt; 4 &lt;named list [18]&gt;\n#&gt; 5 &lt;named list [18]&gt;\n#&gt; 6 &lt;named list [18]&gt;\n#&gt; # ℹ 24 more rows\n\nThe json column contains named elements, so we’ll start by widening it:json 列包含命名元素，所以我们先将其加宽：\n\nchars |&gt; \n  unnest_wider(json)\n#&gt; # A tibble: 30 × 18\n#&gt;   url                    id name            gender culture    born           \n#&gt;   &lt;chr&gt;               &lt;int&gt; &lt;chr&gt;           &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;          \n#&gt; 1 https://www.anapio…  1022 Theon Greyjoy   Male   \"Ironborn\" \"In 278 AC or …\n#&gt; 2 https://www.anapio…  1052 Tyrion Lannist… Male   \"\"         \"In 273 AC, at…\n#&gt; 3 https://www.anapio…  1074 Victarion Grey… Male   \"Ironborn\" \"In 268 AC or …\n#&gt; 4 https://www.anapio…  1109 Will            Male   \"\"         \"\"             \n#&gt; 5 https://www.anapio…  1166 Areo Hotah      Male   \"Norvoshi\" \"In 257 AC or …\n#&gt; 6 https://www.anapio…  1267 Chett           Male   \"\"         \"At Hag's Mire\"\n#&gt; # ℹ 24 more rows\n#&gt; # ℹ 12 more variables: died &lt;chr&gt;, alive &lt;lgl&gt;, titles &lt;list&gt;, …\n\nAnd selecting a few columns to make it easier to read:\n然后选择几列以便于阅读：\n\ncharacters &lt;- chars |&gt; \n  unnest_wider(json) |&gt; \n  select(id, name, gender, culture, born, died, alive)\ncharacters\n#&gt; # A tibble: 30 × 7\n#&gt;      id name              gender culture    born              died           \n#&gt;   &lt;int&gt; &lt;chr&gt;             &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;             &lt;chr&gt;          \n#&gt; 1  1022 Theon Greyjoy     Male   \"Ironborn\" \"In 278 AC or 27… \"\"             \n#&gt; 2  1052 Tyrion Lannister  Male   \"\"         \"In 273 AC, at C… \"\"             \n#&gt; 3  1074 Victarion Greyjoy Male   \"Ironborn\" \"In 268 AC or be… \"\"             \n#&gt; 4  1109 Will              Male   \"\"         \"\"                \"In 297 AC, at…\n#&gt; 5  1166 Areo Hotah        Male   \"Norvoshi\" \"In 257 AC or be… \"\"             \n#&gt; 6  1267 Chett             Male   \"\"         \"At Hag's Mire\"   \"In 299 AC, at…\n#&gt; # ℹ 24 more rows\n#&gt; # ℹ 1 more variable: alive &lt;lgl&gt;\n\nThis dataset contains also many list-columns:\n这个数据集也包含许多列表列：\n\nchars |&gt; \n  unnest_wider(json) |&gt; \n  select(id, where(is.list))\n#&gt; # A tibble: 30 × 8\n#&gt;      id titles    aliases    allegiances books     povBooks tvSeries playedBy\n#&gt;   &lt;int&gt; &lt;list&gt;    &lt;list&gt;     &lt;list&gt;      &lt;list&gt;    &lt;list&gt;   &lt;list&gt;   &lt;list&gt;  \n#&gt; 1  1022 &lt;chr [2]&gt; &lt;chr [4]&gt;  &lt;chr [1]&gt;   &lt;chr [3]&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   \n#&gt; 2  1052 &lt;chr [2]&gt; &lt;chr [11]&gt; &lt;chr [1]&gt;   &lt;chr [2]&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   \n#&gt; 3  1074 &lt;chr [2]&gt; &lt;chr [1]&gt;  &lt;chr [1]&gt;   &lt;chr [3]&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   \n#&gt; 4  1109 &lt;chr [1]&gt; &lt;chr [1]&gt;  &lt;NULL&gt;      &lt;chr [1]&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   \n#&gt; 5  1166 &lt;chr [1]&gt; &lt;chr [1]&gt;  &lt;chr [1]&gt;   &lt;chr [3]&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   \n#&gt; 6  1267 &lt;chr [1]&gt; &lt;chr [1]&gt;  &lt;NULL&gt;      &lt;chr [2]&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   \n#&gt; # ℹ 24 more rows\n\nLet’s explore the titles column. It’s an unnamed list-column, so we’ll unnest it into rows:\n我们来探究一下 titles 列。它是一个未命名的列表列，所以我们将其展开为多行：\n\nchars |&gt; \n  unnest_wider(json) |&gt; \n  select(id, titles) |&gt; \n  unnest_longer(titles)\n#&gt; # A tibble: 59 × 2\n#&gt;      id titles                                              \n#&gt;   &lt;int&gt; &lt;chr&gt;                                               \n#&gt; 1  1022 Prince of Winterfell                                \n#&gt; 2  1022 Lord of the Iron Islands (by law of the green lands)\n#&gt; 3  1052 Acting Hand of the King (former)                    \n#&gt; 4  1052 Master of Coin (former)                             \n#&gt; 5  1074 Lord Captain of the Iron Fleet                      \n#&gt; 6  1074 Master of the Iron Victory                          \n#&gt; # ℹ 53 more rows\n\nYou might expect to see this data in its own table because it would be easy to join to the characters data as needed. Let’s do that, which requires little cleaning: removing the rows containing empty strings and renaming titles to title since each row now only contains a single title.\n你可能期望在一个独立的表中看到这些数据，因为这样很容易根据需要将其连接到角色数据上。我们来动手实现，这需要做一些清理：移除包含空字符串的行，并将 titles 重命名为 title，因为现在每行只包含一个头衔。\n\ntitles &lt;- chars |&gt; \n  unnest_wider(json) |&gt; \n  select(id, titles) |&gt; \n  unnest_longer(titles) |&gt; \n  filter(titles != \"\") |&gt; \n  rename(title = titles)\ntitles\n#&gt; # A tibble: 52 × 2\n#&gt;      id title                                               \n#&gt;   &lt;int&gt; &lt;chr&gt;                                               \n#&gt; 1  1022 Prince of Winterfell                                \n#&gt; 2  1022 Lord of the Iron Islands (by law of the green lands)\n#&gt; 3  1052 Acting Hand of the King (former)                    \n#&gt; 4  1052 Master of Coin (former)                             \n#&gt; 5  1074 Lord Captain of the Iron Fleet                      \n#&gt; 6  1074 Master of the Iron Victory                          \n#&gt; # ℹ 46 more rows\n\nYou could imagine creating a table like this for each of the list-columns, then using joins to combine them with the character data as you need it.\n你可以想象为每个列表列创建这样一个表，然后在需要时使用连接操作将它们与角色数据合并。\n\n23.4.3 Deeply nested\nWe’ll finish off these case studies with a list-column that’s very deeply nested and requires repeated rounds of unnest_wider() and unnest_longer() to unravel: gmaps_cities. This is a two column tibble containing five city names and the results of using Google’s geocoding API to determine their location:\n我们将通过一个深度嵌套的列表列来结束这些案例研究，它需要反复调用 unnest_wider() 和 unnest_longer() 来解开：gmaps_cities。这是一个两列的 tibble，包含五个城市名称以及使用谷歌的地理编码 API 来确定其位置的结果：\n\ngmaps_cities\n#&gt; # A tibble: 5 × 2\n#&gt;   city       json            \n#&gt;   &lt;chr&gt;      &lt;list&gt;          \n#&gt; 1 Houston    &lt;named list [2]&gt;\n#&gt; 2 Washington &lt;named list [2]&gt;\n#&gt; 3 New York   &lt;named list [2]&gt;\n#&gt; 4 Chicago    &lt;named list [2]&gt;\n#&gt; 5 Arlington  &lt;named list [2]&gt;\n\njson is a list-column with internal names, so we start with an unnest_wider():json 是一个带有内部名称的列表列，所以我们从 unnest_wider() 开始：\n\ngmaps_cities |&gt; \n  unnest_wider(json)\n#&gt; # A tibble: 5 × 3\n#&gt;   city       results    status\n#&gt;   &lt;chr&gt;      &lt;list&gt;     &lt;chr&gt; \n#&gt; 1 Houston    &lt;list [1]&gt; OK    \n#&gt; 2 Washington &lt;list [2]&gt; OK    \n#&gt; 3 New York   &lt;list [1]&gt; OK    \n#&gt; 4 Chicago    &lt;list [1]&gt; OK    \n#&gt; 5 Arlington  &lt;list [2]&gt; OK\n\nThis gives us the status and the results. We’ll drop the status column since they’re all OK; in a real analysis, you’d also want to capture all the rows where status != \"OK\" and figure out what went wrong. results is an unnamed list, with either one or two elements (we’ll see why shortly) so we’ll unnest it into rows:\n这样我们得到了 status 和 results。我们将丢弃 status 列，因为它们的值都是 OK；在实际分析中，你还需要捕获所有 status != \"OK\" 的行，并找出问题所在。results 是一个未命名的列表，包含一个或两个元素（我们很快会看到原因），所以我们将其展开为多行：\n\ngmaps_cities |&gt; \n  unnest_wider(json) |&gt; \n  select(-status) |&gt; \n  unnest_longer(results)\n#&gt; # A tibble: 7 × 2\n#&gt;   city       results         \n#&gt;   &lt;chr&gt;      &lt;list&gt;          \n#&gt; 1 Houston    &lt;named list [5]&gt;\n#&gt; 2 Washington &lt;named list [5]&gt;\n#&gt; 3 Washington &lt;named list [5]&gt;\n#&gt; 4 New York   &lt;named list [5]&gt;\n#&gt; 5 Chicago    &lt;named list [5]&gt;\n#&gt; 6 Arlington  &lt;named list [5]&gt;\n#&gt; # ℹ 1 more row\n\nNow results is a named list, so we’ll use unnest_wider():\n现在 results 是一个命名列表，所以我们将使用 unnest_wider()：\n\nlocations &lt;- gmaps_cities |&gt; \n  unnest_wider(json) |&gt; \n  select(-status) |&gt; \n  unnest_longer(results) |&gt; \n  unnest_wider(results)\nlocations\n#&gt; # A tibble: 7 × 6\n#&gt;   city       address_components formatted_address   geometry        \n#&gt;   &lt;chr&gt;      &lt;list&gt;             &lt;chr&gt;               &lt;list&gt;          \n#&gt; 1 Houston    &lt;list [4]&gt;         Houston, TX, USA    &lt;named list [4]&gt;\n#&gt; 2 Washington &lt;list [2]&gt;         Washington, USA     &lt;named list [4]&gt;\n#&gt; 3 Washington &lt;list [4]&gt;         Washington, DC, USA &lt;named list [4]&gt;\n#&gt; 4 New York   &lt;list [3]&gt;         New York, NY, USA   &lt;named list [4]&gt;\n#&gt; 5 Chicago    &lt;list [4]&gt;         Chicago, IL, USA    &lt;named list [4]&gt;\n#&gt; 6 Arlington  &lt;list [4]&gt;         Arlington, TX, USA  &lt;named list [4]&gt;\n#&gt; # ℹ 1 more row\n#&gt; # ℹ 2 more variables: place_id &lt;chr&gt;, types &lt;list&gt;\n\nNow we can see why two cities got two results: Washington matched both Washington state and Washington, DC, and Arlington matched Arlington, Virginia and Arlington, Texas.\n现在我们可以看到为什么有两个城市得到了两个结果：华盛顿 (Washington) 匹配了华盛顿州和华盛顿特区，而阿灵顿 (Arlington) 匹配了弗吉尼亚州的阿灵顿和德克萨斯州的阿灵顿。\nThere are a few different places we could go from here. We might want to determine the exact location of the match, which is stored in the geometry list-column:\n从这里开始，我们可以有几个不同的方向。我们可能想确定匹配的精确位置，它存储在 geometry 列表列中：\n\nlocations |&gt; \n  select(city, formatted_address, geometry) |&gt; \n  unnest_wider(geometry)\n#&gt; # A tibble: 7 × 6\n#&gt;   city       formatted_address   bounds           location     location_type\n#&gt;   &lt;chr&gt;      &lt;chr&gt;               &lt;list&gt;           &lt;list&gt;       &lt;chr&gt;        \n#&gt; 1 Houston    Houston, TX, USA    &lt;named list [2]&gt; &lt;named list&gt; APPROXIMATE  \n#&gt; 2 Washington Washington, USA     &lt;named list [2]&gt; &lt;named list&gt; APPROXIMATE  \n#&gt; 3 Washington Washington, DC, USA &lt;named list [2]&gt; &lt;named list&gt; APPROXIMATE  \n#&gt; 4 New York   New York, NY, USA   &lt;named list [2]&gt; &lt;named list&gt; APPROXIMATE  \n#&gt; 5 Chicago    Chicago, IL, USA    &lt;named list [2]&gt; &lt;named list&gt; APPROXIMATE  \n#&gt; 6 Arlington  Arlington, TX, USA  &lt;named list [2]&gt; &lt;named list&gt; APPROXIMATE  \n#&gt; # ℹ 1 more row\n#&gt; # ℹ 1 more variable: viewport &lt;list&gt;\n\nThat gives us new bounds (a rectangular region) and location (a point). We can unnest location to see the latitude (lat) and longitude (lng):\n这样就得到了新的 bounds（一个矩形区域）和 location（一个点）。我们可以展开 location 来查看纬度 (lat) 和经度 (lng)：\n\nlocations |&gt; \n  select(city, formatted_address, geometry) |&gt; \n  unnest_wider(geometry) |&gt; \n  unnest_wider(location)\n#&gt; # A tibble: 7 × 7\n#&gt;   city       formatted_address   bounds             lat    lng location_type\n#&gt;   &lt;chr&gt;      &lt;chr&gt;               &lt;list&gt;           &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;        \n#&gt; 1 Houston    Houston, TX, USA    &lt;named list [2]&gt;  29.8  -95.4 APPROXIMATE  \n#&gt; 2 Washington Washington, USA     &lt;named list [2]&gt;  47.8 -121.  APPROXIMATE  \n#&gt; 3 Washington Washington, DC, USA &lt;named list [2]&gt;  38.9  -77.0 APPROXIMATE  \n#&gt; 4 New York   New York, NY, USA   &lt;named list [2]&gt;  40.7  -74.0 APPROXIMATE  \n#&gt; 5 Chicago    Chicago, IL, USA    &lt;named list [2]&gt;  41.9  -87.6 APPROXIMATE  \n#&gt; 6 Arlington  Arlington, TX, USA  &lt;named list [2]&gt;  32.7  -97.1 APPROXIMATE  \n#&gt; # ℹ 1 more row\n#&gt; # ℹ 1 more variable: viewport &lt;list&gt;\n\nExtracting the bounds requires a few more steps:\n提取边界需要更多几个步骤：\n\nlocations |&gt; \n  select(city, formatted_address, geometry) |&gt; \n  unnest_wider(geometry) |&gt; \n  # focus on the variables of interest\n  select(!location:viewport) |&gt;\n  unnest_wider(bounds)\n#&gt; # A tibble: 7 × 4\n#&gt;   city       formatted_address   northeast        southwest       \n#&gt;   &lt;chr&gt;      &lt;chr&gt;               &lt;list&gt;           &lt;list&gt;          \n#&gt; 1 Houston    Houston, TX, USA    &lt;named list [2]&gt; &lt;named list [2]&gt;\n#&gt; 2 Washington Washington, USA     &lt;named list [2]&gt; &lt;named list [2]&gt;\n#&gt; 3 Washington Washington, DC, USA &lt;named list [2]&gt; &lt;named list [2]&gt;\n#&gt; 4 New York   New York, NY, USA   &lt;named list [2]&gt; &lt;named list [2]&gt;\n#&gt; 5 Chicago    Chicago, IL, USA    &lt;named list [2]&gt; &lt;named list [2]&gt;\n#&gt; 6 Arlington  Arlington, TX, USA  &lt;named list [2]&gt; &lt;named list [2]&gt;\n#&gt; # ℹ 1 more row\n\nWe then rename southwest and northeast (the corners of the rectangle) so we can use names_sep to create short but evocative names:\n然后我们重命名 southwest 和 northeast（矩形的角点），这样我们就可以使用 names_sep 来创建简短但富有表现力的名称：\n\nlocations |&gt; \n  select(city, formatted_address, geometry) |&gt; \n  unnest_wider(geometry) |&gt; \n  select(!location:viewport) |&gt;\n  unnest_wider(bounds) |&gt; \n  rename(ne = northeast, sw = southwest) |&gt; \n  unnest_wider(c(ne, sw), names_sep = \"_\") \n#&gt; # A tibble: 7 × 6\n#&gt;   city       formatted_address   ne_lat ne_lng sw_lat sw_lng\n#&gt;   &lt;chr&gt;      &lt;chr&gt;                &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 Houston    Houston, TX, USA      30.1  -95.0   29.5  -95.8\n#&gt; 2 Washington Washington, USA       49.0 -117.    45.5 -125. \n#&gt; 3 Washington Washington, DC, USA   39.0  -76.9   38.8  -77.1\n#&gt; 4 New York   New York, NY, USA     40.9  -73.7   40.5  -74.3\n#&gt; 5 Chicago    Chicago, IL, USA      42.0  -87.5   41.6  -87.9\n#&gt; 6 Arlington  Arlington, TX, USA    32.8  -97.0   32.6  -97.2\n#&gt; # ℹ 1 more row\n\nNote how we unnest two columns simultaneously by supplying a vector of variable names to unnest_wider().\n注意我们是如何通过向 unnest_wider() 提供一个变量名向量来同时展开两列的。\nOnce you’ve discovered the path to get to the components you’re interested in, you can extract them directly using another tidyr function, hoist():\n一旦你找到了获取感兴趣组件的路径，你就可以使用 tidyr 的另一个函数 hoist() 直接提取它们：\n\nlocations |&gt; \n  select(city, formatted_address, geometry) |&gt; \n  hoist(\n    geometry,\n    ne_lat = c(\"bounds\", \"northeast\", \"lat\"),\n    sw_lat = c(\"bounds\", \"southwest\", \"lat\"),\n    ne_lng = c(\"bounds\", \"northeast\", \"lng\"),\n    sw_lng = c(\"bounds\", \"southwest\", \"lng\"),\n  )\n\nIf these case studies have whetted your appetite for more real-life rectangling, you can see a few more examples in vignette(\"rectangling\", package = \"tidyr\").\n如果这些案例研究激发了你对更多现实世界中数据规整化的兴趣，你可以在 vignette(\"rectangling\", package = \"tidyr\") 中看到更多示例。\n\n23.4.4 Exercises\n\nRoughly estimate when gh_repos was created. Why can you only roughly estimate the date?\nThe owner column of gh_repo contains a lot of duplicated information because each owner can have many repos. Can you construct an owners data frame that contains one row for each owner? (Hint: does distinct() work with list-cols?)\nFollow the steps used for titles to create similar tables for the aliases, allegiances, books, and TV series for the Game of Thrones characters.\n\nExplain the following code line-by-line. Why is it interesting? Why does it work for got_chars but might not work in general?\n\ntibble(json = got_chars) |&gt; \n  unnest_wider(json) |&gt; \n  select(id, where(is.list)) |&gt; \n  pivot_longer(\n    where(is.list), \n    names_to = \"name\", \n    values_to = \"value\"\n  ) |&gt;  \n  unnest_longer(value)\n\n\nIn gmaps_cities, what does address_components contain? Why does the length vary between rows? Unnest it appropriately to figure it out. (Hint: types always appears to contain two elements. Does unnest_wider() make it easier to work with than unnest_longer()?) .",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Hierarchical data</span>"
    ]
  },
  {
    "objectID": "rectangling.html#json",
    "href": "rectangling.html#json",
    "title": "23  Hierarchical data",
    "section": "\n23.5 JSON",
    "text": "23.5 JSON\nAll of the case studies in the previous section were sourced from wild-caught JSON. JSON is short for javascript object notation and is the way that most web APIs return data. It’s important to understand it because while JSON and R’s data types are pretty similar, there isn’t a perfect 1-to-1 mapping, so it’s good to understand a bit about JSON if things go wrong.\n上一节中的所有案例研究都源于从网络上获取的 JSON 数据。JSON 是 javascript object notation（JavaScript 对象表示法）的缩写，是大多数 Web API 返回数据的方式。理解它很重要，因为虽然 JSON 和 R 的数据类型非常相似，但它们之间并非完美的 1 对 1 映射，所以如果出现问题，对 JSON 有所了解会很有帮助。\n\n23.5.1 Data types\nJSON is a simple format designed to be easily read and written by machines, not humans. It has six key data types. Four of them are scalars:\nJSON 是一种简单的格式，设计用于机器的轻松读写，而非人类。它有六种关键的数据类型。其中四种是标量：\n\nThe simplest type is a null (null) which plays the same role as NA in R. It represents the absence of data.\n最简单的类型是空值（null），它扮演着与 R 中 NA 相同的角色。它表示数据的缺失。\nA string is much like a string in R, but must always use double quotes. 字符串 (string) 很像 R 中的字符串，但必须始终使用双引号。\nA number is similar to R’s numbers: they can use integer (e.g., 123), decimal (e.g., 123.45), or scientific (e.g., 1.23e3) notation. JSON doesn’t support Inf, -Inf, or NaN.数字 (number) 类似于 R 中的数字：它们可以使用整数（例如，123）、小数（例如，123.45）或科学（例如，1.23e3）记数法。JSON 不支持 Inf、-Inf 或 NaN。\nA boolean is similar to R’s TRUE and FALSE, but uses lowercase true and false.布尔值 (boolean) 类似于 R 的 TRUE 和 FALSE，但使用小写的 true 和 false。\n\nJSON’s strings, numbers, and booleans are pretty similar to R’s character, numeric, and logical vectors. The main difference is that JSON’s scalars can only represent a single value. To represent multiple values you need to use one of the two remaining types: arrays and objects.\nJSON 的字符串、数字和布尔值与 R 的字符、数值和逻辑向量非常相似。主要区别在于 JSON 的标量只能表示单个值。要表示多个值，你需要使用剩下的两种类型之一：数组和对象。\nBoth arrays and objects are similar to lists in R; the difference is whether or not they’re named. An array is like an unnamed list, and is written with []. For example [1, 2, 3] is an array containing 3 numbers, and [null, 1, \"string\", false] is an array that contains a null, a number, a string, and a boolean. An object is like a named list, and is written with {}. The names (keys in JSON terminology) are strings, so must be surrounded by quotes. For example, {\"x\": 1, \"y\": 2} is an object that maps x to 1 and y to 2.\n数组和对象都类似于 R 中的列表；区别在于它们是否有名称。 数组 (array) 就像一个未命名的列表，用 [] 书写。例如 [1, 2, 3] 是一个包含 3 个数字的数组，而 [null, 1, \"string\", false] 是一个包含空值、数字、字符串和布尔值的数组。 对象 (object) 就像一个命名列表，用 {} 书写。名称（在 JSON 术语中称为键 (keys)）是字符串，因此必须用引号括起来。例如，{\"x\": 1, \"y\": 2} 是一个将 x 映射到 1，y 映射到 2 的对象。\nNote that JSON doesn’t have any native way to represent dates or date-times, so they’re often stored as strings, and you’ll need to use readr::parse_date() or readr::parse_datetime() to turn them into the correct data structure. Similarly, JSON’s rules for representing floating point numbers in JSON are a little imprecise, so you’ll also sometimes find numbers stored in strings. Apply readr::parse_double() as needed to get the correct variable type.\n请注意，JSON 没有任何原生方式来表示日期或日期时间，因此它们通常以字符串形式存储，你需要使用 readr::parse_date() 或 readr::parse_datetime() 将它们转换为正确的数据结构。同样，JSON 表示浮点数的规则有些不精确，所以你有时也会发现数字以字符串形式存储。需要时，应用 readr::parse_double() 以获取正确的变量类型。\n\n23.5.2 jsonlite\nTo convert JSON into R data structures, we recommend the jsonlite package, by Jeroen Ooms. We’ll use only two jsonlite functions: read_json() and parse_json(). In real life, you’ll use read_json() to read a JSON file from disk. For example, the repurrsive package also provides the source for gh_user as a JSON file and you can read it with read_json():\n要将 JSON 转换为 R 数据结构，我们推荐 Jeroen Ooms 开发的 jsonlite 包。我们将只使用两个 jsonlite 函数：read_json() 和 parse_json()。在实际应用中，你会使用 read_json() 从磁盘读取 JSON 文件。例如，repurrsive 包也以 JSON 文件的形式提供了 gh_user 的源数据，你可以用 read_json() 读取它：\n\n# A path to a json file inside the package:\ngh_users_json()\n#&gt; [1] \"C:/Users/14913/AppData/Local/R/win-library/4.5/repurrrsive/extdata/gh_users.json\"\n\n# Read it with read_json()\ngh_users2 &lt;- read_json(gh_users_json())\n\n# Check it's the same as the data we were using previously\nidentical(gh_users, gh_users2)\n#&gt; [1] TRUE\n\nIn this book, we’ll also use parse_json(), since it takes a string containing JSON, which makes it good for generating simple examples. To get started, here are three simple JSON datasets, starting with a number, then putting a few numbers in an array, then putting that array in an object:\n在本书中，我们也会使用 parse_json()，因为它接受包含 JSON 的字符串，这使得它很适合生成简单的示例。作为开始，这里有三个简单的 JSON 数据集，从一个数字开始，然后将几个数字放入一个数组，再将该数组放入一个对象中：\n\nstr(parse_json('1'))\n#&gt;  int 1\nstr(parse_json('[1, 2, 3]'))\n#&gt; List of 3\n#&gt;  $ : int 1\n#&gt;  $ : int 2\n#&gt;  $ : int 3\nstr(parse_json('{\"x\": [1, 2, 3]}'))\n#&gt; List of 1\n#&gt;  $ x:List of 3\n#&gt;   ..$ : int 1\n#&gt;   ..$ : int 2\n#&gt;   ..$ : int 3\n\njsonlite has another important function called fromJSON(). We don’t use it here because it performs automatic simplification (simplifyVector = TRUE). This often works well, particularly in simple cases, but we think you’re better off doing the rectangling yourself so you know exactly what’s happening and can more easily handle the most complicated nested structures.\njsonlite 还有另一个重要的函数叫做 fromJSON()。我们在这里不使用它，因为它会执行自动简化 (simplifyVector = TRUE)。这在简单情况下通常效果很好，但我们认为最好还是自己进行数据规整化，这样你才能确切地知道发生了什么，并能更容易地处理最复杂的嵌套结构。\n\n23.5.3 Starting the rectangling process\nIn most cases, JSON files contain a single top-level array, because they’re designed to provide data about multiple “things”, e.g., multiple pages, or multiple records, or multiple results. In this case, you’ll start your rectangling with tibble(json) so that each element becomes a row:\n在大多数情况下，JSON 文件包含一个顶层数组，因为它们旨在提供关于多个“事物”的数据，例如多个页面、多个记录或多个结果。在这种情况下，你将通过 tibble(json) 开始你的数据规整化过程，使每个元素都成为一行：\n\njson &lt;- '[\n  {\"name\": \"John\", \"age\": 34},\n  {\"name\": \"Susan\", \"age\": 27}\n]'\ndf &lt;- tibble(json = parse_json(json))\ndf\n#&gt; # A tibble: 2 × 1\n#&gt;   json            \n#&gt;   &lt;list&gt;          \n#&gt; 1 &lt;named list [2]&gt;\n#&gt; 2 &lt;named list [2]&gt;\n\ndf |&gt; \n  unnest_wider(json)\n#&gt; # A tibble: 2 × 2\n#&gt;   name    age\n#&gt;   &lt;chr&gt; &lt;int&gt;\n#&gt; 1 John     34\n#&gt; 2 Susan    27\n\nIn rarer cases, the JSON file consists of a single top-level JSON object, representing one “thing”. In this case, you’ll need to kick off the rectangling process by wrapping it in a list, before you put it in a tibble.\n在较少见的情况下，JSON 文件由一个顶层 JSON 对象组成，代表一个“事物”。在这种情况下，在将其放入 tibble 之前，你需要通过将其包装在列表中来启动数据规整化过程。\n\njson &lt;- '{\n  \"status\": \"OK\", \n  \"results\": [\n    {\"name\": \"John\", \"age\": 34},\n    {\"name\": \"Susan\", \"age\": 27}\n ]\n}\n'\ndf &lt;- tibble(json = list(parse_json(json)))\ndf\n#&gt; # A tibble: 1 × 1\n#&gt;   json            \n#&gt;   &lt;list&gt;          \n#&gt; 1 &lt;named list [2]&gt;\n\ndf |&gt; \n  unnest_wider(json) |&gt; \n  unnest_longer(results) |&gt; \n  unnest_wider(results)\n#&gt; # A tibble: 2 × 3\n#&gt;   status name    age\n#&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;int&gt;\n#&gt; 1 OK     John     34\n#&gt; 2 OK     Susan    27\n\nAlternatively, you can reach inside the parsed JSON and start with the bit that you actually care about:\n或者，你可以深入解析后的 JSON 内部，从你真正关心的部分开始：\n\ndf &lt;- tibble(results = parse_json(json)$results)\ndf |&gt; \n  unnest_wider(results)\n#&gt; # A tibble: 2 × 2\n#&gt;   name    age\n#&gt;   &lt;chr&gt; &lt;int&gt;\n#&gt; 1 John     34\n#&gt; 2 Susan    27\n\n\n23.5.4 Exercises\n\n\nRectangle the df_col and df_row below. They represent the two ways of encoding a data frame in JSON.\n\njson_col &lt;- parse_json('\n  {\n    \"x\": [\"a\", \"x\", \"z\"],\n    \"y\": [10, null, 3]\n  }\n')\njson_row &lt;- parse_json('\n  [\n    {\"x\": \"a\", \"y\": 10},\n    {\"x\": \"x\", \"y\": null},\n    {\"x\": \"z\", \"y\": 3}\n  ]\n')\n\ndf_col &lt;- tibble(json = list(json_col)) \ndf_row &lt;- tibble(json = json_row)",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Hierarchical data</span>"
    ]
  },
  {
    "objectID": "rectangling.html#summary",
    "href": "rectangling.html#summary",
    "title": "23  Hierarchical data",
    "section": "\n23.6 Summary",
    "text": "23.6 Summary\nIn this chapter, you learned what lists are, how you can generate them from JSON files, and how to turn them into rectangular data frames. Surprisingly we only need two new functions: unnest_longer() to put list elements into rows and unnest_wider() to put list elements into columns. It doesn’t matter how deeply nested the list-column is; all you need to do is repeatedly call these two functions.\n在本章中，你学习了什么是列表，如何从 JSON 文件生成它们，以及如何将它们转换为规整的数据框。令人惊讶的是，我们只需要两个新函数：unnest_longer() 用于将列表元素放入行，unnest_wider() 用于将列表元素放入列。列表列的嵌套深度无关紧要；你所需要做的就是重复调用这两个函数。\nJSON is the most common data format returned by web APIs. What happens if the website doesn’t have an API, but you can see data you want on the website? That’s the topic of the next chapter: web scraping, extracting data from HTML webpages.\nJSON 是 Web API 返回的最常见的数据格式。如果网站没有 API，但你可以在网站上看到你想要的数据，那该怎么办？这就是下一章的主题：网页抓取，即从 HTML 网页中提取数据。",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Hierarchical data</span>"
    ]
  },
  {
    "objectID": "rectangling.html#footnotes",
    "href": "rectangling.html#footnotes",
    "title": "23  Hierarchical data",
    "section": "",
    "text": "This is an RStudio feature.↩︎",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Hierarchical data</span>"
    ]
  },
  {
    "objectID": "webscraping.html",
    "href": "webscraping.html",
    "title": "24  Web scraping",
    "section": "",
    "text": "24.1 Introduction\nThis chapter introduces you to the basics of web scraping with rvest. Web scraping is a very useful tool for extracting data from web pages. Some websites will offer an API, a set of structured HTTP requests that return data as JSON, which you handle using the techniques from Chapter 23. Where possible, you should use the API1, because typically it will give you more reliable data. Unfortunately, however, programming with web APIs is out of scope for this book. Instead, we are teaching scraping, a technique that works whether or not a site provides an API.\n本章将向您介绍使用 rvest 进行网页抓取的基础知识。网页抓取（Web scraping）是从网页中提取数据的非常有用的工具。有些网站会提供 API，这是一组结构化的 HTTP 请求，以 JSON 格式返回数据，您可以使用 Chapter 23 中的技术来处理这些数据。在可能的情况下，您应该使用 API 1，因为它通常会为您提供更可靠的数据。然而，不幸的是，使用 Web API 进行编程超出了本书的范围。因此，我们教授的是抓取技术，无论网站是否提供 API，这种技术都适用。\nIn this chapter, we’ll first discuss the ethics and legalities of scraping before we dive into the basics of HTML. You’ll then learn the basics of CSS selectors to locate specific elements on the page, and how to use rvest functions to get data from text and attributes out of HTML and into R. We’ll then discuss some techniques to figure out what CSS selector you need for the page you’re scraping, before finishing up with a couple of case studies, and a brief discussion of dynamic websites.\n在本章中，我们将在深入探讨 HTML 基础知识之前，首先讨论抓取的道德和法律问题。然后，您将学习 CSS 选择器的基础知识，以在页面上定位特定元素，以及如何使用 rvest 函数从 HTML 的文本和属性中提取数据并导入 R。接着，我们将讨论一些技巧，以确定您需要为正在抓取的页面使用哪种 CSS 选择器，最后通过几个案例研究，并简要讨论动态网站。",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Web scraping</span>"
    ]
  },
  {
    "objectID": "webscraping.html#introduction",
    "href": "webscraping.html#introduction",
    "title": "24  Web scraping",
    "section": "",
    "text": "24.1.1 Prerequisites\nIn this chapter, we’ll focus on tools provided by rvest. rvest is a member of the tidyverse, but is not a core member so you’ll need to load it explicitly. We’ll also load the full tidyverse since we’ll find it generally useful working with the data we’ve scraped.\n在本章中，我们将重点介绍 rvest 提供的工具。rvest 是 tidyverse 的成员，但不是核心成员，因此您需要显式加载它。我们还将加载完整的 tidyverse，因为在处理我们抓取的数据时，它通常会很有用。\n\nlibrary(tidyverse)\nlibrary(rvest)",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Web scraping</span>"
    ]
  },
  {
    "objectID": "webscraping.html#scraping-ethics-and-legalities",
    "href": "webscraping.html#scraping-ethics-and-legalities",
    "title": "24  Web scraping",
    "section": "\n24.2 Scraping ethics and legalities",
    "text": "24.2 Scraping ethics and legalities\nBefore we get started discussing the code you’ll need to perform web scraping, we need to talk about whether it’s legal and ethical for you to do so. Overall, the situation is complicated with regards to both of these.\n在我们开始讨论执行网页抓取所需的代码之前，我们需要谈谈这样做是否合法和道德。总的来说，在这两个方面，情况都很复杂。\nLegalities depend a lot on where you live. However, as a general principle, if the data is public, non-personal, and factual, you’re likely to be ok2. These three factors are important because they’re connected to the site’s terms and conditions, personally identifiable information, and copyright, as we’ll discuss below.\n合法性在很大程度上取决于您居住的地方。然而，作为一般原则，如果数据是公开的、非个人的和事实性的，您很可能是可以的2。这三个因素很重要，因为它们与网站的服务条款、个人身份信息和版权有关，我们将在下面讨论。\nIf the data isn’t public, non-personal, or factual or you’re scraping the data specifically to make money with it, you’ll need to talk to a lawyer. In any case, you should be respectful of the resources of the server hosting the pages you are scraping. Most importantly, this means that if you’re scraping many pages, you should make sure to wait a little between each request. One easy way to do so is to use the polite package by Dmytro Perepolkin. It will automatically pause between requests and cache the results so you never ask for the same page twice.\n如果数据不是公开的、非个人的或事实性的，或者您抓取数据是专门为了赚钱，那么您需要咨询律师。在任何情况下，您都应该尊重托管您正在抓取的页面的服务器资源。最重要的是，这意味着如果您要抓取许多页面，您应该确保在每个请求之间稍作等待。一个简单的方法是使用 Dmytro Perepolkin 的 polite 包。它会自动在请求之间暂停，并缓存结果，这样您就永远不会两次请求同一个页面。\n\n24.2.1 Terms of service\nIf you look closely, you’ll find many websites include a “terms and conditions” or “terms of service” link somewhere on the page, and if you read that page closely you’ll often discover that the site specifically prohibits web scraping. These pages tend to be a legal land grab where companies make very broad claims. It’s polite to respect these terms of service where possible, but take any claims with a grain of salt.\n如果您仔细查看，您会发现许多网站在页面的某个地方包含“条款和条件”或“服务条款”的链接，如果您仔细阅读该页面，您通常会发现该网站明确禁止网页抓取。这些页面往往是公司提出非常宽泛主张的法律圈地。在可能的情况下，尊重这些服务条款是礼貌的，但对任何主张都要持保留态度。\nUS courts have generally found that simply putting the terms of service in the footer of the website isn’t sufficient for you to be bound by them, e.g., HiQ Labs v. LinkedIn. Generally, to be bound to the terms of service, you must have taken some explicit action like creating an account or checking a box. This is why whether or not the data is public is important; if you don’t need an account to access them, it is unlikely that you are bound to the terms of service. Note, however, the situation is rather different in Europe where courts have found that terms of service are enforceable even if you don’t explicitly agree to them.\n美国法院通常认为，仅仅将服务条款放在网站页脚并不足以使您受其约束，例如 HiQ Labs v. LinkedIn。通常，要受服务条款的约束，您必须采取一些明确的行动，如创建帐户或勾选复选框。这就是为什么数据是否公开很重要的原因；如果您不需要帐户即可访问它们，那么您就不太可能受服务条款的约束。但请注意，在欧洲情况有所不同，法院认为即使您没有明确同意，服务条款也是可强制执行的。\n\n24.2.2 Personally identifiable information\nEven if the data is public, you should be extremely careful about scraping personally identifiable information like names, email addresses, phone numbers, dates of birth, etc. Europe has particularly strict laws about the collection or storage of such data (GDPR), and regardless of where you live you’re likely to be entering an ethical quagmire. For example, in 2016, a group of researchers scraped public profile information (e.g., usernames, age, gender, location, etc.) about 70,000 people on the dating site OkCupid and they publicly released these data without any attempts for anonymization. While the researchers felt that there was nothing wrong with this since the data were already public, this work was widely condemned due to ethics concerns around identifiability of users whose information was released in the dataset. If your work involves scraping personally identifiable information, we strongly recommend reading about the OkCupid study3 as well as similar studies with questionable research ethics involving the acquisition and release of personally identifiable information.\n即使数据是公开的，您在抓取姓名、电子邮件地址、电话号码、出生日期等个人身份信息时也应极其谨慎。欧洲对这类数据的收集或存储有特别严格的法律（GDPR），无论您住在哪里，您都可能陷入道德困境。例如，2016年，一群研究人员从交友网站 OkCupid 上抓取了约 70,000 人的公开个人资料信息（如用户名、年龄、性别、地点等），并在未进行任何匿名化处理的情况下公开发布了这些数据。尽管研究人员认为这样做没有错，因为数据已经是公开的，但这项工作因涉及数据集中用户信息的可识别性而受到广泛谴责。如果您的工作涉及抓取个人身份信息，我们强烈建议您阅读关于 OkCupid 研究的资料3，以及涉及获取和发布个人身份信息存在可疑研究伦理的类似研究。\n\n24.2.3 Copyright\nFinally, you also need to worry about copyright law. Copyright law is complicated, but it’s worth taking a look at the US law which describes exactly what’s protected: “[…] original works of authorship fixed in any tangible medium of expression, […]”. It then goes on to describe specific categories that it applies like literary works, musical works, motion pictures and more. Notably absent from copyright protection are data. This means that as long as you limit your scraping to facts, copyright protection does not apply. (But note that Europe has a separate “sui generis” right that protects databases.)\n最后，您还需要担心版权法。版权法很复杂，但值得一看的是美国法律，它确切地描述了受保护的内容：“[…] 固定在任何有形表达媒介上的原创作者作品 […]”。然后它继续描述了其适用的具体类别，如文学作品、音乐作品、电影等。值得注意的是，数据不在版权保护之列。这意味着，只要您将抓取范围限制在事实上，版权保护就不适用。（但请注意，欧洲有一项单独的“数据库权”（sui generis）来保护数据库。）\nAs a brief example, in the US, lists of ingredients and instructions are not copyrightable, so copyright can not be used to protect a recipe. But if that list of recipes is accompanied by substantial novel literary content, that is copyrightable. This is why when you’re looking for a recipe on the internet there’s always so much content beforehand.\n举个简短的例子，在美国，配料清单和说明是不受版权保护的，因此版权不能用来保护食谱。但如果那份食谱清单附有大量新颖的文学内容，那么这些内容就是受版权保护的。这就是为什么当您在网上查找食谱时，总会先看到那么多内容。\nIf you do need to scrape original content (like text or images), you may still be protected under the doctrine of fair use. Fair use is not a hard and fast rule, but weighs up a number of factors. It’s more likely to apply if you are collecting the data for research or non-commercial purposes and if you limit what you scrape to just what you need.\n如果您确实需要抓取原创内容（如文本或图像），您可能仍然受到合理使用原则的保护。合理使用不是一成不变的规则，而是权衡多种因素的结果。如果您是为研究或非商业目的收集数据，并且只抓取您需要的内容，那么它就更有可能适用。",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Web scraping</span>"
    ]
  },
  {
    "objectID": "webscraping.html#html-basics",
    "href": "webscraping.html#html-basics",
    "title": "24  Web scraping",
    "section": "\n24.3 HTML basics",
    "text": "24.3 HTML basics\nTo scrape webpages, you need to first understand a little bit about HTML, the language that describes web pages. HTML stands for HyperText Markup Language and looks something like this:\n要抓取网页，您首先需要了解一些关于 HTML 的知识，这是一种描述网页的语言。HTML 是超文本标记语言（HyperText Markup Language）的缩写，看起来像这样：\n&lt;html&gt;\n&lt;head&gt;\n  &lt;title&gt;Page title&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1 id='first'&gt;A heading&lt;/h1&gt;\n  &lt;p&gt;Some text &amp; &lt;b&gt;some bold text.&lt;/b&gt;&lt;/p&gt;\n  &lt;img src='myimg.png' width='100' height='100'&gt;\n&lt;/body&gt;\nHTML has a hierarchical structure formed by elements which consist of a start tag (e.g., &lt;tag&gt;), optional attributes (id='first'), an end tag4 (like &lt;/tag&gt;), and contents (everything in between the start and end tag).\nHTML 具有由元素（elements）构成的层次结构，元素由开始标签（例如 &lt;tag&gt;）、可选的属性（attributes）（例如 id='first'）、结束标签4（例如 &lt;/tag&gt;）和内容（contents）（开始和结束标签之间的所有内容）组成。\nSince &lt; and &gt; are used for start and end tags, you can’t write them directly. Instead you have to use the HTML escapes &gt; (greater than) and &lt; (less than). And since those escapes use &, if you want a literal ampersand you have to escape it as &amp;. There are a wide range of possible HTML escapes but you don’t need to worry about them too much because rvest automatically handles them for you.\n由于 &lt; 和 &gt; 用于开始和结束标签，因此不能直接书写它们。您必须使用 HTML 转义符（escapes），即 &gt;（大于）和 &lt;（小于）。又因为这些转义符使用了 &，所以如果您想表示一个字面意义上的与号（ampersand），就必须将其转义为 &amp;。HTML 有很多种可能的转义符，但您不必太过担心，因为 rvest 会自动为您处理。\nWeb scraping is possible because most pages that contain data that you want to scrape generally have a consistent structure.\n网页抓取之所以可行，是因为大多数包含您想要抓取数据地网页通常具有一致的结构。\n\n24.3.1 Elements\nThere are over 100 HTML elements. Some of the most important are:\nHTML 元素超过 100 种。其中一些最重要的包括：\n\nEvery HTML page must be in an &lt;html&gt; element, and it must have two children: &lt;head&gt;, which contains document metadata like the page title, and &lt;body&gt;, which contains the content you see in the browser.\n每个 HTML 页面都必须位于一个 &lt;html&gt; 元素中，并且它必须有两个子元素：&lt;head&gt;，包含文档元数据，如页面标题；以及 &lt;body&gt;，包含您在浏览器中看到的内容。\nBlock tags like &lt;h1&gt; (heading 1), &lt;section&gt; (section), &lt;p&gt; (paragraph), and &lt;ol&gt; (ordered list) form the overall structure of the page.\n块级标签，如 &lt;h1&gt;（一级标题）、&lt;section&gt;（节）、&lt;p&gt;（段落）和 &lt;ol&gt;（有序列表），构成了页面的整体结构。\nInline tags like &lt;b&gt; (bold), &lt;i&gt; (italics), and &lt;a&gt; (link) format text inside block tags.\n内联标签，如 &lt;b&gt;（粗体）、&lt;i&gt;（斜体）和 &lt;a&gt;（链接），用于格式化块级标签内的文本。\n\nIf you encounter a tag that you’ve never seen before, you can find out what it does with a little googling. Another good place to start are the MDN Web Docs which describe just about every aspect of web programming.\n如果您遇到一个从未见过的标签，可以通过 Google 搜索来了解它的作用。另一个很好的起点是 MDN Web Docs，它几乎描述了网页编程的方方面面。\nMost elements can have content in between their start and end tags. This content can either be text or more elements. For example, the following HTML contains paragraph of text, with one word in bold.\n大多数元素都可以在其开始和结束标签之间包含内容。这个内容可以是文本，也可以是更多的元素。例如，下面的 HTML 包含一个文本段落，其中一个词是粗体。\n&lt;p&gt;\n  Hi! My &lt;b&gt;name&lt;/b&gt; is Hadley.\n&lt;/p&gt;\nThe children are the elements it contains, so the &lt;p&gt; element above has one child, the &lt;b&gt; element. The &lt;b&gt; element has no children, but it does have contents (the text “name”).子元素（children）是它所包含的元素，所以上面 &lt;p&gt; 元素有一个子元素，即 &lt;b&gt; 元素。&lt;b&gt; 元素没有子元素，但它有内容（文本“name”）。\n\n24.3.2 Attributes\nTags can have named attributes which look like name1='value1' name2='value2'. Two of the most important attributes are id and class, which are used in conjunction with CSS (Cascading Style Sheets) to control the visual appearance of the page. These are often useful when scraping data off a page. Attributes are also used to record the destination of links (the href attribute of &lt;a&gt; elements) and the source of images (the src attribute of the &lt;img&gt; element).\n标签可以有名为属性（attributes）的参数，其形式如 name1='value1' name2='value2'。其中最重要的两个属性是 id 和 class，它们与 CSS（层叠样式表）结合使用，以控制页面的视觉外观。在从页面抓取数据时，这些属性通常很有用。属性还用于记录链接的目的地（&lt;a&gt; 元素的 href 属性）和图像的来源（&lt;img&gt; 元素的 src 属性）。",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Web scraping</span>"
    ]
  },
  {
    "objectID": "webscraping.html#extracting-data",
    "href": "webscraping.html#extracting-data",
    "title": "24  Web scraping",
    "section": "\n24.4 Extracting data",
    "text": "24.4 Extracting data\nTo get started scraping, you’ll need the URL of the page you want to scrape, which you can usually copy from your web browser. You’ll then need to read the HTML for that page into R with read_html(). This returns an xml_document5 object which you’ll then manipulate using rvest functions:\n要开始抓取，您需要目标页面的 URL，通常可以从您的网络浏览器中复制。然后，您需要使用 read_html() 将该页面的 HTML 读入 R。这将返回一个 xml_document5 对象，您将使用 rvest 函数对其进行操作：\n\nhtml &lt;- read_html(\"http://rvest.tidyverse.org/\")\nhtml\n#&gt; {html_document}\n#&gt; &lt;html lang=\"en\"&gt;\n#&gt; [1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UT ...\n#&gt; [2] &lt;body&gt;\\n    &lt;a href=\"#container\" class=\"visually-hidden-focusable\"&gt;Ski ...\n\nrvest also includes a function that lets you write HTML inline. We’ll use this a bunch in this chapter as we teach how the various rvest functions work with simple examples.\nrvest 还包含一个允许您内联编写 HTML 的函数。在本章中，我们将大量使用它，通过简单的例子来教授各种 rvest 函数的工作方式。\n\nhtml &lt;- minimal_html(\"\n  &lt;p&gt;This is a paragraph&lt;/p&gt;\n  &lt;ul&gt;\n    &lt;li&gt;This is a bulleted list&lt;/li&gt;\n  &lt;/ul&gt;\n\")\nhtml\n#&gt; {html_document}\n#&gt; &lt;html&gt;\n#&gt; [1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UT ...\n#&gt; [2] &lt;body&gt;\\n&lt;p&gt;This is a paragraph&lt;/p&gt;\\n  &lt;ul&gt;\\n&lt;li&gt;This is a bulleted lis ...\n\nNow that you have the HTML in R, it’s time to extract the data of interest. You’ll first learn about the CSS selectors that allow you to identify the elements of interest and the rvest functions that you can use to extract data from them. Then we’ll briefly cover HTML tables, which have some special tools.\n现在您已经在 R 中获得了 HTML，是时候提取感兴趣的数据了。您将首先了解 CSS 选择器，它允许您识别感兴趣的元素，以及可以用来从中提取数据的 rvest 函数。然后我们将简要介绍 HTML 表格，它有一些特殊的工具。\n\n24.4.1 Find elements\nCSS is short for cascading style sheets, and is a tool for defining the visual styling of HTML documents. CSS includes a miniature language for selecting elements on a page called CSS selectors. CSS selectors define patterns for locating HTML elements, and are useful for scraping because they provide a concise way of describing which elements you want to extract.\nCSS 是层叠样式表（cascading style sheets）的缩写，是用于定义 HTML 文档视觉样式的工具。CSS 包含一种用于在页面上选择元素的微型语言，称为 CSS 选择器（CSS selectors）。CSS 选择器定义了定位 HTML 元素的模式，在抓取数据时非常有用，因为它们提供了一种简洁的方式来描述您想要提取的元素。\nWe’ll come back to CSS selectors in more detail in Section 24.5, but luckily you can get a long way with just three:\n我们将在 Section 24.5 中更详细地回到 CSS 选择器，但幸运的是，仅用以下三个就可以走得很远：\n\np selects all &lt;p&gt; elements.p 选择所有的 &lt;p&gt; 元素。\n.title selects all elements with class “title”..title 选择所有 class 为 “title” 的元素。\n#title selects the element with the id attribute that equals “title”. Id attributes must be unique within a document, so this will only ever select a single element.#title 选择 id 属性等于 “title” 的元素。Id 属性在文档中必须是唯一的，所以这只会选择一个元素。\n\nLet’s try out these selectors with a simple example:\n让我们用一个简单的例子来试试这些选择器：\n\nhtml &lt;- minimal_html(\"\n  &lt;h1&gt;This is a heading&lt;/h1&gt;\n  &lt;p id='first'&gt;This is a paragraph&lt;/p&gt;\n  &lt;p class='important'&gt;This is an important paragraph&lt;/p&gt;\n\")\n\nUse html_elements() to find all elements that match the selector:\n使用 html_elements() 查找所有匹配选择器的元素：\n\nhtml |&gt; html_elements(\"p\")\n#&gt; {xml_nodeset (2)}\n#&gt; [1] &lt;p id=\"first\"&gt;This is a paragraph&lt;/p&gt;\n#&gt; [2] &lt;p class=\"important\"&gt;This is an important paragraph&lt;/p&gt;\nhtml |&gt; html_elements(\".important\")\n#&gt; {xml_nodeset (1)}\n#&gt; [1] &lt;p class=\"important\"&gt;This is an important paragraph&lt;/p&gt;\nhtml |&gt; html_elements(\"#first\")\n#&gt; {xml_nodeset (1)}\n#&gt; [1] &lt;p id=\"first\"&gt;This is a paragraph&lt;/p&gt;\n\nAnother important function is html_element() which always returns the same number of outputs as inputs. If you apply it to a whole document it’ll give you the first match:\n另一个重要的函数是 html_element()，它总是返回与输入相同数量的输出。如果将其应用于整个文档，它将给出第一个匹配项：\n\nhtml |&gt; html_element(\"p\")\n#&gt; {html_node}\n#&gt; &lt;p id=\"first\"&gt;\n\nThere’s an important difference between html_element() and html_elements() when you use a selector that doesn’t match any elements. html_elements() returns a vector of length 0, where html_element() returns a missing value. This will be important shortly.\n当您使用一个不匹配任何元素的选择器时，html_element() 和 html_elements() 之间有一个重要的区别。html_elements() 返回一个长度为 0 的向量，而 html_element() 返回一个缺失值。这一点很快就会变得很重要。\n\nhtml |&gt; html_elements(\"b\")\n#&gt; {xml_nodeset (0)}\nhtml |&gt; html_element(\"b\")\n#&gt; {xml_missing}\n#&gt; &lt;NA&gt;\n\n\n24.4.2 Nesting selections\nIn most cases, you’ll use html_elements() and html_element() together, typically using html_elements() to identify elements that will become observations then using html_element() to find elements that will become variables. Let’s see this in action using a simple example. Here we have an unordered list (&lt;ul&gt;) where each list item (&lt;li&gt;) contains some information about four characters from StarWars:\n在大多数情况下，您会同时使用 html_elements() 和 html_element()，通常使用 html_elements() 来识别将成为观测值的元素，然后使用 html_element() 来查找将成为变量的元素。让我们通过一个简单的例子来看看它的实际应用。这里我们有一个无序列表（&lt;ul&gt;），其中每个列表项（&lt;li&gt;）都包含有关《星球大战》中四个角色的一些信息：\n\nhtml &lt;- minimal_html(\"\n  &lt;ul&gt;\n    &lt;li&gt;&lt;b&gt;C-3PO&lt;/b&gt; is a &lt;i&gt;droid&lt;/i&gt; that weighs &lt;span class='weight'&gt;167 kg&lt;/span&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;b&gt;R4-P17&lt;/b&gt; is a &lt;i&gt;droid&lt;/i&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;b&gt;R2-D2&lt;/b&gt; is a &lt;i&gt;droid&lt;/i&gt; that weighs &lt;span class='weight'&gt;96 kg&lt;/span&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;b&gt;Yoda&lt;/b&gt; weighs &lt;span class='weight'&gt;66 kg&lt;/span&gt;&lt;/li&gt;\n  &lt;/ul&gt;\n  \")\n\nWe can use html_elements() to make a vector where each element corresponds to a different character:\n我们可以使用 html_elements() 创建一个向量，其中每个元素对应一个不同的角色：\n\ncharacters &lt;- html |&gt; html_elements(\"li\")\ncharacters\n#&gt; {xml_nodeset (4)}\n#&gt; [1] &lt;li&gt;\\n&lt;b&gt;C-3PO&lt;/b&gt; is a &lt;i&gt;droid&lt;/i&gt; that weighs &lt;span class=\"weight\"&gt; ...\n#&gt; [2] &lt;li&gt;\\n&lt;b&gt;R4-P17&lt;/b&gt; is a &lt;i&gt;droid&lt;/i&gt;\\n&lt;/li&gt;\n#&gt; [3] &lt;li&gt;\\n&lt;b&gt;R2-D2&lt;/b&gt; is a &lt;i&gt;droid&lt;/i&gt; that weighs &lt;span class=\"weight\"&gt; ...\n#&gt; [4] &lt;li&gt;\\n&lt;b&gt;Yoda&lt;/b&gt; weighs &lt;span class=\"weight\"&gt;66 kg&lt;/span&gt;\\n&lt;/li&gt;\n\nTo extract the name of each character, we use html_element(), because when applied to the output of html_elements() it’s guaranteed to return one response per element:\n要提取每个角色的名字，我们使用 html_element()，因为当它应用于 html_elements() 的输出时，可以保证为每个元素返回一个响应：\n\ncharacters |&gt; html_element(\"b\")\n#&gt; {xml_nodeset (4)}\n#&gt; [1] &lt;b&gt;C-3PO&lt;/b&gt;\n#&gt; [2] &lt;b&gt;R4-P17&lt;/b&gt;\n#&gt; [3] &lt;b&gt;R2-D2&lt;/b&gt;\n#&gt; [4] &lt;b&gt;Yoda&lt;/b&gt;\n\nThe distinction between html_element() and html_elements() isn’t important for name, but it is important for weight. We want to get one weight for each character, even if there’s no weight &lt;span&gt;. That’s what html_element() does:\n对于名字来说，html_element() 和 html_elements() 之间的区别并不重要，但对于体重来说，这个区别很重要。我们希望为每个角色获取一个体重，即使没有体重 &lt;span&gt; 标签。这正是 html_element() 所做的：\n\ncharacters |&gt; html_element(\".weight\")\n#&gt; {xml_nodeset (4)}\n#&gt; [1] &lt;span class=\"weight\"&gt;167 kg&lt;/span&gt;\n#&gt; [2] NA\n#&gt; [3] &lt;span class=\"weight\"&gt;96 kg&lt;/span&gt;\n#&gt; [4] &lt;span class=\"weight\"&gt;66 kg&lt;/span&gt;\n\nhtml_elements() finds all weight &lt;span&gt;s that are children of characters. There’s only three of these, so we lose the connection between names and weights:html_elements() 会找到 characters 的所有子元素中的体重 &lt;span&gt;。这里只有三个，所以我们失去了名字和体重之间的联系：\n\ncharacters |&gt; html_elements(\".weight\")\n#&gt; {xml_nodeset (3)}\n#&gt; [1] &lt;span class=\"weight\"&gt;167 kg&lt;/span&gt;\n#&gt; [2] &lt;span class=\"weight\"&gt;96 kg&lt;/span&gt;\n#&gt; [3] &lt;span class=\"weight\"&gt;66 kg&lt;/span&gt;\n\nNow that you’ve selected the elements of interest, you’ll need to extract the data, either from the text contents or some attributes.\n既然您已经选择了感兴趣的元素，接下来就需要从文本内容或某些属性中提取数据。\n\n24.4.3 Text and attributes\nhtml_text2()6 extracts the plain text contents of an HTML element:html_text2()6 提取 HTML 元素的纯文本内容：\n\ncharacters |&gt; \n  html_element(\"b\") |&gt; \n  html_text2()\n#&gt; [1] \"C-3PO\"  \"R4-P17\" \"R2-D2\"  \"Yoda\"\n\ncharacters |&gt; \n  html_element(\".weight\") |&gt; \n  html_text2()\n#&gt; [1] \"167 kg\" NA       \"96 kg\"  \"66 kg\"\n\nNote that any escapes will be automatically handled; you’ll only ever see HTML escapes in the source HTML, not in the data returned by rvest.\n请注意，任何转义字符都会被自动处理；你只会在源 HTML 中看到 HTML 转义字符，而不会在 rvest 返回的数据中看到。\nhtml_attr() extracts data from attributes:html_attr() 从属性中提取数据：\n\nhtml &lt;- minimal_html(\"\n  &lt;p&gt;&lt;a href='https://en.wikipedia.org/wiki/Cat'&gt;cats&lt;/a&gt;&lt;/p&gt;\n  &lt;p&gt;&lt;a href='https://en.wikipedia.org/wiki/Dog'&gt;dogs&lt;/a&gt;&lt;/p&gt;\n\")\n\nhtml |&gt; \n  html_elements(\"p\") |&gt; \n  html_element(\"a\") |&gt; \n  html_attr(\"href\")\n#&gt; [1] \"https://en.wikipedia.org/wiki/Cat\" \"https://en.wikipedia.org/wiki/Dog\"\n\nhtml_attr() always returns a string, so if you’re extracting numbers or dates, you’ll need to do some post-processing.html_attr() 总是返回一个字符串，所以如果你要提取数字或日期，就需要进行一些后处理。\n\n24.4.4 Tables\nIf you’re lucky, your data will be already stored in an HTML table, and it’ll be a matter of just reading it from that table. It’s usually straightforward to recognize a table in your browser: it’ll have a rectangular structure of rows and columns, and you can copy and paste it into a tool like Excel.\n如果你幸运的话，你的数据可能已经存储在 HTML 表格中，那么问题就简化为从该表格中读取数据。在浏览器中识别表格通常很简单：它会有一个由行和列组成的矩形结构，你可以将其复制并粘贴到像 Excel 这样的工具中。\nHTML tables are built up from four main elements: &lt;table&gt;, &lt;tr&gt; (table row), &lt;th&gt; (table heading), and &lt;td&gt; (table data). Here’s a simple HTML table with two columns and three rows:\nHTML 表格由四个主要元素构成：&lt;table&gt;、&lt;tr&gt; (table row，表格行)、&lt;th&gt; (table heading，表头) 和 &lt;td&gt; (table data，表格数据)。下面是一个包含两列三行的简单 HTML 表格：\n\nhtml &lt;- minimal_html(\"\n  &lt;table class='mytable'&gt;\n    &lt;tr&gt;&lt;th&gt;x&lt;/th&gt;    &lt;th&gt;y&lt;/th&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td&gt;1.5&lt;/td&gt; &lt;td&gt;2.7&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td&gt;4.9&lt;/td&gt; &lt;td&gt;1.3&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td&gt;7.2&lt;/td&gt; &lt;td&gt;8.1&lt;/td&gt;&lt;/tr&gt;\n  &lt;/table&gt;\n  \")\n\nrvest provides a function that knows how to read this sort of data: html_table(). It returns a list containing one tibble for each table found on the page. Use html_element() to identify the table you want to extract:\nrvest 提供了一个知道如何读取这类数据的函数：html_table()。它返回一个列表，其中包含页面上找到的每个表格对应的一个 tibble。使用 html_element() 来识别你想要提取的表格：\n\nhtml |&gt; \n  html_element(\".mytable\") |&gt; \n  html_table()\n#&gt; # A tibble: 3 × 2\n#&gt;       x     y\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1   1.5   2.7\n#&gt; 2   4.9   1.3\n#&gt; 3   7.2   8.1\n\nNote that x and y have automatically been converted to numbers. This automatic conversion doesn’t always work, so in more complex scenarios you may want to turn it off with convert = FALSE and then do your own conversion.\n注意 x 和 y 已被自动转换为数字。这种自动转换并不总是有效，因此在更复杂的场景中，你可能需要使用 convert = FALSE 将其关闭，然后自己进行转换。",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Web scraping</span>"
    ]
  },
  {
    "objectID": "webscraping.html#sec-css-selectors",
    "href": "webscraping.html#sec-css-selectors",
    "title": "24  Web scraping",
    "section": "\n24.5 Finding the right selectors",
    "text": "24.5 Finding the right selectors\nFiguring out the selector you need for your data is typically the hardest part of the problem. You’ll often need to do some experimenting to find a selector that is both specific (i.e. it doesn’t select things you don’t care about) and sensitive (i.e. it does select everything you care about). Lots of trial and error is a normal part of the process! There are two main tools that are available to help you with this process: SelectorGadget and your browser’s developer tools.\n找出数据所需的选择器通常是问题中最困难的部分。你通常需要进行一些实验，以找到一个既具体 (specific)（即它不会选择你不在乎的东西）又敏感 (sensitive)（即它确实选择了你关心的所有东西）的选择器。大量的反复试验是这个过程的正常部分！有两个主要工具可以帮助你完成这个过程：SelectorGadget 和你浏览器的开发者工具。\nSelectorGadget is a javascript bookmarklet that automatically generates CSS selectors based on the positive and negative examples that you provide. It doesn’t always work, but when it does, it’s magic! You can learn how to install and use SelectorGadget either by reading https://rvest.tidyverse.org/articles/selectorgadget.html or watching Mine’s video at https://www.youtube.com/watch?v=PetWV5g1Xsc.SelectorGadget 是一个 javascript 书签工具，它可以根据你提供的正面和负面示例自动生成 CSS 选择器。它并不总是有效，但一旦有效，效果就如同魔法！你可以通过阅读 https://rvest.tidyverse.org/articles/selectorgadget.html 或观看 Mine 在 https://www.youtube.com/watch?v=PetWV5g1Xsc 上的视频来学习如何安装和使用 SelectorGadget。\nEvery modern browser comes with some toolkit for developers, but we recommend Chrome, even if it isn’t your regular browser: its web developer tools are some of the best and they’re immediately available. Right click on an element on the page and click Inspect. This will open an expandable view of the complete HTML page, centered on the element that you just clicked. You can use this to explore the page and get a sense of what selectors might work. Pay particular attention to the class and id attributes, since these are often used to form the visual structure of the page, and hence make for good tools to extract the data that you’re looking for.\n每个现代浏览器都带有一些面向开发者的工具包，但我们推荐使用 Chrome，即使它不是你的常用浏览器：它的 Web 开发者工具是最好的之一，而且可以立即使用。在页面上的一个元素上右键单击，然后点击 Inspect (检查)。这将打开一个可展开的完整 HTML 页面视图，并以你刚刚点击的元素为中心。你可以用它来探索页面，并了解哪些选择器可能有效。要特别注意 class 和 id 属性，因为它们通常用于构成页面的视觉结构，因此是提取你所寻找数据的好工具。\nInside the Elements view, you can also right click on an element and choose Copy as Selector to generate a selector that will uniquely identify the element of interest.\n在“元素”(Elements) 视图中，你还可以在一个元素上右键单击并选择 Copy as Selector (复制为选择器)，以生成一个能唯一标识目标元素的选择器。\nIf either SelectorGadget or Chrome DevTools have generated a CSS selector that you don’t understand, try Selectors Explained which translates CSS selectors into plain English. If you find yourself doing this a lot, you might want to learn more about CSS selectors generally. We recommend starting with the fun CSS dinner tutorial and then referring to the MDN web docs.\n如果 SelectorGadget 或 Chrome 开发者工具生成了你看不懂的 CSS 选择器，可以试试 Selectors Explained，它能将 CSS 选择器翻译成通俗易懂的英语。如果你发现自己经常这样做，你可能需要更全面地学习 CSS 选择器。我们推荐从有趣的 CSS dinner 教程开始，然后参考 MDN web docs。",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Web scraping</span>"
    ]
  },
  {
    "objectID": "webscraping.html#putting-it-all-together",
    "href": "webscraping.html#putting-it-all-together",
    "title": "24  Web scraping",
    "section": "\n24.6 Putting it all together",
    "text": "24.6 Putting it all together\nLet’s put this all together to scrape some websites. There’s some risk that these examples may no longer work when you run them — that’s the fundamental challenge of web scraping; if the structure of the site changes, then you’ll have to change your scraping code.\n让我们把所有这些整合起来，去爬取一些网站。当你运行这些示例时，它们可能不再有效，这存在一定的风险——这是网络爬取的根本挑战；如果网站的结构发生变化，你就必须修改你的爬取代码。\n\n24.6.1 StarWars\nrvest includes a very simple example in vignette(\"starwars\"). This is a simple page with minimal HTML so it’s a good place to start. I’d encourage you to navigate to that page now and use “Inspect Element” to inspect one of the headings that’s the title of a Star Wars movie. Use the keyboard or mouse to explore the hierarchy of the HTML and see if you can get a sense of the shared structure used by each movie.\nrvest 在 vignette(\"starwars\") 中包含了一个非常简单的例子。这是一个 HTML 极简的简单页面，所以是一个很好的起点。我鼓励你现在就导航到那个页面，并使用“检查元素”(Inspect Element) 来检查其中一个作为星球大战电影标题的标题。使用键盘或鼠标来探索 HTML 的层次结构，看看你是否能了解每部电影所使用的共享结构。\nYou should be able to see that each movie has a shared structure that looks like this:\n你应该能够看到每部电影都有一个共享的结构，看起来像这样：\n&lt;section&gt;\n  &lt;h2 data-id=\"1\"&gt;The Phantom Menace&lt;/h2&gt;\n  &lt;p&gt;Released: 1999-05-19&lt;/p&gt;\n  &lt;p&gt;Director: &lt;span class=\"director\"&gt;George Lucas&lt;/span&gt;&lt;/p&gt;\n  \n  &lt;div class=\"crawl\"&gt;\n    &lt;p&gt;...&lt;/p&gt;\n    &lt;p&gt;...&lt;/p&gt;\n    &lt;p&gt;...&lt;/p&gt;\n  &lt;/div&gt;\n&lt;/section&gt;\nOur goal is to turn this data into a 7 row data frame with variables title, year, director, and intro. We’ll start by reading the HTML and extracting all the &lt;section&gt; elements:\n我们的目标是将这些数据转换成一个包含 title、year、director 和 intro 变量的 7 行数据框。我们将从读取 HTML 并提取所有 &lt;section&gt; 元素开始：\n\nurl &lt;- \"https://rvest.tidyverse.org/articles/starwars.html\"\nhtml &lt;- read_html(url)\n\nsection &lt;- html |&gt; html_elements(\"section\")\nsection\n#&gt; {xml_nodeset (7)}\n#&gt; [1] &lt;section&gt;&lt;h2 data-id=\"1\"&gt;\\nThe Phantom Menace\\n&lt;/h2&gt;\\n&lt;p&gt;\\nReleased: 1 ...\n#&gt; [2] &lt;section&gt;&lt;h2 data-id=\"2\"&gt;\\nAttack of the Clones\\n&lt;/h2&gt;\\n&lt;p&gt;\\nReleased: ...\n#&gt; [3] &lt;section&gt;&lt;h2 data-id=\"3\"&gt;\\nRevenge of the Sith\\n&lt;/h2&gt;\\n&lt;p&gt;\\nReleased:  ...\n#&gt; [4] &lt;section&gt;&lt;h2 data-id=\"4\"&gt;\\nA New Hope\\n&lt;/h2&gt;\\n&lt;p&gt;\\nReleased: 1977-05-2 ...\n#&gt; [5] &lt;section&gt;&lt;h2 data-id=\"5\"&gt;\\nThe Empire Strikes Back\\n&lt;/h2&gt;\\n&lt;p&gt;\\nReleas ...\n#&gt; [6] &lt;section&gt;&lt;h2 data-id=\"6\"&gt;\\nReturn of the Jedi\\n&lt;/h2&gt;\\n&lt;p&gt;\\nReleased: 1 ...\n#&gt; [7] &lt;section&gt;&lt;h2 data-id=\"7\"&gt;\\nThe Force Awakens\\n&lt;/h2&gt;\\n&lt;p&gt;\\nReleased: 20 ...\n\nThis retrieves seven elements matching the seven movies found on that page, suggesting that using section as a selector is good. Extracting the individual elements is straightforward since the data is always found in the text. It’s just a matter of finding the right selector:\n这段代码检索到七个元素，与页面上找到的七部电影相匹配，这表明使用 section 作为选择器是很好的。提取单个元素很简单，因为数据总是在文本中找到。这只是找到正确选择器的问题：\n\nsection |&gt; html_element(\"h2\") |&gt; html_text2()\n#&gt; [1] \"The Phantom Menace\"      \"Attack of the Clones\"   \n#&gt; [3] \"Revenge of the Sith\"     \"A New Hope\"             \n#&gt; [5] \"The Empire Strikes Back\" \"Return of the Jedi\"     \n#&gt; [7] \"The Force Awakens\"\n\nsection |&gt; html_element(\".director\") |&gt; html_text2()\n#&gt; [1] \"George Lucas\"     \"George Lucas\"     \"George Lucas\"    \n#&gt; [4] \"George Lucas\"     \"Irvin Kershner\"   \"Richard Marquand\"\n#&gt; [7] \"J. J. Abrams\"\n\nOnce we’ve done that for each component, we can wrap all the results up into a tibble:\n为每个组件完成此操作后，我们可以将所有结果包装到一个 tibble 中：\n\ntibble(\n  title = section |&gt; \n    html_element(\"h2\") |&gt; \n    html_text2(),\n  released = section |&gt; \n    html_element(\"p\") |&gt; \n    html_text2() |&gt; \n    str_remove(\"Released: \") |&gt; \n    parse_date(),\n  director = section |&gt; \n    html_element(\".director\") |&gt; \n    html_text2(),\n  intro = section |&gt; \n    html_element(\".crawl\") |&gt; \n    html_text2()\n)\n#&gt; # A tibble: 7 × 4\n#&gt;   title                   released   director         intro                  \n#&gt;   &lt;chr&gt;                   &lt;date&gt;     &lt;chr&gt;            &lt;chr&gt;                  \n#&gt; 1 The Phantom Menace      1999-05-19 George Lucas     \"Turmoil has engulfed …\n#&gt; 2 Attack of the Clones    2002-05-16 George Lucas     \"There is unrest in th…\n#&gt; 3 Revenge of the Sith     2005-05-19 George Lucas     \"War! The Republic is …\n#&gt; 4 A New Hope              1977-05-25 George Lucas     \"It is a period of civ…\n#&gt; 5 The Empire Strikes Back 1980-05-17 Irvin Kershner   \"It is a dark time for…\n#&gt; 6 Return of the Jedi      1983-05-25 Richard Marquand \"Luke Skywalker has re…\n#&gt; # ℹ 1 more row\n\nWe did a little more processing of released to get a variable that will be easy to use later in our analysis.\n我们对 released 进行了更多的处理，以得到一个在后续分析中更易于使用的变量。\n\n24.6.2 IMDB top films\nFor our next task we’ll tackle something a little trickier, extracting the top 250 movies from the internet movie database (IMDb). At the time we wrote this chapter, the page looked like Figure 24.1.\n在我们的下一个任务中，我们将处理一个稍微棘手的问题，即从互联网电影数据库 (IMDb) 中提取排名前 250 的电影。在我们撰写本章时，该页面的外观如 Figure 24.1 所示。\n\n\n\n\n\n\n\nFigure 24.1: Screenshot of the IMDb top movies web page taken on 2022-12-05.\n\n\n\n\nThis data has a clear tabular structure so it’s worth starting with html_table():\n这些数据具有清晰的表格结构，因此值得从 html_table() 开始：\n\nurl &lt;- \"https://web.archive.org/web/20220201012049/https://www.imdb.com/chart/top/\"\nhtml &lt;- read_html(url)\n\ntable &lt;- html |&gt; \n  html_element(\"table\") |&gt; \n  html_table()\ntable\n#&gt; # A tibble: 250 × 5\n#&gt;   ``    `Rank & Title`                    `IMDb Rating` `Your Rating`   ``   \n#&gt;   &lt;lgl&gt; &lt;chr&gt;                                     &lt;dbl&gt; &lt;chr&gt;           &lt;lgl&gt;\n#&gt; 1 NA    \"1.\\n      The Shawshank Redempt…           9.2 \"12345678910\\n… NA   \n#&gt; 2 NA    \"2.\\n      The Godfather\\n      …           9.1 \"12345678910\\n… NA   \n#&gt; 3 NA    \"3.\\n      The Godfather: Part I…           9   \"12345678910\\n… NA   \n#&gt; 4 NA    \"4.\\n      The Dark Knight\\n    …           9   \"12345678910\\n… NA   \n#&gt; 5 NA    \"5.\\n      12 Angry Men\\n       …           8.9 \"12345678910\\n… NA   \n#&gt; 6 NA    \"6.\\n      Schindler's List\\n   …           8.9 \"12345678910\\n… NA   \n#&gt; # ℹ 244 more rows\n\nThis includes a few empty columns, but overall does a good job of capturing the information from the table. However, we need to do some more processing to make it easier to use. First, we’ll rename the columns to be easier to work with, and remove the extraneous whitespace in rank and title. We will do this with select() (instead of rename()) to do the renaming and selecting of just these two columns in one step. Then we’ll remove the new lines and extra spaces, and then apply separate_wider_regex() (from Section 15.3.4) to pull out the title, year, and rank into their own variables.\n这其中包含一些空列，但总体上很好地捕获了表格中的信息。然而，我们需要进行更多的处理以使其更易于使用。首先，我们将重命名列名以便于操作，并移除排名和标题中多余的空白。我们将使用 select() (而不是 rename()) 来一步完成重命名和仅选择这两列的操作。然后，我们将移除换行符和多余的空格，接着应用 separate_wider_regex() (来自 Section 15.3.4) 将标题、年份和排名提取到各自的变量中。\n\nratings &lt;- table |&gt;\n  select(\n    rank_title_year = `Rank & Title`,\n    rating = `IMDb Rating`\n  ) |&gt; \n  mutate(\n    rank_title_year = str_replace_all(rank_title_year, \"\\n +\", \" \")\n  ) |&gt; \n  separate_wider_regex(\n    rank_title_year,\n    patterns = c(\n      rank = \"\\\\d+\", \"\\\\. \",\n      title = \".+\", \" +\\\\(\",\n      year = \"\\\\d+\", \"\\\\)\"\n    )\n  )\nratings\n#&gt; # A tibble: 250 × 4\n#&gt;   rank  title                    year  rating\n#&gt;   &lt;chr&gt; &lt;chr&gt;                    &lt;chr&gt;  &lt;dbl&gt;\n#&gt; 1 1     The Shawshank Redemption 1994     9.2\n#&gt; 2 2     The Godfather            1972     9.1\n#&gt; 3 3     The Godfather: Part II   1974     9  \n#&gt; 4 4     The Dark Knight          2008     9  \n#&gt; 5 5     12 Angry Men             1957     8.9\n#&gt; 6 6     Schindler's List         1993     8.9\n#&gt; # ℹ 244 more rows\n\nEven in this case where most of the data comes from table cells, it’s still worth looking at the raw HTML. If you do so, you’ll discover that we can add a little extra data by using one of the attributes. This is one of the reasons it’s worth spending a little time spelunking the source of the page; you might find extra data, or might find a parsing route that’s slightly easier.\n即使在这种大部分数据来自表格单元格的情况下，查看原始 HTML 仍然是值得的。如果你这样做，你会发现我们可以通过使用其中一个属性来添加一些额外的数据。这就是花点时间研究页面源代码的原因之一；你可能会发现额外的数据，或者找到一个稍微容易一些的解析路径。\n\nhtml |&gt; \n  html_elements(\"td strong\") |&gt; \n  head() |&gt; \n  html_attr(\"title\")\n#&gt; [1] \"9.2 based on 2,536,415 user ratings\"\n#&gt; [2] \"9.1 based on 1,745,675 user ratings\"\n#&gt; [3] \"9.0 based on 1,211,032 user ratings\"\n#&gt; [4] \"9.0 based on 2,486,931 user ratings\"\n#&gt; [5] \"8.9 based on 749,563 user ratings\"  \n#&gt; [6] \"8.9 based on 1,295,705 user ratings\"\n\nWe can combine this with the tabular data and again apply separate_wider_regex() to extract out the bit of data we care about:\n我们可以将其与表格数据结合起来，并再次应用 separate_wider_regex() 来提取我们关心的那部分数据：\n\nratings |&gt;\n  mutate(\n    rating_n = html |&gt; html_elements(\"td strong\") |&gt; html_attr(\"title\")\n  ) |&gt; \n  separate_wider_regex(\n    rating_n,\n    patterns = c(\n      \"[0-9.]+ based on \",\n      number = \"[0-9,]+\",\n      \" user ratings\"\n    )\n  ) |&gt; \n  mutate(\n    number = parse_number(number)\n  )\n#&gt; # A tibble: 250 × 5\n#&gt;   rank  title                    year  rating  number\n#&gt;   &lt;chr&gt; &lt;chr&gt;                    &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 1     The Shawshank Redemption 1994     9.2 2536415\n#&gt; 2 2     The Godfather            1972     9.1 1745675\n#&gt; 3 3     The Godfather: Part II   1974     9   1211032\n#&gt; 4 4     The Dark Knight          2008     9   2486931\n#&gt; 5 5     12 Angry Men             1957     8.9  749563\n#&gt; 6 6     Schindler's List         1993     8.9 1295705\n#&gt; # ℹ 244 more rows",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Web scraping</span>"
    ]
  },
  {
    "objectID": "webscraping.html#dynamic-sites",
    "href": "webscraping.html#dynamic-sites",
    "title": "24  Web scraping",
    "section": "\n24.7 Dynamic sites",
    "text": "24.7 Dynamic sites\nSo far we have focused on websites where html_elements() returns what you see in the browser and discussed how to parse what it returns and how to organize that information in tidy data frames. From time-to-time, however, you’ll hit a site where html_elements() and friends don’t return anything like what you see in the browser. In many cases, that’s because you’re trying to scrape a website that dynamically generates the content of the page with javascript. This doesn’t currently work with rvest, because rvest downloads the raw HTML and doesn’t run any javascript.\n到目前为止，我们一直专注于那些 html_elements() 返回你在浏览器中看到的内容的网站，并讨论了如何解析其返回内容以及如何将这些信息组织成整洁的数据框。然而，你偶尔会遇到一个网站，其中 html_elements() 及其相关函数返回的内容与你在浏览器中看到的完全不同。在许多情况下，这是因为你试图爬取一个使用 javascript 动态生成页面内容的网站。这目前不适用于 rvest，因为 rvest 下载的是原始 HTML，不运行任何 javascript。\nIt’s still possible to scrape these types of sites, but rvest needs to use a more expensive process: fully simulating the web browser including running all javascript. This functionality is not available at the time of writing, but it’s something we’re actively working on and might be available by the time you read this. It uses the chromote package which actually runs the Chrome browser in the background, and gives you additional tools to interact with the site, like a human typing text and clicking buttons. Check out the rvest website for more details.\n仍然可以爬取这类网站，但 rvest 需要使用一个更昂贵的过程：完全模拟网络浏览器，包括运行所有 javascript。在撰写本文时，此功能尚不可用，但这是我们正在积极开发的功能，当你阅读本文时可能已经可用。它使用 chromote 包，该包实际上在后台运行 Chrome 浏览器，并为你提供与网站交互的额外工具，就像人类输入文本和点击按钮一样。请查看 rvest 网站 以获取更多详细信息。",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Web scraping</span>"
    ]
  },
  {
    "objectID": "webscraping.html#summary",
    "href": "webscraping.html#summary",
    "title": "24  Web scraping",
    "section": "\n24.8 Summary",
    "text": "24.8 Summary\nIn this chapter, you’ve learned about the why, the why not, and the how of scraping data from web pages. First, you’ve learned about the basics of HTML and using CSS selectors to refer to specific elements, then you’ve learned about using the rvest package to get data out of HTML into R. We then demonstrated web scraping with two case studies: a simpler scenario on scraping data on StarWars films from the rvest package website and a more complex scenario on scraping the top 250 films from IMDB.\n在本章中，你学习了从网页上爬取数据的原因、不应爬取的情况以及如何爬取。首先，你学习了 HTML 的基础知识和使用 CSS 选择器来引用特定元素，然后你学习了使用 rvest 包将数据从 HTML 中提取到 R 中。接着，我们通过两个案例研究演示了网络爬取：一个是在 rvest 包网站上爬取星球大战电影数据的较简单场景，另一个是在 IMDB 上爬取排名前 250 部电影的较复杂场景。\nTechnical details of scraping data off the web can be complex, particularly when dealing with sites, however legal and ethical considerations can be even more complex. It’s important for you to educate yourself about both of these before setting out to scrape data.\n从网络上爬取数据的技术细节可能很复杂，尤其是在处理网站时，然而法律和道德方面的考虑可能更为复杂。在开始爬取数据之前，对这两方面进行自我教育是非常重要的。\nThis brings us to the end of the import part of the book where you’ve learned techniques to get data from where it lives (spreadsheets, databases, JSON files, and web sites) into a tidy form in R. Now it’s time to turn our sights to a new topic: making the most of R as a programming language.\n这就结束了本书的导入部分，你已经学习了从数据所在之处（电子表格、数据库、JSON 文件和网站）获取数据并将其整理成 R 中整洁形式的技术。现在是时候将我们的目光转向一个新主题：充分利用 R 作为一种编程语言。",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Web scraping</span>"
    ]
  },
  {
    "objectID": "webscraping.html#footnotes",
    "href": "webscraping.html#footnotes",
    "title": "24  Web scraping",
    "section": "",
    "text": "And many popular APIs already have CRAN packages that wrap them, so start with a little research first!↩︎\nObviously we’re not lawyers, and this is not legal advice.     But this is the best summary we can give having read a bunch about this topic.↩︎\nOne example of an article on the OkCupid study was published by Wired, https://www.wired.com/2016/05/okcupid-study-reveals-perils-big-data-science.↩︎\nA number of tags (including &lt;p&gt; and &lt;li&gt;) don’t require end tags, but we think it’s best to include them because it makes seeing the structure of the HTML a little easier.↩︎\nThis class comes from the xml2 package.     xml2 is a low-level package that rvest builds on top of.↩︎\nrvest also provides html_text() but you should almost always use html_text2() since it does a better job of converting nested HTML to text.↩︎",
    "crumbs": [
      "Import",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Web scraping</span>"
    ]
  },
  {
    "objectID": "program.html",
    "href": "program.html",
    "title": "Program",
    "section": "",
    "text": "In this part of the book, you’ll improve your programming skills. Programming is a cross-cutting skill needed for all data science work: you must use a computer to do data science; you cannot do it in your head, or with pencil and paper.\n在本书的这一部分，你将提升你的编程技能。编程是所有数据科学工作所需的一项贯穿性技能：你必须使用计算机来做数据科学；你无法在头脑中，或用纸笔完成它。\n\n\n\n\n\n\n\nFigure 1: Programming is the water in which all the other components swim.\n\n\n\n\nProgramming produces code, and code is a tool of communication. Obviously code tells the computer what you want it to do. But it also communicates meaning to other humans. Thinking about code as a vehicle for communication is important because every project you do is fundamentally collaborative. Even if you’re not working with other people, you’ll definitely be working with future-you! Writing clear code is important so that others (like future-you) can understand why you tackled an analysis in the way you did. That means getting better at programming also involves getting better at communicating. Over time, you want your code to become not just easier to write, but easier for others to read.\n编程产生代码，而代码是一种沟通工具。显然，代码告诉计算机你希望它做什么。但它也向其他人类传达意义。将代码视为一种沟通媒介非常重要，因为你做的每个项目本质上都是协作性的。即使你没有和别人一起工作，你也肯定会和未来的你一起工作！编写清晰的代码很重要，这样其他人（比如未来的你）才能理解你为什么用那种方式处理分析。这意味着，提高编程能力也包括提高沟通能力。随着时间的推移，你希望你的代码不仅更容易编写，也更容易为他人阅读。\nIn the following three chapters, you’ll learn skills to improve your programming skills:\n在接下来的三章中，你将学到提升编程技能的技巧：\n\nCopy-and-paste is a powerful tool, but you should avoid doing it more than twice. Repeating yourself in code is dangerous because it can easily lead to errors and inconsistencies. Instead, in 25  Functions, you’ll learn how to write functions which let you extract out repeated tidyverse code so that it can be easily reused.\n复制粘贴是一个强大的工具，但你应该避免重复使用它超过两次。在代码中重复自己是危险的，因为它很容易导致错误和不一致。因此，在 25  Functions 中，你将学习如何编写函数 (functions)，这能让你提取出重复的 tidyverse 代码，以便于轻松重用。\nFunctions extract out repeated code, but you often need to repeat the same actions on different inputs. You need tools for iteration that let you do similar things again and again. These tools include for loops and functional programming, which you’ll learn about in 26  Iteration.\n函数提取了重复的代码，但你经常需要对不同的输入重复相同的操作。你需要迭代 (iteration) 工具，让你能一遍又一遍地做类似的事情。这些工具包括 for 循环和函数式编程，你将在 26  Iteration 中学习它们。\nAs you read more code written by others, you’ll see more code that doesn’t use the tidyverse. In 27  A field guide to base R, you’ll learn some of the most important base R functions that you’ll see in the wild.\n当你阅读更多他人编写的代码时，你会看到更多不使用 tidyverse 的代码。在 27  A field guide to base R 中，你将学习一些你在实际中会看到的最重要的基础 R 函数。\n\nThe goal of these chapters is to teach you the minimum about programming that you need for data science. Once you have mastered the material here, we strongly recommend that you continue to invest in your programming skills. We’ve written two books that you might find helpful. Hands on Programming with R, by Garrett Grolemund, is an introduction to R as a programming language and is a great place to start if R is your first programming language. Advanced R by Hadley Wickham dives into the details of R the programming language; it’s a great place to start if you have existing programming experience and a great next step once you’ve internalized the ideas in these chapters.\n这些章节的目标是教会你数据科学所需的最低限度的编程知识。一旦你掌握了这里的内容，我们强烈建议你继续投资于你的编程技能。我们写了两本书，你可能会觉得有帮助。Garrett Grolemund 的 Hands on Programming with R 是 R 作为一种编程语言的入门介绍，如果 R 是你的第一门编程语言，这是一个很好的起点。Hadley Wickham 的 Advanced R 深入探讨了 R 编程语言的细节；如果你有现成的编程经验，这是一个很好的起点，也是你在内化了这些章节的思想后的一个很好的下一步。",
    "crumbs": [
      "Program"
    ]
  },
  {
    "objectID": "functions.html",
    "href": "functions.html",
    "title": "25  Functions",
    "section": "",
    "text": "25.1 Introduction\nOne of the best ways to improve your reach as a data scientist is to write functions. Functions allow you to automate common tasks in a more powerful and general way than copy-and-pasting. Writing a function has four big advantages over using copy-and-paste:\n作为一名数据科学家，编写函数是提升能力最好的方法之一。函数允许你以比复制粘贴更强大、更通用的方式来自动化常见任务。与复制粘贴相比，编写函数有四大优势：\nA good rule of thumb is to consider writing a function whenever you’ve copied and pasted a block of code more than twice (i.e. you now have three copies of the same code). In this chapter, you’ll learn about three useful types of functions:\n一个好的经验法则是，当你复制粘贴一段代码超过两次（即你现在有三份相同的代码）时，就应该考虑编写一个函数。在本章中，你将学习三种有用的函数类型： Vector functions take one or more vectors as input and return a vector as output.\n向量函数 (Vector functions) 将一个或多个向量作为输入，并返回一个向量作为输出。\nEach of these sections includes many examples to help you generalize the patterns that you see. These examples wouldn’t be possible without the help of folks of twitter, and we encourage you to follow the links in the comments to see the original inspirations. You might also want to read the original motivating tweets for general functions and plotting functions to see even more functions.\n这些部分中的每一个都包含许多示例，以帮助你归纳所看到的模式。没有 Twitter 上朋友们的帮助，这些示例是不可能完成的，我们鼓励你点击评论中的链接，查看最初的灵感来源。你可能还想阅读关于 通用函数 和 绘图函数 的原始推文，以看到更多函数。",
    "crumbs": [
      "Program",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "functions.html#introduction",
    "href": "functions.html#introduction",
    "title": "25  Functions",
    "section": "",
    "text": "You can give a function an evocative name that makes your code easier to understand.\n你可以给函数起一个能唤起记忆的名称，使你的代码更容易理解。\nAs requirements change, you only need to update code in one place, instead of many.\n当需求变更时，你只需要在一个地方更新代码，而不是多个地方。\nYou eliminate the chance of making incidental mistakes when you copy and paste (i.e. updating a variable name in one place, but not in another).\n你消除了在复制粘贴时犯下偶然错误的机会（例如，在一个地方更新了变量名，但在另一个地方没有更新）。\nIt makes it easier to reuse work from project-to-project, increasing your productivity over time.\n这使得在项目之间重用工作变得更加容易，从而随着时间的推移提高你的生产力。\n\n\n\nData frame functions take a data frame as input and return a data frame as output.\n数据框函数 (Data frame functions) 将一个数据框作为输入，并返回一个数据框作为输出。\nPlot functions that take a data frame as input and return a plot as output.\n绘图函数 (Plot functions) 将一个数据框作为输入，并返回一个图表作为输出。\n\n\n\n25.1.1 Prerequisites\nWe’ll wrap up a variety of functions from around the tidyverse. We’ll also use nycflights13 as a source of familiar data to use our functions with.\n我们将整合 tidyverse 中的各种函数。我们还将使用 nycflights13 作为我们熟悉的，用于函数处理的数据源。\n\nlibrary(tidyverse)\nlibrary(nycflights13)",
    "crumbs": [
      "Program",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "functions.html#vector-functions",
    "href": "functions.html#vector-functions",
    "title": "25  Functions",
    "section": "\n25.2 Vector functions",
    "text": "25.2 Vector functions\nWe’ll begin with vector functions: functions that take one or more vectors and return a vector result. For example, take a look at this code. What does it do?\n我们将从向量函数开始：即接受一个或多个向量并返回一个向量结果的函数。例如，看看这段代码。它是做什么的？\n\ndf &lt;- tibble(\n  a = rnorm(5),\n  b = rnorm(5),\n  c = rnorm(5),\n  d = rnorm(5),\n)\n\ndf |&gt; mutate(\n  a = (a - min(a, na.rm = TRUE)) / \n    (max(a, na.rm = TRUE) - min(a, na.rm = TRUE)),\n  b = (b - min(a, na.rm = TRUE)) / \n    (max(b, na.rm = TRUE) - min(b, na.rm = TRUE)),\n  c = (c - min(c, na.rm = TRUE)) / \n    (max(c, na.rm = TRUE) - min(c, na.rm = TRUE)),\n  d = (d - min(d, na.rm = TRUE)) / \n    (max(d, na.rm = TRUE) - min(d, na.rm = TRUE)),\n)\n#&gt; # A tibble: 5 × 4\n#&gt;       a       b     c     d\n#&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 0.339  0.387  0.291 0    \n#&gt; 2 0.880 -0.613  0.611 0.557\n#&gt; 3 0     -0.0833 1     0.752\n#&gt; 4 0.795 -0.0822 0     1    \n#&gt; 5 1     -0.0952 0.580 0.394\n\nYou might be able to puzzle out that this rescales each column to have a range from 0 to 1. But did you spot the mistake? When Hadley wrote this code he made an error when copying-and-pasting and forgot to change an a to a b. Preventing this type of mistake is one very good reason to learn how to write functions.\n你或许能猜到这是将每一列重新缩放到 0 到 1 的范围。但你发现错误了吗？Hadley 在编写这段代码时，在复制粘贴时犯了一个错误，忘记将一个 a 改成 b。避免这类错误是学习编写函数的一个很好的理由。\n\n25.2.1 Writing a function\nTo write a function you need to first analyse your repeated code to figure what parts are constant and what parts vary. If we take the code above and pull it outside of mutate(), it’s a little easier to see the pattern because each repetition is now one line:\n要编写一个函数，你首先需要分析你重复的代码，找出哪些部分是常量，哪些部分是变量。如果我们把上面的代码从 mutate() 中抽离出来，模式会更容易看清，因为现在每次重复都只有一行：\n\n(a - min(a, na.rm = TRUE)) / (max(a, na.rm = TRUE) - min(a, na.rm = TRUE))\n(b - min(b, na.rm = TRUE)) / (max(b, na.rm = TRUE) - min(b, na.rm = TRUE))\n(c - min(c, na.rm = TRUE)) / (max(c, na.rm = TRUE) - min(c, na.rm = TRUE))\n(d - min(d, na.rm = TRUE)) / (max(d, na.rm = TRUE) - min(d, na.rm = TRUE))  \n\nTo make this a bit clearer we can replace the bit that varies with █:\n为了更清晰地说明，我们可以用 █ 替换变化的部分：\n\n(█ - min(█, na.rm = TRUE)) / (max(█, na.rm = TRUE) - min(█, na.rm = TRUE))\n\nTo turn this into a function you need three things:\n要把这个变成一个函数，你需要三样东西：\n\nA name. Here we’ll use rescale01 because this function rescales a vector to lie between 0 and 1.\n一个名称。这里我们使用 rescale01，因为这个函数将一个向量重新缩放到 0 和 1 之间。\nThe arguments. The arguments are things that vary across calls and our analysis above tells us that we have just one. We’ll call it x because this is the conventional name for a numeric vector.参数。参数是每次调用中变化的东西，我们上面的分析告诉我们只有一个。我们称它为 x，因为这是数值向量的常规名称。\nThe body. The body is the code that’s repeated across all the calls.函数体。函数体是所有调用中重复的代码。\n\nThen you create a function by following the template:\n然后你按照以下模板创建一个函数：\n\nname &lt;- function(arguments) {\n  body\n}\n\nFor this case that leads to:\n对于这种情况，我们得到：\n\nrescale01 &lt;- function(x) {\n  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))\n}\n\nAt this point you might test with a few simple inputs to make sure you’ve captured the logic correctly:\n此时，你可能会用一些简单的输入来测试，以确保你已经正确地捕获了逻辑：\n\nrescale01(c(-10, 0, 10))\n#&gt; [1] 0.0 0.5 1.0\nrescale01(c(1, 2, 3, NA, 5))\n#&gt; [1] 0.00 0.25 0.50   NA 1.00\n\nThen you can rewrite the call to mutate() as:\n然后你可以将对 mutate() 的调用重写为：\n\ndf |&gt; mutate(\n  a = rescale01(a),\n  b = rescale01(b),\n  c = rescale01(c),\n  d = rescale01(d),\n)\n#&gt; # A tibble: 5 × 4\n#&gt;       a     b     c     d\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 0.339 1     0.291 0    \n#&gt; 2 0.880 0     0.611 0.557\n#&gt; 3 0     0.530 1     0.752\n#&gt; 4 0.795 0.531 0     1    \n#&gt; 5 1     0.518 0.580 0.394\n\n(In Chapter 26, you’ll learn how to use across() to reduce the duplication even further so all you need is df |&gt; mutate(across(a:d, rescale01))).\n（在 Chapter 26 中，你将学习如何使用 across() 来进一步减少重复，这样你只需要 df |&gt; mutate(across(a:d, rescale01))）。\n\n25.2.2 Improving our function\nYou might notice that the rescale01() function does some unnecessary work — instead of computing min() twice and max() once we could instead compute both the minimum and maximum in one step with range():\n你可能会注意到 rescale01() 函数做了一些不必要的工作——与其计算两次 min() 和一次 max()，我们可以用 range() 一步计算出最小值和最大值：\n\nrescale01 &lt;- function(x) {\n  rng &lt;- range(x, na.rm = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\n\nOr you might try this function on a vector that includes an infinite value:\n或者你可以在包含无穷大值的向量上尝试这个函数：\n\nx &lt;- c(1:10, Inf)\nrescale01(x)\n#&gt;  [1]   0   0   0   0   0   0   0   0   0   0 NaN\n\nThat result is not particularly useful so we could ask range() to ignore infinite values:\n这个结果不是特别有用，所以我们可以让 range() 忽略无穷大值：\n\nrescale01 &lt;- function(x) {\n  rng &lt;- range(x, na.rm = TRUE, finite = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\n\nrescale01(x)\n#&gt;  [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444 0.5555556 0.6666667\n#&gt;  [8] 0.7777778 0.8888889 1.0000000       Inf\n\nThese changes illustrate an important benefit of functions: because we’ve moved the repeated code into a function, we only need to make the change in one place.\n这些改变说明了函数的一个重要好处：因为我们已经将重复的代码移入了一个函数中，我们只需要在一个地方进行修改。\n\n25.2.3 Mutate functions\nNow that you’ve got the basic idea of functions, let’s take a look at a whole bunch of examples. We’ll start by looking at “mutate” functions, i.e. functions that work well inside of mutate() and filter() because they return an output of the same length as the input.\n既然你已经掌握了函数的基本概念，让我们来看一大堆例子。我们将从“mutate”函数开始，即那些在 mutate() 和 filter() 中工作得很好的函数，因为它们返回的输出与输入长度相同。\nLet’s start with a simple variation of rescale01(). Maybe you want to compute the Z-score, rescaling a vector to have a mean of zero and a standard deviation of one:\n让我们从 rescale01() 的一个简单变体开始。也许你想要计算 Z-score，将一个向量重新缩放，使其均值为零，标准差为一：\n\nz_score &lt;- function(x) {\n  (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)\n}\n\nOr maybe you want to wrap up a straightforward case_when() and give it a useful name. For example, this clamp() function ensures all values of a vector lie in between a minimum or a maximum:\n或者，你可能想包装一个简单的 case_when() 并给它一个有用的名字。例如，这个 clamp() 函数确保向量的所有值都介于最小值和最大值之间：\n\nclamp &lt;- function(x, min, max) {\n  case_when(\n    x &lt; min ~ min,\n    x &gt; max ~ max,\n    .default = x\n  )\n}\n\nclamp(1:10, min = 3, max = 7)\n#&gt;  [1] 3 3 3 4 5 6 7 7 7 7\n\nOf course functions don’t just need to work with numeric variables. You might want to do some repeated string manipulation. Maybe you need to make the first character upper case:\n当然，函数不只适用于数值变量。你可能想做一些重复的字符串操作。也许你需要将第一个字符大写：\n\nfirst_upper &lt;- function(x) {\n  str_sub(x, 1, 1) &lt;- str_to_upper(str_sub(x, 1, 1))\n  x\n}\n\nfirst_upper(\"hello\")\n#&gt; [1] \"Hello\"\n\nOr maybe you want to strip percent signs, commas, and dollar signs from a string before converting it into a number:\n或者，你可能想在将字符串转换为数字之前，去除其中的百分号、逗号和美元符号：\n\n# https://twitter.com/NVlabormarket/status/1571939851922198530\nclean_number &lt;- function(x) {\n  is_pct &lt;- str_detect(x, \"%\")\n  num &lt;- x |&gt; \n    str_remove_all(\"%\") |&gt; \n    str_remove_all(\",\") |&gt; \n    str_remove_all(fixed(\"$\")) |&gt; \n    as.numeric()\n  if_else(is_pct, num / 100, num)\n}\n\nclean_number(\"$12,300\")\n#&gt; [1] 12300\nclean_number(\"45%\")\n#&gt; [1] 0.45\n\nSometimes your functions will be highly specialized for one data analysis step. For example, if you have a bunch of variables that record missing values as 997, 998, or 999, you might want to write a function to replace them with NA:\n有时你的函数会为一个数据分析步骤高度特化。例如，如果你有一堆变量将缺失值记录为 997、998 或 999，你可能想写一个函数将它们替换为 NA：\n\nfix_na &lt;- function(x) {\n  if_else(x %in% c(997, 998, 999), NA, x)\n}\n\nWe’ve focused on examples that take a single vector because we think they’re the most common. But there’s no reason that your function can’t take multiple vector inputs.\n我们专注于接受单个向量的示例，因为我们认为它们最常见。但没有理由你的函数不能接受多个向量输入。\n\n25.2.4 Summary functions\nAnother important family of vector functions is summary functions, functions that return a single value for use in summarize(). Sometimes this can just be a matter of setting a default argument or two:\n向量函数的另一个重要家族是摘要函数，即在 summarize() 中使用并返回单个值的函数。有时这可能只是设置一个或两个默认参数的问题：\n\ncommas &lt;- function(x) {\n  str_flatten(x, collapse = \", \", last = \" and \")\n}\n\ncommas(c(\"cat\", \"dog\", \"pigeon\"))\n#&gt; [1] \"cat, dog and pigeon\"\n\nOr you might wrap up a simple computation, like for the coefficient of variation, which divides the standard deviation by the mean:\n或者你可能想包装一个简单的计算，比如变异系数，它是标准差除以均值：\n\ncv &lt;- function(x, na.rm = FALSE) {\n  sd(x, na.rm = na.rm) / mean(x, na.rm = na.rm)\n}\n\ncv(runif(100, min = 0, max = 50))\n#&gt; [1] 0.5196276\ncv(runif(100, min = 0, max = 500))\n#&gt; [1] 0.5652554\n\nOr maybe you just want to make a common pattern easier to remember by giving it a memorable name:\n或者，你可能只是想通过给一个常用模式起一个容易记住的名字，来让它更容易被记住：\n\n# https://twitter.com/gbganalyst/status/1571619641390252033\nn_missing &lt;- function(x) {\n  sum(is.na(x))\n}\n\nYou can also write functions with multiple vector inputs. For example, maybe you want to compute the mean absolute percentage error to help you compare model predictions with actual values:\n你也可以编写具有多个向量输入的函数。例如，也许你想计算平均绝对百分比误差 (mean absolute percentage error) 来帮助你比较模型预测值与实际值：\n\n# https://twitter.com/neilgcurrie/status/1571607727255834625\nmape &lt;- function(actual, predicted) {\n  sum(abs((actual - predicted) / actual)) / length(actual)\n}\n\n\n\n\n\n\n\nRStudio\n\n\n\nOnce you start writing functions, there are two RStudio shortcuts that are super useful:\n一旦你开始编写函数，有两个 RStudio 快捷键会非常有用：\n\nTo find the definition of a function that you’ve written, place the cursor on the name of the function and press F2.\n要查找你编写的函数的定义，请将光标放在函数名称上，然后按 F2。\nTo quickly jump to a function, press Ctrl + . to open the fuzzy file and function finder and type the first few letters of your function name. You can also navigate to files, Quarto sections, and more, making it a very handy navigation tool.\n要快速跳转到某个函数，请按 Ctrl + . 打开模糊文件和函数查找器，然后输入函数名称的前几个字母。你还可以用它导航到文件、Quarto 章节等，使其成为一个非常方便的导航工具。\n\n\n\n\n25.2.5 Exercises\n\n\nPractice turning the following code snippets into functions. Think about what each function does. What would you call it? How many arguments does it need?\n\nmean(is.na(x))\nmean(is.na(y))\nmean(is.na(z))\n\nx / sum(x, na.rm = TRUE)\ny / sum(y, na.rm = TRUE)\nz / sum(z, na.rm = TRUE)\n\nround(x / sum(x, na.rm = TRUE) * 100, 1)\nround(y / sum(y, na.rm = TRUE) * 100, 1)\nround(z / sum(z, na.rm = TRUE) * 100, 1)\n\n\nIn the second variant of rescale01(), infinite values are left unchanged. Can you rewrite rescale01() so that -Inf is mapped to 0, and Inf is mapped to 1?\nGiven a vector of birthdates, write a function to compute the age in years.\nWrite your own functions to compute the variance and skewness of a numeric vector. You can look up the definitions on Wikipedia or elsewhere.\nWrite both_na(), a summary function that takes two vectors of the same length and returns the number of positions that have an NA in both vectors.\n\nRead the documentation to figure out what the following functions do. Why are they useful even though they are so short?\n\nis_directory &lt;- function(x) {\n  file.info(x)$isdir\n}\nis_readable &lt;- function(x) {\n  file.access(x, 4) == 0\n}",
    "crumbs": [
      "Program",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "functions.html#data-frame-functions",
    "href": "functions.html#data-frame-functions",
    "title": "25  Functions",
    "section": "\n25.3 Data frame functions",
    "text": "25.3 Data frame functions\nVector functions are useful for pulling out code that’s repeated within a dplyr verb. But you’ll often also repeat the verbs themselves, particularly within a large pipeline. When you notice yourself copying and pasting multiple verbs multiple times, you might think about writing a data frame function. Data frame functions work like dplyr verbs: they take a data frame as the first argument, some extra arguments that say what to do with it, and return a data frame or a vector.\n向量函数对于提取在 dplyr 动词中重复出现的代码很有用。但是，你也经常会重复使用动词本身，尤其是在大型管道中。当你发现自己多次复制和粘贴多个动词时，你可能会考虑编写一个数据框函数。数据框函数的工作方式类似于 dplyr 动词：它们将数据框作为第一个参数，以及一些额外的参数来说明如何处理它，并返回一个数据框或一个向量。\nTo let you write a function that uses dplyr verbs, we’ll first introduce you to the challenge of indirection and how you can overcome it with embracing, {{ }}. With this theory under your belt, we’ll then show you a bunch of examples to illustrate what you might do with it.\n为了让你能够编写使用 dplyr 动词的函数，我们将首先向你介绍间接引用 (indirection) 的挑战，以及如何通过拥抱 (embracing) {{ }} 来克服它。掌握了这一理论后，我们将向你展示一系列示例，以说明你可以用它做什么。\n\n25.3.1 Indirection and tidy evaluation\nWhen you start writing functions that use dplyr verbs you rapidly hit the problem of indirection. Let’s illustrate the problem with a very simple function: grouped_mean(). The goal of this function is to compute the mean of mean_var grouped by group_var:\n当你开始编写使用 dplyr 动词的函数时，你会很快遇到间接引用的问题。让我们用一个非常简单的函数 grouped_mean() 来说明这个问题。该函数的目标是计算按 group_var 分组的 mean_var 的均值：\n\ngrouped_mean &lt;- function(df, group_var, mean_var) {\n  df |&gt; \n    group_by(group_var) |&gt; \n    summarize(mean(mean_var))\n}\n\nIf we try and use it, we get an error:\n如果我们尝试使用它，我们会得到一个错误：\n\ndiamonds |&gt; grouped_mean(cut, carat)\n#&gt; Error in `group_by()`:\n#&gt; ! Must group by variables found in `.data`.\n#&gt; ✖ Column `group_var` is not found.\n\nTo make the problem a bit more clear, we can use a made up data frame:\n为了让问题更清楚一些，我们可以使用一个虚构的数据框：\n\ndf &lt;- tibble(\n  mean_var = 1,\n  group_var = \"g\",\n  group = 1,\n  x = 10,\n  y = 100\n)\n\ndf |&gt; grouped_mean(group, x)\n#&gt; # A tibble: 1 × 2\n#&gt;   group_var `mean(mean_var)`\n#&gt;   &lt;chr&gt;                &lt;dbl&gt;\n#&gt; 1 g                        1\ndf |&gt; grouped_mean(group, y)\n#&gt; # A tibble: 1 × 2\n#&gt;   group_var `mean(mean_var)`\n#&gt;   &lt;chr&gt;                &lt;dbl&gt;\n#&gt; 1 g                        1\n\nRegardless of how we call grouped_mean() it always does df |&gt; group_by(group_var) |&gt; summarize(mean(mean_var)), instead of df |&gt; group_by(group) |&gt; summarize(mean(x)) or df |&gt; group_by(group) |&gt; summarize(mean(y)). This is a problem of indirection, and it arises because dplyr uses tidy evaluation to allow you to refer to the names of variables inside your data frame without any special treatment.\n无论我们如何调用 grouped_mean()，它总是执行 df |&gt; group_by(group_var) |&gt; summarize(mean(mean_var))，而不是 df |&gt; group_by(group) |&gt; summarize(mean(x)) 或 df |&gt; group_by(group) |&gt; summarize(mean(y))。这是一个间接引用的问题，它的出现是因为 dplyr 使用 整洁求值 (tidy evaluation) 来允许你引用数据框内的变量名而无需任何特殊处理。\nTidy evaluation is great 95% of the time because it makes your data analyses very concise as you never have to say which data frame a variable comes from; it’s obvious from the context. The downside of tidy evaluation comes when we want to wrap up repeated tidyverse code into a function. Here we need some way to tell group_by() and summarize() not to treat group_var and mean_var as the name of the variables, but instead look inside them for the variable we actually want to use.\n整洁求值在 95% 的情况下都很好用，因为它让你的数据分析非常简洁，你永远不必说明一个变量来自哪个数据框；这从上下文中是显而易见的。整洁求值的缺点在于当我们要将重复的 tidyverse 代码封装成函数时。在这里，我们需要一种方法告诉 group_by() 和 summarize() 不要将 group_var 和 mean_var 视为变量的名称，而是查看它们内部以找到我们实际想要使用的变量。\nTidy evaluation includes a solution to this problem called embracing 🤗. Embracing a variable means to wrap it in braces so (e.g.) var becomes {{ var }}. Embracing a variable tells dplyr to use the value stored inside the argument, not the argument as the literal variable name. One way to remember what’s happening is to think of {{ }} as looking down a tunnel — {{ var }} will make a dplyr function look inside of var rather than looking for a variable called var.\n整洁求值包含一个解决此问题的方法，称为 拥抱 (embracing) 🤗。拥抱一个变量意味着将其用花括号括起来，例如 var 变成 {{ var }}。拥抱一个变量会告诉 dplyr 使用存储在参数内的值，而不是将参数本身作为字面上的变量名。记住正在发生什么的一种方法是，将 {{ }} 想象成在看一条隧道 — {{ var }} 会让 dplyr 函数查看 var 的内部，而不是寻找一个名为 var 的变量。\nSo to make grouped_mean() work, we need to surround group_var and mean_var with {{ }}:\n因此，为了让 grouped_mean() 正常工作，我们需要用 {{ }} 将 group_var 和 mean_var 包围起来：\n\ngrouped_mean &lt;- function(df, group_var, mean_var) {\n  df |&gt; \n    group_by({{ group_var }}) |&gt; \n    summarize(mean({{ mean_var }}))\n}\n\ndf |&gt; grouped_mean(group, x)\n#&gt; # A tibble: 1 × 2\n#&gt;   group `mean(x)`\n#&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1     1        10\n\nSuccess!\n成功了！\n\n25.3.2 When to embrace?\nSo the key challenge in writing data frame functions is figuring out which arguments need to be embraced. Fortunately, this is easy because you can look it up from the documentation 😄. There are two terms to look for in the docs which correspond to the two most common sub-types of tidy evaluation:\n因此，编写数据框函数的关键挑战是弄清楚哪些参数需要被拥抱。幸运的是，这很容易，因为你可以从文档中查到 😄。在文档中需要注意两个术语，它们对应于整洁求值最常见的两种子类型：\n\nData-masking: this is used in functions like arrange(), filter(), and summarize() that compute with variables.数据屏蔽 (Data-masking)：这用于像 arrange()、filter() 和 summarize() 这样需要对变量进行计算的函数。\nTidy-selection: this is used for functions like select(), relocate(), and rename() that select variables.整洁选择 (Tidy-selection)：这用于像 select()、relocate() 和 rename() 这样需要选择变量的函数。\n\nYour intuition about which arguments use tidy evaluation should be good for many common functions — just think about whether you can compute (e.g., x + 1) or select (e.g., a:x).\n对于许多常用函数，你关于哪些参数使用整洁求值的直觉应该是准确的——只需考虑你是在进行计算（例如 x + 1）还是在进行选择（例如 a:x）。\nIn the following sections, we’ll explore the sorts of handy functions you might write once you understand embracing.\n在接下来的部分，一旦你理解了拥抱操作，我们将会探讨你可以编写的各种便捷函数。\n\n25.3.3 Common use cases\nIf you commonly perform the same set of summaries when doing initial data exploration, you might consider wrapping them up in a helper function:\n如果你在进行初步数据探索时经常执行同一组汇总操作，你可能会考虑将它们封装到一个辅助函数中：\n\nsummary6 &lt;- function(data, var) {\n  data |&gt; summarize(\n    min = min({{ var }}, na.rm = TRUE),\n    mean = mean({{ var }}, na.rm = TRUE),\n    median = median({{ var }}, na.rm = TRUE),\n    max = max({{ var }}, na.rm = TRUE),\n    n = n(),\n    n_miss = sum(is.na({{ var }})),\n    .groups = \"drop\"\n  )\n}\n\ndiamonds |&gt; summary6(carat)\n#&gt; # A tibble: 1 × 6\n#&gt;     min  mean median   max     n n_miss\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;\n#&gt; 1   0.2 0.798    0.7  5.01 53940      0\n\n(Whenever you wrap summarize() in a helper, we think it’s good practice to set .groups = \"drop\" to both avoid the message and leave the data in an ungrouped state.)\n（无论何时将 summarize() 包装在辅助函数中，我们都认为将 .groups = \"drop\" 设置为好习惯，这样既可以避免消息提示，又能使数据处于未分组状态。）\nThe nice thing about this function is, because it wraps summarize(), you can use it on grouped data:\n这个函数的好处在于，因为它包装了 summarize()，所以你可以对分组数据使用它：\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summary6(carat)\n#&gt; # A tibble: 5 × 7\n#&gt;   cut         min  mean median   max     n n_miss\n#&gt;   &lt;ord&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;\n#&gt; 1 Fair       0.22 1.05    1     5.01  1610      0\n#&gt; 2 Good       0.23 0.849   0.82  3.01  4906      0\n#&gt; 3 Very Good  0.2  0.806   0.71  4    12082      0\n#&gt; 4 Premium    0.2  0.892   0.86  4.01 13791      0\n#&gt; 5 Ideal      0.2  0.703   0.54  3.5  21551      0\n\nFurthermore, since the arguments to summarize are data-masking, so is the var argument to summary6(). That means you can also summarize computed variables:\n此外，由于 summarize 的参数是数据屏蔽的，summary6() 的 var 参数也是如此。这意味着你也可以对计算得出的变量进行汇总：\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summary6(log10(carat))\n#&gt; # A tibble: 5 × 7\n#&gt;   cut          min    mean  median   max     n n_miss\n#&gt;   &lt;ord&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;\n#&gt; 1 Fair      -0.658 -0.0273  0      0.700  1610      0\n#&gt; 2 Good      -0.638 -0.133  -0.0862 0.479  4906      0\n#&gt; 3 Very Good -0.699 -0.164  -0.149  0.602 12082      0\n#&gt; 4 Premium   -0.699 -0.125  -0.0655 0.603 13791      0\n#&gt; 5 Ideal     -0.699 -0.225  -0.268  0.544 21551      0\n\nTo summarize multiple variables, you’ll need to wait until Section 26.2, where you’ll learn how to use across().\n要汇总多个变量，你需要等到 Section 26.2 章节，届时你将学习如何使用 across()。\nAnother popular summarize() helper function is a version of count() that also computes proportions:\n另一个流行的 summarize() 辅助函数是 count() 的一个版本，它还能计算比例：\n\n# https://twitter.com/Diabb6/status/1571635146658402309\ncount_prop &lt;- function(df, var, sort = FALSE) {\n  df |&gt;\n    count({{ var }}, sort = sort) |&gt;\n    mutate(prop = n / sum(n))\n}\n\ndiamonds |&gt; count_prop(clarity)\n#&gt; # A tibble: 8 × 3\n#&gt;   clarity     n   prop\n#&gt;   &lt;ord&gt;   &lt;int&gt;  &lt;dbl&gt;\n#&gt; 1 I1        741 0.0137\n#&gt; 2 SI2      9194 0.170 \n#&gt; 3 SI1     13065 0.242 \n#&gt; 4 VS2     12258 0.227 \n#&gt; 5 VS1      8171 0.151 \n#&gt; 6 VVS2     5066 0.0939\n#&gt; # ℹ 2 more rows\n\nThis function has three arguments: df, var, and sort, and only var needs to be embraced because it’s passed to count() which uses data-masking for all variables. Note that we use a default value for sort so that if the user doesn’t supply their own value it will default to FALSE.\n这个函数有三个参数：df、var 和 sort，只有 var 需要被拥抱，因为它被传递给了 count()，而 count() 对所有变量都使用数据屏蔽。请注意，我们为 sort 使用了默认值，这样如果用户不提供自己的值，它将默认为 FALSE。\nOr maybe you want to find the sorted unique values of a variable for a subset of the data. Rather than supplying a variable and a value to do the filtering, we’ll allow the user to supply a condition:\n或者，你可能想要为数据的子集查找变量的已排序唯一值。与其提供一个变量和一个值来进行筛选，不如让用户提供一个条件：\n\nunique_where &lt;- function(df, condition, var) {\n  df |&gt; \n    filter({{ condition }}) |&gt; \n    distinct({{ var }}) |&gt; \n    arrange({{ var }})\n}\n\n# Find all the destinations in December\nflights |&gt; unique_where(month == 12, dest)\n#&gt; # A tibble: 96 × 1\n#&gt;   dest \n#&gt;   &lt;chr&gt;\n#&gt; 1 ABQ  \n#&gt; 2 ALB  \n#&gt; 3 ATL  \n#&gt; 4 AUS  \n#&gt; 5 AVL  \n#&gt; 6 BDL  \n#&gt; # ℹ 90 more rows\n\nHere we embrace condition because it’s passed to filter() and var because it’s passed to distinct() and arrange().\n这里我们拥抱 condition 是因为它被传递给了 filter()，拥抱 var 是因为它被传递给了 distinct() 和 arrange()。\nWe’ve made all these examples to take a data frame as the first argument, but if you’re working repeatedly with the same data, it can make sense to hardcode it. For example, the following function always works with the flights dataset and always selects time_hour, carrier, and flight since they form the compound primary key that allows you to identify a row.\n我们所有的示例都将数据框作为第一个参数，但是如果你重复使用相同的数据，将其硬编码可能更有意义。例如，下面的函数总是处理 flights 数据集，并且总是选择 time_hour、carrier 和 flight，因为它们构成了可以识别一行的复合主键。\n\nsubset_flights &lt;- function(rows, cols) {\n  flights |&gt; \n    filter({{ rows }}) |&gt; \n    select(time_hour, carrier, flight, {{ cols }})\n}\n\n\n25.3.4 Data-masking vs. tidy-selection\nSometimes you want to select variables inside a function that uses data-masking. For example, imagine you want to write a count_missing() that counts the number of missing observations in rows. You might try writing something like:\n有时你想在一个使用数据屏蔽的函数内部选择变量。例如，假设你想编写一个 count_missing() 函数，用于计算行中缺失观测值的数量。你可能会尝试这样写：\n\ncount_missing &lt;- function(df, group_vars, x_var) {\n  df |&gt; \n    group_by({{ group_vars }}) |&gt; \n    summarize(\n      n_miss = sum(is.na({{ x_var }})),\n      .groups = \"drop\"\n    )\n}\n\nflights |&gt; \n  count_missing(c(year, month, day), dep_time)\n#&gt; Error in `group_by()`:\n#&gt; ℹ In argument: `c(year, month, day)`.\n#&gt; Caused by error:\n#&gt; ! `c(year, month, day)` must be size 336776 or 1, not 1010328.\n\nThis doesn’t work because group_by() uses data-masking, not tidy-selection. We can work around that problem by using the handy pick() function, which allows you to use tidy-selection inside data-masking functions:\n这不起作用，因为 group_by() 使用的是数据屏蔽 (data-masking)，而不是整洁选择 (tidy-selection)。我们可以通过使用方便的 pick() 函数来解决这个问题，它允许你在数据屏蔽函数内部使用整洁选择：\n\ncount_missing &lt;- function(df, group_vars, x_var) {\n  df |&gt; \n    group_by(pick({{ group_vars }})) |&gt; \n    summarize(\n      n_miss = sum(is.na({{ x_var }})),\n      .groups = \"drop\"\n    )\n}\n\nflights |&gt; \n  count_missing(c(year, month, day), dep_time)\n#&gt; # A tibble: 365 × 4\n#&gt;    year month   day n_miss\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n#&gt; 1  2013     1     1      4\n#&gt; 2  2013     1     2      8\n#&gt; 3  2013     1     3     10\n#&gt; 4  2013     1     4      6\n#&gt; 5  2013     1     5      3\n#&gt; 6  2013     1     6      1\n#&gt; # ℹ 359 more rows\n\nAnother convenient use of pick() is to make a 2d table of counts. Here we count using all the variables in the rows and columns, then use pivot_wider() to rearrange the counts into a grid:pick() 的另一个便捷用途是制作一个二维计数表。这里我们使用 rows 和 columns 中的所有变量进行计数，然后使用 pivot_wider() 将计数重新排列成一个网格：\n\n# https://twitter.com/pollicipes/status/1571606508944719876\ncount_wide &lt;- function(data, rows, cols) {\n  data |&gt; \n    count(pick(c({{ rows }}, {{ cols }}))) |&gt; \n    pivot_wider(\n      names_from = {{ cols }}, \n      values_from = n,\n      names_sort = TRUE,\n      values_fill = 0\n    )\n}\n\ndiamonds |&gt; count_wide(c(clarity, color), cut)\n#&gt; # A tibble: 56 × 7\n#&gt;   clarity color  Fair  Good `Very Good` Premium Ideal\n#&gt;   &lt;ord&gt;   &lt;ord&gt; &lt;int&gt; &lt;int&gt;       &lt;int&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1 I1      D         4     8           5      12    13\n#&gt; 2 I1      E         9    23          22      30    18\n#&gt; 3 I1      F        35    19          13      34    42\n#&gt; 4 I1      G        53    19          16      46    16\n#&gt; 5 I1      H        52    14          12      46    38\n#&gt; 6 I1      I        34     9           8      24    17\n#&gt; # ℹ 50 more rows\n\nWhile our examples have mostly focused on dplyr, tidy evaluation also underpins tidyr, and if you look at the pivot_wider() docs you can see that names_from uses tidy-selection.\n虽然我们的示例主要集中在 dplyr 上，但整洁求值 (tidy evaluation) 也是 tidyr 的基础，如果你查看 pivot_wider() 的文档，你会发现 names_from 使用了整洁选择 (tidy-selection)。\n\n25.3.5 Exercises\n\n\nUsing the datasets from nycflights13, write a function that:\n\n\nFinds all flights that were cancelled (i.e. is.na(arr_time)) or delayed by more than an hour.\n\nflights |&gt; filter_severe()\n\n\n\nCounts the number of cancelled flights and the number of flights delayed by more than an hour.\n\nflights |&gt; group_by(dest) |&gt; summarize_severe()\n\n\n\nFinds all flights that were cancelled or delayed by more than a user supplied number of hours:\n\nflights |&gt; filter_severe(hours = 2)\n\n\n\nSummarizes the weather to compute the minimum, mean, and maximum, of a user supplied variable:\n\nweather |&gt; summarize_weather(temp)\n\n\n\nConverts the user supplied variable that uses clock time (e.g., dep_time, arr_time, etc.) into a decimal time (i.e. hours + (minutes / 60)).\n\nflights |&gt; standardize_time(sched_dep_time)\n\n\n\n\nFor each of the following functions list all arguments that use tidy evaluation and describe whether they use data-masking or tidy-selection: distinct(), count(), group_by(), rename_with(), slice_min(), slice_sample().\n\nGeneralize the following function so that you can supply any number of variables to count.\n\ncount_prop &lt;- function(df, var, sort = FALSE) {\n  df |&gt;\n    count({{ var }}, sort = sort) |&gt;\n    mutate(prop = n / sum(n))\n}",
    "crumbs": [
      "Program",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "functions.html#plot-functions",
    "href": "functions.html#plot-functions",
    "title": "25  Functions",
    "section": "\n25.4 Plot functions",
    "text": "25.4 Plot functions\nInstead of returning a data frame, you might want to return a plot.\n除了返回一个数据框，你可能还想返回一个图表。\nFortunately, you can use the same techniques with ggplot2, because aes() is a data-masking function.\n幸运的是，你可以在 ggplot2 中使用相同的技术，因为 aes() 是一个数据掩码 (data-masking) 函数。\nFor example, imagine that you’re making a lot of histograms:\n例如，假设你正在制作大量的直方图：\n\ndiamonds |&gt; \n  ggplot(aes(x = carat)) +\n  geom_histogram(binwidth = 0.1)\n\ndiamonds |&gt; \n  ggplot(aes(x = carat)) +\n  geom_histogram(binwidth = 0.05)\n\nWouldn’t it be nice if you could wrap this up into a histogram function?\n如果你能把这些代码封装成一个直方图函数，那岂不是很好？\nThis is easy as pie once you know that aes() is a data-masking function and you need to embrace:\n一旦你知道 aes() 是一个数据掩码函数并且需要使用 embracing {{}}，这就变得易如反掌了：\n\nhistogram &lt;- function(df, var, binwidth = NULL) {\n  df |&gt; \n    ggplot(aes(x = {{ var }})) + \n    geom_histogram(binwidth = binwidth)\n}\n\ndiamonds |&gt; histogram(carat, 0.1)\n\n\n\n\n\n\n\nNote that histogram() returns a ggplot2 plot, meaning you can still add on additional components if you want.\n请注意，histogram() 返回的是一个 ggplot2 图表对象，这意味着你仍然可以根据需要添加额外的组件。\nJust remember to switch from |&gt; to +:\n只要记得从 |&gt; 切换到 + 即可：\n\ndiamonds |&gt; \n  histogram(carat, 0.1) +\n  labs(x = \"Size (in carats)\", y = \"Number of diamonds\")\n\n\n25.4.1 More variables\nIt’s straightforward to add more variables to the mix.\n在函数中添加更多变量是件很简单的事。\nFor example, maybe you want an easy way to eyeball whether or not a dataset is linear by overlaying a smooth line and a straight line:\n例如，你可能想通过叠加一条平滑曲线和一条直线来快速目测一个数据集是否呈线性关系：\n\n# https://twitter.com/tyler_js_smith/status/1574377116988104704\nlinearity_check &lt;- function(df, x, y) {\n  df |&gt;\n    ggplot(aes(x = {{ x }}, y = {{ y }})) +\n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x, color = \"red\", se = FALSE) +\n    geom_smooth(method = \"lm\", formula = y ~ x, color = \"blue\", se = FALSE) \n}\n\nstarwars |&gt; \n  filter(mass &lt; 1000) |&gt; \n  linearity_check(mass, height)\n\n\n\n\n\n\n\nOr maybe you want an alternative to colored scatterplots for very large datasets where overplotting is a problem:\n或者，对于因数据点过多而存在过度绘制 (overplotting) 问题的大型数据集，你可能想要一种替代彩色散点图的方法：\n\n# https://twitter.com/ppaxisa/status/1574398423175921665\nhex_plot &lt;- function(df, x, y, z, bins = 20, fun = \"mean\") {\n  df |&gt; \n    ggplot(aes(x = {{ x }}, y = {{ y }}, z = {{ z }})) + \n    stat_summary_hex(\n      aes(color = after_scale(fill)), # make border same color as fill\n      bins = bins, \n      fun = fun,\n    )\n}\n\ndiamonds |&gt; hex_plot(carat, price, depth)\n\n\n\n\n\n\n\n\n25.4.2 Combining with other tidyverse\nSome of the most useful helpers combine a dash of data manipulation with ggplot2.\n一些最有用的辅助函数会将少量数据处理与 ggplot2 结合起来。\nFor example, if you might want to do a vertical bar chart where you automatically sort the bars in frequency order using fct_infreq().\n例如，你可能想绘制一个垂直条形图，并使用 fct_infreq() 自动按频率顺序对条形进行排序。\nSince the bar chart is vertical, we also need to reverse the usual order to get the highest values at the top:\n由于条形图是垂直的，我们还需要反转通常的顺序，才能让最高的值显示在顶部：\n\nsorted_bars &lt;- function(df, var) {\n  df |&gt; \n    mutate({{ var }} := fct_rev(fct_infreq({{ var }})))  |&gt;\n    ggplot(aes(y = {{ var }})) +\n    geom_bar()\n}\n\ndiamonds |&gt; sorted_bars(clarity)\n\n\n\n\n\n\n\nWe have to use a new operator here, := (commonly referred to as the “walrus operator”), because we are generating the variable name based on user-supplied data.\n我们在这里必须使用一个新的运算符 :=（通常被称为“海象运算符”），因为我们是根据用户提供的数据来生成变量名的。\nVariable names go on the left hand side of =, but R’s syntax doesn’t allow anything to the left of = except for a single literal name.\n变量名位于 = 的左侧，但 R 的语法不允许在 = 左侧出现除单个字面名称之外的任何内容。\nTo work around this problem, we use the special operator := which tidy evaluation treats in exactly the same way as =.\n为了解决这个问题，我们使用了特殊的运算符 :=，整洁求值 (tidy evaluation) 会将其与 = 完全同等对待。\nOr maybe you want to make it easy to draw a bar plot just for a subset of the data:\n又或者，你可能想让绘制数据子集的条形图变得更容易：\n\nconditional_bars &lt;- function(df, condition, var) {\n  df |&gt; \n    filter({{ condition }}) |&gt; \n    ggplot(aes(x = {{ var }})) + \n    geom_bar()\n}\n\ndiamonds |&gt; conditional_bars(cut == \"Good\", clarity)\n\n\n\n\n\n\n\nYou can also get creative and display data summaries in other ways.\n你也可以发挥创意，用其他方式来展示数据摘要。\nYou can find a cool application at https://gist.github.com/GShotwell/b19ef520b6d56f61a830fabb3454965b; it uses the axis labels to display the highest value.\n你可以在 https://gist.github.com/GShotwell/b19ef520b6d56f61a830fabb3454965b 找到一个很酷的应用；它使用坐标轴标签来显示最高值。\nAs you learn more about ggplot2, the power of your functions will continue to increase.\n随着你对 ggplot2 的了解越来越多，你的函数的功能也会越来越强大。\nWe’ll finish with a more complicated case: labelling the plots you create.\n最后，我们来看一个更复杂的情况：为你创建的图表添加标签。\n\n25.4.3 Labeling\nRemember the histogram function we showed you earlier?\n还记得我们前面展示的直方图函数吗？\n\nhistogram &lt;- function(df, var, binwidth = NULL) {\n  df |&gt; \n    ggplot(aes(x = {{ var }})) + \n    geom_histogram(binwidth = binwidth)\n}\n\nWouldn’t it be nice if we could label the output with the variable and the bin width that was used?\n如果我们能用所使用的变量和组距 (bin width) 来标记输出，那岂不是很好？\nTo do so, we’re going to have to go under the covers of tidy evaluation and use a function from the package we haven’t talked about yet: rlang.\n要做到这一点，我们必须深入了解整洁求值 (tidy evaluation) 的内部机制，并使用一个我们尚未讨论过的包中的函数：rlang。\nrlang is a low-level package that’s used by just about every other package in the tidyverse because it implements tidy evaluation (as well as many other useful tools).\nrlang 是一个底层包，几乎 tidyverse 中的所有其他包都在使用它，因为它实现了整洁求值 (tidy evaluation)（以及许多其他有用的工具）。\nTo solve the labeling problem we can use rlang::englue().\n为了解决添加标签的问题，我们可以使用 rlang::englue()。\nThis works similarly to str_glue(), so any value wrapped in { } will be inserted into the string.\n它的工作方式类似于 str_glue()，因此任何用 { } 包装的值都将被插入到字符串中。\nBut it also understands {{ }}, which automatically inserts the appropriate variable name:\n但它还能理解 {{ }}，该语法会自动插入相应的变量名：\n\nhistogram &lt;- function(df, var, binwidth) {\n  label &lt;- rlang::englue(\"A histogram of {{var}} with binwidth {binwidth}\")\n  \n  df |&gt; \n    ggplot(aes(x = {{ var }})) + \n    geom_histogram(binwidth = binwidth) + \n    labs(title = label)\n}\n\ndiamonds |&gt; histogram(carat, 0.1)\n\n\n\n\n\n\n\nYou can use the same approach in any other place where you want to supply a string in a ggplot2 plot.\n你可以在任何需要在 ggplot2 图表中提供字符串的地方使用同样的方法。\n\n25.4.4 Exercises\nBuild up a rich plotting function by incrementally implementing each of the steps below:\n\nDraw a scatterplot given dataset and x and y variables.\nAdd a line of best fit (i.e. a linear model with no standard errors).\nAdd a title.",
    "crumbs": [
      "Program",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "functions.html#style",
    "href": "functions.html#style",
    "title": "25  Functions",
    "section": "\n25.5 Style",
    "text": "25.5 Style\nR doesn’t care what your function or arguments are called but the names make a big difference for humans.\nR 并不关心你的函数或参数叫什么名字，但这些名字对人类读者来说却至关重要。\nIdeally, the name of your function will be short, but clearly evoke what the function does.\n理想情况下，函数名应该简短，但能清晰地表达函数的功能。\nThat’s hard!\n这很难！\nBut it’s better to be clear than short, as RStudio’s autocomplete makes it easy to type long names.\n但清晰比简短更重要，因为 RStudio 的自动补全功能可以让你轻松输入长名称。\nGenerally, function names should be verbs, and arguments should be nouns.\n通常，函数名应该是动词，参数应该是名词。\nThere are some exceptions: nouns are ok if the function computes a very well known noun (i.e. mean() is better than compute_mean()), or accessing some property of an object (i.e. coef() is better than get_coefficients()).\n也有一些例外：如果函数计算的是一个众所周知的名词（例如 mean() 就比 compute_mean() 好），或者用于访问对象的某个属性（例如 coef() 就比 get_coefficients() 好），那么使用名词作为函数名也是可以的。\nUse your best judgement and don’t be afraid to rename a function if you figure out a better name later.\n请运用你的最佳判断力，如果以后想到了更好的名字，不要害怕重命名函数。\n\n# Too short\nf()\n\n# Not a verb, or descriptive\nmy_awesome_function()\n\n# Long, but clear\nimpute_missing()\ncollapse_years()\n\nR also doesn’t care about how you use white space in your functions but future readers will.\nR 也不关心你在函数中如何使用空白，但未来的读者会在意。\nContinue to follow the rules from Chapter 4.\n请继续遵循 Chapter 4 中的规则。\nAdditionally, function() should always be followed by squiggly brackets ({}), and the contents should be indented by an additional two spaces.\n此外，function() 后面应始终紧跟花括号 ({})，并且其中的内容应额外缩进两个空格。\nThis makes it easier to see the hierarchy in your code by skimming the left-hand margin.\n这样一来，通过浏览代码的左边距，就可以更容易地看清代码的层级结构。\n\n# Missing extra two spaces\ndensity &lt;- function(color, facets, binwidth = 0.1) {\ndiamonds |&gt; \n  ggplot(aes(x = carat, y = after_stat(density), color = {{ color }})) +\n  geom_freqpoly(binwidth = binwidth) +\n  facet_wrap(vars({{ facets }}))\n}\n\n# Pipe indented incorrectly\ndensity &lt;- function(color, facets, binwidth = 0.1) {\n  diamonds |&gt; \n  ggplot(aes(x = carat, y = after_stat(density), color = {{ color }})) +\n  geom_freqpoly(binwidth = binwidth) +\n  facet_wrap(vars({{ facets }}))\n}\n\nAs you can see we recommend putting extra spaces inside of {{ }}.\n正如你所见，我们建议在 {{ }} 内部加上额外的空格。\nThis makes it very obvious that something unusual is happening.\n这使得一些不寻常的操作变得非常显眼。\n\n25.5.1 Exercises\n\n\nRead the source code for each of the following two functions, puzzle out what they do, and then brainstorm better names.\n\nf1 &lt;- function(string, prefix) {\n  str_sub(string, 1, str_length(prefix)) == prefix\n}\n\nf3 &lt;- function(x, y) {\n  rep(y, length.out = length(x))\n}\n\n\nTake a function that you’ve written recently and spend 5 minutes brainstorming a better name for it and its arguments.\nMake a case for why norm_r(), norm_d() etc. would be better than rnorm(), dnorm(). Make a case for the opposite. How could you make the names even clearer?",
    "crumbs": [
      "Program",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "functions.html#summary",
    "href": "functions.html#summary",
    "title": "25  Functions",
    "section": "\n25.6 Summary",
    "text": "25.6 Summary\nIn this chapter, you learned how to write functions for three useful scenarios: creating a vector, creating a data frame, or creating a plot.\n在本章中，你学习了如何针对三种有用的场景编写函数：创建向量、创建数据框或创建图表。\nAlong the way you saw many examples, which hopefully started to get your creative juices flowing, and gave you some ideas for where functions might help your analysis code.\n在此过程中，你看到了许多示例，希望这些示例能激发你的创造力，并让你对函数如何帮助你的分析代码有了一些想法。\nWe have only shown you the bare minimum to get started with functions and there’s much more to learn.\n我们只向你展示了函数入门所需的最基本知识，还有更多内容有待学习。\nA few places to learn more are:\n以下是一些可以深入学习的地方：\n\nTo learn more about programming with tidy evaluation, see useful recipes in programming with dplyr and programming with tidyr and learn more about the theory in What is data-masking and why do I need {{?.\n要了解有关使用整洁求值 (tidy evaluation) 编程的更多信息，请参阅 programming with dplyr 和 programming with tidyr 中的实用方法，并在 什么是数据掩码 (data-masking) 以及为什么我需要 {{? 中学习更多相关理论。\nTo learn more about reducing duplication in your ggplot2 code, read the Programming with ggplot2 chapter of the ggplot2 book.\n要了解有关减少 ggplot2 代码重复的更多信息，请阅读 ggplot2 书籍中的 使用 ggplot2 编程 一章。\nFor more advice on function style, see the tidyverse style guide.\n有关函数风格的更多建议，请参阅 tidyverse 风格指南。\n\nIn the next chapter, we’ll dive into iteration which gives you further tools for reducing code duplication.\n在下一章中，我们将深入探讨迭代，它将为你提供更多减少代码重复的工具。",
    "crumbs": [
      "Program",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "iteration.html",
    "href": "iteration.html",
    "title": "26  Iteration",
    "section": "",
    "text": "26.1 Introduction\nIn this chapter, you’ll learn tools for iteration, repeatedly performing the same action on different objects.\n在本章中，你将学习迭代的工具，即对不同的对象重复执行相同的操作。\nIteration in R generally tends to look rather different from other programming languages because so much of it is implicit and we get it for free.\nR 中的迭代通常看起来与其他编程语言大不相同，因为其中很多是隐式的，我们可以免费获得。\nFor example, if you want to double a numeric vector x in R, you can just write 2 * x.\n例如，如果你想在 R 中将一个数值向量 x 的值加倍，你只需写 2 * x。\nIn most other languages, you’d need to explicitly double each element of x using some sort of for loop.\n在大多数其他语言中，你需要使用某种 for 循环来显式地将 x 的每个元素加倍。\nThis book has already given you a small but powerful number of tools that perform the same action for multiple “things”:\n本书已经为你提供了一些小而强大的工具，可以对多个“事物”执行相同的操作：\nNow it’s time to learn some more general tools, often called functional programming tools because they are built around functions that take other functions as inputs.\n现在是时候学习一些更通用的工具了，这些工具通常被称为函数式编程 (functional programming) 工具，因为它们是围绕着接受其他函数作为输入的函数构建的。\nLearning functional programming can easily veer into the abstract, but in this chapter we’ll keep things concrete by focusing on three common tasks: modifying multiple columns, reading multiple files, and saving multiple objects.\n学习函数式编程很容易变得抽象，但在本章中，我们将通过关注三个常见任务来保持具体性：修改多个列、读取多个文件和保存多个对象。",
    "crumbs": [
      "Program",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "iteration.html#introduction",
    "href": "iteration.html#introduction",
    "title": "26  Iteration",
    "section": "",
    "text": "facet_wrap() and facet_grid() draws a plot for each subset.facet_wrap() 和 facet_grid() 为每个子集绘制一张图。\ngroup_by() plus summarize() computes summary statistics for each subset.group_by() 加上 summarize() 为每个子集计算汇总统计量。\nunnest_wider() and unnest_longer() create new rows and columns for each element of a list-column.unnest_wider() 和 unnest_longer() 为列表列的每个元素创建新的行和列。\n\n\n\n\n26.1.1 Prerequisites\nIn this chapter, we’ll focus on tools provided by dplyr and purrr, both core members of the tidyverse.\n在本章中，我们将重点介绍由 dplyr 和 purrr 提供的工具，它们都是 tidyverse 的核心成员。\nYou’ve seen dplyr before, but purrr is new.\n你之前见过 dplyr，但 purrr 是新的。\nWe’re just going to use a couple of purrr functions in this chapter, but it’s a great package to explore as you improve your programming skills.\n在本章中，我们只会使用几个 purrr 函数，但随着你编程技能的提高，它是一个非常值得探索的包。\n\nlibrary(tidyverse)",
    "crumbs": [
      "Program",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "iteration.html#sec-across",
    "href": "iteration.html#sec-across",
    "title": "26  Iteration",
    "section": "\n26.2 Modifying multiple columns",
    "text": "26.2 Modifying multiple columns\nImagine you have this simple tibble and you want to count the number of observations and compute the median of every column.\n假设你有这个简单的 tibble，并且你想要计算观测值的数量并计算每一列的中位数。\n\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\nYou could do it with copy-and-paste:\n你可以通过复制粘贴来完成：\n\ndf |&gt; summarize(\n  n = n(),\n  a = median(a),\n  b = median(b),\n  c = median(c),\n  d = median(d),\n)\n#&gt; # A tibble: 1 × 5\n#&gt;       n      a      b       c     d\n#&gt;   &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1    10 -0.246 -0.287 -0.0567 0.144\n\nThat breaks our rule of thumb to never copy and paste more than twice, and you can imagine that this will get very tedious if you have tens or even hundreds of columns.\n这违反了我们“绝不复制粘贴超过两次”的经验法则，而且你可以想象，如果你有几十甚至几百列，这将变得非常繁琐。\nInstead, you can use across():\n相反，你可以使用 across()：\n\ndf |&gt; summarize(\n  n = n(),\n  across(a:d, median),\n)\n#&gt; # A tibble: 1 × 5\n#&gt;       n      a      b       c     d\n#&gt;   &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1    10 -0.246 -0.287 -0.0567 0.144\n\nacross() has three particularly important arguments, which we’ll discuss in detail in the following sections.across() 有三个特别重要的参数，我们将在接下来的部分详细讨论。\nYou’ll use the first two every time you use across(): the first argument, .cols, specifies which columns you want to iterate over, and the second argument, .fns, specifies what to do with each column.\n每次使用 across() 时，你都会用到前两个参数：第一个参数 .cols 指定了你想要迭代的列，第二个参数 .fns 指定了对每一列做什么。\nYou can use the .names argument when you need additional control over the names of output columns, which is particularly important when you use across() with mutate().\n当你需要对输出列的名称进行额外控制时，可以使用 .names 参数，这在使用 across() 和 mutate() 时尤其重要。\nWe’ll also discuss two important variations, if_any() and if_all(), which work with filter().\n我们还将讨论两个重要的变体，if_any() 和 if_all()，它们与 filter() 一起使用。\n\n26.2.1 Selecting columns with .cols\n\nThe first argument to across(), .cols, selects the columns to transform.across() 的第一个参数 .cols 用于选择要转换的列。\nThis uses the same specifications as select(), Section 3.3.2, so you can use functions like starts_with() and ends_with() to select columns based on their name.\n它使用与 select() 相同的规范，见 Section 3.3.2，所以你可以使用像 starts_with() 和 ends_with() 这样的函数来根据列名选择列。\nThere are two additional selection techniques that are particularly useful for across(): everything() and where().\n还有两种额外的选择技术对 across() 特别有用：everything() 和 where()。\neverything() is straightforward: it selects every (non-grouping) column:everything() 很直接：它选择每一个（非分组）列：\n\ndf &lt;- tibble(\n  grp = sample(2, 10, replace = TRUE),\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ndf |&gt; \n  group_by(grp) |&gt; \n  summarize(across(everything(), median))\n#&gt; # A tibble: 2 × 5\n#&gt;     grp       a       b     c     d\n#&gt;   &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1 -0.0935 -0.0163 0.363 0.364\n#&gt; 2     2  0.312  -0.0576 0.208 0.565\n\nNote grouping columns (grp here) are not included in across(), because they’re automatically preserved by summarize().\n注意分组列（此处的 grp）不包含在 across() 中，因为它们被 summarize() 自动保留了。\nwhere() allows you to select columns based on their type:where() 允许你根据列的类型来选择列：\n\nwhere(is.numeric) selects all numeric columns.where(is.numeric) 选择所有数值型列。\nwhere(is.character) selects all string columns.where(is.character) 选择所有字符型列。\nwhere(is.Date) selects all date columns.where(is.Date) 选择所有日期型列。\nwhere(is.POSIXct) selects all date-time columns.where(is.POSIXct) 选择所有日期时间型列。\nwhere(is.logical) selects all logical columns.where(is.logical) 选择所有逻辑型列。\n\nJust like other selectors, you can combine these with Boolean algebra.\n就像其他选择器一样，你可以将它们与布尔代数结合使用。\nFor example, !where(is.numeric) selects all non-numeric columns, and starts_with(\"a\") & where(is.logical) selects all logical columns whose name starts with “a”.\n例如，!where(is.numeric) 选择所有非数值型列，而 starts_with(\"a\") & where(is.logical) 选择所有名称以 “a” 开头的逻辑型列。\n\n26.2.2 Calling a single function\nThe second argument to across() defines how each column will be transformed.across() 的第二个参数定义了每一列将如何被转换。\nIn simple cases, as above, this will be a single existing function.\n在简单的情况下，如上所述，这将是一个单一的现有函数。\nThis is a pretty special feature of R: we’re passing one function (median, mean, str_flatten, …) to another function (across).\n这是 R 的一个相当特殊的特性：我们将一个函数（median、mean、str_flatten 等）传递给另一个函数 (across)。\nThis is one of the features that makes R a functional programming language.\n这是使 R 成为一门函数式编程语言的特性之一。\nIt’s important to note that we’re passing this function to across(), so across() can call it; we’re not calling it ourselves.\n重要的是要注意，我们是将这个函数传递给 across()，以便 across() 可以调用它；我们不是自己调用它。\nThat means the function name should never be followed by ().\n这意味着函数名后面不应该跟 ()。\nIf you forget, you’ll get an error:\n如果你忘了，你会得到一个错误：\n\ndf |&gt; \n  group_by(grp) |&gt; \n  summarize(across(everything(), median()))\n#&gt; Error in `summarize()`:\n#&gt; ℹ In argument: `across(everything(), median())`.\n#&gt; Caused by error in `median.default()`:\n#&gt; ! argument \"x\" is missing, with no default\n\nThis error arises because you’re calling the function with no input, e.g.:\n这个错误的出现是因为你在没有输入的情况下调用了函数，例如：\n\nmedian()\n#&gt; Error in median.default(): argument \"x\" is missing, with no default\n\n\n26.2.3 Calling multiple functions\nIn more complex cases, you might want to supply additional arguments or perform multiple transformations.\n在更复杂的情况下，你可能想要提供额外的参数或执行多个转换。\nLet’s motivate this problem with a simple example: what happens if we have some missing values in our data?\n让我们用一个简单的例子来引出这个问题：如果我们的数据中有一些缺失值会发生什么？\nmedian() propagates those missing values, giving us a suboptimal output:median() 会传播这些缺失值，导致一个次优的输出：\n\nrnorm_na &lt;- function(n, n_na, mean = 0, sd = 1) {\n  sample(c(rnorm(n - n_na, mean = mean, sd = sd), rep(NA, n_na)))\n}\n\ndf_miss &lt;- tibble(\n  a = rnorm_na(5, 1),\n  b = rnorm_na(5, 1),\n  c = rnorm_na(5, 2),\n  d = rnorm(5)\n)\ndf_miss |&gt; \n  summarize(\n    across(a:d, median),\n    n = n()\n  )\n#&gt; # A tibble: 1 × 5\n#&gt;       a     b     c     d     n\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n#&gt; 1    NA    NA    NA  1.15     5\n\nIt would be nice if we could pass along na.rm = TRUE to median() to remove these missing values.\n如果我们能将 na.rm = TRUE 传递给 median() 来移除这些缺失值，那就太好了。\nTo do so, instead of calling median() directly, we need to create a new function that calls median() with the desired arguments:\n要做到这一点，我们不能直接调用 median()，而是需要创建一个新函数，用所需的参数来调用 median()：\n\ndf_miss |&gt; \n  summarize(\n    across(a:d, function(x) median(x, na.rm = TRUE)),\n    n = n()\n  )\n#&gt; # A tibble: 1 × 5\n#&gt;       a     b      c     d     n\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 0.139 -1.11 -0.387  1.15     5\n\nThis is a little verbose, so R comes with a handy shortcut: for this sort of throw away, or anonymous[^6], function you can replace function with \\[^7]:\n这有点冗长，所以 R 提供了一个方便的快捷方式：对于这种一次性使用的，或者说匿名 (anonymous)6 的函数，你可以用 \\ 替换 function7：\n\ndf_miss |&gt; \n  summarize(\n    across(a:d, \\(x) median(x, na.rm = TRUE)),\n    n = n()\n  )\n\nIn either case, across() effectively expands to the following code:\n在任何一种情况下，across() 实际上都等同于展开成以下代码：\n\ndf_miss |&gt; \n  summarize(\n    a = median(a, na.rm = TRUE),\n    b = median(b, na.rm = TRUE),\n    c = median(c, na.rm = TRUE),\n    d = median(d, na.rm = TRUE),\n    n = n()\n  )\n\nWhen we remove the missing values from the median(), it would be nice to know just how many values were removed.\n当我们从 median() 中移除缺失值时，如果能知道移除了多少个值就更好了。\nWe can find that out by supplying two functions to across(): one to compute the median and the other to count the missing values.\n我们可以通过向 across() 提供两个函数来做到这一点：一个用于计算中位数，另一个用于计算缺失值的数量。\nYou supply multiple functions by using a named list to .fns:\n你可以通过使用一个命名的列表作为 .fns 来提供多个函数：\n\ndf_miss |&gt; \n  summarize(\n    across(a:d, list(\n      median = \\(x) median(x, na.rm = TRUE),\n      n_miss = \\(x) sum(is.na(x))\n    )),\n    n = n()\n  )\n#&gt; # A tibble: 1 × 9\n#&gt;   a_median a_n_miss b_median b_n_miss c_median c_n_miss d_median d_n_miss\n#&gt;      &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;\n#&gt; 1    0.139        1    -1.11        1   -0.387        2     1.15        0\n#&gt; # ℹ 1 more variable: n &lt;int&gt;\n\nIf you look carefully, you might intuit that the columns are named using a glue specification (Section 14.3.2) like {.col}_{.fn} where .col is the name of the original column and .fn is the name of the function.\n如果你仔细观察，你可能会直觉地认为列是根据一个类似 {.col}_{.fn} 的 glue 规范 (Section 14.3.2) 来命名的，其中 .col 是原始列的名称，.fn 是函数的名称。\nThat’s not a coincidence!\n那不是巧合！\nAs you’ll learn in the next section, you can use the .names argument to supply your own glue spec.\n正如你将在下一节中学到的，你可以使用 .names 参数来提供你自己的 glue 规范。\n\n26.2.4 Column names\nThe result of across() is named according to the specification provided in the .names argument.across() 的结果是根据 .names 参数中提供的规范来命名的。\nWe could specify our own if we wanted the name of the function to come first[^3]:\n如果我们希望函数名排在前面，我们可以自己指定[^3]：\n\ndf_miss |&gt; \n  summarize(\n    across(\n      a:d,\n      list(\n        median = \\(x) median(x, na.rm = TRUE),\n        n_miss = \\(x) sum(is.na(x))\n      ),\n      .names = \"{.fn}_{.col}\"\n    ),\n    n = n(),\n  )\n#&gt; # A tibble: 1 × 9\n#&gt;   median_a n_miss_a median_b n_miss_b median_c n_miss_c median_d n_miss_d\n#&gt;      &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;\n#&gt; 1    0.139        1    -1.11        1   -0.387        2     1.15        0\n#&gt; # ℹ 1 more variable: n &lt;int&gt;\n\nThe .names argument is particularly important when you use across() with mutate().\n当您将 across() 与 mutate() 结合使用时，.names 参数尤其重要。\nBy default, the output of across() is given the same names as the inputs.\n默认情况下，across() 的输出被赋予与输入相同的名称。\nThis means that across() inside of mutate() will replace existing columns.\n这意味着 mutate() 中的 across() 将替换现有的列。\nFor example, here we use coalesce() to replace NAs with 0:\n例如，在这里我们使用 coalesce() 将 NA 替换为 0：\n\ndf_miss |&gt; \n  mutate(\n    across(a:d, \\(x) coalesce(x, 0))\n  )\n#&gt; # A tibble: 5 × 4\n#&gt;        a      b      c     d\n#&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  0.434 -1.25   0     1.60 \n#&gt; 2  0     -1.43  -0.297 0.776\n#&gt; 3 -0.156 -0.980  0     1.15 \n#&gt; 4 -2.61  -0.683 -0.785 2.13 \n#&gt; 5  1.11   0     -0.387 0.704\n\nIf you’d like to instead create new columns, you can use the .names argument to give the output new names:\n如果你想创建新列，可以使用 .names 参数为输出赋予新名称：\n\ndf_miss |&gt; \n  mutate(\n    across(a:d, \\(x) coalesce(x, 0), .names = \"{.col}_na_zero\")\n  )\n#&gt; # A tibble: 5 × 8\n#&gt;        a      b      c     d a_na_zero b_na_zero c_na_zero d_na_zero\n#&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1  0.434 -1.25  NA     1.60      0.434    -1.25      0         1.60 \n#&gt; 2 NA     -1.43  -0.297 0.776     0        -1.43     -0.297     0.776\n#&gt; 3 -0.156 -0.980 NA     1.15     -0.156    -0.980     0         1.15 \n#&gt; 4 -2.61  -0.683 -0.785 2.13     -2.61     -0.683    -0.785     2.13 \n#&gt; 5  1.11  NA     -0.387 0.704     1.11      0        -0.387     0.704\n\n\n26.2.5 Filtering\nacross() is a great match for summarize() and mutate() but it’s more awkward to use with filter(), because you usually combine multiple conditions with either | or &.across() 与 summarize() 和 mutate() 配合得很好，但与 filter() 一起使用时就比较尴尬，因为你通常需要用 | 或 & 来组合多个条件。\nIt’s clear that across() can help to create multiple logical columns, but then what?\n很明显 across() 可以帮助创建多个逻辑列，但之后呢？\nSo dplyr provides two variants of across() called if_any() and if_all():\n因此 dplyr 提供了 across() 的两个变体，名为 if_any() 和 if_all()：\n\n# same as df_miss |&gt; filter(is.na(a) | is.na(b) | is.na(c) | is.na(d))\ndf_miss |&gt; filter(if_any(a:d, is.na))\n#&gt; # A tibble: 4 × 4\n#&gt;        a      b      c     d\n#&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  0.434 -1.25  NA     1.60 \n#&gt; 2 NA     -1.43  -0.297 0.776\n#&gt; 3 -0.156 -0.980 NA     1.15 \n#&gt; 4  1.11  NA     -0.387 0.704\n\n# same as df_miss |&gt; filter(is.na(a) & is.na(b) & is.na(c) & is.na(d))\ndf_miss |&gt; filter(if_all(a:d, is.na))\n#&gt; # A tibble: 0 × 4\n#&gt; # ℹ 4 variables: a &lt;dbl&gt;, b &lt;dbl&gt;, c &lt;dbl&gt;, d &lt;dbl&gt;\n\n\n26.2.6 across() in functions\nacross() is particularly useful to program with because it allows you to operate on multiple columns.across() 在编程中特别有用，因为它允许你对多个列进行操作。\nFor example, Jacob Scott uses this little helper which wraps a bunch of lubridate functions to expand all date columns into year, month, and day columns:\n例如，Jacob Scott 使用这个小辅助函数，它封装了一系列 lubridate 函数，将所有日期列扩展为年、月、日列：\n\nexpand_dates &lt;- function(df) {\n  df |&gt; \n    mutate(\n      across(where(is.Date), list(year = year, month = month, day = mday))\n    )\n}\n\ndf_date &lt;- tibble(\n  name = c(\"Amy\", \"Bob\"),\n  date = ymd(c(\"2009-08-03\", \"2010-01-16\"))\n)\n\ndf_date |&gt; \n  expand_dates()\n#&gt; # A tibble: 2 × 5\n#&gt;   name  date       date_year date_month date_day\n#&gt;   &lt;chr&gt; &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;    &lt;int&gt;\n#&gt; 1 Amy   2009-08-03      2009          8        3\n#&gt; 2 Bob   2010-01-16      2010          1       16\n\nacross() also makes it easy to supply multiple columns in a single argument because the first argument uses tidy-select; you just need to remember to embrace that argument, as we discussed in Section 25.3.2.across() 也使得在单个参数中提供多个列变得容易，因为第一个参数使用了整洁选择（tidy-select）；你只需要记住拥抱（embrace）那个参数，正如我们在 Section 25.3.2 中讨论的那样。\nFor example, this function will compute the means of numeric columns by default.\n例如，这个函数默认会计算数值列的均值。\nBut by supplying the second argument you can choose to summarize just selected columns:\n但通过提供第二个参数，你可以选择只对选定的列进行汇总：\n\nsummarize_means &lt;- function(df, summary_vars = where(is.numeric)) {\n  df |&gt; \n    summarize(\n      across({{ summary_vars }}, \\(x) mean(x, na.rm = TRUE)),\n      n = n(),\n      .groups = \"drop\"\n    )\n}\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summarize_means()\n#&gt; # A tibble: 5 × 9\n#&gt;   cut       carat depth table price     x     y     z     n\n#&gt;   &lt;ord&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 Fair      1.05   64.0  59.1 4359.  6.25  6.18  3.98  1610\n#&gt; 2 Good      0.849  62.4  58.7 3929.  5.84  5.85  3.64  4906\n#&gt; 3 Very Good 0.806  61.8  58.0 3982.  5.74  5.77  3.56 12082\n#&gt; 4 Premium   0.892  61.3  58.7 4584.  5.97  5.94  3.65 13791\n#&gt; 5 Ideal     0.703  61.7  56.0 3458.  5.51  5.52  3.40 21551\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summarize_means(c(carat, x:z))\n#&gt; # A tibble: 5 × 6\n#&gt;   cut       carat     x     y     z     n\n#&gt;   &lt;ord&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 Fair      1.05   6.25  6.18  3.98  1610\n#&gt; 2 Good      0.849  5.84  5.85  3.64  4906\n#&gt; 3 Very Good 0.806  5.74  5.77  3.56 12082\n#&gt; 4 Premium   0.892  5.97  5.94  3.65 13791\n#&gt; 5 Ideal     0.703  5.51  5.52  3.40 21551\n\n\n26.2.7 Compare with pivot_longer()\n\nBefore we go on, it’s worth pointing out an interesting connection between across() and pivot_longer() (Section 5.3). In many cases, you perform the same calculations by first pivoting the data and then performing the operations by group rather than by column. For example, take this multi-function summary:\n在我们继续之前，有必要指出 across() 和 pivot_longer() (Section 5.3) 之间一个有趣的联系。在许多情况下，你可以通过先转换数据，然后按组而不是按列执行操作来完成相同的计算。例如，看这个多函数摘要：\n\ndf |&gt; \n  summarize(across(a:d, list(median = median, mean = mean)))\n#&gt; # A tibble: 1 × 8\n#&gt;   a_median a_mean b_median b_mean c_median c_mean d_median d_mean\n#&gt;      &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1   0.0380  0.205  -0.0163 0.0910    0.260 0.0716    0.540  0.508\n\nWe could compute the same values by pivoting longer and then summarizing:\n我们可以通过先加长数据再进行汇总来计算相同的值：\n\nlong &lt;- df |&gt; \n  pivot_longer(a:d) |&gt; \n  group_by(name) |&gt; \n  summarize(\n    median = median(value),\n    mean = mean(value)\n  )\nlong\n#&gt; # A tibble: 4 × 3\n#&gt;   name   median   mean\n#&gt;   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 a      0.0380 0.205 \n#&gt; 2 b     -0.0163 0.0910\n#&gt; 3 c      0.260  0.0716\n#&gt; 4 d      0.540  0.508\n\nAnd if you wanted the same structure as across() you could pivot again:\n如果你想要和 across() 一样的结构，你可以再次进行转换：\n\nlong |&gt; \n  pivot_wider(\n    names_from = name,\n    values_from = c(median, mean),\n    names_vary = \"slowest\",\n    names_glue = \"{name}_{.value}\"\n  )\n#&gt; # A tibble: 1 × 8\n#&gt;   a_median a_mean b_median b_mean c_median c_mean d_median d_mean\n#&gt;      &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1   0.0380  0.205  -0.0163 0.0910    0.260 0.0716    0.540  0.508\n\nThis is a useful technique to know about because sometimes you’ll hit a problem that’s not currently possible to solve with across(): when you have groups of columns that you want to compute with simultaneously. For example, imagine that our data frame contains both values and weights and we want to compute a weighted mean:\n这是一个很有用的技巧，因为有时你会遇到一个目前无法用 across() 解决的问题：当你有多组列需要同时进行计算时。例如，假设我们的数据框同时包含值和权重，我们想要计算加权平均值：\n\ndf_paired &lt;- tibble(\n  a_val = rnorm(10),\n  a_wts = runif(10),\n  b_val = rnorm(10),\n  b_wts = runif(10),\n  c_val = rnorm(10),\n  c_wts = runif(10),\n  d_val = rnorm(10),\n  d_wts = runif(10)\n)\n\nThere’s currently no way to do this with across()[^4], but it’s relatively straightforward with pivot_longer():\n目前没有办法用 across() 做到这一点4，但用 pivot_longer() 就相对直接了：\n\ndf_long &lt;- df_paired |&gt; \n  pivot_longer(\n    everything(), \n    names_to = c(\"group\", \".value\"), \n    names_sep = \"_\"\n  )\ndf_long\n#&gt; # A tibble: 40 × 3\n#&gt;   group    val   wts\n#&gt;   &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 a      0.715 0.518\n#&gt; 2 b     -0.709 0.691\n#&gt; 3 c      0.718 0.216\n#&gt; 4 d     -0.217 0.733\n#&gt; 5 a     -1.09  0.979\n#&gt; 6 b     -0.209 0.675\n#&gt; # ℹ 34 more rows\n\ndf_long |&gt; \n  group_by(group) |&gt; \n  summarize(mean = weighted.mean(val, wts))\n#&gt; # A tibble: 4 × 2\n#&gt;   group    mean\n#&gt;   &lt;chr&gt;   &lt;dbl&gt;\n#&gt; 1 a      0.126 \n#&gt; 2 b     -0.0704\n#&gt; 3 c     -0.360 \n#&gt; 4 d     -0.248\n\nIf needed, you could pivot_wider() this back to the original form.\n如果需要，你可以用 pivot_wider() 将其转换回原始形式。\n\n26.2.8 Exercises\n\n\nPractice your across() skills by:\n\nComputing the number of unique values in each column of palmerpenguins::penguins.\nComputing the mean of every column in mtcars.\nGrouping diamonds by cut, clarity, and color then counting the number of observations and computing the mean of each numeric column.\n\n\nWhat happens if you use a list of functions in across(), but don’t name them? How is the output named?\nAdjust expand_dates() to automatically remove the date columns after they’ve been expanded. Do you need to embrace any arguments?\n\nExplain what each step of the pipeline in this function does. What special feature of where() are we taking advantage of?\n\nshow_missing &lt;- function(df, group_vars, summary_vars = everything()) {\n  df |&gt; \n    group_by(pick({{ group_vars }})) |&gt; \n    summarize(\n      across({{ summary_vars }}, \\(x) sum(is.na(x))),\n      .groups = \"drop\"\n    ) |&gt;\n    select(where(\\(x) any(x &gt; 0)))\n}\nnycflights13::flights |&gt; show_missing(c(year, month, day))",
    "crumbs": [
      "Program",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "iteration.html#reading-multiple-files",
    "href": "iteration.html#reading-multiple-files",
    "title": "26  Iteration",
    "section": "\n26.3 Reading multiple files",
    "text": "26.3 Reading multiple files\nIn the previous section, you learned how to use dplyr::across() to repeat a transformation on multiple columns. In this section, you’ll learn how to use purrr::map() to do something to every file in a directory. Let’s start with a little motivation: imagine you have a directory full of excel spreadsheets[^2] you want to read. You could do it with copy and paste:\n在上一节中，你学习了如何使用 dplyr::across() 对多列重复进行转换。在本节中，你将学习如何使用 purrr::map() 对目录中的每个文件执行操作。让我们从一个小的动机开始：想象一下，你有一个装满了你想要读取的 Excel 电子表格的目录[^2]。你可以通过复制粘贴来完成：\n\ndata2019 &lt;- readxl::read_excel(\"data/y2019.xlsx\")\ndata2020 &lt;- readxl::read_excel(\"data/y2020.xlsx\")\ndata2021 &lt;- readxl::read_excel(\"data/y2021.xlsx\")\ndata2022 &lt;- readxl::read_excel(\"data/y2022.xlsx\")\n\nAnd then use dplyr::bind_rows() to combine them all together:\n然后使用 dplyr::bind_rows() 将它们全部合并在一起：\n\ndata &lt;- bind_rows(data2019, data2020, data2021, data2022)\n\nYou can imagine that this would get tedious quickly, especially if you had hundreds of files, not just four. The following sections show you how to automate this sort of task. There are three basic steps: use list.files() to list all the files in a directory, then use purrr::map() to read each of them into a list, then use purrr::list_rbind() to combine them into a single data frame. We’ll then discuss how you can handle situations of increasing heterogeneity, where you can’t do exactly the same thing to every file.\n你可以想象，这很快就会变得乏味，特别是如果你有数百个文件，而不仅仅是四个。接下来的部分将向你展示如何自动化这类任务。有三个基本步骤：使用 list.files() 列出目录中的所有文件，然后使用 purrr::map() 将每个文件读入一个列表，再使用 purrr::list_rbind() 将它们合并成一个单一的数据框。然后，我们将讨论如何处理异质性增加的情况，即你不能对每个文件都做完全相同的事情。\n\n26.3.1 Listing files in a directory\nAs the name suggests, list.files() lists the files in a directory. You’ll almost always use three arguments:\n顾名思义，list.files() 会列出目录中的文件。你几乎总是会使用三个参数：\n\nThe first argument, path, is the directory to look in.\n第一个参数 path 是要查找的目录。\npattern is a regular expression used to filter the file names. The most common pattern is something like [.]xlsx$ or [.]csv$ to find all files with a specified extension.pattern 是一个用于筛选文件名的正则表达式。最常见的模式是像 [.]xlsx$ 或 [.]csv$ 这样的，用来查找所有具有指定扩展名的文件。\nfull.names determines whether or not the directory name should be included in the output.\nYou almost always want this to be TRUE.full.names 决定了目录名是否应包含在输出中。你几乎总是希望这个值为 TRUE。\n\nTo make our motivating example concrete, this book contains a folder with 12 excel spreadsheets containing data from the gapminder package. Each file contains one year’s worth of data for 142 countries. We can list them all with the appropriate call to list.files():\n为了让我们的激励示例更具体，本书包含一个文件夹，里面有 12 个 Excel 电子表格，其中包含来自 gapminder 包的数据。每个文件包含 142 个国家一年的数据。我们可以使用对 list.files() 的适当调用来列出所有这些文件：\n\npaths &lt;- list.files(\"data/gapminder\", pattern = \"[.]xlsx$\", full.names = TRUE)\npaths\n#&gt;  [1] \"data/gapminder/1952.xlsx\" \"data/gapminder/1957.xlsx\"\n#&gt;  [3] \"data/gapminder/1962.xlsx\" \"data/gapminder/1967.xlsx\"\n#&gt;  [5] \"data/gapminder/1972.xlsx\" \"data/gapminder/1977.xlsx\"\n#&gt;  [7] \"data/gapminder/1982.xlsx\" \"data/gapminder/1987.xlsx\"\n#&gt;  [9] \"data/gapminder/1992.xlsx\" \"data/gapminder/1997.xlsx\"\n#&gt; [11] \"data/gapminder/2002.xlsx\" \"data/gapminder/2007.xlsx\"\n\n\n26.3.2 Lists\nNow that we have these 12 paths, we could call read_excel() 12 times to get 12 data frames:\n现在我们有了这 12 个路径，我们可以调用 read_excel() 12 次来获取 12 个数据框：\n\ngapminder_1952 &lt;- readxl::read_excel(\"data/gapminder/1952.xlsx\")\ngapminder_1957 &lt;- readxl::read_excel(\"data/gapminder/1957.xlsx\")\ngapminder_1962 &lt;- readxl::read_excel(\"data/gapminder/1962.xlsx\")\n  ...,\ngapminder_2007 &lt;- readxl::read_excel(\"data/gapminder/2007.xlsx\")\n\nBut putting each sheet into its own variable is going to make it hard to work with them a few steps down the road. Instead, they’ll be easier to work with if we put them into a single object. A list is the perfect tool for this job:\n但是，将每个工作表放入其自己的变量中，会在后续步骤中难以处理。相反，如果我们将它们放入一个单一的对象中，处理起来会更容易。列表 (list) 是完成这项工作的完美工具：\n\nfiles &lt;- list(\n  readxl::read_excel(\"data/gapminder/1952.xlsx\"),\n  readxl::read_excel(\"data/gapminder/1957.xlsx\"),\n  readxl::read_excel(\"data/gapminder/1962.xlsx\"),\n  ...,\n  readxl::read_excel(\"data/gapminder/2007.xlsx\")\n)\n\nNow that you have these data frames in a list, how do you get one out? You can use files[[i]] to extract the i&lt;sup&gt;th&lt;/sup&gt; element:\n现在你已经将这些数据框放在一个列表中了，你该如何取出一个呢？你可以使用 files[[i]] 来提取第 i&lt;sup&gt;th&lt;/sup&gt; 个元素：\n\nfiles[[3]]\n#&gt; # A tibble: 142 × 5\n#&gt;   country     continent lifeExp      pop gdpPercap\n#&gt;   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 Afghanistan Asia         32.0 10267083      853.\n#&gt; 2 Albania     Europe       64.8  1728137     2313.\n#&gt; 3 Algeria     Africa       48.3 11000948     2551.\n#&gt; 4 Angola      Africa       34    4826015     4269.\n#&gt; 5 Argentina   Americas     65.1 21283783     7133.\n#&gt; 6 Australia   Oceania      70.9 10794968    12217.\n#&gt; # ℹ 136 more rows\n\nWe’ll come back to [[ in more detail in Section 27.3.\n我们将在 Section 27.3 中更详细地回过头来讨论 [[。\n\n26.3.3 purrr::map() and list_rbind()\n\nThe code to collect those data frames in a list “by hand” is basically just as tedious to type as code that reads the files one-by-one. Happily, we can use purrr::map() to make even better use of our paths vector. map() is similar toacross(), but instead of doing something to each column in a data frame, it does something to each element of a vector.map(x, f) is shorthand for:\n“手动”收集那些数据框到列表中的代码，基本上和逐个读取文件的代码一样乏味。幸运的是，我们可以使用 purrr::map() 来更好地利用我们的 paths 向量。map() 类似于 across()，但它不是对数据框的每一列执行操作，而是对向量的每个元素执行操作。map(x, f) 是以下代码的简写：\n\nlist(\n  f(x[[1]]),\n  f(x[[2]]),\n  ...,\n  f(x[[n]])\n)\n\nSo we can use map() to get a list of 12 data frames:\n所以我们可以使用 map() 来得到一个包含 12 个数据框的列表：\n\nfiles &lt;- map(paths, readxl::read_excel)\nlength(files)\n#&gt; [1] 12\n\nfiles[[1]]\n#&gt; # A tibble: 142 × 5\n#&gt;   country     continent lifeExp      pop gdpPercap\n#&gt;   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 Afghanistan Asia         28.8  8425333      779.\n#&gt; 2 Albania     Europe       55.2  1282697     1601.\n#&gt; 3 Algeria     Africa       43.1  9279525     2449.\n#&gt; 4 Angola      Africa       30.0  4232095     3521.\n#&gt; 5 Argentina   Americas     62.5 17876956     5911.\n#&gt; 6 Australia   Oceania      69.1  8691212    10040.\n#&gt; # ℹ 136 more rows\n\n(This is another data structure that doesn’t display particularly compactly with str() so you might want to load it into RStudio and inspect it with View()).\n（这是另一个用 str() 显示不够紧凑的数据结构，所以你可能想将它加载到 RStudio 中并用 View() 来检查它）。\nNow we can use purrr::list_rbind() to combine that list of data frames into a single data frame:\n现在我们可以使用 purrr::list_rbind() 将那个数据框列表合并成一个单一的数据框：\n\nlist_rbind(files)\n#&gt; # A tibble: 1,704 × 5\n#&gt;   country     continent lifeExp      pop gdpPercap\n#&gt;   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 Afghanistan Asia         28.8  8425333      779.\n#&gt; 2 Albania     Europe       55.2  1282697     1601.\n#&gt; 3 Algeria     Africa       43.1  9279525     2449.\n#&gt; 4 Angola      Africa       30.0  4232095     3521.\n#&gt; 5 Argentina   Americas     62.5 17876956     5911.\n#&gt; 6 Australia   Oceania      69.1  8691212    10040.\n#&gt; # ℹ 1,698 more rows\n\nOr we could do both steps at once in a pipeline:\n或者我们可以在一个管道中一次性完成这两个步骤：\n\npaths |&gt; \n  map(readxl::read_excel) |&gt; \n  list_rbind()\n\nWhat if we want to pass in extra arguments to read_excel()? We use the same technique that we used with across(). For example, it’s often useful to peek at the first few rows of the data with n_max = 1:\n如果我们想给 read_excel() 传递额外的参数怎么办？我们使用与 across() 相同的技术。例如，用 n_max = 1 来查看数据的前几行通常很有用：\n\npaths |&gt; \n  map(\\(path) readxl::read_excel(path, n_max = 1)) |&gt; \n  list_rbind()\n#&gt; # A tibble: 12 × 5\n#&gt;   country     continent lifeExp      pop gdpPercap\n#&gt;   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 Afghanistan Asia         28.8  8425333      779.\n#&gt; 2 Afghanistan Asia         30.3  9240934      821.\n#&gt; 3 Afghanistan Asia         32.0 10267083      853.\n#&gt; 4 Afghanistan Asia         34.0 11537966      836.\n#&gt; 5 Afghanistan Asia         36.1 13079460      740.\n#&gt; 6 Afghanistan Asia         38.4 14880372      786.\n#&gt; # ℹ 6 more rows\n\nThis makes it clear that something is missing: there’s no year column because that value is recorded in the path, not in the individual files. We’ll tackle that problem next.\n这清楚地表明有些东西丢失了：没有 year 列，因为该值记录在路径中，而不是在单个文件中。我们接下来将解决这个问题。\n\n26.3.4 Data in the path\nSometimes the name of the file is data itself. In this example, the file name contains the year, which is not otherwise recorded in the individual files. To get that column into the final data frame, we need to do two things:\n有时文件名本身就是数据。在这个例子中，文件名包含了年份，而这个信息在单个文件中并没有记录。为了将这一列加入到最终的数据框中，我们需要做两件事：\nFirst, we name the vector of paths. The easiest way to do this is with the set_names() function, which can take a function. Here we use basename() to extract just the file name from the full path:\n首先，我们为路径向量命名。最简单的方法是使用 set_names() 函数，它可以接受一个函数作为参数。这里我们使用 basename() 从完整路径中提取文件名：\n\npaths |&gt; set_names(basename) \n#&gt;                  1952.xlsx                  1957.xlsx \n#&gt; \"data/gapminder/1952.xlsx\" \"data/gapminder/1957.xlsx\" \n#&gt;                  1962.xlsx                  1967.xlsx \n#&gt; \"data/gapminder/1962.xlsx\" \"data/gapminder/1967.xlsx\" \n#&gt;                  1972.xlsx                  1977.xlsx \n#&gt; \"data/gapminder/1972.xlsx\" \"data/gapminder/1977.xlsx\" \n#&gt;                  1982.xlsx                  1987.xlsx \n#&gt; \"data/gapminder/1982.xlsx\" \"data/gapminder/1987.xlsx\" \n#&gt;                  1992.xlsx                  1997.xlsx \n#&gt; \"data/gapminder/1992.xlsx\" \"data/gapminder/1997.xlsx\" \n#&gt;                  2002.xlsx                  2007.xlsx \n#&gt; \"data/gapminder/2002.xlsx\" \"data/gapminder/2007.xlsx\"\n\nThose names are automatically carried along by all the map functions, so the list of data frames will have those same names:\n这些名称会自动被所有 map 函数沿用，所以数据框列表也会有相同的名称：\n\nfiles &lt;- paths |&gt; \n  set_names(basename) |&gt; \n  map(readxl::read_excel)\n\nThat makes this call to map() shorthand for:\n这使得对 map() 的调用成为以下代码的简写：\n\nfiles &lt;- list(\n  \"1952.xlsx\" = readxl::read_excel(\"data/gapminder/1952.xlsx\"),\n  \"1957.xlsx\" = readxl::read_excel(\"data/gapminder/1957.xlsx\"),\n  \"1962.xlsx\" = readxl::read_excel(\"data/gapminder/1962.xlsx\"),\n  ...,\n  \"2007.xlsx\" = readxl::read_excel(\"data/gapminder/2007.xlsx\")\n)\n\nYou can also use [[ to extract elements by name:\n你也可以使用 [[ 按名称提取元素：\n\nfiles[[\"1962.xlsx\"]]\n#&gt; # A tibble: 142 × 5\n#&gt;   country     continent lifeExp      pop gdpPercap\n#&gt;   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 Afghanistan Asia         32.0 10267083      853.\n#&gt; 2 Albania     Europe       64.8  1728137     2313.\n#&gt; 3 Algeria     Africa       48.3 11000948     2551.\n#&gt; 4 Angola      Africa       34    4826015     4269.\n#&gt; 5 Argentina   Americas     65.1 21283783     7133.\n#&gt; 6 Australia   Oceania      70.9 10794968    12217.\n#&gt; # ℹ 136 more rows\n\nThen we use the names_to argument to list_rbind() to tell it to save the names into a new column called year then use readr::parse_number() to extract the number from the string.\n然后我们使用 list_rbind() 的 names_to 参数，告诉它将名称保存到一个名为 year 的新列中，然后使用 readr::parse_number() 从字符串中提取数字。\n\npaths |&gt; \n  set_names(basename) |&gt; \n  map(readxl::read_excel) |&gt; \n  list_rbind(names_to = \"year\") |&gt; \n  mutate(year = parse_number(year))\n#&gt; # A tibble: 1,704 × 6\n#&gt;    year country     continent lifeExp      pop gdpPercap\n#&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1  1952 Afghanistan Asia         28.8  8425333      779.\n#&gt; 2  1952 Albania     Europe       55.2  1282697     1601.\n#&gt; 3  1952 Algeria     Africa       43.1  9279525     2449.\n#&gt; 4  1952 Angola      Africa       30.0  4232095     3521.\n#&gt; 5  1952 Argentina   Americas     62.5 17876956     5911.\n#&gt; 6  1952 Australia   Oceania      69.1  8691212    10040.\n#&gt; # ℹ 1,698 more rows\n\nIn more complicated cases, there might be other variables stored in the directory name, or maybe the file name contains multiple bits of data. In that case, use set_names() (without any arguments) to record the full path, and then use tidyr::separate_wider_delim() and friends to turn them into useful columns.\n在更复杂的情况下，目录名中可能存储了其他变量，或者文件名可能包含多个数据片段。在这种情况下，使用 set_names()（不带任何参数）来记录完整路径，然后使用 tidyr::separate_wider_delim() 及其相关函数将它们转换成有用的列。\n\npaths |&gt; \n  set_names() |&gt; \n  map(readxl::read_excel) |&gt; \n  list_rbind(names_to = \"year\") |&gt; \n  separate_wider_delim(year, delim = \"/\", names = c(NA, \"dir\", \"file\")) |&gt; \n  separate_wider_delim(file, delim = \".\", names = c(\"file\", \"ext\"))\n#&gt; # A tibble: 1,704 × 8\n#&gt;   dir       file  ext   country     continent lifeExp      pop gdpPercap\n#&gt;   &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 gapminder 1952  xlsx  Afghanistan Asia         28.8  8425333      779.\n#&gt; 2 gapminder 1952  xlsx  Albania     Europe       55.2  1282697     1601.\n#&gt; 3 gapminder 1952  xlsx  Algeria     Africa       43.1  9279525     2449.\n#&gt; 4 gapminder 1952  xlsx  Angola      Africa       30.0  4232095     3521.\n#&gt; 5 gapminder 1952  xlsx  Argentina   Americas     62.5 17876956     5911.\n#&gt; 6 gapminder 1952  xlsx  Australia   Oceania      69.1  8691212    10040.\n#&gt; # ℹ 1,698 more rows\n\n\n26.3.5 Save your work\nNow that you’ve done all this hard work to get to a nice tidy data frame, it’s a great time to save your work:\n既然你已经费了这么多功夫得到了一个整洁的数据框，现在是保存你工作成果的好时机：\n\ngapminder &lt;- paths |&gt; \n  set_names(basename) |&gt; \n  map(readxl::read_excel) |&gt; \n  list_rbind(names_to = \"year\") |&gt; \n  mutate(year = parse_number(year))\n\nwrite_csv(gapminder, \"gapminder.csv\")\n\nNow when you come back to this problem in the future, you can read in a single csv file. For large and richer datasets, using parquet might be a better choice than .csv, as discussed in Section 22.4.\n现在，当你将来再次处理这个问题时，你可以直接读取一个 CSV 文件。对于大型且更丰富的数据集，使用 Parquet 可能是比 .csv 更好的选择，正如在 Section 22.4 中讨论的那样。\nIf you’re working in a project, we suggest calling the file that does this sort of data prep work something like 0-cleanup.R. The 0 in the file name suggests that this should be run before anything else.\n如果你在一个项目中工作，我们建议将执行此类数据准备工作的文件命名为 0-cleanup.R 之类的名称。文件名中的 0 暗示这个文件应该在其他任何文件之前运行。\nIf your input data files change over time, you might consider learning a tool like targets to set up your data cleaning code to automatically re-run whenever one of the input files is modified.\n如果你的输入数据文件随时间变化，你可能需要考虑学习一个像 targets 这样的工具，来设置你的数据清理代码，以便在任何输入文件被修改时自动重新运行。\n\n26.3.6 Many simple iterations\nHere we’ve just loaded the data directly from disk, and were lucky enough to get a tidy dataset. In most cases, you’ll need to do some additional tidying, and you have two basic options: you can do one round of iteration with a complex function, or do multiple rounds of iteration with simple functions. In our experience most folks reach first for one complex iteration, but you’re often better by doing multiple simple iterations.\n在这里，我们只是直接从磁盘加载了数据，并且幸运地得到了一个整洁的数据集。在大多数情况下，你需要进行一些额外的整理，你有两个基本选择：你可以用一个复杂的函数进行一轮迭代，或者用简单的函数进行多轮迭代。根据我们的经验，大多数人首先会选择进行一次复杂的迭代，但通常通过进行多次简单的迭代会更好。\nFor example, imagine that you want to read in a bunch of files, filter out missing values, pivot, and then combine. One way to approach the problem is to write a function that takes a file and does all those steps then call map() once:\n例如，想象一下你想读入一堆文件，过滤掉缺失值，进行数据透视，然后合并。一种解决问题的方法是编写一个函数，该函数接受一个文件并执行所有这些步骤，然后调用 map() 一次：\n\nprocess_file &lt;- function(path) {\n  df &lt;- read_csv(path)\n  \n  df |&gt; \n    filter(!is.na(id)) |&gt; \n    mutate(id = tolower(id)) |&gt; \n    pivot_longer(jan:dec, names_to = \"month\")\n}\n\npaths |&gt; \n  map(process_file) |&gt; \n  list_rbind()\n\nAlternatively, you could perform each step of process_file() to every file:\n或者，你可以对每个文件执行 process_file() 的每一步：\n\npaths |&gt; \n  map(read_csv) |&gt; \n  map(\\(df) df |&gt; filter(!is.na(id))) |&gt; \n  map(\\(df) df |&gt; mutate(id = tolower(id))) |&gt; \n  map(\\(df) df |&gt; pivot_longer(jan:dec, names_to = \"month\")) |&gt; \n  list_rbind()\n\nWe recommend this approach because it stops you getting fixated on getting the first file right before moving on to the rest. By considering all of the data when doing tidying and cleaning, you’re more likely to think holistically and end up with a higher quality result.\n我们推荐这种方法，因为它能防止你在处理其他文件之前，过分执着于把第一个文件处理好。在进行数据整理和清洗时，通过考虑所有数据，你更有可能进行整体思考，并最终得到更高质量的结果。\nIn this particular example, there’s another optimization you could make, by binding all the data frames together earlier. Then you can rely on regular dplyr behavior:\n在这个特定的例子中，你还可以进行另一个优化，即更早地将所有数据框绑定在一起。然后你就可以依赖常规的 dplyr 行为：\n\npaths |&gt; \n  map(read_csv) |&gt; \n  list_rbind() |&gt; \n  filter(!is.na(id)) |&gt; \n  mutate(id = tolower(id)) |&gt; \n  pivot_longer(jan:dec, names_to = \"month\")\n\n\n26.3.7 Heterogeneous data\nUnfortunately, sometimes it’s not possible to go from map() straight to list_rbind() because the data frames are so heterogeneous that list_rbind() either fails or yields a data frame that’s not very useful. In that case, it’s still useful to start by loading all of the files:\n不幸的是，有时无法直接从 map() 转到 list_rbind()，因为数据框的异构性太强，导致 list_rbind() 要么失败，要么产生一个不太有用的数据框。在这种情况下，从加载所有文件开始仍然是很有用的：\n\nfiles &lt;- paths |&gt; \n  map(readxl::read_excel) \n\nThen a very useful strategy is to capture the structure of the data frames so that you can explore it using your data science skills. One way to do so is with this handy df_types function[^1] that returns a tibble with one row for each column:\n然后，一个非常有用的策略是捕获数据框的结构，以便你可以运用数据科学技能对其进行探索。一种方法是使用这个方便的 df_types 函数[^1]，它会返回一个 tibble，每行对应一列：\n\ndf_types &lt;- function(df) {\n  tibble(\n    col_name = names(df), \n    col_type = map_chr(df, vctrs::vec_ptype_full),\n    n_miss = map_int(df, \\(x) sum(is.na(x)))\n  )\n}\n\ndf_types(gapminder)\n#&gt; # A tibble: 6 × 3\n#&gt;   col_name  col_type  n_miss\n#&gt;   &lt;chr&gt;     &lt;chr&gt;      &lt;int&gt;\n#&gt; 1 year      double         0\n#&gt; 2 country   character      0\n#&gt; 3 continent character      0\n#&gt; 4 lifeExp   double         0\n#&gt; 5 pop       double         0\n#&gt; 6 gdpPercap double         0\n\nYou can then apply this function to all of the files, and maybe do some pivoting to make it easier to see where the differences are. For example, this makes it easy to verify that the gapminder spreadsheets that we’ve been working with are all quite homogeneous:\n然后，你可以将此函数应用于所有文件，并可能进行一些透视操作，以便更容易地查看差异所在。例如，这可以轻松验证我们一直在使用的 gapminder 电子表格都非常同质：\n\nfiles |&gt; \n  map(df_types) |&gt; \n  list_rbind(names_to = \"file_name\") |&gt; \n  select(-n_miss) |&gt; \n  pivot_wider(names_from = col_name, values_from = col_type)\n#&gt; # A tibble: 12 × 6\n#&gt;   file_name country   continent lifeExp pop    gdpPercap\n#&gt;   &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;    \n#&gt; 1 1952.xlsx character character double  double double   \n#&gt; 2 1957.xlsx character character double  double double   \n#&gt; 3 1962.xlsx character character double  double double   \n#&gt; 4 1967.xlsx character character double  double double   \n#&gt; 5 1972.xlsx character character double  double double   \n#&gt; 6 1977.xlsx character character double  double double   \n#&gt; # ℹ 6 more rows\n\nIf the files have heterogeneous formats, you might need to do more processing before you can successfully merge them. Unfortunately, we’re now going to leave you to figure that out on your own, but you might want to read about map_if() and map_at(). map_if() allows you to selectively modify elements of a list based on their values; map_at() allows you to selectively modify elements based on their names.\n如果文件具有异构格式，你可能需要进行更多处理才能成功合并它们。不幸的是，现在我们将让你自己去解决这个问题，但你可能想了解一下 map_if() 和 map_at()。map_if() 允许你根据列表元素的值选择性地修改它们；map_at() 允许你根据列表元素的名称选择性地修改它们。\n\n26.3.8 Handling failures\nSometimes the structure of your data might be sufficiently wild that you can’t even read all the files with a single command. And then you’ll encounter one of the downsides of map(): it succeeds or fails as a whole. map() will either successfully read all of the files in a directory or fail with an error, reading zero files. This is annoying: why does one failure prevent you from accessing all the other successes?\n有时，你的数据结构可能非常混乱，以至于你甚至无法用一个命令读取所有文件。然后你会遇到 map() 的一个缺点：它要么整体成功，要么整体失败。map() 要么成功读取目录中的所有文件，要么因错误而失败，读取零个文件。这很烦人：为什么一个失败会阻止你访问所有其他成功的结果？\nLuckily, purrr comes with a helper to tackle this problem: possibly(). possibly() is what’s known as a function operator: it takes a function and returns a function with modified behavior. In particular, possibly() changes a function from erroring to returning a value that you specify:\n幸运的是，purrr 提供了一个辅助函数来解决这个问题：possibly()。possibly() 被称为函数操作符：它接受一个函数并返回一个行为被修改了的函数。具体来说，possibly() 将一个会出错的函数更改为返回你指定的值：\n\nfiles &lt;- paths |&gt; \n  map(possibly(\\(path) readxl::read_excel(path), NULL))\n\ndata &lt;- files |&gt; list_rbind()\n\nThis works particularly well here because list_rbind(), like many tidyverse functions, automatically ignores NULLs.\n这在这里特别有效，因为 list_rbind() 和许多 tidyverse 函数一样，会自动忽略 NULL 值。\nNow you have all the data that can be read easily, and it’s time to tackle the hard part of figuring out why some files failed to load and what to do about it. Start by getting the paths that failed:\n现在你已经有了所有可以轻松读取的数据，是时候解决困难的部分了：弄清楚为什么有些文件加载失败以及如何处理。首先获取失败的路径：\n\nfailed &lt;- map_vec(files, is.null)\npaths[failed]\n#&gt; character(0)\n\nThen call the import function again for each failure and figure out what went wrong.\n然后对每个失败的文件再次调用导入函数，找出问题所在。",
    "crumbs": [
      "Program",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "iteration.html#saving-multiple-outputs",
    "href": "iteration.html#saving-multiple-outputs",
    "title": "26  Iteration",
    "section": "\n26.4 Saving multiple outputs",
    "text": "26.4 Saving multiple outputs\nIn the last section, you learned about map(), which is useful for reading multiple files into a single object. In this section, we’ll now explore sort of the opposite problem: how can you take one or more R objects and save it to one or more files? We’ll explore this challenge using three examples:\n在上一节中，你学习了 map()，它对于将多个文件读入单个对象很有用。在本节中，我们将探讨一个相反的问题：如何将一个或多个 R 对象保存到一个或多个文件中？我们将通过三个例子来探讨这个挑战：\n\nSaving multiple data frames into one database.\n将多个数据框保存到一个数据库中。\nSaving multiple data frames into multiple .csv files.\n将多个数据框保存到多个 .csv 文件中。\nSaving multiple plots to multiple .png files.\n将多个图保存到多个 .png 文件中。\n\n\n26.4.1 Writing to a database\nSometimes when working with many files at once, it’s not possible to fit all your data into memory at once, and you can’t do map(files, read_csv). One approach to deal with this problem is to load your data into a database so you can access just the bits you need with dbplyr.\n有时，当一次处理多个文件时，不可能将所有数据一次性装入内存，也就无法执行 map(files, read_csv)。解决这个问题的一种方法是将数据加载到数据库中，这样你就可以使用 dbplyr 只访问你需要的部分。\nIf you’re lucky, the database package you’re using will provide a handy function that takes a vector of paths and loads them all into the database. This is the case with duckdb’s duckdb_read_csv():\n如果幸运的话，你使用的数据库包会提供一个方便的函数，该函数接受一个路径向量并将它们全部加载到数据库中。duckdb 的 duckdb_read_csv() 就是这种情况：\n\ncon &lt;- DBI::dbConnect(duckdb::duckdb())\nduckdb::duckdb_read_csv(con, \"gapminder\", paths)\n\nThis would work well here, but we don’t have csv files, instead we have excel spreadsheets. So we’re going to have to do it “by hand”. Learning to do it by hand will also help you when you have a bunch of csvs and the database that you’re working with doesn’t have one function that will load them all in.\n这在这里会很有效，但我们没有 csv 文件，而是 excel 电子表格。所以我们必须“手动”来做。当你有一堆 csv 文件，而你正在使用的数据库没有一个能将它们全部加载的函数时，学会手动操作也会对你有所帮助。\nWe need to start by creating a table that we will fill in with data. The easiest way to do this is by creating a template, a dummy data frame that contains all the columns we want, but only a sampling of the data. For the gapminder data, we can make that template by reading a single file and adding the year to it:\n我们需要从创建一个我们将用数据填充的表开始。最简单的方法是创建一个模板，一个包含我们想要的所有列但只有少量示例数据的虚拟数据框。对于 gapminder 数据，我们可以通过读取单个文件并向其添加年份来制作该模板：\n\ntemplate &lt;- readxl::read_excel(paths[[1]])\ntemplate$year &lt;- 1952\ntemplate\n#&gt; # A tibble: 142 × 6\n#&gt;   country     continent lifeExp      pop gdpPercap  year\n#&gt;   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Afghanistan Asia         28.8  8425333      779.  1952\n#&gt; 2 Albania     Europe       55.2  1282697     1601.  1952\n#&gt; 3 Algeria     Africa       43.1  9279525     2449.  1952\n#&gt; 4 Angola      Africa       30.0  4232095     3521.  1952\n#&gt; 5 Argentina   Americas     62.5 17876956     5911.  1952\n#&gt; 6 Australia   Oceania      69.1  8691212    10040.  1952\n#&gt; # ℹ 136 more rows\n\nNow we can connect to the database, and use DBI::dbCreateTable() to turn our template into a database table:\n现在我们可以连接到数据库，并使用 DBI::dbCreateTable() 将我们的模板转换成一个数据库表：\n\ncon &lt;- DBI::dbConnect(duckdb::duckdb())\nDBI::dbCreateTable(con, \"gapminder\", template)\n\ndbCreateTable() doesn’t use the data in template, just the variable names and types. So if we inspect the gapminder table now you’ll see that it’s empty but it has the variables we need with the types we expect:dbCreateTable() 不会使用 template 中的数据，只使用变量名和类型。所以如果我们现在检查 gapminder 表，你会看到它是空的，但它拥有我们需要的变量和我们期望的类型：\n\ncon |&gt; tbl(\"gapminder\")\n#&gt; # Source:   table&lt;gapminder&gt; [?? x 6]\n#&gt; # Database: DuckDB v1.3.1 [14913@Windows 10 x64:R 4.5.1/:memory:]\n#&gt; # ℹ 6 variables: country &lt;chr&gt;, continent &lt;chr&gt;, lifeExp &lt;dbl&gt;, pop &lt;dbl&gt;,\n#&gt; #   gdpPercap &lt;dbl&gt;, year &lt;dbl&gt;\n\nNext, we need a function that takes a single file path, reads it into R, and adds the result to the gapminder table. We can do that by combining read_excel() with DBI::dbAppendTable():\n接下来，我们需要一个函数，它接受单个文件路径，将其读入 R，并将结果添加到 gapminder 表中。我们可以通过结合 read_excel() 和 DBI::dbAppendTable() 来实现：\n\nappend_file &lt;- function(path) {\n  df &lt;- readxl::read_excel(path)\n  df$year &lt;- parse_number(basename(path))\n  \n  DBI::dbAppendTable(con, \"gapminder\", df)\n}\n\nNow we need to call append_file() once for each element of paths. That’s certainly possible with map():\n现在我们需要对 paths 的每个元素调用一次 append_file()。这当然可以用 map() 实现：\n\npaths |&gt; map(append_file)\n\nBut we don’t care about the output of append_file(), so instead of map() it’s slightly nicer to use walk(). walk() does exactly the same thing as map() but throws the output away:\n但我们不关心 append_file() 的输出，所以使用 walk() 会比 map() 更简洁一些。walk() 的作用与 map() 完全相同，但会丢弃输出：\n\npaths |&gt; walk(append_file)\n\nNow we can see if we have all the data in our table:\n现在我们可以看看我们的表中是否包含了所有数据：\n\ncon |&gt; \n  tbl(\"gapminder\") |&gt; \n  count(year)\n#&gt; # Source:   SQL [?? x 2]\n#&gt; # Database: DuckDB v1.3.1 [14913@Windows 10 x64:R 4.5.1/:memory:]\n#&gt;    year     n\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  1967   142\n#&gt; 2  1972   142\n#&gt; 3  1992   142\n#&gt; 4  1997   142\n#&gt; 5  2002   142\n#&gt; 6  1987   142\n#&gt; # ℹ more rows\n\n\n26.4.2 Writing csv files\nThe same basic principle applies if we want to write multiple csv files, one for each group. Let’s imagine that we want to take the ggplot2::diamonds data and save one csv file for each clarity. First we need to make those individual datasets. There are many ways you could do that, but there’s one way we particularly like: group_nest().\n如果我们想为每个组别写入多个 csv 文件，同样的基本原则也适用。假设我们想要获取 ggplot2::diamonds 数据，并为每个 clarity 保存一个 csv 文件。首先，我们需要创建这些独立的数据集。有很多方法可以做到这一点，但我们特别喜欢一种方法：group_nest()。\n\nby_clarity &lt;- diamonds |&gt; \n  group_nest(clarity)\n\nby_clarity\n#&gt; # A tibble: 8 × 2\n#&gt;   clarity               data\n#&gt;   &lt;ord&gt;   &lt;list&lt;tibble[,9]&gt;&gt;\n#&gt; 1 I1               [741 × 9]\n#&gt; 2 SI2            [9,194 × 9]\n#&gt; 3 SI1           [13,065 × 9]\n#&gt; 4 VS2           [12,258 × 9]\n#&gt; 5 VS1            [8,171 × 9]\n#&gt; 6 VVS2           [5,066 × 9]\n#&gt; # ℹ 2 more rows\n\nThis gives us a new tibble with eight rows and two columns. clarity is our grouping variable and data is a list-column containing one tibble for each unique value of clarity:\n这给了我们一个有八行两列的新 tibble。clarity 是我们的分组变量，data 是一个列表列 (list-column)，其中包含对应 clarity 每个唯一值的一个 tibble：\n\nby_clarity$data[[1]]\n#&gt; # A tibble: 741 × 9\n#&gt;   carat cut       color depth table price     x     y     z\n#&gt;   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  0.32 Premium   E      60.9    58   345  4.38  4.42  2.68\n#&gt; 2  1.17 Very Good J      60.2    61  2774  6.83  6.9   4.13\n#&gt; 3  1.01 Premium   F      61.8    60  2781  6.39  6.36  3.94\n#&gt; 4  1.01 Fair      E      64.5    58  2788  6.29  6.21  4.03\n#&gt; 5  0.96 Ideal     F      60.7    55  2801  6.37  6.41  3.88\n#&gt; 6  1.04 Premium   G      62.2    58  2801  6.46  6.41  4   \n#&gt; # ℹ 735 more rows\n\nWhile we’re here, let’s create a column that gives the name of output file, using mutate() and str_glue():\n趁此机会，让我们使用 mutate() 和 str_glue() 创建一个包含输出文件名的列：\n\nby_clarity &lt;- by_clarity |&gt; \n  mutate(path = str_glue(\"diamonds-{clarity}.csv\"))\n\nby_clarity\n#&gt; # A tibble: 8 × 3\n#&gt;   clarity               data path             \n#&gt;   &lt;ord&gt;   &lt;list&lt;tibble[,9]&gt;&gt; &lt;glue&gt;           \n#&gt; 1 I1               [741 × 9] diamonds-I1.csv  \n#&gt; 2 SI2            [9,194 × 9] diamonds-SI2.csv \n#&gt; 3 SI1           [13,065 × 9] diamonds-SI1.csv \n#&gt; 4 VS2           [12,258 × 9] diamonds-VS2.csv \n#&gt; 5 VS1            [8,171 × 9] diamonds-VS1.csv \n#&gt; 6 VVS2           [5,066 × 9] diamonds-VVS2.csv\n#&gt; # ℹ 2 more rows\n\nSo if we were going to save these data frames by hand, we might write something like:\n因此，如果我们打算手动保存这些数据框，我们可能会写出类似这样的代码：\n\nwrite_csv(by_clarity$data[[1]], by_clarity$path[[1]])\nwrite_csv(by_clarity$data[[2]], by_clarity$path[[2]])\nwrite_csv(by_clarity$data[[3]], by_clarity$path[[3]])\n...\nwrite_csv(by_clarity$by_clarity[[8]], by_clarity$path[[8]])\n\nThis is a little different to our previous uses of map() because there are two arguments that are changing, not just one. That means we need a new function: map2(), which varies both the first and second arguments. And because we again don’t care about the output, we want walk2() rather than map2(). That gives us:\n这与我们之前使用 map() 的情况略有不同，因为有两个参数在变化，而不仅仅是一个。这意味着我们需要一个新的函数：map2()，它可以同时改变第一个和第二个参数。而且因为我们同样不关心输出，所以我们想要用 walk2() 而不是 map2()。这样我们得到：\n\nwalk2(by_clarity$data, by_clarity$path, write_csv)\n\n\n26.4.3 Saving plots\nWe can take the same basic approach to create many plots. Let’s first make a function that draws the plot we want:\n我们可以采取同样的基本方法来创建多个图。首先，让我们创建一个函数来绘制我们想要的图：\n\ncarat_histogram &lt;- function(df) {\n  ggplot(df, aes(x = carat)) + geom_histogram(binwidth = 0.1)  \n}\n\ncarat_histogram(by_clarity$data[[1]])\n\n\n\n\n\n\n\nNow we can use map() to create a list of many plots[^5] and their eventual file paths:\n现在我们可以使用 map() 来创建许多图的列表5及其最终的文件路径：\n\nby_clarity &lt;- by_clarity |&gt; \n  mutate(\n    plot = map(data, carat_histogram),\n    path = str_glue(\"clarity-{clarity}.png\")\n  )\n\nThen use walk2() with ggsave() to save each plot:\n然后使用 walk2() 和 ggsave() 来保存每个图：\n\nwalk2(\n  by_clarity$path,\n  by_clarity$plot,\n  \\(path, plot) ggsave(path, plot, width = 6, height = 6)\n)\n\nThis is shorthand for:\n这是以下代码的简写：\n\nggsave(by_clarity$path[[1]], by_clarity$plot[[1]], width = 6, height = 6)\nggsave(by_clarity$path[[2]], by_clarity$plot[[2]], width = 6, height = 6)\nggsave(by_clarity$path[[3]], by_clarity$plot[[3]], width = 6, height = 6)\n...\nggsave(by_clarity$path[[8]], by_clarity$plot[[8]], width = 6, height = 6)",
    "crumbs": [
      "Program",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "iteration.html#summary",
    "href": "iteration.html#summary",
    "title": "26  Iteration",
    "section": "\n26.5 Summary",
    "text": "26.5 Summary\nIn this chapter, you’ve seen how to use explicit iteration to solve three problems that come up frequently when doing data science: manipulating multiple columns, reading multiple files, and saving multiple outputs. But in general, iteration is a super power: if you know the right iteration technique, you can easily go from fixing one problem to fixing all the problems. Once you’ve mastered the techniques in this chapter, we highly recommend learning more by reading the Functionals chapter of Advanced R and consulting the purrr website.\n在本章中，你已经学习了如何使用显式迭代来解决数据科学中经常出现的三个问题：操作多个列、读取多个文件以及保存多个输出。但总的来说，迭代是一项超能力：如果你掌握了正确的迭代技巧，你就可以轻松地从解决一个问题扩展到解决所有问题。一旦你掌握了本章的技巧，我们强烈建议你通过阅读 Advanced R 的 函数式编程 (Functionals) 章节 和查阅 purrr 网站 来学习更多内容。\nIf you know much about iteration in other languages, you might be surprised that we didn’t discuss the for loop. That’s because R’s orientation towards data analysis changes how we iterate: in most cases you can rely on an existing idiom to do something to each columns or each group. And when you can’t, you can often use a functional programming tool like map() that does something to each element of a list. However, you will see for loops in wild-caught code, so you’ll learn about them in the next chapter where we’ll discuss some important base R tools.\n如果你对其他语言中的迭代很了解，你可能会惊讶于我们没有讨论 for 循环。这是因为 R 面向数据分析的特性改变了我们迭代的方式：在大多数情况下，你可以依赖现有的惯用法来对每一列或每一组执行操作。当你无法这样做时，你通常可以使用像 map() 这样的函数式编程工具，它会对列表的每个元素执行某些操作。然而，你会在实际代码中看到 for 循环，所以你将在下一章中学习它们，届时我们将讨论一些重要的 R 基础工具。",
    "crumbs": [
      "Program",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "base-R.html",
    "href": "base-R.html",
    "title": "27  A field guide to base R",
    "section": "",
    "text": "27.1 Introduction\nTo finish off the programming section, we’re going to give you a quick tour of the most important base R functions that we don’t otherwise discuss in the book.\n为了结束编程部分，我们将带你快速浏览一下本书中未曾讨论过的最重要的基础 R 函数。\nThese tools are particularly useful as you do more programming and will help you read code you’ll encounter in the wild.\n当你进行更多编程时，这些工具特别有用，它们将帮助你阅读在实际中遇到的代码。\nThis is a good place to remind you that the tidyverse is not the only way to solve data science problems.\n这里正好可以提醒你，tidyverse 并不是解决数据科学问题的唯一方法。\nWe teach the tidyverse in this book because tidyverse packages share a common design philosophy, increasing the consistency across functions, and making each new function or package a little easier to learn and use.\n我们在本书中教授 tidyverse，因为 tidyverse 的包共享一个共同的设计理念，增加了函数之间的一致性，使得学习和使用每个新函数或包都变得更容易一些。\nIt’s not possible to use the tidyverse without using base R, so we’ve actually already taught you a lot of base R functions: from library() to load packages, to sum() and mean() for numeric summaries, to the factor, date, and POSIXct data types, and of course all the basic operators like +, -, /, *, |, &, and !.\n不使用基础 R 是不可能使用 tidyverse 的，所以我们实际上已经教了你很多基础 R 函数：从加载包的 library()，到用于数值摘要的 sum() 和 mean()，再到因子、日期和 POSIXct 数据类型，当然还有所有基本运算符，如 +、-、/、*、|、& 和 !。\nWhat we haven’t focused on so far is base R workflows, so we will highlight a few of those in this chapter.\n到目前为止，我们还没有重点关注基础 R 的工作流程，所以我们将在本章中重点介绍其中的一些。\nAfter you read this book, you’ll learn other approaches to the same problems using base R, data.table, and other packages.\n读完这本书后，你将学习到使用基础 R、data.table 和其他包来解决同样问题的其他方法。\nYou’ll undoubtedly encounter these other approaches when you start reading R code written by others, particularly if you’re using StackOverflow.\n当你开始阅读他人编写的 R 代码时，毫无疑问你会遇到这些其他方法，尤其是在使用 StackOverflow 时。\nIt’s 100% okay to write code that uses a mix of approaches, and don’t let anyone tell you otherwise!\n编写混合使用多种方法的代码是 100% 可以的，不要让任何人告诉你别的！\nIn this chapter, we’ll focus on four big topics: subsetting with [, subsetting with [[ and $, the apply family of functions, and for loops.\n在本章中，我们将重点关注四个大主题：使用 [ 进行子集提取，使用 [[ 和 $ 进行子集提取，apply 函数族，以及 for 循环。\nTo finish off, we’ll briefly discuss two essential plotting functions.\n最后，我们将简要讨论两个基本的绘图函数。",
    "crumbs": [
      "Program",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>A field guide to base R</span>"
    ]
  },
  {
    "objectID": "base-R.html#introduction",
    "href": "base-R.html#introduction",
    "title": "27  A field guide to base R",
    "section": "",
    "text": "27.1.1 Prerequisites\nThis package focuses on base R so doesn’t have any real prerequisites, but we’ll load the tidyverse in order to explain some of the differences.\n这个包专注于基础 R，所以没有真正的先决条件，但我们会加载 tidyverse 以便解释一些差异。\n\nlibrary(tidyverse)",
    "crumbs": [
      "Program",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>A field guide to base R</span>"
    ]
  },
  {
    "objectID": "base-R.html#sec-subset-many",
    "href": "base-R.html#sec-subset-many",
    "title": "27  A field guide to base R",
    "section": "\n27.2 Selecting multiple elements with [\n",
    "text": "27.2 Selecting multiple elements with [\n\n[ is used to extract sub-components from vectors and data frames, and is called like x[i] or x[i, j].[ 用于从向量和数据框中提取子组件，调用方式如 x[i] 或 x[i, j]。\nIn this section, we’ll introduce you to the power of [, first showing you how you can use it with vectors, then how the same principles extend in a straightforward way to two-dimensional (2d) structures like data frames.\n在本节中，我们将向你介绍 [ 的强大功能，首先展示如何将其用于向量，然后展示相同的原则如何直接扩展到二维（2D）结构，如数据框。\nWe’ll then help you cement that knowledge by showing how various dplyr verbs are special cases of [.\n然后，我们将通过展示各种 dplyr 动词如何是 [ 的特例来帮助你巩固这些知识。\n\n27.2.1 Subsetting vectors\nThere are five main types of things that you can subset a vector with, i.e., that can be the i in x[i]:\n你可以用五种主要类型的东西来对向量进行子集提取，即 x[i] 中的 i 可以是：\n\n\nA vector of positive integers.一个正整数向量。\nSubsetting with positive integers keeps the elements at those positions:\n用正整数进行子集提取会保留那些位置上的元素：\n\nx &lt;- c(\"one\", \"two\", \"three\", \"four\", \"five\")\nx[c(3, 2, 5)]\n#&gt; [1] \"three\" \"two\"   \"five\"\n\nBy repeating a position, you can actually make a longer output than input, making the term “subsetting” a bit of a misnomer.\n通过重复一个位置，你实际上可以使输出比输入更长，这使得“子集提取”这个术语有点用词不当。\n\nx[c(1, 1, 5, 5, 5, 2)]\n#&gt; [1] \"one\"  \"one\"  \"five\" \"five\" \"five\" \"two\"\n\n\n\nA vector of negative integers.一个负整数向量。\nNegative values drop the elements at the specified positions:\n负值会删除指定位置的元素：\n\nx[c(-1, -3, -5)]\n#&gt; [1] \"two\"  \"four\"\n\n\n\nA logical vector.一个逻辑向量。\nSubsetting with a logical vector keeps all values corresponding to a TRUE value.\n用逻辑向量进行子集提取会保留所有对应 TRUE 值的值。\nThis is most often useful in conjunction with the comparison functions.\n这在与比较函数结合使用时最有用。\n\nx &lt;- c(10, 3, NA, 5, 8, 1, NA)\n\n# All non-missing values of x\nx[!is.na(x)]\n#&gt; [1] 10  3  5  8  1\n\n# All even (or missing!) values of x\nx[x %% 2 == 0]\n#&gt; [1] 10 NA  8 NA\n\nUnlike filter(), NA indices will be included in the output as NAs.\n与 filter() 不同，NA 索引将作为 NA 包含在输出中。\n\n\nA character vector.一个字符向量。\nIf you have a named vector, you can subset it with a character vector:\n如果你有一个命名的向量，你可以用一个字符向量来对它进行子集提取：\n\nx &lt;- c(abc = 1, def = 2, xyz = 5)\nx[c(\"xyz\", \"def\")]\n#&gt; xyz def \n#&gt;   5   2\n\nAs with subsetting with positive integers, you can use a character vector to duplicate individual entries.\n与使用正整数进行子集提取一样，你可以使用字符向量来复制单个条目。\n\nNothing.什么都不提供。\nThe final type of subsetting is nothing, x[], which returns the complete x.\n最后一种子集提取是什么都不提供，即 x[]，它返回完整的 x。\n\nThis is not useful for subsetting vectors, but as we’ll see shortly, it is useful when subsetting 2d structures like tibbles.\n这对于向量的子集提取没有用，但正如我们稍后将看到的，它在对像 tibble 这样的二维结构进行子集提取时很有用。\n\n27.2.2 Subsetting data frames\nThere are quite a few different ways1 that you can use [ with a data frame, but the most important way is to select rows and columns independently with df[rows, cols]. Here rows and cols are vectors as described above.\n你可以用多种不同的方式1 对数据框使用 [，但最重要的方式是使用 df[rows, cols] 独立地选择行和列。这里的 rows 和 cols 是如上所述的向量。\nFor example, df[rows, ] and df[, cols] select just rows or just columns, using the empty subset to preserve the other dimension.\n例如，df[rows, ] 和 df[, cols] 只选择行或列，使用空的子集来保留另一维度。\nHere are a couple of examples:\n这里有几个例子：\n\ndf &lt;- tibble(\n  x = 1:3, \n  y = c(\"a\", \"e\", \"f\"), \n  z = runif(3)\n)\n\n# Select first row and second column\ndf[1, 2]\n#&gt; # A tibble: 1 × 1\n#&gt;   y    \n#&gt;   &lt;chr&gt;\n#&gt; 1 a\n\n# Select all rows and columns x and y\ndf[, c(\"x\" , \"y\")]\n#&gt; # A tibble: 3 × 2\n#&gt;       x y    \n#&gt;   &lt;int&gt; &lt;chr&gt;\n#&gt; 1     1 a    \n#&gt; 2     2 e    \n#&gt; 3     3 f\n\n# Select rows where `x` is greater than 1 and all columns\ndf[df$x &gt; 1, ]\n#&gt; # A tibble: 2 × 3\n#&gt;       x y         z\n#&gt;   &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;\n#&gt; 1     2 e     0.834\n#&gt; 2     3 f     0.601\n\nWe’ll come back to $ shortly, but you should be able to guess what df$x does from the context: it extracts the x variable from df.\n我们稍后会回到 $，但你应该能从上下文中猜出 df$x 的作用：它从 df 中提取 x 变量。\nWe need to use it here because [ doesn’t use tidy evaluation, so you need to be explicit about the source of the x variable.\n我们在这里需要使用它，因为 [ 不使用整洁评估 (tidy evaluation)，所以你需要明确 x 变量的来源。\nThere’s an important difference between tibbles and data frames when it comes to [.\n在 [ 的使用上，tibble 和数据框之间有一个重要的区别。\nIn this book, we’ve mainly used tibbles, which are data frames, but they tweak some behaviors to make your life a little easier.\n在本书中，我们主要使用 tibble，它是数据框，但它们调整了一些行为，让你的生活更轻松一些。\nIn most places, you can use “tibble” and “data frame” interchangeably, so when we want to draw particular attention to R’s built-in data frame, we’ll write data.frame.\n在大多数地方，你可以互换使用 “tibble” 和 “data frame”，所以当我们想特别指出 R 的内置数据框时，我们会写 data.frame。\nIf df is a data.frame, then df[, cols] will return a vector if col selects a single column and a data frame if it selects more than one column.\n如果 df 是一个 data.frame，那么如果 col 选择单个列，df[, cols] 将返回一个向量；如果选择多个列，则返回一个数据框。\nIf df is a tibble, then [ will always return a tibble.\n如果 df 是一个 tibble，那么 [ 将总是返回一个 tibble。\n\ndf1 &lt;- data.frame(x = 1:3)\ndf1[, \"x\"]\n#&gt; [1] 1 2 3\n\ndf2 &lt;- tibble(x = 1:3)\ndf2[, \"x\"]\n#&gt; # A tibble: 3 × 1\n#&gt;       x\n#&gt;   &lt;int&gt;\n#&gt; 1     1\n#&gt; 2     2\n#&gt; 3     3\n\nOne way to avoid this ambiguity with data.frames is to explicitly specify drop = FALSE:\n要避免 data.frame 的这种不确定性，一种方法是明确指定 drop = FALSE：\n\ndf1[, \"x\" , drop = FALSE]\n#&gt;   x\n#&gt; 1 1\n#&gt; 2 2\n#&gt; 3 3\n\n\n27.2.3 dplyr equivalents\nSeveral dplyr verbs are special cases of [:\n有几个 dplyr 动词是 [ 的特例：\n\n\nfilter() is equivalent to subsetting the rows with a logical vector, taking care to exclude missing values:filter() 等同于使用逻辑向量对行进行子集提取，并注意排除缺失值：\n\ndf &lt;- tibble(\n  x = c(2, 3, 1, 1, NA), \n  y = letters[1:5], \n  z = runif(5)\n)\ndf |&gt; filter(x &gt; 1)\n\n# same as\ndf[!is.na(df$x) & df$x &gt; 1, ]\n\nAnother common technique in the wild is to use which() for its side-effect of dropping missing values: df[which(df$x &gt; 1), ].\n在实践中，另一种常见的技巧是使用 which()，利用其可以丢弃缺失值的副作用：df[which(df$x &gt; 1), ]。\n\n\narrange() is equivalent to subsetting the rows with an integer vector, usually created with order():arrange() 等同于使用一个整数向量对行进行子集提取，这个向量通常由 order() 创建：\n\ndf |&gt; arrange(x, y)\n\n# same as\ndf[order(df$x, df$y), ]\n\nYou can use order(decreasing = TRUE) to sort all columns in descending order or -rank(col) to sort columns in decreasing order individually.\n你可以使用 order(decreasing = TRUE) 按降序对所有列进行排序，或者使用 -rank(col) 单独按降序对列进行排序。\n\nBoth select() and relocate() are similar to subsetting the columns with a character vector:select() 和 relocate() 都类似于使用字符向量对列进行子集提取：\n\n\ndf |&gt; select(x, z)\n\n# same as\ndf[, c(\"x\", \"z\")]\n\n\n\nBase R also provides a function that combines the features of filter() and select()2 called subset():\n基础 R 还提供了一个名为 subset() 的函数，它结合了 filter() 和 select() 的功能2：\n\ndf |&gt; \n  filter(x &gt; 1) |&gt; \n  select(y, z)\n#&gt; # A tibble: 2 × 2\n#&gt;   y           z\n#&gt;   &lt;chr&gt;   &lt;dbl&gt;\n#&gt; 1 a     0.157  \n#&gt; 2 b     0.00740\n\n\n# same as\ndf |&gt; subset(x &gt; 1, c(y, z))\n\nThis function was the inspiration for much of dplyr’s syntax.\n这个函数是 dplyr 许多语法的灵感来源。\n\n27.2.4 Exercises\n\n\nCreate functions that take a vector as input and return:\n\nThe elements at even-numbered positions.\nEvery element except the last value.\nOnly even values (and no missing values).\n\n\nWhy is x[-which(x &gt; 0)] not the same as x[x &lt;= 0]? Read the documentation for which() and do some experiments to figure it out.",
    "crumbs": [
      "Program",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>A field guide to base R</span>"
    ]
  },
  {
    "objectID": "base-R.html#sec-subset-one",
    "href": "base-R.html#sec-subset-one",
    "title": "27  A field guide to base R",
    "section": "\n27.3 Selecting a single element with $ and [[\n",
    "text": "27.3 Selecting a single element with $ and [[\n\n[, which selects many elements, is paired with [[ and $, which extract a single element.\n用于选择多个元素的 [ 与用于提取单个元素的 [[ 和 $ 配对使用。\nIn this section, we’ll show you how to use [[ and $ to pull columns out of data frames, discuss a couple more differences between data.frames and tibbles, and emphasize some important differences between [ and [[ when used with lists.\n在本节中，我们将向你展示如何使用 [[ 和 $ 从数据框中提取列，讨论 data.frame 和 tibble 之间的更多差异，并强调在与列表一起使用时 [ 和 [[ 之间的一些重要区别。\n\n27.3.1 Data frames\n[[ and $ can be used to extract columns out of a data frame.[[ 和 $ 可以用来从数据框中提取列。\n[[ can access by position or by name, and $ is specialized for access by name:[[ 可以通过位置或名称访问，而 $ 则专门用于通过名称访问：\n\ntb &lt;- tibble(\n  x = 1:4,\n  y = c(10, 4, 1, 21)\n)\n\n# by position\ntb[[1]]\n#&gt; [1] 1 2 3 4\n\n# by name\ntb[[\"x\"]]\n#&gt; [1] 1 2 3 4\ntb$x\n#&gt; [1] 1 2 3 4\n\nThey can also be used to create new columns, the base R equivalent of mutate():\n它们也可以用来创建新列，这相当于基础 R 中的 mutate()：\n\ntb$z &lt;- tb$x + tb$y\ntb\n#&gt; # A tibble: 4 × 3\n#&gt;       x     y     z\n#&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1    10    11\n#&gt; 2     2     4     6\n#&gt; 3     3     1     4\n#&gt; 4     4    21    25\n\nThere are several other base R approaches to creating new columns including with transform(), with(), and within().\n还有其他几种基础 R 的方法可以创建新列，包括使用 transform()、with() 和 within()。\nHadley collected a few examples at https://gist.github.com/hadley/1986a273e384fb2d4d752c18ed71bedf.\nHadley 在 https://gist.github.com/hadley/1986a273e384fb2d4d752c18ed71bedf 收集了一些例子。\nUsing $ directly is convenient when performing quick summaries.\n在进行快速摘要时，直接使用 $ 很方便。\nFor example, if you just want to find the size of the biggest diamond or the possible values of cut, there’s no need to use summarize():\n例如，如果你只想找到最大钻石的尺寸或 cut 的可能值，就不需要使用 summarize()：\n\nmax(diamonds$carat)\n#&gt; [1] 5.01\n\nlevels(diamonds$cut)\n#&gt; [1] \"Fair\"      \"Good\"      \"Very Good\" \"Premium\"   \"Ideal\"\n\ndplyr also provides an equivalent to [[/$ that we didn’t mention in Chapter 3: pull().\ndplyr 也提供了一个等价于 [[/$ 的函数，我们在 Chapter 3 中没有提到：pull()。\npull() takes either a variable name or variable position and returns just that column.pull() 接受变量名或变量位置，并只返回那一列。\nThat means we could rewrite the above code to use the pipe:\n这意味着我们可以重写上面的代码来使用管道：\n\ndiamonds |&gt; pull(carat) |&gt; max()\n#&gt; [1] 5.01\n\ndiamonds |&gt; pull(cut) |&gt; levels()\n#&gt; [1] \"Fair\"      \"Good\"      \"Very Good\" \"Premium\"   \"Ideal\"\n\n\n27.3.2 Tibbles\nThere are a couple of important differences between tibbles and base data.frames when it comes to $. Data frames match the prefix of any variable names (so-called partial matching) and don’t complain if a column doesn’t exist:\n在使用 $ 时，tibble 和基础 data.frame 之间有几个重要的区别。数据框会匹配任何变量名的前缀（所谓的部分匹配 (partial matching)），并且如果列不存在也不会报错：\n\ndf &lt;- data.frame(x1 = 1)\ndf$x\n#&gt; [1] 1\ndf$z\n#&gt; NULL\n\nTibbles are more strict: they only ever match variable names exactly and they will generate a warning if the column you are trying to access doesn’t exist:\nTibble 更为严格：它们只精确匹配变量名，并且如果你尝试访问的列不存在，它们会生成一个警告：\n\ntb &lt;- tibble(x1 = 1)\n\ntb$x\n#&gt; Warning: Unknown or uninitialised column: `x`.\n#&gt; NULL\ntb$z\n#&gt; Warning: Unknown or uninitialised column: `z`.\n#&gt; NULL\n\nFor this reason we sometimes joke that tibbles are lazy and surly: they do less and complain more.\n因此，我们有时开玩笑说 tibble 既懒惰又暴躁：它们做得更少，抱怨得更多。\n\n27.3.3 Lists\n[[ and $ are also really important for working with lists, and it’s important to understand how they differ from [. Let’s illustrate the differences with a list named l:[[ 和 $ 在处理列表时也非常重要，理解它们与 [ 的区别至关重要。让我们用一个名为 l 的列表来说明这些差异：\n\nl &lt;- list(\n  a = 1:3, \n  b = \"a string\", \n  c = pi, \n  d = list(-1, -5)\n)\n\n\n\n[ extracts a sub-list. It doesn’t matter how many elements you extract, the result will always be a list.[ 提取一个子列表。无论你提取多少个元素，结果总是一个列表。\n\nstr(l[1:2])\n#&gt; List of 2\n#&gt;  $ a: int [1:3] 1 2 3\n#&gt;  $ b: chr \"a string\"\n\nstr(l[1])\n#&gt; List of 1\n#&gt;  $ a: int [1:3] 1 2 3\n\nstr(l[4])\n#&gt; List of 1\n#&gt;  $ d:List of 2\n#&gt;   ..$ : num -1\n#&gt;   ..$ : num -5\n\nLike with vectors, you can subset with a logical, integer, or character vector.\n与向量一样，你可以使用逻辑型、整型或字符型向量进行子集提取。\n\n\n[[ and $ extract a single component from a list. They remove a level of hierarchy from the list.[[ 和 $ 从列表中提取单个组件。它们会从列表中移除一个层级。\n\nstr(l[[1]])\n#&gt;  int [1:3] 1 2 3\n\nstr(l[[4]])\n#&gt; List of 2\n#&gt;  $ : num -1\n#&gt;  $ : num -5\n\nstr(l$a)\n#&gt;  int [1:3] 1 2 3\n\n\n\nThe difference between [ and [[ is particularly important for lists because [[ drills down into the list while [ returns a new, smaller list. To help you remember the difference, take a look at the unusual pepper shaker shown in Figure 27.1. If this pepper shaker is your list pepper, then, pepper[1] is a pepper shaker containing a single pepper packet. pepper[2] would look the same, but would contain the second packet. pepper[1:2] would be a pepper shaker containing two pepper packets. pepper[[1]] would extract the pepper packet itself.[ 和 [[ 之间的区别对于列表尤其重要，因为 [[ 会深入到列表中，而 [ 返回一个新的、更小的列表。为了帮助你记住这个区别，请看 Figure 27.1 中展示的那个不寻常的胡椒瓶。如果这个胡椒瓶是你的列表 pepper，那么 pepper[1] 就是一个包含单个胡椒包的胡椒瓶。pepper[2] 看起来一样，但会包含第二个包。pepper[1:2] 将是一个包含两个胡椒包的胡椒瓶。而 pepper[[1]] 则会提取出胡椒包本身。\n\n\n\n\n\n\n\nFigure 27.1: (Left) A pepper shaker that Hadley once found in his hotel room. (Middle) pepper[1]. (Right) pepper[[1]]\n\n\n\n\nThis same principle applies when you use 1d [ with a data frame: df[\"x\"] returns a one-column data frame and df[[\"x\"]] returns a vector.\n当你对数据框使用一维 [ 时，同样的原则也适用：df[\"x\"] 返回一个单列数据框，而 df[[\"x\"]] 返回一个向量。\n\n27.3.4 Exercises\n\nWhat happens when you use [[ with a positive integer that’s bigger than the length of the vector? What happens when you subset with a name that doesn’t exist?\nWhat would pepper[[1]][1] be? What about pepper[[1]][[1]]?",
    "crumbs": [
      "Program",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>A field guide to base R</span>"
    ]
  },
  {
    "objectID": "base-R.html#apply-family",
    "href": "base-R.html#apply-family",
    "title": "27  A field guide to base R",
    "section": "\n27.4 Apply family",
    "text": "27.4 Apply family\nIn Chapter 26, you learned tidyverse techniques for iteration like dplyr::across() and the map family of functions. In this section, you’ll learn about their base equivalents, the apply family. In this context apply and map are synonyms because another way of saying “map a function over each element of a vector” is “apply a function over each element of a vector”. Here we’ll give you a quick overview of this family so you can recognize them in the wild.\n在 Chapter 26 中，你学习了 tidyverse 的迭代技术，如 dplyr::across() 和 map 系列函数。在本节中，你将学习它们在基础 R 中的等价物，即 apply 家族。在这种情况下，apply 和 map 是同义词，因为“将函数映射到向量的每个元素上”的另一种说法是“将函数应用于向量的每个元素上”。这里我们将快速介绍这个家族，以便你在实际中能认出它们。\nThe most important member of this family is lapply(), which is very similar to purrr::map()3. In fact, because we haven’t used any of map()’s more advanced features, you can replace every map() call in Chapter 26 with lapply().\n这个家族中最重要的成员是 lapply()，它与 purrr::map() 非常相似3。事实上，因为我们没有使用 map() 的任何更高级的功能，你可以用 lapply() 替换 Chapter 26 中的每一个 map() 调用。\nThere’s no exact base R equivalent to across() but you can get close by using [ with lapply(). This works because under the hood, data frames are lists of columns, so calling lapply() on a data frame applies the function to each column.\n基础 R 中没有与 across() 完全等价的函数，但你可以通过将 [ 与 lapply() 结合使用来接近它的功能。这样做是可行的，因为在底层，数据框是列的列表，所以对数据框调用 lapply() 会将函数应用于每一列。\n\ndf &lt;- tibble(a = 1, b = 2, c = \"a\", d = \"b\", e = 4)\n\n# First find numeric columns\nnum_cols &lt;- sapply(df, is.numeric)\nnum_cols\n#&gt;     a     b     c     d     e \n#&gt;  TRUE  TRUE FALSE FALSE  TRUE\n\n# Then transform each column with lapply() then replace the original values\ndf[, num_cols] &lt;- lapply(df[, num_cols, drop = FALSE], \\(x) x * 2)\ndf\n#&gt; # A tibble: 1 × 5\n#&gt;       a     b c     d         e\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n#&gt; 1     2     4 a     b         8\n\nThe code above uses a new function, sapply(). It’s similar to lapply() but it always tries to simplify the result, hence the s in its name, here producing a logical vector instead of a list. We don’t recommend using it for programming, because the simplification can fail and give you an unexpected type, but it’s usually fine for interactive use. purrr has a similar function called map_vec() that we didn’t mention in Chapter 26.\n上面的代码使用了一个新函数 sapply()。它与 lapply() 相似，但它总是尝试简化结果，因此其名称中带有 s，这里它产生一个逻辑向量而不是一个列表。我们不建议在编程中使用它，因为简化可能会失败并给你一个意想不到的类型，但对于交互式使用来说通常没问题。purrr 有一个类似的函数叫做 map_vec()，我们在 Chapter 26 中没有提到。\nBase R provides a stricter version of sapply() called vapply(), short for vector apply. It takes an additional argument that specifies the expected type, ensuring that simplification occurs the same way regardless of the input. For example, we could replace the sapply() call above with this vapply() where we specify that we expect is.numeric() to return a logical vector of length 1:\n基础 R 提供了一个更严格的 sapply() 版本，名为 vapply()，是 vector apply 的缩写。它接受一个额外的参数来指定预期的类型，确保无论输入如何，简化都以相同的方式发生。例如，我们可以用这个 vapply() 替换上面的 sapply() 调用，我们在其中指定我们期望 is.numeric() 返回一个长度为 1 的逻辑向量：\n\nvapply(df, is.numeric, logical(1))\n#&gt;     a     b     c     d     e \n#&gt;  TRUE  TRUE FALSE FALSE  TRUE\n\nThe distinction between sapply() and vapply() is really important when they’re inside a function (because it makes a big difference to the function’s robustness to unusual inputs), but it doesn’t usually matter in data analysis.sapply() 和 vapply() 之间的区别在它们位于函数内部时非常重要（因为它对函数对异常输入的鲁棒性有很大影响），但在数据分析中通常无关紧要。\nAnother important member of the apply family is tapply() which computes a single grouped summary:\napply 家族的另一个重要成员是 tapply()，它计算单个分组摘要：\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summarize(price = mean(price))\n#&gt; # A tibble: 5 × 2\n#&gt;   cut       price\n#&gt;   &lt;ord&gt;     &lt;dbl&gt;\n#&gt; 1 Fair      4359.\n#&gt; 2 Good      3929.\n#&gt; 3 Very Good 3982.\n#&gt; 4 Premium   4584.\n#&gt; 5 Ideal     3458.\n\ntapply(diamonds$price, diamonds$cut, mean)\n#&gt;      Fair      Good Very Good   Premium     Ideal \n#&gt;  4358.758  3928.864  3981.760  4584.258  3457.542\n\nUnfortunately tapply() returns its results in a named vector which requires some gymnastics if you want to collect multiple summaries and grouping variables into a data frame (it’s certainly possible to not do this and just work with free floating vectors, but in our experience that just delays the work). If you want to see how you might use tapply() or other base techniques to perform other grouped summaries, Hadley has collected a few techniques in a gist.\n不幸的是，tapply() 以命名向量的形式返回其结果，如果你想将多个摘要和分组变量收集到一个数据框中，就需要一些技巧（当然可以不这样做，只使用自由浮动的向量，但根据我们的经验，这只是推迟了工作）。如果你想看看如何使用 tapply() 或其他基础技术来执行其他分组摘要，Hadley 在 一个 gist 中收集了一些技巧。\nThe final member of the apply family is the titular apply(), which works with matrices and arrays. In particular, watch out for apply(df, 2, something), which is a slow and potentially dangerous way of doing lapply(df, something). This rarely comes up in data science because we usually work with data frames and not matrices.\napply 家族的最后一个成员是同名的 apply()，它用于处理矩阵和数组。特别要注意 apply(df, 2, something)，这是一种缓慢且可能危险的方式来执行 lapply(df, something)。这在数据科学中很少出现，因为我们通常处理数据框而不是矩阵。",
    "crumbs": [
      "Program",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>A field guide to base R</span>"
    ]
  },
  {
    "objectID": "base-R.html#for-loops",
    "href": "base-R.html#for-loops",
    "title": "27  A field guide to base R",
    "section": "\n27.5 for loops",
    "text": "27.5 for loops\nfor loops are the fundamental building block of iteration that both the apply and map families use under the hood. for loops are powerful and general tools that are important to learn as you become a more experienced R programmer. The basic structure of a for loop looks like this:for 循环是迭代的基本构建块，apply 和 map 家族在底层都使用了它。for 循环是强大而通用的工具，随着你成为一名更有经验的 R 程序员，学习它们非常重要。for 循环的基本结构如下：\n\nfor (element in vector) {\n  # do something with element\n}\n\nThe most straightforward use of for loops is to achieve the same effect as walk(): call some function with a side-effect on each element of a list. For example, in Section 26.4.1 instead of using walk():for 循环最直接的用途是实现与 walk() 相同的效果：对列表的每个元素调用某个具有副作用的函数。例如，在 Section 26.4.1 中，我们可以不使用 walk()：\n\npaths |&gt; walk(append_file)\n\nWe could have used a for loop:\n而是使用 for 循环：\n\nfor (path in paths) {\n  append_file(path)\n}\n\nThings get a little trickier if you want to save the output of the for loop, for example reading all of the excel files in a directory like we did in Chapter 26:\n如果你想保存 for 循环的输出，事情会变得稍微复杂一些，例如，像我们在 Chapter 26 中所做的那样，读取目录中所有的 excel 文件：\n\npaths &lt;- dir(\"data/gapminder\", pattern = \"\\\\.xlsx$\", full.names = TRUE)\nfiles &lt;- map(paths, readxl::read_excel)\n\nThere are a few different techniques that you can use, but we recommend being explicit about what the output is going to look like upfront. In this case, we’re going to want a list the same length as paths, which we can create with vector():\n你可以使用几种不同的技术，但我们建议预先明确输出的样子。在这种情况下，我们需要一个与 paths 长度相同的列表，我们可以用 vector() 创建它：\n\nfiles &lt;- vector(\"list\", length(paths))\n\nThen instead of iterating over the elements of paths, we’ll iterate over their indices, using seq_along() to generate one index for each element of paths:\n然后，我们不遍历 paths 的元素，而是遍历它们的索引，使用 seq_along() 为 paths 的每个元素生成一个索引：\n\nseq_along(paths)\n#&gt;  [1]  1  2  3  4  5  6  7  8  9 10 11 12\n\nUsing the indices is important because it allows us to link to each position in the input with the corresponding position in the output:\n使用索引很重要，因为它允许我们将输入中的每个位置与输出中相应的位置链接起来：\n\nfor (i in seq_along(paths)) {\n  files[[i]] &lt;- readxl::read_excel(paths[[i]])\n}\n\nTo combine the list of tibbles into a single tibble you can use do.call() + rbind():\n要将 tibble 列表合并为单个 tibble，你可以使用 do.call() + rbind()：\n\ndo.call(rbind, files)\n#&gt; # A tibble: 1,704 × 5\n#&gt;   country     continent lifeExp      pop gdpPercap\n#&gt;   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 Afghanistan Asia         28.8  8425333      779.\n#&gt; 2 Albania     Europe       55.2  1282697     1601.\n#&gt; 3 Algeria     Africa       43.1  9279525     2449.\n#&gt; 4 Angola      Africa       30.0  4232095     3521.\n#&gt; 5 Argentina   Americas     62.5 17876956     5911.\n#&gt; 6 Australia   Oceania      69.1  8691212    10040.\n#&gt; # ℹ 1,698 more rows\n\nRather than making a list and saving the results as we go, a simpler approach is to build up the data frame piece-by-piece:\n与其创建一个列表并随时保存结果，一个更简单的方法是逐个构建数据框：\n\nout &lt;- NULL\nfor (path in paths) {\n  out &lt;- rbind(out, readxl::read_excel(path))\n}\n\nWe recommend avoiding this pattern because it can become very slow when the vector is very long. This is the source of the persistent canard that for loops are slow: they’re not, but iteratively growing a vector is.\n我们建议避免这种模式，因为当向量非常长时，它会变得非常慢。这就是 for 循环很慢这个谣言的来源：它们本身不慢，但迭代地增长一个向量是慢的。",
    "crumbs": [
      "Program",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>A field guide to base R</span>"
    ]
  },
  {
    "objectID": "base-R.html#plots",
    "href": "base-R.html#plots",
    "title": "27  A field guide to base R",
    "section": "\n27.6 Plots",
    "text": "27.6 Plots\nMany R users who don’t otherwise use the tidyverse prefer ggplot2 for plotting due to helpful features like sensible defaults, automatic legends, and a modern look. However, base R plotting functions can still be useful because they’re so concise — it takes very little typing to do a basic exploratory plot.\n许多不使用 tidyverse 的 R 用户也喜欢用 ggplot2 来绘图，因为它有许多有用的功能，比如合理的默认设置、自动图例和现代的外观。然而，基础 R 的绘图函数仍然很有用，因为它们非常简洁——做一个基本的探索性图表只需要很少的输入。\nThere are two main types of base plot you’ll see in the wild: scatterplots and histograms, produced with plot() and hist() respectively. Here’s a quick example from the diamonds dataset:\n你在实践中会看到两种主要的基础图类型：散点图和直方图，分别用 plot() 和 hist() 生成。这里有一个来自 diamonds 数据集的快速示例：\n# Left\nhist(diamonds$carat)\n\n# Right\nplot(diamonds$carat, diamonds$price)\n\n\n\n\n\n\n\n\n\n\nNote that base plotting functions work with vectors, so you need to pull columns out of the data frame using $ or some other technique.\n请注意，基础绘图函数是作用于向量的，所以你需要使用 $ 或其他技术从数据框中提取列。",
    "crumbs": [
      "Program",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>A field guide to base R</span>"
    ]
  },
  {
    "objectID": "base-R.html#summary",
    "href": "base-R.html#summary",
    "title": "27  A field guide to base R",
    "section": "\n27.7 Summary",
    "text": "27.7 Summary\nIn this chapter, we’ve shown you a selection of base R functions useful for subsetting and iteration. Compared to approaches discussed elsewhere in the book, these functions tend to have more of a “vector” flavor than a “data frame” flavor because base R functions tend to take individual vectors, rather than a data frame and some column specification. This often makes life easier for programming and so becomes more important as you write more functions and begin to write your own packages.\n在本章中，我们向你展示了一些用于子集和迭代的基础 R 函数。与本书其他地方讨论的方法相比，这些函数更具“向量”风格而非“数据框”风格，因为基础 R 函数倾向于接受单个向量，而不是数据框和一些列规范。这通常使编程生活更轻松，因此在你编写更多函数并开始编写自己的包时变得更加重要。\nThis chapter concludes the programming section of the book. You’ve made a solid start on your journey to becoming not just a data scientist who uses R, but a data scientist who can program in R. We hope these chapters have sparked your interest in programming and that you’re looking forward to learning more outside of this book.\n本章结束了本书的编程部分。你已经在成为一名不仅使用 R 的数据科学家，而且是能够用 R 编程 的数据科学家的道路上迈出了坚实的一步。我们希望这些章节能激发你对编程的兴趣，并期待你在本书之外学习更多内容。",
    "crumbs": [
      "Program",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>A field guide to base R</span>"
    ]
  },
  {
    "objectID": "base-R.html#footnotes",
    "href": "base-R.html#footnotes",
    "title": "27  A field guide to base R",
    "section": "",
    "text": "Read https://adv-r.hadley.nz/subsetting.html#subset-multiple to see how you can also subset a data frame like it is a 1d object and how you can subset it with a matrix.↩︎\nBut it doesn’t handle grouped data frames differently and it doesn’t support selection helper functions like starts_with().↩︎\nIt just lacks convenient features like progress bars and reporting which element caused the problem if there’s an error.↩︎",
    "crumbs": [
      "Program",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>A field guide to base R</span>"
    ]
  },
  {
    "objectID": "communicate.html",
    "href": "communicate.html",
    "title": "Communicate",
    "section": "",
    "text": "So far, you’ve learned the tools to get your data into R, tidy it into a form convenient for analysis, and then understand your data through transformation, and visualization.\n到目前为止，你已经学习了将数据导入 R、将其整理成便于分析的形式，然后通过转换和可视化来理解数据的工具。\nHowever, it doesn’t matter how great your analysis is unless you can explain it to others: you need to communicate your results.\n然而，无论你的分析有多出色，除非你能向他人解释清楚，否则都毫无意义：你需要沟通你的结果。\n\n\n\n\n\n\n\nFigure 1: Communication is the final part of the data science process; if you can’t communicate your results to other humans, it doesn’t matter how great your analysis is.\n\n\n\n\nCommunication is the theme of the following two chapters:\n沟通是接下来两章的主题：\n\nIn 28  Quarto, you will learn about Quarto, a tool for integrating prose, code, and results.\nYou can use Quarto for analyst-to-analyst communication as well as analyst-to-decision-maker communication.\nThanks to the power of Quarto formats, you can even use the same document for both purposes.\n在 28  Quarto 中，你将学习 Quarto，这是一个用于整合文字、代码和结果的工具。\n你可以使用 Quarto 进行分析师与分析师之间的沟通，以及分析师与决策者之间的沟通。\n得益于 Quarto 格式的强大功能，你甚至可以为这两种目的使用同一份文档。\nIn 29  Quarto formats, you’ll learn a little about the many other varieties of outputs you can produce using Quarto, including dashboards, websites, and books.\n在 29  Quarto formats 中，你将学习一些关于使用 Quarto 可以生成的许多其他类型的输出，包括仪表板、网站和书籍。\n\nThese chapters focus mostly on the technical mechanics of communication, not the really hard problems of communicating your thoughts to other humans.\n这些章节主要关注沟通的技术层面，而不是将你的想法传达给其他人的真正难题。\nHowever, there are lot of other great books about communication, which we’ll point you to at the end of each chapter.\n不过，关于沟通还有很多其他优秀的书籍，我们会在每章末尾向你推荐。",
    "crumbs": [
      "Communicate"
    ]
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "28  Quarto",
    "section": "",
    "text": "28.1 Introduction\nQuarto provides a unified authoring framework for data science, combining your code, its results, and your prose. Quarto documents are fully reproducible and support dozens of output formats, like PDFs, Word files, presentations, and more.\nQuarto 为数据科学提供了一个统一的创作框架，它结合了你的代码、代码的运行结果以及你的文字说明。Quarto 文档是完全可复现的，并支持数十种输出格式，如 PDF、Word 文件、演示文稿等。\nQuarto files are designed to be used in three ways:\nQuarto 文件设计用于以下三种方式：\nQuarto is a command line interface tool, not an R package. This means that help is, by-and-large, not available through ?. Instead, as you work through this chapter, and use Quarto in the future, you should refer to the Quarto documentation.\nQuarto 是一个命令行界面工具，而不是一个 R 包。这意味着，总的来说，你无法通过 ? 来获取帮助。相反，在学习本章和将来使用 Quarto 时，你应该参考 Quarto 官方文档。\nIf you’re an R Markdown user, you might be thinking “Quarto sounds a lot like R Markdown”. You’re not wrong! Quarto unifies the functionality of many packages from the R Markdown ecosystem (rmarkdown, bookdown, distill, xaringan, etc.) into a single consistent system as well as extends it with native support for multiple programming languages like Python and Julia in addition to R. In a way, Quarto reflects everything that was learned from expanding and supporting the R Markdown ecosystem over a decade.\n如果你是 R Markdown 用户，你可能会想“Quarto 听起来很像 R Markdown”。你没说错！Quarto 将 R Markdown 生态系统中的许多包（rmarkdown、bookdown、distill、xaringan 等）的功能统一到一个单一、一致的系统中，并通过对 R 之外的多种编程语言（如 Python 和 Julia）的原生支持来扩展它。在某种程度上，Quarto 反映了十多年来在扩展和支持 R Markdown 生态系统过程中学到的一切。",
    "crumbs": [
      "Communicate",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#introduction",
    "href": "quarto.html#introduction",
    "title": "28  Quarto",
    "section": "",
    "text": "For communicating to decision-makers, who want to focus on the conclusions, not the code behind the analysis.\n用于与决策者沟通，他们希望专注于结论，而不是分析背后的代码。\nFor collaborating with other data scientists (including future you!), who are interested in both your conclusions, and how you reached them (i.e. the code).\n用于与其他数据科学家（包括未来的你！）合作，他们对你的结论以及你如何得出这些结论（即代码）都感兴趣。\nAs an environment in which to do data science, as a modern-day lab notebook where you can capture not only what you did, but also what you were thinking.\n作为一个从事数据科学的环境，就像一个现代化的实验室笔记本，你不仅可以记录你做了什么，还可以记录你的想法。\n\n\n\n\n28.1.1 Prerequisites\nYou need the Quarto command line interface (Quarto CLI), but you don’t need to explicitly install it or load it, as RStudio automatically does both when needed.\n你需要 Quarto 命令行界面 (Quarto CLI)，但你不需要显式地安装或加载它，因为 RStudio 会在需要时自动完成这两项工作。",
    "crumbs": [
      "Communicate",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#quarto-basics",
    "href": "quarto.html#quarto-basics",
    "title": "28  Quarto",
    "section": "\n28.2 Quarto basics",
    "text": "28.2 Quarto basics\nThis is a Quarto file – a plain text file that has the extension .qmd:\n这是一个 Quarto 文件——一个扩展名为 .qmd 的纯文本文件：\n\n---\ntitle: \"Diamond sizes\"\ndate: 2022-09-12\nformat: html\n---\n\n```{r}\n#| label: setup\n#| include: false\n\nlibrary(tidyverse)\n\nsmaller &lt;- diamonds |&gt; \n  filter(carat &lt;= 2.5)\n```\n\nWe have data about `r nrow(diamonds)` diamonds.\nOnly `r nrow(diamonds) - nrow(smaller)` are larger than 2.5 carats.\nThe distribution of the remainder is shown below:\n\n```{r}\n#| label: plot-smaller-diamonds\n#| echo: false\n\nsmaller |&gt; \n  ggplot(aes(x = carat)) + \n  geom_freqpoly(binwidth = 0.01)\n```\n\nIt contains three important types of content:\n它包含三种重要的内容类型：\n\nAn (optional) YAML header surrounded by ---s.\n一个（可选的）由 --- 包围的 YAML 头。\nChunks of R code surrounded by ```.\n由 ``` 包围的 R 代码块。\nText mixed with simple text formatting like # heading and _italics_.\n混合了简单文本格式的文本，如 # 标题 和 _斜体_。\n\nFigure 28.1 shows a .qmd document in RStudio with notebook interface where code and output are interleaved. You can run each code chunk by clicking the Run icon (it looks like a play button at the top of the chunk), or by pressing Cmd/Ctrl + Shift + Enter. RStudio executes the code and displays the results inline with the code.Figure 28.1 展示了 RStudio 中一个采用笔记本界面的 .qmd 文档，其中代码和输出交错显示。你可以通过点击运行图标（它看起来像代码块顶部的播放按钮），或按 Cmd/Ctrl + Shift + Enter 来运行每个代码块。RStudio 会执行代码并将结果内联显示在代码旁边。\n\n\n\n\n\n\n\nFigure 28.1: A Quarto document in RStudio. Code and output interleaved in the document, with the plot output appearing right underneath the code.\n\n\n\n\nIf you don’t like seeing your plots and output in your document and would rather make use of RStudio’s Console and Plot panes, you can click on the gear icon next to “Render” and switch to “Chunk Output in Console”, as shown in Figure 28.2.\n如果你不喜欢在文档中看到你的绘图和输出，而更愿意使用 RStudio 的控制台 (Console) 和绘图 (Plot) 窗格，你可以点击“Render”旁边的齿轮图标，并切换到“Chunk Output in Console”，如 Figure 28.2 所示。\n\n\n\n\n\n\n\nFigure 28.2: A Quarto document in RStudio with the plot output in the Plots pane.\n\n\n\n\nTo produce a complete report containing all text, code, and results, click “Render” or press Cmd/Ctrl + Shift + K. You can also do this programmatically with quarto::quarto_render(\"diamond-sizes.qmd\"). This will display the report in the viewer pane as shown in Figure 28.3 and create an HTML file.\n要生成一个包含所有文本、代码和结果的完整报告，请点击“Render”或按 Cmd/Ctrl + Shift + K。你也可以通过编程方式使用 quarto::quarto_render(\"diamond-sizes.qmd\") 来实现。这将在查看器窗格中显示报告，如 Figure 28.3 所示，并创建一个 HTML 文件。\n\n\n\n\n\n\n\nFigure 28.3: A Quarto document in RStudio with the rendered document in the Viewer pane.\n\n\n\n\nWhen you render the document, Quarto sends the .qmd file to knitr, https://yihui.org/knitr/, which executes all of the code chunks and creates a new markdown (.md) document which includes the code and its output. The markdown file generated by knitr is then processed by pandoc, https://pandoc.org, which is responsible for creating the finished file. This process is shown in Figure 28.4. The advantage of this two step workflow is that you can create a very wide range of output formats, as you’ll learn about in Chapter 29.\n当你渲染文档时，Quarto 会将 .qmd 文件发送给 knitr (https://yihui.org/knitr/)，它会执行所有代码块并创建一个新的 Markdown (.md) 文档，其中包含代码及其输出。然后，由 knitr 生成的 Markdown 文件会被 pandoc (https://pandoc.org) 处理，pandoc 负责创建最终文件。这个过程如 Figure 28.4 所示。这种两步工作流的优点是你可以创建非常广泛的输出格式，你将在 Chapter 29 中学到相关内容。\n\n\n\n\n\n\n\nFigure 28.4: Diagram of Quarto workflow from qmd, to knitr, to md, to pandoc, to output in PDF, MS Word, or HTML formats.\n\n\n\n\nTo get started with your own .qmd file, select File &gt; New File &gt; Quarto Document… in the menu bar. RStudio will launch a wizard that you can use to pre-populate your file with useful content that reminds you how the key features of Quarto work.\n要开始创建你自己的 .qmd 文件，请在菜单栏中选择 File &gt; New File &gt; Quarto Document…。RStudio 将启动一个向导，你可以用它来预填充文件，其中包含有用的内容，提醒你 Quarto 的关键功能是如何工作的。\nThe following sections dive into the three components of a Quarto document in more details: the markdown text, the code chunks, and the YAML header.\n以下各节将更详细地探讨 Quarto 文档的三个组成部分：Markdown 文本、代码块和 YAML 头。\n\n28.2.1 Exercises\n\nCreate a new Quarto document using File &gt; New File &gt; Quarto Document. Read the instructions. Practice running the chunks individually. Then render the document by clicking the appropriate button and then by using the appropriate keyboard short cut. Verify that you can modify the code, re-run it, and see modified output.\nCreate one new Quarto document for each of the three built-in formats: HTML, PDF and Word. Render each of the three documents. How do the outputs differ? How do the inputs differ? (You may need to install LaTeX in order to build the PDF output — RStudio will prompt you if this is necessary.)",
    "crumbs": [
      "Communicate",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#visual-editor",
    "href": "quarto.html#visual-editor",
    "title": "28  Quarto",
    "section": "\n28.3 Visual editor",
    "text": "28.3 Visual editor\nThe Visual editor in RStudio provides a WYSIWYM interface for authoring Quarto documents. Under the hood, prose in Quarto documents (.qmd files) is written in Markdown, a lightweight set of conventions for formatting plain text files. In fact, Quarto uses Pandoc markdown (a slightly extended version of Markdown that Quarto understands), including tables, citations, cross-references, footnotes, divs/spans, definition lists, attributes, raw HTML/TeX, and more as well as support for executing code cells and viewing their output inline. While Markdown is designed to be easy to read and write, as you will see in Section 28.4, it still requires learning new syntax. Therefore, if you’re new to computational documents like .qmd files but have experience using tools like Google Docs or MS Word, the easiest way to get started with Quarto in RStudio is the visual editor.\nRStudio 中的可视化编辑器为创作 Quarto 文档提供了一个 WYSIWYM (所见即所意) 界面。实际上，Quarto 文档（.qmd 文件）中的文字是使用 Markdown 编写的，这是一种用于格式化纯文本文件的轻量级约定。事实上，Quarto 使用的是 Pandoc markdown（Quarto 能理解的 Markdown 的一个略微扩展的版本），它包括表格、引文、交叉引用、脚注、divs/spans、定义列表、属性、原始 HTML/TeX 等等，并且支持执行代码单元格并内联查看其输出。虽然 Markdown 被设计得易于读写，正如你将在 Section 28.4 中看到的，它仍然需要学习新的语法。因此，如果你是初次接触像 .qmd 文件这样的计算文档，但有使用 Google Docs 或 MS Word 等工具的经验，那么在 RStudio 中开始使用 Quarto 的最简单方法就是可视化编辑器。\nIn the visual editor you can either use the buttons on the menu bar to insert images, tables, cross-references, etc. or you can use the catch-all &lt;kbd&gt;⌘&lt;/kbd&gt; + &lt;kbd&gt;/&lt;/kbd&gt; or &lt;kbd&gt;Ctrl&lt;/kbd&gt; + &lt;kbd&gt;/&lt;/kbd&gt; shortcut to insert just about anything. If you are at the beginning of a line (as shown in Figure 28.5), you can also enter just &lt;kbd&gt;/&lt;/kbd&gt; to invoke the shortcut.\n在可视化编辑器中，你可以使用菜单栏上的按钮来插入图像、表格、交叉引用等，也可以使用万能快捷键 &lt;kbd&gt;⌘&lt;/kbd&gt; + &lt;kbd&gt;/&lt;/kbd&gt; 或 &lt;kbd&gt;Ctrl&lt;/kbd&gt; + &lt;kbd&gt;/&lt;/kbd&gt; 来插入几乎任何东西。如果你位于一行的开头（如 Figure 28.5 所示），你也可以只输入 &lt;kbd&gt;/&lt;/kbd&gt; 来调用该快捷方式。\n\n\n\n\n\n\n\nFigure 28.5: Quarto visual editor.\n\n\n\n\nInserting images and customizing how they are displayed is also facilitated with the visual editor. You can either paste an image from your clipboard directly into the visual editor (and RStudio will place a copy of that image in the project directory and link to it) or you can use the visual editor’s Insert &gt; Figure / Image menu to browse to the image you want to insert or paste it’s URL. In addition, using the same menu you can resize the image as well as add a caption, alternative text, and a link.\n可视化编辑器也方便了插入图像和自定义其显示方式。你可以直接将剪贴板中的图像粘贴到可视化编辑器中（RStudio 会将该图像的副本放置在项目目录中并链接到它），也可以使用可视化编辑器的 Insert &gt; Figure / Image 菜单来浏览要插入的图像或粘贴其 URL。此外，使用同一菜单，你还可以调整图像大小，以及添加标题、替代文本和链接。\nThe visual editor has many more features that we haven’t enumerated here that you might find useful as you gain experience authoring with it.\n可视化编辑器还有许多我们在此未列举的功能，随着你创作经验的增加，你可能会发现它们很有用。\nMost importantly, while the visual editor displays your content with formatting, under the hood, it saves your content in plain Markdown and you can switch back and forth between the visual and source editors to view and edit your content using either tool.\n最重要的是，虽然可视化编辑器会带格式地显示你的内容，但实际上它以纯 Markdown 格式保存你的内容，你可以在可视化编辑器和源代码编辑器之间来回切换，使用任一工具查看和编辑你的内容。\n\n28.3.1 Exercises\n\nRe-create the document in Figure 28.5 using the visual editor.\nUsing the visual editor, insert a code chunk using the Insert menu and then the insert anything tool.\nUsing the visual editor, figure out how to:\n\nAdd a footnote.\nAdd a horizontal rule.\nAdd a block quote.\n\n\nIn the visual editor, go to Insert &gt; Citation and insert a citation to the paper titled Welcome to the Tidyverse using its DOI (digital object identifier), which is 10.21105/joss.01686. Render the document and observe how the reference shows up in the document. What change do you observe in the YAML of your document?",
    "crumbs": [
      "Communicate",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#sec-source-editor",
    "href": "quarto.html#sec-source-editor",
    "title": "28  Quarto",
    "section": "\n28.4 Source editor",
    "text": "28.4 Source editor\nYou can also edit Quarto documents using the Source editor in RStudio, without the assist of the Visual editor. While the Visual editor will feel familiar to those with experience writing in tools like Google docs, the Source editor will feel familiar to those with experience writing R scripts or R Markdown documents. The Source editor can also be useful for debugging any Quarto syntax errors since it’s often easier to catch these in plain text.\n你也可以在 RStudio 中使用源代码编辑器来编辑 Quarto 文档，而无需可视化编辑器的辅助。对于有使用 Google Docs 等工具写作经验的人来说，可视化编辑器会感觉很熟悉；而对于有编写 R 脚本或 R Markdown 文档经验的人来说，源代码编辑器会感觉很熟悉。源代码编辑器对于调试任何 Quarto 语法错误也很有用，因为在纯文本中通常更容易发现这些错误。\nThe guide below shows how to use Pandoc’s Markdown for authoring Quarto documents in the source editor.\n下面的指南展示了如何在源代码编辑器中使用 Pandoc’s Markdown 来创作 Quarto 文档。\n\n## Text formatting\n\n*italic* **bold** ~~strikeout~~ `code`\n\nsuperscript^2^ subscript~2~\n\n[underline]{.underline} [small caps]{.smallcaps}\n\n## Headings\n\n# 1st Level Header\n\n## 2nd Level Header\n\n### 3rd Level Header\n\n## Lists\n\n-   Bulleted list item 1\n\n-   Item 2\n\n    -   Item 2a\n\n    -   Item 2b\n\n1.  Numbered list item 1\n\n2.  Item 2.\n    The numbers are incremented automatically in the output.\n\n## Links and images\n\n&lt;http://example.com&gt;\n\n[linked phrase](http://example.com)\n\n![optional caption text](quarto.png){fig-alt=\"Quarto logo and the word quarto spelled in small case letters\"}\n\n## Tables\n\n| First Header | Second Header |\n|--------------|---------------|\n| Content Cell | Content Cell  |\n| Content Cell | Content Cell  |\n\nThe best way to learn these is simply to try them out. It will take a few days, but soon they will become second nature, and you won’t need to think about them. If you forget, you can get to a handy reference sheet with Help &gt; Markdown Quick Reference.\n学习这些的最好方法就是亲自尝试。这可能需要几天时间，但很快它们就会成为你的第二天性，你将不再需要刻意去想它们。如果你忘记了，可以通过 Help &gt; Markdown Quick Reference 打开一个方便的参考表。\n\n28.4.1 Exercises\n\nPractice what you’ve learned by creating a brief CV. The title should be your name, and you should include headings for (at least) education or employment. Each of the sections should include a bulleted list of jobs/degrees. Highlight the year in bold.\n\nUsing the source editor and the Markdown quick reference, figure out how to:\n\nAdd a footnote.\nAdd a horizontal rule.\nAdd a block quote.\n\n\nCopy and paste the contents of diamond-sizes.qmd from https://github.com/hadley/r4ds/tree/main/quarto in to a local R Quarto document. Check that you can run it, then add text after the frequency polygon that describes its most striking features.\nCreate a document in a Google doc or MS Word (or locate a document you have created previously) with some content in it such as headings, hyperlinks, formatted text, etc. Copy the contents of this document and paste it into a Quarto document in the visual editor. Then, switch over to the source editor and inspect the source code.",
    "crumbs": [
      "Communicate",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#code-chunks",
    "href": "quarto.html#code-chunks",
    "title": "28  Quarto",
    "section": "\n28.5 Code chunks",
    "text": "28.5 Code chunks\nTo run code inside a Quarto document, you need to insert a chunk. There are three ways to do so:\n要在 Quarto 文档中运行代码，你需要插入一个代码块。有三种方法可以做到这一点：\n\nThe keyboard shortcut Cmd + Option + I / Ctrl + Alt + I.\n键盘快捷键 Cmd + Option + I / Ctrl + Alt + I。\nThe “Insert” button icon in the editor toolbar.\n编辑器工具栏中的“Insert”按钮图标。\nBy manually typing the chunk delimiters ```{r} and ```.\n手动输入代码块的分隔符 ```{r} 和 ```。\n\nWe’d recommend you learn the keyboard shortcut. It will save you a lot of time in the long run!\n我们建议你学习这个键盘快捷键。从长远来看，它会为你节省大量时间！\nYou can continue to run the code using the keyboard shortcut that by now (we hope!) you know and love: Cmd/Ctrl + Enter. However, chunks get a new keyboard shortcut: Cmd/Ctrl + Shift + Enter, which runs all the code in the chunk. Think of a chunk like a function. A chunk should be relatively self-contained, and focused around a single task.\n你可以继续使用你现在（我们希望！）已经熟知并喜爱的键盘快捷键来运行代码：Cmd/Ctrl + Enter。然而，代码块有了一个新的键盘快捷键：Cmd/Ctrl + Shift + Enter，它会运行代码块中的所有代码。可以把代码块想象成一个函数。一个代码块应该是相对独立的，并专注于一个单一的任务。\nThe following sections describe the chunk header which consists of ```{r}, followed by an optional chunk label and various other chunk options, each on their own line, marked by #|.\n以下各节描述了代码块的头部，它由 ```{r} 组成，后面跟着一个可选的代码块标签和各种其他的代码块选项，每个选项都独占一行，并以 #| 标记。\n\n28.5.1 Chunk label\nChunks can be given an optional label, e.g.\n代码块可以被赋予一个可选的标签，例如：\n\n```{r}\n#| label: simple-addition\n1 + 1\n```\n#&gt; [1] 2\n\nThis has three advantages:\n这有三个优点：\n\n\nYou can more easily navigate to specific chunks using the drop-down code navigator in the bottom-left of the script editor:\n你可以使用脚本编辑器左下角的下拉代码导航器更轻松地导航到特定的代码块：\n{r}     #| echo: false     #| out-width: \"30%\"     #| fig-alt: |     #|   Snippet of RStudio IDE showing only the drop-down code navigator      #|   which shows three chunks. Chunk 1 is setup. Chunk 2 is cars and      #|   it is in a section called Quarto. Chunk 3 is pressure and it is in      #|   a section called Including plots.     knitr::include_graphics(\"screenshots/quarto-chunk-nav.png\")\n\nGraphics produced by the chunks will have useful names that make them easier to use elsewhere. More on that in Section 28.6.\n代码块生成的图形将具有有用的名称，使它们更易于在其他地方使用。更多相关内容请参见 Section 28.6。\nYou can set up networks of cached chunks to avoid re-performing expensive computations on every run. More on that in Section 28.8.\n你可以设置缓存代码块网络，以避免在每次运行时重新执行耗时的计算。更多相关内容请参见 Section 28.8。\n\nYour chunk labels should be short but evocative and should not contain spaces. We recommend using dashes (-) to separate words (instead of underscores, _) and avoiding other special characters in chunk labels.\n你的代码块标签应该简短但具有描述性，并且不应包含空格。我们建议使用破折号 (-) 来分隔单词（而不是下划线 _），并避免在代码块标签中使用其他特殊字符。\nYou are generally free to label your chunk however you like, but there is one chunk name that imbues special behavior: setup. When you’re in a notebook mode, the chunk named setup will be run automatically once, before any other code is run.\n你通常可以随意命名你的代码块，但有一个代码块名称具有特殊的行为：setup。当你在笔记本模式下时，名为 setup 的代码块会在运行任何其他代码之前自动运行一次。\nAdditionally, chunk labels cannot be duplicated. Each chunk label must be unique.\n此外，代码块标签不能重复。每个代码块标签必须是唯一的。\n\n28.5.2 Chunk options\nChunk output can be customized with options, fields supplied to chunk header. Knitr provides almost 60 options that you can use to customize your code chunks. Here we’ll cover the most important chunk options that you’ll use frequently. You can see the full list at https://yihui.org/knitr/options.\n代码块的输出可以通过选项（提供给代码块头部的字段）进行自定义。Knitr 提供了近 60 个选项，你可以用来自定义你的代码块。在这里，我们将介绍你将频繁使用的最重要的代码块选项。你可以在 https://yihui.org/knitr/options 查看完整列表。\nThe most important set of options controls if your code block is executed and what results are inserted in the finished report:\n最重要的一组选项控制着你的代码块是否被执行，以及哪些结果被插入到最终的报告中：\n\neval: false prevents code from being evaluated.eval: false 可防止代码被执行。\n(And obviously if the code is not run, no results will be generated).\n（很明显，如果代码没有运行，就不会生成任何结果）。\nThis is useful for displaying example code, or for disabling a large block of code without commenting each line.\n这对于显示示例代码，或在不逐行注释的情况下禁用大段代码非常有用。\ninclude: false runs the code, but doesn’t show the code or results in the final document.include: false 会运行代码，但不会在最终文档中显示代码或结果。\nUse this for setup code that you don’t want cluttering your report.\n可将此选项用于你不想让报告显得杂乱的设置代码。\necho: false prevents code, but not the results, from appearing in the finished file.echo: false 可以防止代码（但不会阻止结果）出现在最终文件中。\nUse this when writing reports aimed at people who don’t want to see the underlying R code.\n当编写面向不想看到底层 R 代码的读者的报告时，请使用此选项。\nmessage: false or warning: false prevents messages or warnings from appearing in the finished file.message: false 或 warning: false 可以防止消息或警告出现在最终文件中。\nresults: hide hides printed output; fig-show: hide hides plots.results: hide 隐藏打印输出；fig-show: hide 隐藏绘图。\nerror: true causes the render to continue even if code returns an error.error: true 会使渲染在代码返回错误时也能继续进行。\nThis is rarely something you’ll want to include in the final version of your report, but can be very useful if you need to debug exactly what is going on inside your .qmd.\n你很少会希望在报告的最终版本中包含此选项，但当需要准确调试 .qmd 文件内部情况时，它会非常有用。\nIt’s also useful if you’re teaching R and want to deliberately include an error.\n如果你在教授 R 语言并希望故意引入一个错误，这个选项也很有用。\nThe default, error: false causes rendering to fail if there is a single error in the document.\n默认值 error: false 会在文档中出现单个错误时导致渲染失败。\n\nEach of these chunk options get added to the header of the chunk, following #|, e.g., in the following chunk the result is not printed since eval is set to false.\n这些代码块选项中的每一个都会被添加到代码块的头部，跟在 #| 后面，例如，在下面的代码块中，结果不会被打印出来，因为 eval 被设置为了 false。\n\n```{r}\n#| label: simple-multiplication\n#| eval: false\n2 * 2\n```\n\nThe following table summarizes which types of output each option suppresses:\n下表总结了每个选项抑制的输出类型：\n\n\n\n\n\n\n\n\n\n\n\nOption\nRun code\nShow code\nOutput\nPlots\nMessages\nWarnings\n\n\n\neval: false\nX\n\nX\nX\nX\nX\n\n\ninclude: false\n\nX\nX\nX\nX\nX\n\n\necho: false\n\nX\n\n\n\n\n\n\nresults: hide\n\n\nX\n\n\n\n\n\nfig-show: hide\n\n\n\nX\n\n\n\n\nmessage: false\n\n\n\n\nX\n\n\n\nwarning: false\n\n\n\n\n\nX\n\n\n\n28.5.3 Global options\nAs you work more with knitr, you will discover that some of the default chunk options don’t fit your needs and you want to change them.\n随着你更多地使用 knitr，你会发现一些默认的代码块选项不符合你的需求，你会想要更改它们。\nYou can do this by adding the preferred options in the document YAML, under execute. For example, if you are preparing a report for an audience who does not need to see your code but only your results and narrative, you might set echo: false at the document level. That will hide the code by default, so only showing the chunks you deliberately choose to show (with echo: true). You might consider setting message: false and warning: false, but that would make it harder to debug problems because you wouldn’t see any messages in the final document.\n你可以在文档的 YAML 中，execute 项下添加偏好的选项来做到这一点。例如，如果你正在为一群不需要看你的代码，只关心结果和叙述的读者准备报告，你可以在文档级别设置 echo: false。这样会默认隐藏代码，只显示你特意选择显示的代码块（通过 echo: true）。你可能会考虑设置 message: false 和 warning: false，但这会使调试问题变得更加困难，因为你在最终的文档中看不到任何消息。\ntitle: \"My report\"\nexecute:\n  echo: false\nSince Quarto is designed to be multi-lingual (works with R as well as other languages like Python, Julia, etc.), all of the knitr options are not available at the document execution level since some of them only work with knitr and not other engines Quarto uses for running code in other languages (e.g., Jupyter). You can, however, still set these as global options for your document under the knitr field, under opts_chunk. For example, when writing books and tutorials we set:\n由于 Quarto 被设计为多语言的（既支持 R，也支持 Python、Julia 等其他语言），因此并非所有的 knitr 选项都在文档执行级别可用，因为其中一些选项只适用于 knitr，而不适用于 Quarto 用于运行其他语言代码（例如，Jupyter）的其他引擎。不过，你仍然可以在 knitr 字段下的 opts_chunk 中将它们设置为文档的全局选项。例如，在编写书籍和教程时，我们会这样设置：\ntitle: \"Tutorial\"\nknitr:\n  opts_chunk:\n    comment: \"#&gt;\"\n    collapse: true\nThis uses our preferred comment formatting and ensures that the code and output are kept closely entwined.\n这会使用我们偏好的注释格式，并确保代码和输出紧密地结合在一起。\n\n28.5.4 Inline code\nThere is one other way to embed R code into a Quarto document: directly into the text, with: `r `. This can be very useful if you mention properties of your data in the text. For example, the example document used at the start of the chapter had:\n还有一种将 R 代码嵌入 Quarto 文档的方式：直接嵌入文本中，使用：`r `。如果你在文本中提到数据的属性，这会非常有用。例如，本章开头使用的示例文档中有：\n\nWe have data about `r nrow(diamonds)` diamonds. Only `r nrow(diamonds) - nrow(smaller)` are larger than 2.5 carats. The distribution of the remainder is shown below:\n\nWhen the report is rendered, the results of these computations are inserted into the text:\n当报告被渲染时，这些计算的结果会被插入到文本中：\n\nWe have data about 53940 diamonds. Only 126 are larger than 2.5 carats. The distribution of the remainder is shown below:\n\nWhen inserting numbers into text, format() is your friend. It allows you to set the number of digits so you don’t print to a ridiculous degree of precision, and a big.mark to make numbers easier to read. You might combine these into a helper function:\n当在文本中插入数字时，format() 是你的好帮手。它允许你设置 digits 的数量，这样你就不会打印出精度高到离谱的数字，并且可以使用 big.mark 使数字更易于阅读。你可以将这些组合成一个辅助函数：\n\ncomma &lt;- function(x) format(x, digits = 2, big.mark = \",\")\ncomma(3452345)\n#&gt; [1] \"3,452,345\"\ncomma(.12358124331)\n#&gt; [1] \"0.12\"\n\n\n28.5.5 Exercises\n\nAdd a section that explores how diamond sizes vary by cut, color, and clarity. Assume you’re writing a report for someone who doesn’t know R, and instead of setting echo: false on each chunk, set a global option.\nDownload diamond-sizes.qmd from https://github.com/hadley/r4ds/tree/main/quarto. Add a section that describes the largest 20 diamonds, including a table that displays their most important attributes.\nModify diamonds-sizes.qmd to use label_comma() to produce nicely formatted output. Also include the percentage of diamonds that are larger than 2.5 carats.",
    "crumbs": [
      "Communicate",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#sec-figures",
    "href": "quarto.html#sec-figures",
    "title": "28  Quarto",
    "section": "\n28.6 Figures",
    "text": "28.6 Figures\nThe figures in a Quarto document can be embedded (e.g., a PNG or JPEG file) or generated as a result of a code chunk.\nQuarto 文档中的图形可以是嵌入式的（例如，PNG 或 JPEG 文件），也可以是由代码块生成的结果。\nTo embed an image from an external file, you can use the Insert menu in the Visual Editor in RStudio and select Figure / Image. This will pop open a menu where you can browse to the image you want to insert as well as add alternative text or caption to it and adjust its size. In the visual editor you can also simply paste an image from your clipboard into your document and RStudio will place a copy of that image in your project folder.\n要从外部文件嵌入图像，你可以在 RStudio 的可视化编辑器中使用插入菜单，并选择图形 / 图像。这将弹出一个菜单，你可以在其中浏览要插入的图像，并为其添加替代文本或标题，以及调整其大小。在可视化编辑器中，你也可以简单地将剪贴板中的图像粘贴到文档中，RStudio 会将该图像的副本放置在你的项目文件夹中。\nIf you include a code chunk that generates a figure (e.g., includes a ggplot() call), the resulting figure will be automatically included in your Quarto document.\n如果你包含一个生成图形的代码块（例如，包含一个 ggplot() 调用），生成的图形将自动包含在你的 Quarto 文档中。\n\n28.6.1 Figure sizing\nThe biggest challenge of graphics in Quarto is getting your figures the right size and shape. There are five main options that control figure sizing: fig-width, fig-height, fig-asp, out-width and out-height. Image sizing is challenging because there are two sizes (the size of the figure created by R and the size at which it is inserted in the output document), and multiple ways of specifying the size (i.e. height, width, and aspect ratio: pick two of three).\n在 Quarto 中，图形面临的最大挑战是获得合适的尺寸和形状。有五个主要选项可以控制图形大小：fig-width、fig-height、fig-asp、out-width 和 out-height。图像尺寸调整之所以具有挑战性，是因为存在两种尺寸（R 创建的图形尺寸和插入到输出文档中的尺寸），并且有多种指定尺寸的方式（即高度、宽度和纵横比：三者选其二）。\nWe recommend three of the five options:\n我们推荐五个选项中的三个：\n\nPlots tend to be more aesthetically pleasing if they have consistent width.\nTo enforce this, set fig-width: 6 (6”) and fig-asp: 0.618 (the golden ratio) in the defaults.\nThen in individual chunks, only adjust fig-asp.\n如果图的宽度一致，它们往往在美学上更令人愉悦。\n为了实现这一点，可以在默认设置中设定 fig-width: 6（6英寸）和 fig-asp: 0.618（黄金比例）。\n然后在单个代码块中，只调整 fig-asp。\nControl the output size with out-width and set it to a percentage of the body width of the output document.\nWe suggest to out-width: \"70%\" and fig-align: center.\nThat gives plots room to breathe, without taking up too much space.\n使用 out-width 控制输出尺寸，并将其设置为输出文档正文宽度的百分比。\n我们建议设置为 out-width: \"70%\" 和 fig-align: center。\n这给图表留出了呼吸的空间，而不会占用太多空间。\nTo put multiple plots in a single row, set the layout-ncol to 2 for two plots, 3 for three plots, etc.\nThis effectively sets out-width to “50%” for each of your plots if layout-ncol is 2, “33%” if layout-ncol is 3, etc.\nDepending on what you’re trying to illustrate (e.g., show data or show plot variations), you might also tweak fig-width, as discussed below.\n要将多个图放在一行中，可以将 layout-ncol 设置为 2（表示两个图）、3（表示三个图）等。\n如果 layout-ncol 是 2，这实际上会将每个图的 out-width 设置为 “50%”；如果 layout-ncol 是 3，则设置为 “33%”，以此类推。\n根据你试图说明的内容（例如，显示数据或显示图的变化），你可能还需要调整 fig-width，如下文所述。\n\nIf you find that you’re having to squint to read the text in your plot, you need to tweak fig-width. If fig-width is larger than the size the figure is rendered in the final doc, the text will be too small; if fig-width is smaller, the text will be too big. You’ll often need to do a little experimentation to figure out the right ratio between the fig-width and the eventual width in your document. To illustrate the principle, the following three plots have fig-width of 4, 6, and 8 respectively:\n如果你发现自己需要眯着眼睛才能看清图中的文字，那么你需要调整 fig-width。如果 fig-width 大于最终文档中渲染的图形尺寸，文字会太小；如果 fig-width 更小，文字会太大。你通常需要做一些实验来找出 fig-width 和文档中最终宽度之间的正确比例。为了说明这个原理，下面三个图的 fig-width 分别为 4、6 和 8：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you want to make sure the font size is consistent across all your figures, whenever you set out-width, you’ll also need to adjust fig-width to maintain the same ratio with your default out-width. For example, if your default fig-width is 6 and out-width is “70%”, when you set out-width: \"50%\" you’ll need to set fig-width to 4.3 (6 * 0.5 / 0.7).\n如果你想确保所有图形的字体大小保持一致，那么每当你设置 out-width 时，你也需要调整 fig-width 以保持与默认 out-width 相同的比例。例如，如果你的默认 fig-width 是 6，out-width 是 “70%”，那么当你将 out-width 设置为 “50%” 时，你需要将 fig-width 设置为 4.3 (6 * 0.5 / 0.7)。\nFigure sizing and scaling is an art and science and getting things right can require an iterative trial-and-error approach. You can learn more about figure sizing in the taking control of plot scaling blog post.\n图形的尺寸和缩放是一门艺术和科学，要做到恰到好处可能需要反复试验。你可以在 《掌控绘图缩放》这篇博文 中了解更多关于图形尺寸调整的知识。\n\n28.6.2 Other important options\nWhen mingling code and text, like in this book, you can set fig-show: hold so that plots are shown after the code. This has the pleasant side effect of forcing you to break up large blocks of code with their explanations.\n当像本书这样将代码和文本混合在一起时，你可以设置 fig-show: hold，这样图表就会在代码之后显示。这样做有一个令人愉快的好处，就是迫使你用解释来打断大段的代码。\nTo add a caption to the plot, use fig-cap. In Quarto this will change the figure from inline to “floating”.\n要为图表添加标题，请使用 fig-cap。在 Quarto 中，这会将图形从内联（inline）更改为“浮动”（floating）。\nIf you’re producing PDF output, the default graphics type is PDF. This is a good default because PDFs are high quality vector graphics. However, they can produce very large and slow plots if you are displaying thousands of points. In that case, set fig-format: \"png\" to force the use of PNGs. They are slightly lower quality, but will be much more compact.\n如果你要生成 PDF 输出，默认的图形类型是 PDF。这是一个很好的默认设置，因为 PDF 是高质量的矢量图形。但是，如果你要显示数千个点，它们可能会生成非常大且加载缓慢的图。在这种情况下，可以设置 fig-format: \"png\" 来强制使用 PNG。它们的质量稍低，但会更加紧凑。\nIt’s a good idea to name code chunks that produce figures, even if you don’t routinely label other chunks. The chunk label is used to generate the file name of the graphic on disk, so naming your chunks makes it much easier to pick out plots and reuse in other circumstances (e.g., if you want to quickly drop a single plot into an email).\n为生成图形的代码块命名是一个好主意，即使你并不常规地为其他代码块添加标签。代码块标签用于生成磁盘上图形文件的名称，因此为代码块命名可以让你更容易地挑选出图表并在其他情况下重用（例如，如果你想快速将单个图表放入电子邮件中）。\n\n28.6.3 Exercises\n\nOpen diamond-sizes.qmd in the visual editor, find an image of a diamond, copy it, and paste it into the document. Double click on the image and add a caption. Resize the image and render your document. Observe how the image is saved in your current working directory.\nEdit the label of the code chunk in diamond-sizes.qmd that generates a plot to start with the prefix fig- and add a caption to the figure with the chunk option fig-cap. Then, edit the text above the code chunk to add a cross-reference to the figure with Insert &gt; Cross Reference.\nChange the size of the figure with the following chunk options, one at a time, render your document, and describe how the figure changes.\n\nfig-width: 10\nfig-height: 3\nout-width: \"100%\"\nout-width: \"20%\"",
    "crumbs": [
      "Communicate",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#tables",
    "href": "quarto.html#tables",
    "title": "28  Quarto",
    "section": "\n28.7 Tables",
    "text": "28.7 Tables\nSimilar to figures, you can include two types of tables in a Quarto document. They can be markdown tables that you create directly in your Quarto document (using the Insert Table menu) or they can be tables generated as a result of a code chunk. In this section we will focus on the latter, tables generated via computation.\n与图形类似，你可以在 Quarto 文档中包含两种类型的表格。它们可以是你直接在 Quarto 文档中创建的 markdown 表格（使用“插入表格”菜单），也可以是由代码块生成的结果。在本节中，我们将重点关注后者，即通过计算生成的表格。\nBy default, Quarto prints data frames and matrices as you’d see them in the console:\n默认情况下，Quarto 会像你在控制台中看到的那样打印数据框和矩阵：\n\nmtcars[1:5, ]\n#&gt;                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n#&gt; Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n#&gt; Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n#&gt; Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n#&gt; Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n#&gt; Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\nIf you prefer that data be displayed with additional formatting you can use the knitr::kable() function. The code below generates Table 28.1.\n如果你希望数据以附加格式显示，可以使用 knitr::kable() 函数。下面的代码生成了 Table 28.1。\n\nknitr::kable(mtcars[1:5, ], )\n\n\nTable 28.1: A knitr kable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\n\n\nRead the documentation for ?knitr::kable to see the other ways in which you can customize the table. For even deeper customization, consider the gt, huxtable, reactable, kableExtra, xtable, stargazer, pander, tables, and ascii packages. Each provides a set of tools for returning formatted tables from R code.\n阅读 ?knitr::kable 的文档，了解自定义表格的其他方法。如果需要更深度的定制，可以考虑使用 gt、huxtable、reactable、kableExtra、xtable、stargazer、pander、tables 和 ascii 等包。每个包都提供了一套用于从 R 代码返回格式化表格的工具。\n\n28.7.1 Exercises\n\nOpen diamond-sizes.qmd in the visual editor, insert a code chunk, and add a table with knitr::kable() that shows the first 5 rows of the diamonds data frame.\nDisplay the same table with gt::gt() instead.\nAdd a chunk label that starts with the prefix tbl- and add a caption to the table with the chunk option tbl-cap. Then, edit the text above the code chunk to add a cross-reference to the table with Insert &gt; Cross Reference.",
    "crumbs": [
      "Communicate",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#sec-caching",
    "href": "quarto.html#sec-caching",
    "title": "28  Quarto",
    "section": "\n28.8 Caching",
    "text": "28.8 Caching\nNormally, each render of a document starts from a completely clean slate.\n通常，每次渲染文档都是从一个完全干净的状态开始的。\nThis is great for reproducibility, because it ensures that you’ve captured every important computation in code.\n这对于可复现性来说非常好，因为它确保了你已经在代码中捕获了每一个重要的计算。\nHowever, it can be painful if you have some computations that take a long time.\n然而，如果你有一些需要很长时间才能完成的计算，这可能会很痛苦。\nThe solution is cache: true.\n解决方案是 cache: true。\nYou can enable the Knitr cache at the document level for caching the results of all computations in a document using standard YAML options:\n你可以使用标准的 YAML 选项，在文档级别启用 Knitr 缓存，来缓存文档中所有计算的结果：\n---\ntitle: \"My Document\"\nexecute: \n  cache: true\n---\nYou can also enable caching at the chunk level for caching the results of computation in a specific chunk:\n你也可以在代码块级别启用缓存，来缓存特定代码块中的计算结果：\n\n```{r}\n#| cache: true\n# code for lengthy computation...\n```\n\nWhen set, this will save the output of the chunk to a specially named file on disk.\n设置后，这将把代码块的输出保存到磁盘上的一个特殊命名的文件中。\nOn subsequent runs, knitr will check to see if the code has changed, and if it hasn’t, it will reuse the cached results.\n在后续运行中，knitr 将检查代码是否已更改，如果未更改，它将重用缓存的结果。\nThe caching system must be used with care, because by default it is based on the code only, not its dependencies.\n缓存系统必须谨慎使用，因为默认情况下它只基于代码本身，而不基于其依赖项。\nFor example, here the processed_data chunk depends on the raw-data chunk:\n例如，这里的 processed_data 代码块依赖于 raw-data 代码块：\n``` {{r}}\n#| label: raw-data\n#| cache: true\nrawdata &lt;- readr::read_csv(\"a_very_large_file.csv\")\n```\n``` {{r}}\n#| label: processed_data\n#| cache: true\nprocessed_data &lt;- rawdata |&gt; \n  filter(!is.na(import_var)) |&gt; \n  mutate(new_variable = complicated_transformation(x, y, z))\n```\nCaching the processed_data chunk means that it will get re-run if the dplyr pipeline is changed, but it won’t get rerun if the read_csv() call changes.\n缓存 processed_data 代码块意味着如果 dplyr 管道发生更改，它将重新运行，但如果 read_csv() 调用发生更改，它将不会重新运行。\nYou can avoid that problem with the dependson chunk option:\n你可以使用 dependson 代码块选项来避免这个问题：\n``` {{r}}\n#| label: processed-data\n#| cache: true\n#| dependson: \"raw-data\"\nprocessed_data &lt;- rawdata |&gt; \n  filter(!is.na(import_var)) |&gt; \n  mutate(new_variable = complicated_transformation(x, y, z))\n```\ndependson should contain a character vector of every chunk that the cached chunk depends on.dependson 应该包含一个字符向量，其中包含被缓存的代码块所依赖的每一个代码块。\nKnitr will update the results for the cached chunk whenever it detects that one of its dependencies have changed.\n每当 Knitr 检测到其某个依赖项已更改时，它将更新缓存代码块的结果。\nNote that the chunks won’t update if a_very_large_file.csv changes, because knitr caching only tracks changes within the .qmd file.\n请注意，如果 a_very_large_file.csv 发生更改，代码块不会更新，因为 knitr 缓存只跟踪 .qmd 文件内部的更改。\nIf you want to also track changes to that file you can use the cache.extra option.\n如果你还想跟踪该文件的更改，可以使用 cache.extra 选项。\nThis is an arbitrary R expression that will invalidate the cache whenever it changes.\n这是一个任意的 R 表达式，每当它发生更改时，都会使缓存失效。\nA good function to use is file.mtime(): it returns when it was last modified.\n一个很好用的函数是 file.mtime()：它返回文件的最后修改时间。\nThen you can write:\n然后你可以这样写：\n``` {{r}}\n#| label: raw-data\n#| cache: true\n#| cache.extra: !expr file.mtime(\"a_very_large_file.csv\")\nrawdata &lt;- readr::read_csv(\"a_very_large_file.csv\")\n```\nWe’ve followed the advice of David Robinson to name these chunks: each chunk is named after the primary object that it creates.\n我们遵循了 David Robinson 的建议来命名这些代码块：每个代码块都以它创建的主要对象命名。\nThis makes it easier to understand the dependson specification.\n这使得理解 dependson 的规范变得更加容易。\nAs your caching strategies get progressively more complicated, it’s a good idea to regularly clear out all your caches with knitr::clean_cache().\n随着你的缓存策略变得越来越复杂，定期使用 knitr::clean_cache() 清除所有缓存是个好主意。\n\n28.8.1 Exercises\n\nSet up a network of chunks where d depends on c and b, and both b and c depend on a. Have each chunk print lubridate::now(), set cache: true, then verify your understanding of caching.",
    "crumbs": [
      "Communicate",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#troubleshooting",
    "href": "quarto.html#troubleshooting",
    "title": "28  Quarto",
    "section": "\n28.9 Troubleshooting",
    "text": "28.9 Troubleshooting\nTroubleshooting Quarto documents can be challenging because you are no longer in an interactive R environment, and you will need to learn some new tricks.\n对 Quarto 文档进行故障排除可能具有挑战性，因为你不再处于交互式 R 环境中，需要学习一些新技巧。\nAdditionally, the error could be due to issues with the Quarto document itself or due to the R code in the Quarto document.\n此外，错误可能是由于 Quarto 文档本身的问题，也可能是由于 Quarto 文档中的 R 代码问题。\nOne common error in documents with code chunks is duplicated chunk labels, which are especially pervasive if your workflow involves copying and pasting code chunks.\n带有代码块的文档中一个常见的错误是重复的代码块标签，如果你的工作流程涉及复制和粘贴代码块，这个问题尤其普遍。\nTo address this issue, all you need to do is to change one of your duplicated labels.\n要解决此问题，你只需更改其中一个重复的标签即可。\nIf the errors are due to the R code in the document, the first thing you should always try is to recreate the problem in an interactive session.\n如果错误是由于文档中的 R 代码引起的，你首先应该尝试在交互式会话中重现问题。\nRestart R, then “Run all chunks”, either from the Code menu, under Run region or with the keyboard shortcut Ctrl + Alt + R.\n重启 R，然后“运行所有代码块”，可以从“代码”菜单的“运行区域”下选择，也可以使用键盘快捷键 Ctrl + Alt + R。\nIf you’re lucky, that will recreate the problem, and you can figure out what’s going on interactively.\n如果幸运的话，这将重现问题，你就可以在交互式环境中找出问题所在。\nIf that doesn’t help, there must be something different between your interactive environment and the Quarto environment.\n如果这没有帮助，那么你的交互式环境和 Quarto 环境之间肯定存在差异。\nYou’re going to need to systematically explore the options.\n你将需要系统地探索各种可能性。\nThe most common difference is the working directory: the working directory of a Quarto is the directory in which it lives.\n最常见的区别是工作目录：Quarto 的工作目录是它所在的目录。\nCheck the working directory is what you expect by including getwd() in a chunk.\n通过在代码块中包含 getwd() 来检查工作目录是否符合你的预期。\nNext, brainstorm all the things that might cause the bug.\n接下来，集思广益，想出所有可能导致错误的事情。\nYou’ll need to systematically check that they’re the same in your R session and your Quarto session.\n你需要系统地检查它们在你的 R 会话和 Quarto 会话中是否相同。\nThe easiest way to do that is to set error: true on the chunk causing the problem, then use print() and str() to check that settings are as you expect.\n最简单的方法是在导致问题的代码块上设置 error: true，然后使用 print() 和 str() 来检查设置是否如你所料。",
    "crumbs": [
      "Communicate",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#yaml-header",
    "href": "quarto.html#yaml-header",
    "title": "28  Quarto",
    "section": "\n28.10 YAML header",
    "text": "28.10 YAML header\nYou can control many other “whole document” settings by tweaking the parameters of the YAML header.\n你可以通过调整 YAML 头部的参数来控制许多其他的“整个文档”设置。\nYou might wonder what YAML stands for: it’s “YAML Ain’t Markup Language”, which is designed for representing hierarchical data in a way that’s easy for humans to read and write.\n你可能想知道 YAML 代表什么：它是 “YAML Ain’t Markup Language”（YAML 不是标记语言），旨在以一种易于人类读写的方式表示分层数据。\nQuarto uses it to control many details of the output.\nQuarto 使用它来控制输出的许多细节。\nHere we’ll discuss three: self-contained documents, document parameters, and bibliographies.\n这里我们将讨论三个：自包含文档、文档参数和参考文献。\n\n28.10.1 Self-contained\nHTML documents typically have a number of external dependencies (e.g., images, CSS style sheets, JavaScript, etc.) and, by default, Quarto places these dependencies in a _files folder in the same directory as your .qmd file.\nHTML 文档通常有许多外部依赖项（例如，图像、CSS 样式表、JavaScript 等），默认情况下，Quarto 将这些依赖项放在与你的 .qmd 文件相同目录下的一个 _files 文件夹中。\nIf you publish the HTML file on a hosting platform (e.g., QuartoPub, https://quartopub.com/), the dependencies in this directory are published with your document and hence are available in the published report.\n如果你在托管平台（例如，QuartoPub, https://quartopub.com/）上发布 HTML 文件，此目录中的依赖项将与你的文档一起发布，因此在发布的报告中可用。\nHowever, if you want to email the report to a colleague, you might prefer to have a single, self-contained, HTML document that embeds all of its dependencies.\n然而，如果你想通过电子邮件将报告发送给同事，你可能更喜欢一个单一的、自包含的、嵌入了所有依赖项的 HTML 文档。\nYou can do this by specifying the embed-resources option:\n你可以通过指定 embed-resources 选项来做到这一点：\nformat:\n  html:\n    embed-resources: true\nThe resulting file will be self-contained, such that it will need no external files and no internet access to be displayed properly by a browser.\n生成的文件将是自包含的，因此它不需要任何外部文件，也不需要互联网连接即可被浏览器正常显示。\n\n28.10.2 Parameters\nQuarto documents can include one or more parameters whose values can be set when you render the report.\nQuarto 文档可以包含一个或多个参数，其值可以在你渲染报告时设置。\nParameters are useful when you want to re-render the same report with distinct values for various key inputs.\n当你希望使用不同的关键输入值重新渲染同一份报告时，参数非常有用。\nFor example, you might be producing sales reports per branch, exam results by student, or demographic summaries by country.\n例如，你可能正在按分公司生成销售报告、按学生生成考试成绩或按国家生成人口摘要。\nTo declare one or more parameters, use the params field.\n要声明一个或多个参数，请使用 params 字段。\nThis example uses a my_class parameter to determine which class of cars to display:\n此示例使用 my_class 参数来确定要显示的汽车类别：\n\n---\nformat: html\nparams:\n  my_class: \"suv\"\n---\n\n```{r}\n#| label: setup\n#| include: false\n\nlibrary(tidyverse)\n\nclass &lt;- mpg |&gt; filter(class == params$my_class)\n```\n\n# Fuel economy for `r params$my_class`s\n\n```{r}\n#| message: false\n\nggplot(class, aes(x = displ, y = hwy)) + \n  geom_point() + \n  geom_smooth(se = FALSE)\n```\n\nAs you can see, parameters are available within the code chunks as a read-only list named params.\n如你所见，参数在代码块中以名为 params 的只读列表形式提供。\nYou can write atomic vectors directly into the YAML header.\n你可以将原子向量直接写入 YAML 头部。\nYou can also run arbitrary R expressions by prefacing the parameter value with !expr.\n你还可以通过在参数值前加上 !expr 来运行任意的 R 表达式。\nThis is a good way to specify date/time parameters.\n这是指定日期/时间参数的好方法。\nparams:\n  start: !expr lubridate::ymd(\"2015-01-01\")\n  snapshot: !expr lubridate::ymd_hms(\"2015-01-01 12:30:00\")\n\n28.10.3 Bibliographies and Citations\nQuarto can automatically generate citations and a bibliography in a number of styles.\nQuarto 可以自动生成多种样式的引文和参考文献。\nThe most straightforward way of adding citations and bibliographies to a Quarto document is using the visual editor in RStudio.\n向 Quarto 文档添加引文和参考文献最直接的方法是使用 RStudio 中的可视化编辑器。\nTo add a citation using the visual editor, go to Insert &gt; Citation.\n要使用可视化编辑器添加引文，请转到插入 &gt; 引文 (Insert &gt; Citation)。\nCitations can be inserted from a variety of sources:\n引文可以从多种来源插入：\n\nDOI (Document Object Identifier) references.DOI（文档对象标识符）引用。\nZotero personal or group libraries.Zotero 个人或小组文献库。\nSearches of Crossref, DataCite, or PubMed.\n搜索 Crossref、DataCite 或 PubMed。\nYour document bibliography (a .bib file in the directory of your document)\n你的文档参考文献（文档目录中的一个 .bib 文件）。\n\nUnder the hood, the visual mode uses the standard Pandoc markdown representation for citations (e.g., [@citation]).\n在底层，可视化模式使用标准的 Pandoc markdown 表示法来表示引文（例如，[@citation]）。\nIf you add a citation using one of the first three methods, the visual editor will automatically create a bibliography.bib file for you and add the reference to it.\n如果你使用前三种方法之一添加引文，可视化编辑器将自动为你创建一个 bibliography.bib 文件并将引用添加到其中。\nIt will also add a bibliography field to the document YAML.\n它还会在文档的 YAML 中添加一个 bibliography 字段。\nAs you add more references, this file will get populated with their citations.\n随着你添加更多引用，该文件将填充它们的引文信息。\nYou can also directly edit this file using many common bibliography formats including BibLaTeX, BibTeX, EndNote, Medline.\n你还可以使用许多常见的参考文献格式直接编辑此文件，包括 BibLaTeX、BibTeX、EndNote、Medline。\nTo create a citation within your .qmd file in the source editor, use a key composed of ‘@’ + the citation identifier from the bibliography file.\n要在源编辑器中在你的 .qmd 文件内创建引文，请使用由“@”+ 参考文献文件中的引文标识符组成的键。\nThen place the citation in square brackets.\n然后将引文放在方括号中。\nHere are some examples:\n这里有一些例子：\nSeparate multiple citations with a `;`: Blah blah [@smith04; @doe99].\n\nYou can add arbitrary comments inside the square brackets: \nBlah blah [see @doe99, pp. 33-35; also @smith04, ch. 1].\n\nRemove the square brackets to create an in-text citation: @smith04 \nsays blah, or @smith04 [p. 33] says blah.\n\nAdd a `-` before the citation to suppress the author's name: \nSmith says blah [-@smith04].\nWhen Quarto renders your file, it will build and append a bibliography to the end of your document.\n当 Quarto 渲染你的文件时，它将构建一个参考文献列表并附加到你的文档末尾。\nThe bibliography will contain each of the cited references from your bibliography file, but it will not contain a section heading.\n参考文献列表将包含你的参考文献文件中的每一个被引用的文献，但它不会包含章节标题。\nAs a result it is common practice to end your file with a section header for the bibliography, such as # References or # Bibliography.\n因此，通常的做法是在文件末尾为参考文献添加一个章节标题，例如 # References 或 # Bibliography。\nYou can change the style of your citations and bibliography by referencing a CSL (citation style language) file in the csl field:\n你可以通过在 csl 字段中引用一个 CSL（引文样式语言）文件来更改你的引文和参考文献的样式：\nbibliography: rmarkdown.bib\ncsl: apa.csl\nAs with the bibliography field, your csl file should contain a path to the file.\n与参考文献字段一样，你的 csl 文件应包含文件的路径。\nHere we assume that the csl file is in the same directory as the .qmd file.\n这里我们假设 csl 文件与 .qmd 文件在同一目录中。\nA good place to find CSL style files for common bibliography styles is https://github.com/citation-style-language/styles.\n一个寻找常见参考文献样式 CSL 样式文件的好地方是 https://github.com/citation-style-language/styles。",
    "crumbs": [
      "Communicate",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#workflow",
    "href": "quarto.html#workflow",
    "title": "28  Quarto",
    "section": "\n28.11 Workflow",
    "text": "28.11 Workflow\nEarlier, we discussed a basic workflow for capturing your R code where you work interactively in the console, then capture what works in the script editor.\n之前，我们讨论了捕获 R 代码的基本工作流程，即在控制台中进行交互式工作，然后在脚本编辑器中捕获有效的内容。\nQuarto brings together the console and the script editor, blurring the lines between interactive exploration and long-term code capture.\nQuarto 将控制台和脚本编辑器结合在一起，模糊了交互式探索和长期代码捕获之间的界限。\nYou can rapidly iterate within a chunk, editing and re-executing with Cmd/Ctrl + Shift + Enter.\n你可以在一个代码块内快速迭代，使用 Cmd/Ctrl + Shift + Enter 进行编辑和重新执行。\nWhen you’re happy, you move on and start a new chunk.\n当你满意时，就可以继续前进并开始一个新的代码块。\nQuarto is also important because it so tightly integrates prose and code.\nQuarto 也很重要，因为它将散文和代码如此紧密地结合在一起。\nThis makes it a great analysis notebook because it lets you develop code and record your thoughts.\n这使它成为一个出色的分析笔记，因为它能让你在开发代码的同时记录你的想法。\nAn analysis notebook shares many of the same goals as a classic lab notebook in the physical sciences.\n分析笔记与物理科学中的经典实验笔记有许多相同的目标。\nIt:\n它：\n\nRecords what you did and why you did it. Regardless of how great your memory is, if you don’t record what you do, there will come a time when you have forgotten important details. Write them down so you don’t forget!\n记录你做了什么以及为什么这样做。 无论你的记忆力有多好，如果你不记录你所做的事情，总有一天你会忘记重要的细节。 把它们写下来，这样你就不会忘记了！\nSupports rigorous thinking. You are more likely to come up with a strong analysis if you record your thoughts as you go, and continue to reflect on them. This also saves you time when you eventually write up your analysis to share with others.\n支持严谨的思考。 如果你边做边记录你的想法，并不断反思，你就更有可能得出一个有力的分析。 这也能在你最终撰写分析与他人分享时节省时间。\nHelps others understand your work. It is rare to do data analysis by yourself, and you’ll often be working as part of a team. A lab notebook helps you share not only what you’ve done, but why you did it with your colleagues or lab mates.\n帮助他人理解你的工作。 你很少会独自进行数据分析，通常你会作为团队的一员工作。 实验笔记可以帮助你不仅分享你做了什么，还能与你的同事或实验室伙伴分享你为什么这么做。\n\nMuch of the good advice about using lab notebooks effectively can also be translated to analysis notebooks.\n许多关于有效使用实验笔记的好建议也可以转化为分析笔记。\nWe’ve drawn on our own experiences and Colin Purrington’s advice on lab notebooks (https://colinpurrington.com/tips/lab-notebooks) to come up with the following tips:\n我们借鉴了自己的经验和 Colin Purrington 关于实验笔记的建议 (https://colinpurrington.com/tips/lab-notebooks)，提出了以下技巧：\n\nEnsure each notebook has a descriptive title, an evocative file name, and a first paragraph that briefly describes the aims of the analysis.\n确保每个笔记都有一个描述性的标题、一个引人遐想的文件名，以及一个简要描述分析目标的第一段。\n\nUse the YAML header date field to record the date you started working on the notebook:\n使用 YAML 头部的日期字段来记录你开始使用笔记的日期：\nyaml     date: 2016-08-23\nUse ISO8601 YYYY-MM-DD format so that’s there no ambiguity. Use it even if you don’t normally write dates that way!\n使用 ISO8601 YYYY-MM-DD 格式，这样就不会有任何歧义。 即使你通常不那样写日期，也要使用它！\n\nIf you spend a lot of time on an analysis idea and it turns out to be a dead end, don’t delete it! Write up a brief note about why it failed and leave it in the notebook. That will help you avoid going down the same dead end when you come back to the analysis in the future.\n如果你在一个分析想法上花了很多时间，结果却发现是条死胡同，不要删除它！ 写一个简短的笔记，说明它为什么失败，并把它留在笔记本里。 这将帮助你在将来回到这个分析时，避免重蹈覆辙。\nGenerally, you’re better off doing data entry outside of R. But if you do need to record a small snippet of data, clearly lay it out using tibble::tribble().\n通常情况下，你最好在 R 之外进行数据录入。 但是，如果你确实需要记录一小段数据，请使用 tibble::tribble() 清晰地将其布局。\nIf you discover an error in a data file, never modify it directly, but instead write code to correct the value. Explain why you made the fix.\n如果你在数据文件中发现错误，切勿直接修改它，而应编写代码来修正该值。 解释你为什么进行修复。\nBefore you finish for the day, make sure you can render the notebook. If you’re using caching, make sure to clear the caches. That will let you fix any problems while the code is still fresh in your mind.\n在一天结束之前，确保你可以渲染笔记。 如果你正在使用缓存，请确保清除缓存。 这将让你在代码还记忆犹新的时候解决任何问题。\nIf you want your code to be reproducible in the long-run (i.e. so you can come back to run it next month or next year), you’ll need to track the versions of the packages that your code uses. A rigorous approach is to use renv, https://rstudio.github.io/renv/index.html, which stores packages in your project directory. A quick and dirty hack is to include a chunk that runs sessionInfo() — that won’t let you easily recreate your packages as they are today, but at least you’ll know what they were.\n如果你希望你的代码在长期内是可复现的（即，下个月或明年你回来还能运行它），你需要跟踪你的代码使用的包的版本。 一种严谨的方法是使用 renv (https://rstudio.github.io/renv/index.html)，它将包存储在你的项目目录中。 一个快速而简便的方法是包含一个运行 sessionInfo() 的代码块——这不能让你轻松地重现今天的包，但至少你会知道它们曾经是什么版本。\nYou are going to create many, many, many analysis notebooks over the course of your career. How are you going to organize them so you can find them again in the future? We recommend storing them in individual projects, and coming up with a good naming scheme.\n在你的职业生涯中，你将会创建非常非常多的分析笔记。 你将如何组织它们以便将来能再次找到它们？ 我们建议将它们存储在各自的项目中，并制定一个好的命名方案。",
    "crumbs": [
      "Communicate",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#summary",
    "href": "quarto.html#summary",
    "title": "28  Quarto",
    "section": "\n28.12 Summary",
    "text": "28.12 Summary\nIn this chapter we introduced you to Quarto for authoring and publishing reproducible computational documents that include your code and your prose in one place.\n在本章中，我们向你介绍了 Quarto，用于创作和发布可复现的计算文档，它将你的代码和文字内容集于一处。\nYou’ve learned about writing Quarto documents in RStudio with the visual or the source editor, how code chunks work and how to customize options for them, how to include figures and tables in your Quarto documents, and options for caching for computations.\n你已经学习了如何在 RStudio 中使用可视化或源代码编辑器编写 Quarto 文档，代码块如何工作以及如何为其自定义选项，如何在你的 Quarto 文档中包含图形和表格，以及用于计算的缓存选项。\nAdditionally, you’ve learned about adjusting YAML header options for creating self-contained or parametrized documents as well as including citations and bibliography.\n此外，你还学习了调整 YAML 头部选项以创建自包含或参数化文档，以及包含引文和参考文献。\nWe have also given you some troubleshooting and workflow tips.\n我们还为你提供了一些故障排除和工作流程的提示。\nWhile this introduction should be sufficient to get you started with Quarto, there is still a lot more to learn.\n虽然这个介绍足以让你开始使用 Quarto，但仍有许多东西需要学习。\nQuarto is still relatively young, and is still growing rapidly.\nQuarto 还相对年轻，并且仍在快速发展。\nThe best place to stay on top of innovations is the official Quarto website: https://quarto.org.\n了解最新创新的最佳地点是 Quarto 官方网站：https://quarto.org。\nThere are two important topics that we haven’t covered here: collaboration and the details of accurately communicating your ideas to other humans.\n我们在这里没有涉及两个重要的主题：协作以及如何准确地向他人传达你的想法的细节。\nCollaboration is a vital part of modern data science, and you can make your life much easier by using version control tools, like Git and GitHub.\n协作是现代数据科学至关重要的一部分，通过使用像 Git 和 GitHub 这样的版本控制工具，你可以让你的生活轻松得多。\nWe recommend “Happy Git with R”, a user friendly introduction to Git and GitHub from R users, by Jenny Bryan.\n我们推荐 Jenny Bryan 编写的《Happy Git with R》，这是一本由 R 用户编写的对 Git 和 GitHub 用户友好的入门书。\nThe book is freely available online: https://happygitwithr.com.\n这本书可以在线免费获取：https://happygitwithr.com。\nWe have also not touched on what you should actually write in order to clearly communicate the results of your analysis.\n我们也没有涉及你应该实际写些什么，以便清晰地传达你的分析结果。\nTo improve your writing, we highly recommend reading either Style: Lessons in Clarity and Grace by Joseph M. Williams & Joseph Bizup, or The Sense of Structure: Writing from the Reader’s Perspective by George Gopen.\n为了提高你的写作水平，我们强烈推荐阅读 Joseph M. Williams 和 Joseph Bizup 合著的《风格：清晰与优雅的课程》(Style: Lessons in Clarity and Grace)，或者 George Gopen 的《结构感：从读者的角度写作》(The Sense of Structure: Writing from the Reader’s Perspective)。\nBoth books will help you understand the structure of sentences and paragraphs, and give you the tools to make your writing more clear.\n这两本书都将帮助你理解句子和段落的结构，并为你提供使你的写作更清晰的工具。\n(These books are rather expensive if purchased new, but they’re used by many English classes so there are plenty of cheap second-hand copies).\n（这些书如果买新的会相当昂贵，但很多英语课都使用它们，所以有很多便宜的二手书）。\nGeorge Gopen also has a number of short articles on writing at https://www.georgegopen.com/litigation-articles.html.\nGeorge Gopen 还在 https://www.georgegopen.com/litigation-articles.html 上发表了许多关于写作的短文。\nThey are aimed at lawyers, but almost everything applies to data scientists too.\n它们是针对律师的，但几乎所有内容也同样适用于数据科学家。",
    "crumbs": [
      "Communicate",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto-formats.html",
    "href": "quarto-formats.html",
    "title": "29  Quarto formats",
    "section": "",
    "text": "29.1 Introduction\nSo far, you’ve seen Quarto used to produce HTML documents.\n到目前为止，你已经看到了如何使用 Quarto 来生成 HTML 文档。\nThis chapter gives a brief overview of some of the many other types of output you can produce with Quarto.\n本章将简要概述你可以使用 Quarto 制作的许多其他类型的输出。\nThere are two ways to set the output of a document:\n有两种方法可以设置文档的输出：",
    "crumbs": [
      "Communicate",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Quarto formats</span>"
    ]
  },
  {
    "objectID": "quarto-formats.html#introduction",
    "href": "quarto-formats.html#introduction",
    "title": "29  Quarto formats",
    "section": "",
    "text": "Permanently, by modifying the YAML header:\n永久性地，通过修改 YAML 标题：\nyaml     title: \"Diamond sizes\"     format: html\n\n\nTransiently, by calling quarto::quarto_render() by hand:\n临时性地，通过手动调用 quarto::quarto_render()：\n{r}     #| eval: false     quarto::quarto_render(\"diamond-sizes.qmd\", output_format = \"docx\")\nThis is useful if you want to programmatically produce multiple types of output since the output_format argument can also take a list of values.\n如果你想以编程方式生成多种类型的输出，这会很有用，因为 output_format 参数也可以接受一个值列表。\n{r}     #| eval: false     quarto::quarto_render(\"diamond-sizes.qmd\", output_format = c(\"docx\", \"pdf\"))",
    "crumbs": [
      "Communicate",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Quarto formats</span>"
    ]
  },
  {
    "objectID": "quarto-formats.html#output-options",
    "href": "quarto-formats.html#output-options",
    "title": "29  Quarto formats",
    "section": "\n29.2 Output options",
    "text": "29.2 Output options\nQuarto offers a wide range of output formats.\nQuarto 提供了多种输出格式。\nYou can find the complete list at https://quarto.org/docs/output-formats/all-formats.html.\n你可以在 https://quarto.org/docs/output-formats/all-formats.html 查看完整列表。\nMany formats share some output options (e.g., toc: true for including a table of contents), but others have options that are format specific (e.g., code-fold: true collapses code chunks into a &lt;details&gt; tag for HTML output so the user can display it on demand, it’s not applicable in a PDF or Word document).\n许多格式共享一些输出选项（例如，用于包含目录的 toc: true），但其他格式则有其特定的选项（例如，code-fold: true 会将代码块折叠成一个 HTML 输出的 &lt;details&gt; 标签，以便用户可以按需显示，这在 PDF 或 Word 文档中不适用）。\nTo override the default options, you need to use an expanded format field.\n要覆盖默认选项，你需要使用一个扩展的 format 字段。\nFor example, if you wanted to render an html with a floating table of contents, you’d use:\n例如，如果你想渲染一个带有浮动目录的 html，你可以这样写：\nformat:\n  html:\n    toc: true\n    toc_float: true\nYou can even render to multiple outputs by supplying a list of formats:\n你甚至可以通过提供一个格式列表来渲染成多种输出：\nformat:\n  html:\n    toc: true\n    toc_float: true\n  pdf: default\n  docx: default\nNote the special syntax (pdf: default) if you don’t want to override any default options.\n注意，如果你不想覆盖任何默认选项，可以使用特殊语法 (pdf: default)。\nTo render to all formats specified in the YAML of a document, you can use output_format = \"all\".\n要渲染到文档 YAML 中指定的所有格式，你可以使用 output_format = \"all\"。\n\nquarto::quarto_render(\"diamond-sizes.qmd\", output_format = \"all\")",
    "crumbs": [
      "Communicate",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Quarto formats</span>"
    ]
  },
  {
    "objectID": "quarto-formats.html#documents",
    "href": "quarto-formats.html#documents",
    "title": "29  Quarto formats",
    "section": "\n29.3 Documents",
    "text": "29.3 Documents\nThe previous chapter focused on the default html output.\n上一章重点介绍了默认的 html 输出。\nThere are several basic variations on that theme, generating different types of documents.\n在这个主题上有几种基本的变化，可以生成不同类型的文档。\nFor example:\n例如：\n\npdf makes a PDF with LaTeX (an open-source document layout system), which you’ll need to install. RStudio will prompt you if you don’t already have it.pdf 使用 LaTeX（一个开源的文档排版系统）来创建 PDF，你需要安装它。如果你尚未安装，RStudio 会提示你。\ndocx for Microsoft Word (.docx) documents.docx 用于生成 Microsoft Word (.docx) 文档。\nodt for OpenDocument Text (.odt) documents.odt 用于生成 OpenDocument 文本 (.odt) 文档。\nrtf for Rich Text Format (.rtf) documents.rtf 用于生成富文本格式 (.rtf) 文档。\ngfm for a GitHub Flavored Markdown (.md) document.gfm 用于生成 GitHub Flavored Markdown (.md) 文档。\nipynb for Jupyter Notebooks (.ipynb).ipynb 用于生成 Jupyter Notebooks (.ipynb)。\n\nRemember, when generating a document to share with decision-makers, you can turn off the default display of code by setting global options in the document YAML:\n请记住，在生成与决策者共享的文档时，你可以通过在文档 YAML 中设置全局选项来关闭代码的默认显示：\nexecute:\n  echo: false\nFor html documents another option is to make the code chunks hidden by default, but visible with a click:\n对于 html 文档，另一个选择是让代码块默认隐藏，但可以通过点击来显示：\nformat:\n  html:\n    code: true",
    "crumbs": [
      "Communicate",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Quarto formats</span>"
    ]
  },
  {
    "objectID": "quarto-formats.html#presentations",
    "href": "quarto-formats.html#presentations",
    "title": "29  Quarto formats",
    "section": "\n29.4 Presentations",
    "text": "29.4 Presentations\nYou can also use Quarto to produce presentations.\n你还可以使用 Quarto 来制作演示文稿。\nYou get less visual control than with a tool like Keynote or PowerPoint, but automatically inserting the results of your R code into a presentation can save a huge amount of time.\n与 Keynote 或 PowerPoint 这样的工具相比，你的视觉控制力较弱，但将 R 代码的结果自动插入演示文稿可以节省大量时间。\nPresentations work by dividing your content into slides, with a new slide beginning at each second (##) level header.\n演示文稿的工作方式是将你的内容分成幻灯片，每一张新的幻灯片都以二级 (##) 标题开始。\nAdditionally, first (#) level headers indicate the beginning of a new section with a section title slide that is, by default, centered in the middle.\n此外，一级 (#) 标题表示新章节的开始，其标题幻灯片默认居中。\nQuarto supports a variety of presentation formats, including:\nQuarto 支持多种演示文稿格式，包括：\n\nrevealjs - HTML presentation with revealjsrevealjs - 使用 revealjs 的 HTML 演示文稿\npptx - PowerPoint presentationpptx - PowerPoint 演示文稿\nbeamer - PDF presentation with LaTeX Beamer.beamer - 使用 LaTeX Beamer 的 PDF 演示文稿\n\nYou can read more about creating presentations with Quarto at https://quarto.org/docs/presentations.\n你可以在 https://quarto.org/docs/presentations 阅读更多关于使用 Quarto 创建演示文稿的信息。",
    "crumbs": [
      "Communicate",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Quarto formats</span>"
    ]
  },
  {
    "objectID": "quarto-formats.html#interactivity",
    "href": "quarto-formats.html#interactivity",
    "title": "29  Quarto formats",
    "section": "\n29.5 Interactivity",
    "text": "29.5 Interactivity\nJust like any HTML document, HTML documents created with Quarto can contain interactive components as well.\n就像任何 HTML 文档一样，使用 Quarto 创建的 HTML 文档也可以包含交互式组件。\nHere we introduce two options for including interactivity in your Quarto documents: htmlwidgets and Shiny.\n这里我们介绍在你的 Quarto 文档中加入交互性的两种选择：htmlwidgets 和 Shiny。\n\n29.5.1 htmlwidgets\nHTML is an interactive format, and you can take advantage of that interactivity with htmlwidgets, R functions that produce interactive HTML visualizations.\nHTML 是一种交互式格式，你可以利用 htmlwidgets（生成交互式 HTML 可视化的 R 函数）来利用这种交互性。\nFor example, take the leaflet map below.\n例如，看看下面的 leaflet 地图。\nIf you’re viewing this page on the web, you can drag the map around, zoom in and out, etc.\n如果你正在网页上查看此页面，你可以拖动地图，放大和缩小等。\nYou obviously can’t do that in a book, so Quarto automatically inserts a static screenshot for you.\n在书中你显然不能这样做，所以 Quarto 会自动为你插入一个静态截图。\n\nlibrary(leaflet)\nleaflet() |&gt;\n  setView(174.764, -36.877, zoom = 16) |&gt; \n  addTiles() |&gt;\n  addMarkers(174.764, -36.877, popup = \"Maungawhau\") \n\n\n\n\n\nThe great thing about htmlwidgets is that you don’t need to know anything about HTML or JavaScript to use them.\nhtmlwidgets 的优点在于，你无需了解任何 HTML 或 JavaScript 知识就可以使用它们。\nAll the details are wrapped inside the package, so you don’t need to worry about it.\n所有的细节都被封装在包里，所以你不需要担心。\nThere are many packages that provide htmlwidgets, including:\n有许多提供 htmlwidgets 的包，包括：\n\ndygraphs for interactive time series visualizations.dygraphs 用于交互式时间序列可视化。\nDT for interactive tables.DT 用于交互式表格。\nthreejs for interactive 3d plots.threejs 用于交互式 3D 图。\nDiagrammeR for diagrams (like flow charts and simple node-link diagrams).DiagrammeR 用于图表（如流程图和简单的节点链接图）。\n\nTo learn more about htmlwidgets and see a complete list of packages that provide them visit https://www.htmlwidgets.org.\n要了解更多关于 htmlwidgets 的信息并查看提供它们的包的完整列表，请访问 https://www.htmlwidgets.org。\n\n29.5.2 Shiny\nhtmlwidgets provide client-side interactivity — all the interactivity happens in the browser, independently of R.\nhtmlwidgets 提供客户端 (client-side) 交互性——所有的交互都发生在浏览器中，与 R 无关。\nOn the one hand, that’s great because you can distribute the HTML file without any connection to R.\n一方面，这很好，因为你可以分发 HTML 文件而无需任何与 R 的连接。\nHowever, that fundamentally limits what you can do to things that have been implemented in HTML and JavaScript.\n然而，这从根本上限制了你只能做那些已经在 HTML 和 JavaScript 中实现的事情。\nAn alternative approach is to use shiny, a package that allows you to create interactivity using R code, not JavaScript.\n另一种方法是使用 shiny，这是一个允许你使用 R 代码而不是 JavaScript 来创建交互性的包。\nTo call Shiny code from a Quarto document, add server: shiny to the YAML header:\n要从 Quarto 文档中调用 Shiny 代码，请在 YAML 头部添加 server: shiny：\ntitle: \"Shiny Web App\"\nformat: html\nserver: shiny\nThen you can use the “input” functions to add interactive components to the document:\n然后你可以使用“输入”函数向文档添加交互式组件：\n\nlibrary(shiny)\n\ntextInput(\"name\", \"What is your name?\")\nnumericInput(\"age\", \"How old are you?\", NA, min = 0, max = 150)\n\n\n\n\n\n\n\n\n\nAnd you also need a code chunk with chunk option context: server which contains the code that needs to run in a Shiny server.\n你还需要一个带有 context: server 块选项的代码块，其中包含需要在 Shiny 服务器中运行的代码。\nYou can then refer to the values with input$name and input$age, and the code that uses them will be automatically re-run whenever they change.\n然后你可以用 input$name 和 input$age 来引用这些值，使用它们的代码会在它们改变时自动重新运行。\nWe can’t show you a live shiny app here because shiny interactions occur on the server-side.\n我们无法在这里向你展示一个实时的 Shiny 应用，因为 Shiny 的交互发生在服务器端 (server-side)。\nThis means that you can write interactive apps without knowing JavaScript, but you need a server to run them on.\n这意味着你可以编写交互式应用而无需了解 JavaScript，但你需要一个服务器来运行它们。\nThis introduces a logistical issue: Shiny apps need a Shiny server to be run online.\n这就带来了一个后勤问题：Shiny 应用需要一个 Shiny 服务器才能在线运行。\nWhen you run Shiny apps on your own computer, Shiny automatically sets up a Shiny server for you, but you need a public-facing Shiny server if you want to publish this sort of interactivity online.\n当你在自己的计算机上运行 Shiny 应用时，Shiny 会自动为你设置一个 Shiny 服务器，但如果你想在线发布这种交互性，你需要一个面向公众的 Shiny 服务器。\nThat’s the fundamental trade-off of shiny: you can do anything in a shiny document that you can do in R, but it requires someone to be running R.\n这是 Shiny 的根本权衡：你可以在 Shiny 文档中做任何你在 R 中能做的事情，但这需要有人在运行 R。\nFor learning more about Shiny, we recommend reading Mastering Shiny by Hadley Wickham, https://mastering-shiny.org.\n要了解更多关于 Shiny 的信息，我们推荐阅读 Hadley Wickham 的《精通 Shiny》(Mastering Shiny)，网址：https://mastering-shiny.org。",
    "crumbs": [
      "Communicate",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Quarto formats</span>"
    ]
  },
  {
    "objectID": "quarto-formats.html#websites-and-books",
    "href": "quarto-formats.html#websites-and-books",
    "title": "29  Quarto formats",
    "section": "\n29.6 Websites and books",
    "text": "29.6 Websites and books\nWith a bit of additional infrastructure, you can use Quarto to generate a complete website or book:\n通过一些额外的基础设施，你可以使用 Quarto 生成一个完整的网站或书籍：\n\nPut your .qmd files in a single directory. index.qmd will become the home page.\n将你的 .qmd 文件放在一个单独的目录中。index.qmd 将成为主页。\n\nAdd a YAML file named _quarto.yml that provides the navigation for the site. In this file, set the project type to either book or website, e.g.:\n添加一个名为 _quarto.yml 的 YAML 文件，为网站提供导航。在这个文件中，将 project 类型设置为 book 或 website，例如：\nyaml     project:       type: book\n\n\nFor example, the following _quarto.yml file creates a website from three source files: index.qmd (the home page), viridis-colors.qmd, and terrain-colors.qmd.\n例如，下面的 _quarto.yml 文件从三个源文件创建了一个网站：index.qmd（主页）、viridis-colors.qmd 和 terrain-colors.qmd。\n\nproject:\n  type: website\n\nwebsite:\n  title: \"A website on color scales\"\n  navbar:\n    left:\n      - href: index.qmd\n        text: Home\n      - href: viridis-colors.qmd\n        text: Viridis colors\n      - href: terrain-colors.qmd\n        text: Terrain colors\n\nThe _quarto.yml file you need for a book is very similarly structured.\n你为书籍所需的 _quarto.yml 文件结构非常相似。\nThe following example shows how you can create a book with four chapters that renders to three different outputs (html, pdf, and epub). Once again, the source files are .qmd files.\n下面的例子展示了如何创建一本有四章的书，并渲染成三种不同的输出 (html、pdf 和 epub)。源文件同样是 .qmd 文件。\n\nproject:\n  type: book\n\nbook:\n  title: \"A book on color scales\"\n  author: \"Jane Coloriste\"\n  chapters:\n    - index.qmd\n    - intro.qmd\n    - viridis-colors.qmd\n    - terrain-colors.qmd\n\nformat:\n  html:\n    theme: cosmo\n  pdf: default\n  epub: default\n\nWe recommend that you use an RStudio project for your websites and books.\n我们建议你为你的网站和书籍使用 RStudio 项目。\nBased on the _quarto.yml file, RStudio will recognize the type of project you’re working on, and add a Build tab to the IDE that you can use to render and preview your websites and books.\n根据 _quarto.yml 文件，RStudio 会识别你正在处理的项目类型，并在 IDE 中添加一个 Build 选项卡，你可以用它来渲染和预览你的网站和书籍。\nBoth websites and books can also be rendered using quarto::quarto_render().\n网站和书籍也可以使用 quarto::quarto_render() 进行渲染。\nRead more at https://quarto.org/docs/websites about Quarto websites and https://quarto.org/docs/books about books.\n在 https://quarto.org/docs/websites 阅读更多关于 Quarto 网站的信息，在 https://quarto.org/docs/books 阅读更多关于书籍的信息。",
    "crumbs": [
      "Communicate",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Quarto formats</span>"
    ]
  },
  {
    "objectID": "quarto-formats.html#other-formats",
    "href": "quarto-formats.html#other-formats",
    "title": "29  Quarto formats",
    "section": "\n29.7 Other formats",
    "text": "29.7 Other formats\nQuarto offers even more output formats:\nQuarto 提供了更多的输出格式：\n\nYou can write journal articles using Quarto Journal Templates: https://quarto.org/docs/journals/templates.html.\n你可以使用 Quarto 期刊模板撰写期刊文章：https://quarto.org/docs/journals/templates.html。\nYou can output Quarto documents to Jupyter Notebooks with format: ipynb: https://quarto.org/docs/reference/formats/ipynb.html.\n你可以使用 format: ipynb 将 Quarto 文档输出到 Jupyter Notebooks：https://quarto.org/docs/reference/formats/ipynb.html。\n\nSee https://quarto.org/docs/output-formats/all-formats.html for a list of even more formats.\n请参阅 https://quarto.org/docs/output-formats/all-formats.html 获取更多格式的列表。",
    "crumbs": [
      "Communicate",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Quarto formats</span>"
    ]
  },
  {
    "objectID": "quarto-formats.html#summary",
    "href": "quarto-formats.html#summary",
    "title": "29  Quarto formats",
    "section": "\n29.8 Summary",
    "text": "29.8 Summary\nIn this chapter we presented you a variety of options for communicating your results with Quarto, from static and interactive documents to presentations to websites and books.\n在本章中，我们向你介绍了使用 Quarto 交流成果的多种选择，从静态和交互式文档到演示文稿，再到网站和书籍。\nTo learn more about effective communication in these different formats, we recommend the following resources:\n要了解更多关于在这些不同格式中进行有效沟通的知识，我们推荐以下资源：\n\nTo improve your presentation skills, try Presentation Patterns by Neal Ford, Matthew McCollough, and Nathaniel Schutta. It provides a set of effective patterns (both low- and high-level) that you can apply to improve your presentations.\n为了提高你的演讲技巧，可以试试 Neal Ford、Matthew McCollough 和 Nathaniel Schutta 合著的 Presentation Patterns。它提供了一套行之有效的模式（包括低层次和高层次），你可以应用这些模式来改进你的演讲。\nIf you give academic talks, you might like the Leek group guide to giving talks.\n如果你做学术报告，你可能会喜欢 Leek group guide to giving talks。\nWe haven’t taken it ourselves, but we’ve heard good things about Matt McGarrity’s online course on public speaking: https://www.coursera.org/learn/public-speaking.\n我们自己没有上过，但我们听说过 Matt McGarrity 的在线公开演讲课程的好评：https://www.coursera.org/learn/public-speaking。\nIf you are creating many dashboards, make sure to read Stephen Few’s Information Dashboard Design: The Effective Visual Communication of Data. It will help you create dashboards that are truly useful, not just pretty to look at.\n如果你正在创建许多仪表盘，请务必阅读 Stephen Few 的 Information Dashboard Design: The Effective Visual Communication of Data。它将帮助你创建真正有用而不仅仅是好看的仪表盘。\nEffectively communicating your ideas often benefits from some knowledge of graphic design. Robin Williams’ The Non-Designer’s Design Book is a great place to start.\n有效地传达你的想法通常会受益于一些图形设计知识。Robin Williams 的 The Non-Designer’s Design Book 是一个很好的起点。",
    "crumbs": [
      "Communicate",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Quarto formats</span>"
    ]
  }
]